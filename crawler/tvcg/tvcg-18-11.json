{"10.1109/TVCG.2018.2868403": {"doi": "10.1109/TVCG.2018.2868403", "author": ["L. De Floriani", "D. Schmalstieg"], "title": "Message from the Editor-in-Chief and from the Associate Editor-in-Chief", "year": "2018", "abstract": "Wwelcome to the November 2018 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR), held this year in Munich, Germany, from October 16 to October 20, 2018.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2870548": {"doi": "10.1109/TVCG.2018.2870548", "author": ["D. Chu", "J. L. Gabbard", "J. Grubert", "H. Regenbrecht"], "title": "Message from the ISMAR 2018 Science and Technology Program Chairs and TVCG Guest Editors", "year": "2018", "abstract": "In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the TVCG papers from the 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2018), held October 16\u201320 in Munich, Germany. ISMAR continues the 20-year long tradition of IWAR, ISMR, and ISAR, and is undoubtedly the premier conference for mixed and augmented reality in the world.", "keywords": ["Special issues and sections", "Meetings", "Computer graphics", "Visualization"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2868587": {"doi": "10.1109/TVCG.2018.2868587", "author": ["B. Volmer", "J. Baumeister", "S. Von Itzstein", "I. Bornkessel-Schlesewsky", "M. Schlesewsky", "M. Billinghurst", "B. H. Thomas"], "title": "A Comparison of Predictive Spatial Augmented Reality Cues for Procedural Tasks", "year": "2018", "abstract": "Previous research has demonstrated that Augmented Reality can reduce a user's task response time and mental effort when completing a procedural task. This paper investigates techniques to improve user performance and reduce mental effort by providing projector-based Spatial Augmented Reality predictive cues for future responses. The objective of the two experiments conducted in this study was to isolate the performance and mental effort differences from several different annotation cueing techniques for simple (Experiment 1) and complex (Experiment 2) button-pressing tasks. Comporting with existing cognitive neuroscience literature on prediction, attentional orienting, and interference, we hypothesized that for both simple procedural tasks and complex search-based tasks, having a visual cue guiding to the next task's location would positively impact performance relative to a baseline, no-cue condition. Additionally, we predicted that direction-based cues would provide a more significant positive impact than target-based cues. The results indicated that providing a line to the next task was the most effective technique for improving the users' task time and mental effort in both the simple and complex tasks.", "keywords": ["augmented reality", "cognition", "human computer interaction", "complex tasks", "predictive spatial augmented reality cues", "procedural task", "user performance", "future responses", "mental effort differences", "complex button-pressing tasks", "simple procedural tasks", "complex search-based tasks", "visual cue", "no-cue condition", "direction-based cues", "target-based cues", "projector-based spatial augmented reality predictive cues", "Task analysis", "Visualization", "Resists", "Monitoring", "Augmented reality", "Complexity theory", "Maintenance engineering", "Spatial augmented reality", "predictive cue", "mental effort", "procedural task"], "referenced_by": ["IKEY:8943763", "IKEY:8951930", "IKEY:9284762", "IKEY:9284659"], "referencing": ["IKEY:1240689", "IKEY:8007333", "IKEY:183317", "IKEY:5336486", "IKEY:6162888", "IKEY:6948425", "IKEY:7383340", "IKEY:6671762", "IKEY:5336458", "IKEY:658416", "IKEY:4637331", "IKEY:6064990", "IKEY:7504691", "IKEY:1240689", "IKEY:8007333", "IKEY:183317", "IKEY:5336486", "IKEY:6162888", "IKEY:6948425", "IKEY:7383340", "IKEY:6671762", "IKEY:5336458", "IKEY:658416", "IKEY:4637331", "IKEY:6064990", "IKEY:7504691", "IKEY:1240689", "IKEY:8007333", "IKEY:183317", "IKEY:5336486", "IKEY:6162888", "IKEY:6948425", "IKEY:7383340", "IKEY:6671762", "IKEY:5336458", "IKEY:658416", "IKEY:4637331", "IKEY:6064990", "IKEY:7504691", "10.1145/159544.159587", "10.1145/2371574.2371610", "10.1145/1357054.1357199", "10.1145/642625.642626", "10.1145/2856400.2856416", "10.1145/159544.159587", "10.1145/2371574.2371610", "10.1145/1357054.1357199", "10.1145/642625.642626", "10.1145/2856400.2856416", "10.1145/159544.159587", "10.1145/2371574.2371610", "10.1145/1357054.1357199", "10.1145/642625.642626", "10.1145/2856400.2856416", "10.1007/s00429-012-0475-5", "10.1016/j.neuron.2012.10.038", "10.1201/b10624", "10.1097/PRS.0000000000001056", "10.1146/annurev.ne.18.030195.001205", "10.1111/psyp.12419", "10.1098/rstb.2005.1622", "10.1038/nrn2787", "10.3389/fpsyg.2012.00151", "10.1017/S0140525X04000093", "10.1371/journal.pcbi.1002012", "10.1016/S0364-0213(84)80003-8", "10.1016/S1364-6613(00)01452-2", "10.1146/annurev-psych-122414-033400", "10.1207/S15326985EP3801_8", "10.1037//0022-0663.84.4.429", "10.1037/0033-2909.116.2.220", "10.1080/00335558008248231", "10.1038/4580", "10.1037/0022-3514.32.6.956", "10.1007/978-1-4419-8126-4", "10.1016/j.pmcj.2014.08.010", "10.1007/s40436-015-0131-4", "10.1016/j.cag.2009.06.001", "10.1007/s00429-012-0475-5", "10.1016/j.neuron.2012.10.038", "10.1201/b10624", "10.1097/PRS.0000000000001056", "10.1146/annurev.ne.18.030195.001205", "10.1111/psyp.12419", "10.1098/rstb.2005.1622", "10.1038/nrn2787", "10.3389/fpsyg.2012.00151", "10.1017/S0140525X04000093", "10.1371/journal.pcbi.1002012", "10.1016/S0364-0213(84)80003-8", "10.1016/S1364-6613(00)01452-2", "10.1146/annurev-psych-122414-033400", "10.1207/S15326985EP3801_8", "10.1037//0022-0663.84.4.429", "10.1037/0033-2909.116.2.220", "10.1080/00335558008248231", "10.1038/4580", "10.1037/0022-3514.32.6.956", "10.1007/978-1-4419-8126-4", "10.1016/j.pmcj.2014.08.010", "10.1007/s40436-015-0131-4", "10.1016/j.cag.2009.06.001", "10.1007/s00429-012-0475-5", "10.1016/j.neuron.2012.10.038", "10.1201/b10624", "10.1097/PRS.0000000000001056", "10.1146/annurev.ne.18.030195.001205", "10.1111/psyp.12419", "10.1098/rstb.2005.1622", "10.1038/nrn2787", "10.3389/fpsyg.2012.00151", "10.1017/S0140525X04000093", "10.1371/journal.pcbi.1002012", "10.1016/S0364-0213(84)80003-8", "10.1016/S1364-6613(00)01452-2", "10.1146/annurev-psych-122414-033400", "10.1207/S15326985EP3801_8", "10.1037//0022-0663.84.4.429", "10.1037/0033-2909.116.2.220", "10.1080/00335558008248231", "10.1038/4580", "10.1037/0022-3514.32.6.956", "10.1007/978-1-4419-8126-4", "10.1016/j.pmcj.2014.08.010", "10.1007/s40436-015-0131-4", "10.1016/j.cag.2009.06.001"]}, "10.1109/TVCG.2018.2868570": {"doi": "10.1109/TVCG.2018.2868570", "author": ["K. Rathinavel", "H. Wang", "A. Blate", "H. Fuchs"], "title": "An Extended Depth-at-Field Volumetric Near-Eye Augmented Reality Display", "year": "2018", "abstract": "We introduce an optical design and a rendering pipeline for a full-color volumetric near-eye display which simultaneously presents imagery with near-accurate per-pixel focus across an extended volume ranging from 15cm (6.7 diopters) to 4M (0.25 diopters), allowing the viewer to accommodate freely across this entire depth range. This is achieved using a focus-tunable lens that continuously sweeps a sequence of 280 synchronized binary images from a high-speed, Digital Micromirror Device (DMD) projector and a high-speed, high dynamic range (HDR) light source that illuminates the DMD images with a distinct color and brightness at each binary frame. Our rendering pipeline converts 3-D scene information into a 2-D surface of color voxels, which are decomposed into 280 binary images in a voxel-oriented manner, such that 280 distinct depth positions for full-color voxels can be displayed.", "keywords": ["augmented reality", "display instrumentation", "image colour analysis", "image sequences", "lenses", "micromirrors", "optical focusing", "optical projectors", "rendering (computer graphics)", "depth-at-field volumetric near-eye augmented reality display", "optical design", "focus-tunable lens", "high dynamic range light source", "DMD images", "binary frame", "full-color voxels", "voxel-oriented manner", "3-D scene information", "HDR", "synchronized binary images sequence", "full-color volumetric near-eye display", "distinct depth positions", "rendering pipeline", "digital micromirror device projector", "Three-dimensional displays", "Lenses", "Rendering (computer graphics)", "Pipelines", "Light sources", "Image color analysis", "Hardware", "Near-Eye Displays", "Augmented Reality", "Rendering Pipeline", "Algorithms", "Computer Graphics", "Equipment Design", "Imaging, Three-Dimensional", "Photography", "Video Recording", "Virtual Reality"], "referenced_by": ["IKEY:8794584", "IKEY:8827571", "IKEY:8998139", "IKEY:8999805", "IKEY:9137259", "IKEY:9199563", "IKEY:9284652", "IKEY:9284796"], "referencing": ["IKEY:7829412", "IKEY:6712903", "IKEY:7831415", "IKEY:8218748", "IKEY:7383304", "IKEY:5204084", "IKEY:5280616", "IKEY:7829412", "IKEY:6712903", "IKEY:7831415", "IKEY:8218748", "IKEY:7383304", "IKEY:5204084", "IKEY:5280616", "IKEY:7829412", "IKEY:6712903", "IKEY:7831415", "IKEY:8218748", "IKEY:7383304", "IKEY:5204084", "IKEY:5280616", "10.1145/1186562.1015804", "10.1145/1275808.1276427", "10.1145/2858036.2858140", "10.1145/3023368.3023379", "10.1145/2614066.2614080", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2850414", "10.1145/1186562.1015804", "10.1145/1275808.1276427", "10.1145/2858036.2858140", "10.1145/3023368.3023379", "10.1145/2614066.2614080", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2850414", "10.1145/1186562.1015804", "10.1145/1275808.1276427", "10.1145/2858036.2858140", "10.1145/3023368.3023379", "10.1145/2614066.2614080", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2850414", "10.1364/AO.46.001244", "10.1117/12.480930", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/AO.54.009990", "10.1364/OE.22.013484", "10.1364/OL.34.001642", "10.1364/OE.18.011562", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1073/pnas.1617251114", "10.1117/12.349412", "10.1117/12.2315655", "10.1038/nature25176", "10.1364/AO.46.001244", "10.1117/12.480930", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/AO.54.009990", "10.1364/OE.22.013484", "10.1364/OL.34.001642", "10.1364/OE.18.011562", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1073/pnas.1617251114", "10.1117/12.349412", "10.1117/12.2315655", "10.1038/nature25176", "10.1364/AO.46.001244", "10.1117/12.480930", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/AO.54.009990", "10.1364/OE.22.013484", "10.1364/OL.34.001642", "10.1364/OE.18.011562", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1073/pnas.1617251114", "10.1117/12.349412", "10.1117/12.2315655", "10.1038/nature25176"]}, "10.1109/TVCG.2018.2868568": {"doi": "10.1109/TVCG.2018.2868568", "author": ["A. Ibrahim", "B. Huynh", "J. Downey", "T. H\u00f6llerer", "D. Chun", "J. O'donovan"], "title": "ARbis Pictus: A Study of Vocabulary Learning with Augmented Reality", "year": "2018", "abstract": "We conducted a fundamental user study to assess potential benefits of AR technology for immersive vocabulary learning. With the idea that AR systems will soon be able to label real-world objects in any language in real time, our within-subjects (N=52) lab-based study explores the effect of such an AR vocabulary prompter on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based learning approach. Our results show that the immersive AR experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better on both same-day and 4-day delayed productive recall tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential for language learning in augmented reality, particularly because of the improvement shown in sustained recall compared to the traditional approach.", "keywords": ["augmented reality", "computer aided instruction", "natural language processing", "vocabulary", "augmented reality", "flashcard method", "language learning", "ARbis pictus", "AR technology", "immersive vocabulary learning", "AR vocabulary prompter", "virtual labels", "foreign language", "flashcard-based learning", "Augmented reality", "Vocabulary", "Education", "Task analysis", "Games", "Labeling", "Language learning", "education", "augmented reality", "HCl", "experimentation"], "referenced_by": ["IKEY:8798358", "IKEY:8797804", "IKEY:8951984", "IKEY:9081013", "IKEY:9090575", "IKEY:9284762"], "referencing": ["IKEY:6402573", "IKEY:4538819", "IKEY:7821639", "IKEY:6402573", "IKEY:4538819", "IKEY:7821639", "IKEY:6402573", "IKEY:4538819", "IKEY:7821639", "10.1145/2559206.2581183", "10.1145/1357054.1357080", "10.1145/2858036.2858514", "10.1145/1240866.1240931", "10.1145/2984751.2984776", "10.1145/2702123.2702397", "10.1145/2559206.2581183", "10.1145/1357054.1357080", "10.1145/2858036.2858514", "10.1145/1240866.1240931", "10.1145/2984751.2984776", "10.1145/2702123.2702397", "10.1145/2559206.2581183", "10.1145/1357054.1357080", "10.1145/2858036.2858514", "10.1145/1240866.1240931", "10.1145/2984751.2984776", "10.1145/2702123.2702397", "10.1111/j.1540-4781.1996.tb01159.x", "10.1007/978-1-4614-3185-5_59", "10.1016/j.jenvp.2007.09.002", "10.1016/S0097-8493(03)00028-1", "10.1016/j.sbspro.2016.07.055", "10.1017/CBO9780511816819", "10.1037/0022-0663.86.3.389", "10.1037/0022-0663.91.2.358", "10.1080/09588221.2010.520675", "10.1037/0022-0663.90.1.25", "10.1080/09588221.2012.692384", "10.1111/j.1540-4781.1996.tb01159.x", "10.1007/978-1-4614-3185-5_59", "10.1016/j.jenvp.2007.09.002", "10.1016/S0097-8493(03)00028-1", "10.1016/j.sbspro.2016.07.055", "10.1017/CBO9780511816819", "10.1037/0022-0663.86.3.389", "10.1037/0022-0663.91.2.358", "10.1080/09588221.2010.520675", "10.1037/0022-0663.90.1.25", "10.1080/09588221.2012.692384", "10.1111/j.1540-4781.1996.tb01159.x", "10.1007/978-1-4614-3185-5_59", "10.1016/j.jenvp.2007.09.002", "10.1016/S0097-8493(03)00028-1", "10.1016/j.sbspro.2016.07.055", "10.1017/CBO9780511816819", "10.1037/0022-0663.86.3.389", "10.1037/0022-0663.91.2.358", "10.1080/09588221.2010.520675", "10.1037/0022-0663.90.1.25", "10.1080/09588221.2012.692384"]}, "10.1109/TVCG.2018.2868531": {"doi": "10.1109/TVCG.2018.2868531", "author": ["C. Merenda", "H. Kim", "K. Tanous", "J. L. Gabbard", "B. Feichtl", "T. Misu", "C. Suga"], "title": "Augmented Reality Interface Design Approaches for Goal-directed and Stimulus-driven Driving Tasks", "year": "2018", "abstract": "The automotive industry is rapidly developing new in-vehicle technologies that can provide drivers with information to aid awareness and promote quicker response times. Particularly, vehicles with augmented reality (AR) graphics delivered via head-up displays (HUDs) are nearing mainstream commercial feasibility and will be widely implemented over the next decade. Though AR graphics have been shown to provide tangible benefits to drivers in scenarios like forward collision warnings and navigation, they also create many new perceptual and sensory issues for drivers. For some time now, designers have focused on increasing the realism and quality of virtual graphics delivered via HUDs, and recently have begun testing more advanced 3D HUD systems that deliver volumetric spatial information to drivers. However, the realization of volumetric graphics adds further complexity to the design and delivery of AR cues, and moreover, parameters in this new design space must be clearly and operationally defined and explored. In this work, we present two user studies that examine how driver performance and visual attention are affected when using fixed and animated AR HUD interface design approaches in driving scenarios that require top-down and bottom-up cognitive processing. Results demonstrate that animated design approaches can produce some driving gains (e.g., in goal-directed navigation tasks) but often come at the cost of response time and distance. Our discussion yields AR HUD design recommendations and challenges some of the existing assumptions of world-fixed conformal graphic approaches to design.", "keywords": ["augmented reality", "cognition", "computer animation", "driver information systems", "head-up displays", "user interfaces", "augmented reality interface design approaches", "driving tasks", "automotive industry", "head-up displays", "HUDs", "AR graphics", "sensory issues", "virtual graphics", "volumetric spatial information", "volumetric graphics", "driver performance", "AR HUD interface design approaches", "animated design approaches", "vehicle technologies", "graphic approaches", "3D HUD systems", "bottom-up cognitive processing", "top-down cognitive processing", "Vehicles", "Task analysis", "Visualization", "Navigation", "Roads", "Augmented reality", "Mixed-reality", "augmented reality", "driving", "head-up displays"], "referenced_by": ["IKEY:8943689", "IKEY:9089581", "IKEY:9284762"], "referencing": ["IKEY:8160501", "IKEY:6797341", "IKEY:6704805", "IKEY:8302393", "IKEY:5477182", "IKEY:8160501", "IKEY:6797341", "IKEY:6704805", "IKEY:8302393", "IKEY:5477182", "IKEY:8160501", "IKEY:6797341", "IKEY:6704805", "IKEY:8302393", "IKEY:5477182", "10.1145/2070719.2070721", "10.1145/2070719.2070721", "10.1145/2070719.2070721", "10.1155/2014/380647", "10.1007/s10209-005-0017-5", "10.1016/j.ijhcs.2004.06.002", "10.1016/j.trf.2015.04.011", "10.1518/001872097778543840", "10.2307/3002019", "10.1155/2014/380647", "10.1007/s10209-005-0017-5", "10.1016/j.ijhcs.2004.06.002", "10.1016/j.trf.2015.04.011", "10.1518/001872097778543840", "10.2307/3002019", "10.1155/2014/380647", "10.1007/s10209-005-0017-5", "10.1016/j.ijhcs.2004.06.002", "10.1016/j.trf.2015.04.011", "10.1518/001872097778543840", "10.2307/3002019"]}, "10.1109/TVCG.2018.2868530": {"doi": "10.1109/TVCG.2018.2868530", "author": ["P. Kurth", "V. Lange", "C. Siegl", "M. Stamminger", "F. Bauer"], "title": "Auto-Calibration for Dynamic Multi-Projection Mapping on Arbitrary Surfaces", "year": "2018", "abstract": "The quality of every dynamic multi-projection mapping system is limited by the quality of the projector to tracking device calibration. Common problems with poor calibration result in noticeable artifacts for the user, such as ghosting and seams. In this work we introduce a new, fully automated calibration algorithm that is tailored to reduce these artifacts, based on consumer-grade hardware. We achieve this goal by repurposing a structured-light scanning setup. A structured-light scanner can generate 3D geometry based on a known intrinsic and extrinsic calibration of its components (projector and RGB camera). We revert this process by providing the resulting 3D model to determine the intrinsic and extrinsic parameters of our setup (including those of a variety of tracking systems). Our system matches features and solves for all parameters in a single pass while respecting the lower quality of our sensory input.", "keywords": ["calibration", "optical projectors", "optical scanners", "tracking systems", "dynamic multiprojection mapping system", "consumer-grade hardware", "structured-light scanning setup", "automated calibration algorithm", "3D geometry generation model", "RGB camera", "Cameras", "Calibration", "Target tracking", "Three-dimensional displays", "Distortion", "Heuristic algorithms", "Geometry", "Calibration", "SAR", "multi-projection mapping", "mixed reality"], "referenced_by": ["IKEY:8797846", "IKEY:9284714"], "referencing": ["IKEY:5204319", "IKEY:970539", "IKEY:710808", "IKEY:5204317", "IKEY:7831400", "IKEY:4270475", "IKEY:6375029", "IKEY:7164353", "IKEY:1500323", "IKEY:8007312", "IKEY:8115403", "IKEY:791289", "IKEY:5204319", "IKEY:970539", "IKEY:710808", "IKEY:5204317", "IKEY:7831400", "IKEY:4270475", "IKEY:6375029", "IKEY:7164353", "IKEY:1500323", "IKEY:8007312", "IKEY:8115403", "IKEY:791289", "IKEY:5204319", "IKEY:970539", "IKEY:710808", "IKEY:5204317", "IKEY:7831400", "IKEY:4270475", "IKEY:6375029", "IKEY:7164353", "IKEY:1500323", "IKEY:8007312", "IKEY:8115403", "IKEY:791289", "10.1145/2642918.2647383", "10.1145/1141911.1141977", "10.1145/2642918.2647383", "10.1145/1141911.1141977", "10.1145/2642918.2647383", "10.1145/1141911.1141977", "10.1007/s12541-012-0017-3", "10.1016/j.jvcir.2010.02.005", "10.1016/j.patcog.2014.01.005", "10.1007/s12541-012-0017-3", "10.1016/j.jvcir.2010.02.005", "10.1016/j.patcog.2014.01.005", "10.1007/s12541-012-0017-3", "10.1016/j.jvcir.2010.02.005", "10.1016/j.patcog.2014.01.005"]}, "10.1109/TVCG.2018.2868533": {"doi": "10.1109/TVCG.2018.2868533", "author": ["S. Golodetz", "T. Cavallari", "N. A. Lord", "V. A. Prisacariu", "D. W. Murray", "P. H. S. Torr"], "title": "Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation", "year": "2018", "abstract": "Reconstructing dense, volumetric models of real-world 3D scenes is important for many tasks, but capturing large scenes can take significant time, and the risk of transient changes to the scene goes up as the capture time increases. These are good reasons to want instead to capture several smaller sub-scenes that can be joined to make the whole scene. Achieving this has traditionally been difficult: joining sub-scenes that may never have been viewed from the same angle requires a high-quality camera relocaliser that can cope with novel poses, and tracking drift in each sub-scene can prevent them from being joined to make a consistent overall scene. Recent advances, however, have significantly improved our ability to capture medium-sized sub-scenes with little to no tracking drift: real-time globally consistent reconstruction systems can close loops and re-integrate the scene surface on the fly, whilst new visual-inertial odometry approaches can significantly reduce tracking drift during live reconstruction. Moreover, high-quality regression forest-based relocalisers have recently been made more practical by the introduction of a method to allow them to be trained and used online. In this paper, we leverage these advances to present what to our knowledge is the first system to allow multiple users to collaborate interactively to reconstruct dense, voxel-based models of whole buildings using only consumer-grade hardware, a task that has traditionally been both time-consuming and dependent on the availability of specialised hardware. Using our system, an entire house or lab can be reconstructed in under half an hour and at a far lower cost than was previously possible.", "keywords": ["cameras", "distance measurement", "image reconstruction", "optimisation", "pose estimation", "regression analysis", "large-scale dense 3D reconstruction", "online inter-agent pose optimisation", "volumetric models", "high-quality camera relocaliser", "tracking drift", "medium-sized sub-scenes", "high-quality regression forest-based relocalisers", "visual-inertial odometry approach", "voxel-based model", "consumer-grade hardware", "Robots", "Collaboration", "Three-dimensional displays", "Cameras", "Buildings", "Visualization", "Hardware", "Collaborative", "large-scale", "dense 3D reconstruction", "inter-agent relocalisation", "pose graph optimisation"], "referenced_by": ["IKEY:8643537", "IKEY:8681073", "IKEY:8742334", "IKEY:8791312", "IKEY:8885732", "IKEY:8943776", "IKEY:8956816", "IKEY:9089500", "IKEY:8706568"], "referencing": ["IKEY:7780539", "IKEY:7353389", "IKEY:8099514", "IKEY:7090564", "IKEY:7140075", "IKEY:6631323", "IKEY:5652875", "IKEY:6225356", "IKEY:7759443", "IKEY:7299077", "IKEY:7328080", "IKEY:6696923", "IKEY:6912003", "IKEY:4538852", "IKEY:7057681", "IKEY:8206513", "IKEY:6162880", "IKEY:6629610", "IKEY:7989445", "IKEY:6619221", "IKEY:6907127", "IKEY:7299069", "IKEY:7781775", "IKEY:8099639", "IKEY:7780539", "IKEY:7353389", "IKEY:8099514", "IKEY:7090564", "IKEY:7140075", "IKEY:6631323", "IKEY:5652875", "IKEY:6225356", "IKEY:7759443", "IKEY:7299077", "IKEY:7328080", "IKEY:6696923", "IKEY:6912003", "IKEY:4538852", "IKEY:7057681", "IKEY:8206513", "IKEY:6162880", "IKEY:6629610", "IKEY:7989445", "IKEY:6619221", "IKEY:6907127", "IKEY:7299069", "IKEY:7781775", "IKEY:8099639", "IKEY:7780539", "IKEY:7353389", "IKEY:8099514", "IKEY:7090564", "IKEY:7140075", "IKEY:6631323", "IKEY:5652875", "IKEY:6225356", "IKEY:7759443", "IKEY:7299077", "IKEY:7328080", "IKEY:6696923", "IKEY:6912003", "IKEY:4538852", "IKEY:7057681", "IKEY:8206513", "IKEY:6162880", "IKEY:6629610", "IKEY:7989445", "IKEY:6619221", "IKEY:6907127", "IKEY:7299069", "IKEY:7781775", "IKEY:8099639", "10.1145/3054739", "10.1145/2782782.2792488", "10.1145/3130800.3130824", "10.1145/37402.37422", "10.1145/3150165.3150166", "10.1145/3054739", "10.1145/2782782.2792488", "10.1145/3130800.3130824", "10.1145/37402.37422", "10.1145/3150165.3150166", "10.1145/3054739", "10.1145/2782782.2792488", "10.1145/3130800.3130824", "10.1145/37402.37422", "10.1145/3150165.3150166", "10.1002/rob.21608", "10.1007/978-3-319-10605-2_54", "10.1177/0278364913509675", "10.1007/978-3-319-46484-8_30", "10.1016/j.robot.2012.08.008", "10.1002/rob.21436", "10.1016/j.robot.2013.11.007", "10.1017/S0263574712000021", "10.1002/rob.21620", "10.1016/j.ifacol.2015.06.035", "10.1177/0278364914551008", "10.15607/RSS.2015.XI.001", "10.1002/rob.21608", "10.1007/978-3-319-10605-2_54", "10.1177/0278364913509675", "10.1007/978-3-319-46484-8_30", "10.1016/j.robot.2012.08.008", "10.1002/rob.21436", "10.1016/j.robot.2013.11.007", "10.1017/S0263574712000021", "10.1002/rob.21620", "10.1016/j.ifacol.2015.06.035", "10.1177/0278364914551008", "10.15607/RSS.2015.XI.001", "10.1002/rob.21608", "10.1007/978-3-319-10605-2_54", "10.1177/0278364913509675", "10.1007/978-3-319-46484-8_30", "10.1016/j.robot.2012.08.008", "10.1002/rob.21436", "10.1016/j.robot.2013.11.007", "10.1017/S0263574712000021", "10.1002/rob.21620", "10.1016/j.ifacol.2015.06.035", "10.1177/0278364914551008", "10.15607/RSS.2015.XI.001"]}, "10.1109/TVCG.2018.2868532": {"doi": "10.1109/TVCG.2018.2868532", "author": ["P. Chakravarthula", "D. Dunn", "K. Ak\u015fit", "H. Fuchs"], "title": "FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real World and Virtual Imagery", "year": "2018", "abstract": "We describe a system which corrects dynamically for the focus of the real world surrounding the near-eye display of the user and simultaneously the internal display for augmented synthetic imagery, with an aim of completely replacing the user prescription eyeglasses. The ability to adjust focus for both real and virtual stimuli will be useful for a wide variety of users, but especially for users over 40 years of age who have limited accommodation range. Our proposed solution employs a tunable-focus lens for dynamic prescription vision correction, and a varifocal internal display for setting the virtual imagery at appropriate spatially registered depths. We also demonstrate a proof of concept prototype to verify our design and discuss the challenges to building an auto-focus augmented reality eyeglasses for both real and virtual.", "keywords": ["augmented reality", "eye", "helmet mounted displays", "lenses", "medical computing", "ophthalmic lenses", "optical focusing", "optical tuning", "autofocus augmented reality eyeglasses", "real world imagery", "varifocal internal display", "dynamic prescription vision correction", "tunable-focus lens", "accommodation range", "virtual stimuli", "user prescription eyeglasses", "augmented synthetic imagery", "near-eye display", "virtual imagery", "FocusAR", "Lenses", "Meters", "Liquids", "Prototypes", "Augmented reality", "Glass", "Apertures", "Augmented Reality", "Displays", "Auto-focus", "Focus accommodation", "Prescription correction", "Adult", "Computer Graphics", "Eyeglasses", "Humans", "Image Processing, Computer-Assisted", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8794584", "IKEY:8827571", "IKEY:8951961", "IKEY:8998139", "IKEY:8999805", "IKEY:9123127", "IKEY:9199563", "IKEY:9284794", "IKEY:9284672", "IKEY:9284775", "IKEY:9284652", "IKEY:9284796"], "referencing": ["IKEY:7829412", "IKEY:6671761", "IKEY:7829412", "IKEY:6671761", "IKEY:7829412", "IKEY:6671761", "10.1145/1186562.1015804", "10.1145/3130800.3130815", "10.1145/2766922", "10.1145/2601097.2601122", "10.1145/2858036.2858140", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/2984511.2984517", "10.1145/3130800.3130832", "10.1145/1186562.1015804", "10.1145/3130800.3130815", "10.1145/2766922", "10.1145/2601097.2601122", "10.1145/2858036.2858140", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/2984511.2984517", "10.1145/3130800.3130832", "10.1145/1186562.1015804", "10.1145/3130800.3130815", "10.1145/2766922", "10.1145/2601097.2601122", "10.1145/2858036.2858140", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/2984511.2984517", "10.1145/3130800.3130832", "10.1364/JOSAA.12.000450", "10.1038/s41598-017-02851-5", "10.1002/sdtp.12490", "10.1113/jphysiol.1957.sp005829", "10.1097/00006324-193909000-00002", "10.1364/OE.25.001221", "10.1364/OE.24.013334", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1016/0042-6989(93)90046-Y", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/OE.21.009428", "10.1016/j.jcrs.2009.09.026", "10.1073/pnas.1617251114", "10.1097/00006324-199204000-00002", "10.1097/00006324-193410000-00001", "10.1167/11.8.11", "10.1364/OL.39.001318", "10.1364/OE.24.003929", "10.1364/JOSAA.12.000450", "10.1038/s41598-017-02851-5", "10.1002/sdtp.12490", "10.1113/jphysiol.1957.sp005829", "10.1097/00006324-193909000-00002", "10.1364/OE.25.001221", "10.1364/OE.24.013334", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1016/0042-6989(93)90046-Y", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/OE.21.009428", "10.1016/j.jcrs.2009.09.026", "10.1073/pnas.1617251114", "10.1097/00006324-199204000-00002", "10.1097/00006324-193410000-00001", "10.1167/11.8.11", "10.1364/OL.39.001318", "10.1364/OE.24.003929", "10.1364/JOSAA.12.000450", "10.1038/s41598-017-02851-5", "10.1002/sdtp.12490", "10.1113/jphysiol.1957.sp005829", "10.1097/00006324-193909000-00002", "10.1364/OE.25.001221", "10.1364/OE.24.013334", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1016/0042-6989(93)90046-Y", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/OE.21.009428", "10.1016/j.jcrs.2009.09.026", "10.1073/pnas.1617251114", "10.1097/00006324-199204000-00002", "10.1097/00006324-193410000-00001", "10.1167/11.8.11", "10.1364/OL.39.001318", "10.1364/OE.24.003929"]}, "10.1109/TVCG.2018.2868569": {"doi": "10.1109/TVCG.2018.2868569", "author": ["C. Reichherzer", "A. Cunningham", "J. Walsh", "M. Kohler", "M. Billinghurst", "B. H. Thomas"], "title": "Narrative and Spatial Memory for Jury Viewings in a Reconstructed Virtual Environment", "year": "2018", "abstract": "This paper showcases one way of how virtual reconstruction can be used in a courtroom. The results of a pilot study on narrative and spatial memory are presented in the context of viewing real and virtual copies of a simulated crime scene. Based on current court procedures, three different viewing options were compared: photographs, a real life visit, and a 3D virtual reconstruction of the scene viewed in a Virtual Reality headset. Participants were also given a written narrative that included the spatial locations of stolen goods and were measured on their ability to recall and understand these spatial relationships of those stolen items. The results suggest that Virtual Reality is more reliable for spatial memory compared to photographs and that Virtual Reality provides a compromise for when physical viewing of crime scenes are not possible. We conclude that Virtual Reality is a promising medium for the court.", "keywords": ["criminal law", "forensic science", "virtual reality", "virtual reconstruction", "written narrative", "spatial locations", "spatial relationships", "spatial memory", "photographs", "physical viewing", "jury viewings", "virtual copies", "current court procedures", "virtual reality headset", "crime scene simulation", "narrative memory", "virtual environment reconstruction", "viewing options", "courtroom", "Visualization", "Three-dimensional displays", "Law enforcement", "Virtual environments", "Image reconstruction", "Videos", "Virtual Reality", "virtual environments", "narrative memory", "spatial memory", "crime scene viewing", "Adult", "Crime", "Female", "Humans", "Imaging, Three-Dimensional", "Jurisprudence", "Male", "Mental Recall", "Middle Aged", "Narration", "Spatial Memory", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8951957", "IKEY:9284762"], "referencing": ["IKEY:6790800", "IKEY:6790800", "IKEY:6790800", "10.1145/1670671.1670673", "10.1145/1670671.1670673", "10.1145/1670671.1670673", "10.1017/CBO9780511752896.010", "10.1037/0022-3514.51.2.242", "10.1037/1076-8971.7.3.622", "10.1017/CBO9780511752896", "10.1111/j.1467-9930.2006.00227.x", "10.1111/j.1467-9930.2006.00226.x", "10.1111/j.1556-4029.2010.01671.x", "10.1080/001401397188387", "10.1057/978-1-137-55475-8_14", "10.1207/s15327663jcp1401&amp;2_19", "10.1037/0278-7393.14.3.521", "10.1037/0022-3514.62.2.189", "10.1002/acp.3240", "10.1016/0749-5978(89)90040-X", "10.1080/001401300184378", "10.1089/109493101300117938", "10.1016/j.forsciint.2012.05.015", "10.1007/s12024-014-9605-0", "10.1111/1556-4029.12736", "10.1016/j.acn.2006.10.006", "10.1007/978-1-4614-7485-2_18", "10.1162/pres.1994.3.2.130", "10.1016/S0065-2407(08)60007-5", "10.1089/109493101753235151", "10.1017/CBO9780511752896.010", "10.1037/0022-3514.51.2.242", "10.1037/1076-8971.7.3.622", "10.1017/CBO9780511752896", "10.1111/j.1467-9930.2006.00227.x", "10.1111/j.1467-9930.2006.00226.x", "10.1111/j.1556-4029.2010.01671.x", "10.1080/001401397188387", "10.1057/978-1-137-55475-8_14", "10.1207/s15327663jcp1401&amp;2_19", "10.1037/0278-7393.14.3.521", "10.1037/0022-3514.62.2.189", "10.1002/acp.3240", "10.1016/0749-5978(89)90040-X", "10.1080/001401300184378", "10.1089/109493101300117938", "10.1016/j.forsciint.2012.05.015", "10.1007/s12024-014-9605-0", "10.1111/1556-4029.12736", "10.1016/j.acn.2006.10.006", "10.1007/978-1-4614-7485-2_18", "10.1162/pres.1994.3.2.130", "10.1016/S0065-2407(08)60007-5", "10.1089/109493101753235151", "10.1017/CBO9780511752896.010", "10.1037/0022-3514.51.2.242", "10.1037/1076-8971.7.3.622", "10.1017/CBO9780511752896", "10.1111/j.1467-9930.2006.00227.x", "10.1111/j.1467-9930.2006.00226.x", "10.1111/j.1556-4029.2010.01671.x", "10.1080/001401397188387", "10.1057/978-1-137-55475-8_14", "10.1207/s15327663jcp1401&amp;2_19", "10.1037/0278-7393.14.3.521", "10.1037/0022-3514.62.2.189", "10.1002/acp.3240", "10.1016/0749-5978(89)90040-X", "10.1080/001401300184378", "10.1089/109493101300117938", "10.1016/j.forsciint.2012.05.015", "10.1007/s12024-014-9605-0", "10.1111/1556-4029.12736", "10.1016/j.acn.2006.10.006", "10.1007/978-1-4614-7485-2_18", "10.1162/pres.1994.3.2.130", "10.1016/S0065-2407(08)60007-5", "10.1089/109493101753235151"]}, "10.1109/TVCG.2018.2868581": {"doi": "10.1109/TVCG.2018.2868581", "author": ["D. Yu", "K. Fan", "H. Zhang", "D. Monteiro", "W. Xu", "H. Liang"], "title": "PizzaText: Text Entry for Virtual Reality Systems Using Dual Thumbsticks", "year": "2018", "abstract": "We present PizzaText, a circular keyboard layout technique for text entry in virtual reality (VR) environments that uses the dual thumbsticks of a hand-held game controller. Text entry is a common activity in VR environments but remains challenging with existing techniques and keyboard layouts that is largely based on QWERTY. Our technique makes text entry simple, easy, and efficient, even for novice users. The technique uses a hand-held controller because it is still an important input device for users to interact with VR environments. To allow rapid search of characters, PizzaText divides a circle into slices and each slice contains 4 characters. To enable fast selection, the user uses the right thumbstick for traversing the slices, and the left thumbstick for choosing the letters. The design of PizzaText is based on three criteria: efficiency, learnability, and ease-of-use. In our first study, six potential layouts are considered and evaluated. The results lead to a design with 7 slices and 4 letters per slice. The final design is evaluated in a five-day study with 10 participants. The results show that novice users can achieve an average of 8.59 Words per Minute (WPM), while expert users are able to reach 15.85 WPM, with just two hours of training.", "keywords": ["computer games", "keyboards", "virtual reality", "virtual reality systems", "dual thumbsticks", "circular keyboard layout technique", "virtual reality environments", "hand-held game controller", "VR environments", "keyboard layouts", "hand-held controller", "text entry", "PizzaText", "QWERTY", "Layout", "Keyboards", "Games", "Training", "Virtual reality", "Google", "Fans", "Virtual reality", "text entry", "game controller", "dual-joystick input", "selection keyboard", "circular keyboard layout"], "referenced_by": ["IKEY:8613672", "IKEY:8613634", "IKEY:8642443", "IKEY:8732156", "IKEY:8943750", "IKEY:8943748", "IKEY:9089533", "IKEY:9284762", "IKEY:9284718", "IKEY:9284742"], "referencing": ["IKEY:6246142", "IKEY:8446059", "IKEY:7460039", "IKEY:658467", "IKEY:7274401", "IKEY:8149508", "IKEY:1203567", "IKEY:6246142", "IKEY:8446059", "IKEY:7460039", "IKEY:658467", "IKEY:7274401", "IKEY:8149508", "IKEY:1203567", "IKEY:6246142", "IKEY:8446059", "IKEY:7460039", "IKEY:658467", "IKEY:7274401", "IKEY:8149508", "IKEY:1203567", "10.1145/1133265.1133299", "10.1145/2207676.2208659", "10.1145/1358628.1358821", "10.1145/3173574.3173755", "10.1145/2984511.2984576", "10.1145/765891.766081", "10.1145/1344471.1344483", "10.1145/345513.345262", "10.1145/1028014.1028031", "10.1145/3025453.3025580", "10.1145/2993369.2996330", "10.1145/1268517.1268536", "10.1145/2166966.2166983", "10.1145/642611.642618", "10.1145/3090083", "10.1145/765968.765971", "10.1145/2047196.2047258", "10.1145/288392.288611", "10.1145/238386.238533", "10.1145/1978942.1979303", "10.1145/1124772.1124842", "10.1145/2935334.2935376", "10.1145/348941.348990", "10.1145/642611.642632", "10.1145/3025453.3025783", "10.1145/1124772.1124844", "10.1145/765891.766083", "10.1145/3025453.3025454", "10.1145/2807442.2807504", "10.1145/3025453.3025964", "10.1145/1133265.1133299", "10.1145/2207676.2208659", "10.1145/1358628.1358821", "10.1145/3173574.3173755", "10.1145/2984511.2984576", "10.1145/765891.766081", "10.1145/1344471.1344483", "10.1145/345513.345262", "10.1145/1028014.1028031", "10.1145/3025453.3025580", "10.1145/2993369.2996330", "10.1145/1268517.1268536", "10.1145/2166966.2166983", "10.1145/642611.642618", "10.1145/3090083", "10.1145/765968.765971", "10.1145/2047196.2047258", "10.1145/288392.288611", "10.1145/238386.238533", "10.1145/1978942.1979303", "10.1145/1124772.1124842", "10.1145/2935334.2935376", "10.1145/348941.348990", "10.1145/642611.642632", "10.1145/3025453.3025783", "10.1145/1124772.1124844", "10.1145/765891.766083", "10.1145/3025453.3025454", "10.1145/2807442.2807504", "10.1145/3025453.3025964", "10.1145/1133265.1133299", "10.1145/2207676.2208659", "10.1145/1358628.1358821", "10.1145/3173574.3173755", "10.1145/2984511.2984576", "10.1145/765891.766081", "10.1145/1344471.1344483", "10.1145/345513.345262", "10.1145/1028014.1028031", "10.1145/3025453.3025580", "10.1145/2993369.2996330", "10.1145/1268517.1268536", "10.1145/2166966.2166983", "10.1145/642611.642618", "10.1145/3090083", "10.1145/765968.765971", "10.1145/2047196.2047258", "10.1145/288392.288611", "10.1145/238386.238533", "10.1145/1978942.1979303", "10.1145/1124772.1124842", "10.1145/2935334.2935376", "10.1145/348941.348990", "10.1145/642611.642632", "10.1145/3025453.3025783", "10.1145/1124772.1124844", "10.1145/765891.766083", "10.1145/3025453.3025454", "10.1145/2807442.2807504", "10.1145/3025453.3025964", "10.1177/154193120204602611", "10.1007/978-3-319-20916-6_4", "10.1177/154193120605000909", "10.1207/s15327051hci0701_3", "10.1007/3-540-45756-9_16", "10.1177/154193120204602611", "10.1007/978-3-319-20916-6_4", "10.1177/154193120605000909", "10.1207/s15327051hci0701_3", "10.1007/3-540-45756-9_16", "10.1177/154193120204602611", "10.1007/978-3-319-20916-6_4", "10.1177/154193120605000909", "10.1207/s15327051hci0701_3", "10.1007/3-540-45756-9_16"]}, "10.1109/TVCG.2018.2868559": {"doi": "10.1109/TVCG.2018.2868559", "author": ["L. Qian", "A. Plopski", "N. Navab", "P. Kazanzides"], "title": "Restoring the Awareness in the Occluded Visual Field for Optical See-Through Head-Mounted Displays", "year": "2018", "abstract": "Recent technical advancements support the application of Optical See-Through Head-Mounted Displays (OST-HMDs) in critical situations like navigation and manufacturing. However, while the form-factor of an OST-HMD occupies less of the user's visual field than in the past, it can still result in critical oversights, e.g., missing a pedestrian while driving a car. In this paper, we design and compare two methods to compensate for the loss of awareness due to the occlusion caused by OST-HMDs. Instead of presenting the occluded content to the user, we detect motion that is not visible to the user and highlight its direction either on the edge of the HMD screen, or by activating LEDs placed in the user's peripheral vision. The methods involve an offline stage, where the occluded visual field and location of each indicator and its associated occluded region of interest (OROI) are determined, and an online stage, where an enhanced optical flow algorithm tracks the motion in the occluded visual field. We have implemented both methods on a Microsoft HoloLens and an ODG R-9. Our prototype systems achieved success rates of 100% in an objective evaluation, and 98.90% in a pilot user study. Our methods are able to compensate for the loss of safety-critical information in the occluded visual field for state-of-the-art OST-HMDs and can be extended for their future generations.", "keywords": ["helmet mounted displays", "image sequences", "LED displays", "optical see-through head-mounted displays", "state-of-the-art OST-HMD", "HMD screen", "user peripheral vision", "occluded region of interest", "optical flow algorithm", "Microsoft HoloLens", "ODG R-9", "safety-critical information", "enhanced optical flow algorithm", "associated occluded region", "OST-HMD", "occluded visual field", "Head-mounted displays", "Optical distortion", "Light emitting diodes", "Context awareness", "Image edge detection", "Location awareness", "View Expansion", "Prototype", "Optical See-Through Head-Mounted Display", "Algorithms", "Computer Graphics", "Equipment Design", "Head", "Humans", "Image Processing, Computer-Assisted", "User-Computer Interface", "Virtual Reality", "Visual Fields"], "referenced_by": ["IKEY:9284762"], "referencing": ["IKEY:8446583", "IKEY:6788131", "IKEY:7892248", "IKEY:4160954", "IKEY:6671835", "IKEY:7064856", "IKEY:7756124", "IKEY:7893338", "IKEY:6619022", "IKEY:4079251", "IKEY:217222", "IKEY:8446583", "IKEY:6788131", "IKEY:7892248", "IKEY:4160954", "IKEY:6671835", "IKEY:7064856", "IKEY:7756124", "IKEY:7893338", "IKEY:6619022", "IKEY:4079251", "IKEY:217222", "IKEY:8446583", "IKEY:6788131", "IKEY:7892248", "IKEY:4160954", "IKEY:6671835", "IKEY:7064856", "IKEY:7756124", "IKEY:7893338", "IKEY:6619022", "IKEY:4079251", "IKEY:217222", "10.1145/2407336.2407344", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/1152215.1152261", "10.1145/985921.986035", "10.1145/2582051.2582100", "10.1145/3098279.3122124", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/3098279.3125439", "10.1145/1357054.1357179", "10.1145/2470654.2466112", "10.1145/2638728.2641695", "10.1145/2663806.2663824", "10.1145/2858036.2858339", "10.1145/2929464.2929481", "10.1145/3123021.3123052", "10.1145/2659766.2659771", "10.1145/3131085.3131088", "10.1145/223904.223938", "10.1145/2642918.2647417", "10.1145/2858036.2858212", "10.1145/2407336.2407344", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/1152215.1152261", "10.1145/985921.986035", "10.1145/2582051.2582100", "10.1145/3098279.3122124", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/3098279.3125439", "10.1145/1357054.1357179", "10.1145/2470654.2466112", "10.1145/2638728.2641695", "10.1145/2663806.2663824", "10.1145/2858036.2858339", "10.1145/2929464.2929481", "10.1145/3123021.3123052", "10.1145/2659766.2659771", "10.1145/3131085.3131088", "10.1145/223904.223938", "10.1145/2642918.2647417", "10.1145/2858036.2858212", "10.1145/2407336.2407344", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/1152215.1152261", "10.1145/985921.986035", "10.1145/2582051.2582100", "10.1145/3098279.3122124", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/3098279.3125439", "10.1145/1357054.1357179", "10.1145/2470654.2466112", "10.1145/2638728.2641695", "10.1145/2663806.2663824", "10.1145/2858036.2858339", "10.1145/2929464.2929481", "10.1145/3123021.3123052", "10.1145/2659766.2659771", "10.1145/3131085.3131088", "10.1145/223904.223938", "10.1145/2642918.2647417", "10.1145/2858036.2858212", "10.1007/978-3-319-23234-8_63", "10.1016/j.ijhcs.2007.07.003", "10.1002/cne.902920402", "10.1007/3-540-45103-X_50", "10.4249/scholarpedia.9618", "10.1016/0004-3702(81)90024-2", "10.1001/archopht.1983.01040010371002", "10.1177/154193120605001619", "10.1167/9.10.6", "10.1159/000067554", "10.1007/s11548-017-1564-y", "10.1117/12.197322", "10.1146/annurev-vision-082114-035733", "10.1016/j.tins.2011.07.001", "10.1167/11.5.13", "10.1097/01.ijg.0000151686.89162.28", "10.1080/17489725.2011.579579", "10.1097/00006324-200211000-00009", "10.1889/1.1831704", "10.1007/978-3-319-23234-8_63", "10.1016/j.ijhcs.2007.07.003", "10.1002/cne.902920402", "10.1007/3-540-45103-X_50", "10.4249/scholarpedia.9618", "10.1016/0004-3702(81)90024-2", "10.1001/archopht.1983.01040010371002", "10.1177/154193120605001619", "10.1167/9.10.6", "10.1159/000067554", "10.1007/s11548-017-1564-y", "10.1117/12.197322", "10.1146/annurev-vision-082114-035733", "10.1016/j.tins.2011.07.001", "10.1167/11.5.13", "10.1097/01.ijg.0000151686.89162.28", "10.1080/17489725.2011.579579", "10.1097/00006324-200211000-00009", "10.1889/1.1831704", "10.1007/978-3-319-23234-8_63", "10.1016/j.ijhcs.2007.07.003", "10.1002/cne.902920402", "10.1007/3-540-45103-X_50", "10.4249/scholarpedia.9618", "10.1016/0004-3702(81)90024-2", "10.1001/archopht.1983.01040010371002", "10.1177/154193120605001619", "10.1167/9.10.6", "10.1159/000067554", "10.1007/s11548-017-1564-y", "10.1117/12.197322", "10.1146/annurev-vision-082114-035733", "10.1016/j.tins.2011.07.001", "10.1167/11.5.13", "10.1097/01.ijg.0000151686.89162.28", "10.1080/17489725.2011.579579", "10.1097/00006324-200211000-00009", "10.1889/1.1831704"]}, "10.1109/TVCG.2018.2868591": {"doi": "10.1109/TVCG.2018.2868591", "author": ["K. Kim", "M. Billinghurst", "G. Bruder", "H. B. Duh", "G. F. Welch"], "title": "Revisiting Trends in Augmented Reality Research: A Review of the 2nd Decade of ISMAR (2008\u20132017)", "year": "2018", "abstract": "In 2008, Zhou et al. presented a survey paper summarizing the previous ten years of ISMAR publications, which provided invaluable insights into the research challenges and trends associated with that time period. Ten years later, we review the research that has been presented at ISMAR conferences since the survey of Zhou et al., at a time when both academia and the AR industry are enjoying dramatic technological changes. Here we consider the research results and trends of the last decade of ISMAR by carefully reviewing the ISMAR publications from the period of 2008-2017, in the context of the first ten years. The numbers of papers for different research topics and their impacts by citations were analyzed while reviewing them-which reveals that there is a sharp increase in AR evaluation and rendering research. Based on this review we offer some observations related to potential future research areas or trends, which could be helpful to AR researchers and industry members looking ahead.", "keywords": ["augmented reality", "rendering (computer graphics)", "ISMAR publications", "ISMAR conferences", "augmented reality", "rendering", "International Symposium on Mixed and Augmented Reality", "Market research", "Rendering (computer graphics)", "Calibration", "Augmented reality", "Industries", "Sensors", "Indexes", "Augmented reality", "mixed reality", "survey", "trends"], "referenced_by": [], "referencing": ["10.1145/1508044.1508049", "10.1145/2366145.2366183", "10.1145/258549.258715", "10.1145/3139131.3139151", "10.1145/2185520.2185576", "10.1145/1508044.1508049", "10.1145/2366145.2366183", "10.1145/258549.258715", "10.1145/3139131.3139151", "10.1145/2185520.2185576", "10.1145/1508044.1508049", "10.1145/2366145.2366183", "10.1145/258549.258715", "10.1145/3139131.3139151", "10.1145/2185520.2185576", "10.2196/jmir.6759", "10.1162/pres.1997.6.4.355", "10.1364/3D.2017.JTu1F.1", "10.1007/978-3-642-40483-2_19", "10.3389/frobt.2018.00037", "10.1002/cav.1771", "10.2196/jmir.6759", "10.1162/pres.1997.6.4.355", "10.1364/3D.2017.JTu1F.1", "10.1007/978-3-642-40483-2_19", "10.3389/frobt.2018.00037", "10.1002/cav.1771", "10.2196/jmir.6759", "10.1162/pres.1997.6.4.355", "10.1364/3D.2017.JTu1F.1", "10.1007/978-3-642-40483-2_19", "10.3389/frobt.2018.00037", "10.1002/cav.1771"]}, "10.1109/TVCG.2018.2868597": {"doi": "10.1109/TVCG.2018.2868597", "author": ["P. Pjanic", "S. Willi", "D. Iwai", "A. Grundh\u00f6fer"], "title": "Seamless Multi-Projection Revisited", "year": "2018", "abstract": "This paper introduces a novel photometric compensation technique for inter-projector luminance and chrominance variations. Although it sounds as a classical technical issue, to the best of our knowledge there is no existing solution to alleviate the spatial non-uniformity among strongly heterogeneous projectors at perceptually acceptable quality. Primary goal of our method is increasing the perceived seamlessness of the projection system by automatically generating an improved and consistent visual quality. It builds upon the existing research of multi-projection systems, but instead of working with perceptually non-uniform color spaces such as CIEXYZ, the overall computation is carried out using the RLab [10, pp. 243-254] color appearance model which models the color processing in an adaptive, perceptual manner. Besides, we propose an adaptive color gamut acquisition, spatially varying gamut mapping, and optimization framework for edge blending. The paper describes the overall workflow and detailed algorithm of each component, followed by an evaluation validating the proposed method. The experimental results both qualitatively and quantitatively show the proposed method significant improved the visual quality of projected results of a multi-projection display with projectors with severely heterogeneous color processing.", "keywords": ["colour displays", "image colour analysis", "optical projectors", "photometry", "color appearance model", "RLab color appearance model", "heterogeneous color processing", "heterogeneous projectors", "multiprojection display", "edge blending", "optimization framework", "gamut mapping", "adaptive color gamut acquisition", "nonuniform color spaces", "consistent visual quality", "chrominance variations", "inter-projector luminance", "photometric compensation technique", "seamless multiprojection", "Image color analysis", "Color", "Adaptation models", "Computational modeling", "Visualization", "Optimization", "Three-dimensional displays", "Projector-camera systems", "colorimetric calibration", "3D stereoscopic and multi-user entertainment"], "referenced_by": ["IKEY:9284714"], "referencing": ["IKEY:1388230", "IKEY:4376162", "IKEY:1640443", "IKEY:1467467", "IKEY:885684", "IKEY:1260769", "IKEY:809883", "IKEY:5290744", "IKEY:946632", "IKEY:7164338", "IKEY:4392749", "IKEY:964508", "IKEY:1388230", "IKEY:4376162", "IKEY:1640443", "IKEY:1467467", "IKEY:885684", "IKEY:1260769", "IKEY:809883", "IKEY:5290744", "IKEY:946632", "IKEY:7164338", "IKEY:4392749", "IKEY:964508", "IKEY:1388230", "IKEY:4376162", "IKEY:1640443", "IKEY:1467467", "IKEY:885684", "IKEY:1260769", "IKEY:809883", "IKEY:5290744", "IKEY:946632", "IKEY:7164338", "IKEY:4392749", "IKEY:964508", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1278780.1278807", "10.1145/585740.585765", "10.1145/1037957.1037964", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/769953.769988", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1278780.1278807", "10.1145/585740.585765", "10.1145/1037957.1037964", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/769953.769988", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1278780.1278807", "10.1145/585740.585765", "10.1145/1037957.1037964", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/769953.769988", "10.1117/1.JEI.27.1.011011", "10.1007/BFb0086566", "10.1007/978-3-7091-6242-2_9", "10.1007/978-3-642-15561-1_6", "10.1002/col.20070", "10.1007/s11760-016-0961-y", "10.1007/978-3-642-12304-7_6", "10.1007/s12008-012-0161-0", "10.1117/1.JEI.27.1.011011", "10.1007/BFb0086566", "10.1007/978-3-7091-6242-2_9", "10.1007/978-3-642-15561-1_6", "10.1002/col.20070", "10.1007/s11760-016-0961-y", "10.1007/978-3-642-12304-7_6", "10.1007/s12008-012-0161-0", "10.1117/1.JEI.27.1.011011", "10.1007/BFb0086566", "10.1007/978-3-7091-6242-2_9", "10.1007/978-3-642-15561-1_6", "10.1002/col.20070", "10.1007/s11760-016-0961-y", "10.1007/978-3-642-12304-7_6", "10.1007/s12008-012-0161-0"]}, "10.1109/TVCG.2018.2868594": {"doi": "10.1109/TVCG.2018.2868594", "author": ["T. Piumsomboon", "G. A. Lee", "B. Ens", "B. H. Thomas", "M. Billinghurst"], "title": "Superman vs Giant: A Study on Spatial Perception for a Multi-Scale Mixed Reality Flying Telepresence Interface", "year": "2018", "abstract": "The advancements in Mixed Reality (MR), Unmanned Aerial Vehicle, and multi-scale collaborative virtual environments have led to new interface opportunities for remote collaboration. This paper explores a novel concept of flying telepresence for multi-scale mixed reality remote collaboration. This work could enable remote collaboration at a larger scale such as building construction. We conducted a user study with three experiments. The first experiment compared two interfaces, static and dynamic IPD, on simulator sickness and body size perception. The second experiment tested the user perception of a virtual object size under three levels of IPD and movement gain manipulation with a fixed eye height in a virtual environment having reduced or rich visual cues. Our last experiment investigated the participant's body size perception for two levels of manipulation of the IPDs and heights using stereo video footage to simulate a flying telepresence experience. The studies found that manipulating IPDs and eye height influenced the user's size perception. We present our findings and share the recommendations for designing a multi-scale MR flying telepresence interface.", "keywords": ["autonomous aerial vehicles", "groupware", "human computer interaction", "mobile robots", "telerobotics", "virtual reality", "spatial perception", "multiscale collaborative virtual environments", "unmanned aerial vehicle", "stereo video footage", "interpupillary distance", "multiscale MR flying telepresence interface", "movement gain manipulation", "virtual object size", "user perception", "body size perception", "simulator sickness", "dynamic IPD", "static IPD", "multiscale mixed reality remote collaboration", "Collaboration", "Telepresence", "Avatars", "Navigation", "Magnetic heads", "Virtual environments", "Mixed reality", "remote collaboration", "flying telepresence", "multi-scale collaborative environment", "Adult", "Algorithms", "Computer Graphics", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Space Perception", "Spatial Navigation", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:9089611", "IKEY:9089491", "IKEY:9090619", "IKEY:9284762"], "referencing": ["IKEY:6247833", "IKEY:6788957", "IKEY:7563562", "IKEY:6728902", "IKEY:1667642", "IKEY:816444", "IKEY:7460054", "IKEY:6797503", "IKEY:7050364", "IKEY:1512015", "IKEY:799723", "IKEY:6247833", "IKEY:6788957", "IKEY:7563562", "IKEY:6728902", "IKEY:1667642", "IKEY:816444", "IKEY:7460054", "IKEY:6797503", "IKEY:7050364", "IKEY:1512015", "IKEY:799723", "IKEY:6247833", "IKEY:6788957", "IKEY:7563562", "IKEY:6728902", "IKEY:1667642", "IKEY:816444", "IKEY:7460054", "IKEY:6797503", "IKEY:7050364", "IKEY:1512015", "IKEY:799723", "10.1145/3173574.3173620", "10.1145/3170427.3186495", "10.1145/2557500.2557527", "10.1145/2811266", "10.1145/2077451.2077464", "10.1145/2699254", "10.1145/1450579.1450606", "10.1145/3173574.3173620", "10.1145/3170427.3186495", "10.1145/2557500.2557527", "10.1145/2811266", "10.1145/2077451.2077464", "10.1145/2699254", "10.1145/1450579.1450606", "10.1145/3173574.3173620", "10.1145/3170427.3186495", "10.1145/2557500.2557527", "10.1145/2811266", "10.1145/2077451.2077464", "10.1145/2699254", "10.1145/1450579.1450606", "10.1007/978-3-319-29363-9_14", "10.1002/rob.21581", "10.1016/S0097-8493(01)00117-0", "10.1371/journal.pone.0068594", "10.3758/BF03194530", "10.1037/0096-1523.26.2.582", "10.1177/2041669517708205", "10.2197/ipsjjip.25.133", "10.1177/2041669515593028", "10.1207/s15327108ijap0303_3", "10.1117/12.207547", "10.1007/978-3-319-29363-9_14", "10.1002/rob.21581", "10.1016/S0097-8493(01)00117-0", "10.1371/journal.pone.0068594", "10.3758/BF03194530", "10.1037/0096-1523.26.2.582", "10.1177/2041669517708205", "10.2197/ipsjjip.25.133", "10.1177/2041669515593028", "10.1207/s15327108ijap0303_3", "10.1117/12.207547", "10.1007/978-3-319-29363-9_14", "10.1002/rob.21581", "10.1016/S0097-8493(01)00117-0", "10.1371/journal.pone.0068594", "10.3758/BF03194530", "10.1037/0096-1523.26.2.582", "10.1177/2041669517708205", "10.2197/ipsjjip.25.133", "10.1177/2041669515593028", "10.1207/s15327108ijap0303_3", "10.1117/12.207547"]}, "10.1109/TVCG.2018.2868584": {"doi": "10.1109/TVCG.2018.2868584", "author": ["F. Bork", "C. Schnelzer", "U. Eck", "N. Navab"], "title": "Towards Efficient Visual Guidance in Limited Field-of-View Head-Mounted Displays", "year": "2018", "abstract": "Understanding, navigating, and performing goal-oriented actions in Mixed Reality (MR) environments is a challenging task and requires adequate information conveyance about the location of all virtual objects in a scene. Current Head-Mounted Displays (HMDs) have a limited field-of-view where augmented objects may be displayed. Furthermore, complex MR environments may be comprised of a large number of objects which can be distributed in the extended surrounding space of the user. This paper presents two novel techniques for visually guiding the attention of users towards out-of-view objects in HMD-based MR: the 3D Radar and the Mirror Ball. We evaluate our approaches against existing techniques during three different object collection scenarios, which simulate real-world exploratory and goal-oriented visual search tasks. To better understand how the different visualizations guide the attention of users, we analyzed the head rotation data for all techniques and introduce a novel method to evaluate and classify head rotation trajectories. Our findings provide supporting evidence that the type of visual guidance technique impacts the way users search for virtual objects in MR.", "keywords": ["augmented reality", "helmet mounted displays", "field-of-view head-mounted displays", "Mixed Reality environments", "virtual objects", "augmented objects", "complex MR environments", "out-of-view objects", "real-world exploratory", "goal-oriented visual search tasks", "head rotation data", "head rotation trajectories", "visual guidance technique", "visual guidance", "information conveyance", "Head-Mounted Displays", "visualizations", "object collection scenarios", "goal-oriented actions", "Three-dimensional displays", "Trajectory", "Virtual reality", "Two dimensional displays", "Radar", "Visualization", "Task analysis", "Mixed / Augmented reality", "Visualization design and evaluation methods", "Adult", "Algorithms", "Attention", "Computer Graphics", "Female", "Head", "Humans", "Imaging, Three-Dimensional", "Male", "Middle Aged", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8798025", "IKEY:8797725", "IKEY:8797812", "IKEY:8805467", "IKEY:8943620", "IKEY:8951973", "IKEY:8951955", "IKEY:9089591", "IKEY:9199570", "IKEY:9284762"], "referencing": ["IKEY:4237188", "IKEY:4032601", "IKEY:5354792", "IKEY:1163491", "IKEY:368260", "IKEY:7893338", "IKEY:994784", "IKEY:4237188", "IKEY:4032601", "IKEY:5354792", "IKEY:1163491", "IKEY:368260", "IKEY:7893338", "IKEY:994784", "IKEY:4237188", "IKEY:4032601", "IKEY:5354792", "IKEY:1163491", "IKEY:368260", "IKEY:7893338", "IKEY:994784", "10.1145/1150402.1150411", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/502348.502358", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/1357054.1357179", "10.1145/1240866.1241014", "10.1145/1141911.1142013", "10.1145/3041164.3041198", "10.1145/1842993.1843008", "10.1145/1851600.1851655", "10.1145/2535597.2535608", "10.1145/2642918.2647417", "10.1145/360767.360802", "10.1145/3171221.3171253", "10.1145/563858.563896", "10.1145/765891.766022", "10.1145/1150402.1150411", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/502348.502358", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/1357054.1357179", "10.1145/1240866.1241014", "10.1145/1141911.1142013", "10.1145/3041164.3041198", "10.1145/1842993.1843008", "10.1145/1851600.1851655", "10.1145/2535597.2535608", "10.1145/2642918.2647417", "10.1145/360767.360802", "10.1145/3171221.3171253", "10.1145/563858.563896", "10.1145/765891.766022", "10.1145/1150402.1150411", "10.1145/642611.642695", "10.1145/1124772.1124939", "10.1145/1152215.1152266", "10.1145/502348.502358", "10.1145/3131277.3132175", "10.1145/3131277.3134362", "10.1145/1357054.1357179", "10.1145/1240866.1241014", "10.1145/1141911.1142013", "10.1145/3041164.3041198", "10.1145/1842993.1843008", "10.1145/1851600.1851655", "10.1145/2535597.2535608", "10.1145/2642918.2647417", "10.1145/360767.360802", "10.1145/3171221.3171253", "10.1145/563858.563896", "10.1145/765891.766022", "10.1142/S0218195995000064", "10.1016/j.ijhcs.2007.07.003", "10.1016/B978-012088469-8.50070-X", "10.1177/1071181312561385", "10.1016/j.cag.2011.04.005", "10.1287/opre.28.3.623", "10.1142/S0219843604000058", "10.1007/978-1-4020-2249-4_16", "10.14778/1453856.1453972", "10.1037//0022-0663.84.4.429", "10.1080/17489725.2011.579579", "10.1071/CH9490149", "10.1142/S0218195995000064", "10.1016/j.ijhcs.2007.07.003", "10.1016/B978-012088469-8.50070-X", "10.1177/1071181312561385", "10.1016/j.cag.2011.04.005", "10.1287/opre.28.3.623", "10.1142/S0219843604000058", "10.1007/978-1-4020-2249-4_16", "10.14778/1453856.1453972", "10.1037//0022-0663.84.4.429", "10.1080/17489725.2011.579579", "10.1071/CH9490149", "10.1142/S0218195995000064", "10.1016/j.ijhcs.2007.07.003", "10.1016/B978-012088469-8.50070-X", "10.1177/1071181312561385", "10.1016/j.cag.2011.04.005", "10.1287/opre.28.3.623", "10.1142/S0219843604000058", "10.1007/978-1-4020-2249-4_16", "10.14778/1453856.1453972", "10.1037//0022-0663.84.4.429", "10.1080/17489725.2011.579579", "10.1071/CH9490149"]}, "10.1109/TVCG.2018.2868527": {"doi": "10.1109/TVCG.2018.2868527", "author": ["Y. -W. Cha", "T. Price", "Z. Wei", "X. Lu", "N. Rewkowski", "R. Chabra", "Z. Qin", "H. Kim", "Z. Su", "Y. Liu", "A. Ilie", "A. State", "Z. Xu", "J. -M. Frahm", "H. Fuchs"], "title": "Towards Fully Mobile 3D Face, Body, and Environment Capture Using Only Head-worn Cameras", "year": "2018", "abstract": "We propose a new approach for 3D reconstruction of dynamic indoor and outdoor scenes in everyday environments, leveraging only cameras worn by a user. This approach allows 3D reconstruction of experiences at any location and virtual tours from anywhere. The key innovation of the proposed ego-centric reconstruction system is to capture the wearer's body pose and facial expression from near-body views, e.g. cameras on the user's glasses, and to capture the surrounding environment using outward-facing views. The main challenge of the ego-centric reconstruction, however, is the poor coverage of the near-body views - that is, the user's body and face are observed from vantage points that are convenient for wear but inconvenient for capture. To overcome these challenges, we propose a parametric-model-based approach to user motion estimation. This approach utilizes convolutional neural networks (CNNs) for near-view body pose estimation, and we introduce a CNN-based approach for facial expression estimation that combines audio and video. For each time-point during capture, the intermediate model-based reconstructions from these systems are used to re-target a high-fidelity pre-scanned model of the user. We demonstrate that the proposed self-sufficient, head-worn capture system is capable of reconstructing the wearer's movements and their surrounding environment in both indoor and outdoor situations without any additional views. As a proof of concept, we show how the resulting 3D-plus-time reconstruction can be immersively experienced within a virtual reality system (e.g., the HTC Vive). We expect that the size of the proposed egocentric capture-and-reconstruction system will eventually be reduced to fit within future AR glasses, and will be widely useful for immersive 3D telepresence, virtual tours, and general use-anywhere 3D content creation.", "keywords": ["cameras", "computer vision", "convolution", "face recognition", "feedforward neural nets", "image reconstruction", "mobile computing", "motion estimation", "pose estimation", "solid modelling", "virtual reality", "environment capture", "head-worn cameras", "dynamic indoor scenes", "virtual tours", "ego-centric reconstruction system", "near-body views", "parametric-model-based approach", "user motion estimation", "convolutional neural networks", "CNN-based approach", "facial expression estimation", "intermediate model-based reconstructions", "high-fidelity pre-scanned model", "head-worn capture system", "3D-plus-time reconstruction", "virtual reality system", "immersive 3D telepresence", "use-anywhere 3D content creation", "capture-and-reconstruction system", "fully mobile 3D face capture", "AR glasses", "HTC Vive", "near-view body pose estimation", "Cameras", "Three-dimensional displays", "Image reconstruction", "Face", "Solid modeling", "Pose estimation", "Deformable models", "Telepresence", "Ego-centric Vision", "Motion Capture", "Convolutional Neural Networks", "Facial Expression", "Humans", "Imaging, Three-Dimensional", "Internet", "Neural Networks, Computer", "Posture", "User-Computer Interface", "Video Recording"], "referenced_by": ["IKEY:8945141", "IKEY:9010852", "IKEY:9054812", "IKEY:9288370"], "referencing": ["IKEY:7523411", "IKEY:8237378", "IKEY:8099626", "IKEY:7892243", "IKEY:4270321", "IKEY:6802048", "IKEY:7298647", "IKEY:7898369", "IKEY:5206755", "IKEY:5459161", "IKEY:6162881", "IKEY:6180879", "IKEY:7219438", "IKEY:6162880", "IKEY:7298631", "IKEY:7780814", "IKEY:1640800", "IKEY:4178157", "IKEY:8100178", "IKEY:6165146", "IKEY:8237683", "IKEY:7780880", "IKEY:6126465", "IKEY:6618919", "IKEY:6618870", "IKEY:7780392", "IKEY:7523411", "IKEY:8237378", "IKEY:8099626", "IKEY:7892243", "IKEY:4270321", "IKEY:6802048", "IKEY:7298647", "IKEY:7898369", "IKEY:5206755", "IKEY:5459161", "IKEY:6162881", "IKEY:6180879", "IKEY:7219438", "IKEY:6162880", "IKEY:7298631", "IKEY:7780814", "IKEY:1640800", "IKEY:4178157", "IKEY:8100178", "IKEY:6165146", "IKEY:8237683", "IKEY:7780880", "IKEY:6126465", "IKEY:6618919", "IKEY:6618870", "IKEY:7780392", "IKEY:7523411", "IKEY:8237378", "IKEY:8099626", "IKEY:7892243", "IKEY:4270321", "IKEY:6802048", "IKEY:7298647", "IKEY:7898369", "IKEY:5206755", "IKEY:5459161", "IKEY:6162881", "IKEY:6180879", "IKEY:7219438", "IKEY:6162880", "IKEY:7298631", "IKEY:7780814", "IKEY:1640800", "IKEY:4178157", "IKEY:8100178", "IKEY:6165146", "IKEY:8237683", "IKEY:7780880", "IKEY:6126465", "IKEY:6618919", "IKEY:6618870", "IKEY:7780392", "10.1145/2001269.2001293", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2601097.2601204", "10.1145/237170.237269", "10.1145/1399504.1360697", "10.1145/2897824.2925969", "10.1145/2047196.2047270", "10.1145/2647868.2654889", "10.1145/3072959.3073658", "10.1145/2487228.2487237", "10.1145/2380116.2380139", "10.1145/2508363.2508407", "10.1145/2816795.2818122", "10.1145/2816795.2818013", "10.1145/2980179.2980252", "10.1145/2984511.2984517", "10.1145/2980179.2980235", "10.1145/2393091.2393095", "10.1145/1179352.1141964", "10.1145/1015706.1015736", "10.1145/1399504.1360696", "10.1145/2508363.2508418", "10.1145/2661229.2661286", "10.1145/2001269.2001293", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2601097.2601204", "10.1145/237170.237269", "10.1145/1399504.1360697", "10.1145/2897824.2925969", "10.1145/2047196.2047270", "10.1145/2647868.2654889", "10.1145/3072959.3073658", "10.1145/2487228.2487237", "10.1145/2380116.2380139", "10.1145/2508363.2508407", "10.1145/2816795.2818122", "10.1145/2816795.2818013", "10.1145/2980179.2980252", "10.1145/2984511.2984517", "10.1145/2980179.2980235", "10.1145/2393091.2393095", "10.1145/1179352.1141964", "10.1145/1015706.1015736", "10.1145/1399504.1360696", "10.1145/2508363.2508418", "10.1145/2661229.2661286", "10.1145/2001269.2001293", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2601097.2601204", "10.1145/237170.237269", "10.1145/1399504.1360697", "10.1145/2897824.2925969", "10.1145/2047196.2047270", "10.1145/2647868.2654889", "10.1145/3072959.3073658", "10.1145/2487228.2487237", "10.1145/2380116.2380139", "10.1145/2508363.2508407", "10.1145/2816795.2818122", "10.1145/2816795.2818013", "10.1145/2980179.2980252", "10.1145/2984511.2984517", "10.1145/2980179.2980235", "10.1145/2393091.2393095", "10.1145/1179352.1141964", "10.1145/1015706.1015736", "10.1145/1399504.1360696", "10.1145/2508363.2508418", "10.1145/2661229.2661286", "10.1007/s11263-013-0667-3", "10.1007/s00371-013-0775-7", "10.3311/PPee.7163", "10.1007/978-3-642-33783-3_18", "10.1016/j.patcog.2017.02.018", "10.1007/978-3-319-10605-2_3", "10.1023/A:1014573219977", "10.1007/978-3-319-46487-9_31", "10.1007/978-3-642-33709-3_59", "10.1007/s11263-013-0667-3", "10.1007/s00371-013-0775-7", "10.3311/PPee.7163", "10.1007/978-3-642-33783-3_18", "10.1016/j.patcog.2017.02.018", "10.1007/978-3-319-10605-2_3", "10.1023/A:1014573219977", "10.1007/978-3-319-46487-9_31", "10.1007/978-3-642-33709-3_59", "10.1007/s11263-013-0667-3", "10.1007/s00371-013-0775-7", "10.3311/PPee.7163", "10.1007/978-3-642-33783-3_18", "10.1016/j.patcog.2017.02.018", "10.1007/978-3-319-10605-2_3", "10.1023/A:1014573219977", "10.1007/978-3-319-46487-9_31", "10.1007/978-3-642-33709-3_59"]}}