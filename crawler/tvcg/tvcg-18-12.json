{"10.1109/TVCG.2017.2772238": {"doi": "10.1109/TVCG.2017.2772238", "author": ["D. T. Nguyen", "B. Hua", "L. Yu", "S. Yeung"], "title": "A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation", "year": "2018", "abstract": "Recent advances of 3D acquisition devices have enabled large-scale acquisition of 3D scene data. Such data, if completely and well annotated, can serve as useful ingredients for a wide spectrum of computer vision and graphics works such as data-driven modeling and scene understanding, object detection and recognition. However, annotating a vast amount of 3D scene data remains challenging due to the lack of an effective tool and/or the complexity of 3D scenes (e.g., clutter, varying illumination conditions). This paper aims to build a robust annotation tool that effectively and conveniently enables the segmentation and annotation of massive 3D data. Our tool works by coupling 2D and 3D information via an interactive framework, through which users can provide high-level semantic annotation for objects. We have experimented our tool and found that a typical indoor scene could be well segmented and annotated in less than 30 minutes by using the tool, as opposed to a few hours if done manually. Along with the tool, we created a dataset of over a hundred 3D scenes associated with complete annotations using our tool. Both the tool and dataset are available at http://scenenn.net.", "keywords": ["computer graphics", "computer vision", "image annotation", "image segmentation", "interactive systems", "object detection", "3D scene data", "effective tool", "robust annotation tool", "high-level semantic annotation", "complete annotations", "scene segmentation", "3D acquisition devices", "large-scale acquisition", "scene understanding", "robust 3D-2D interactive tool", "scene annotation", "Three-dimensional displays", "Image segmentation", "Tools", "Two dimensional displays", "Image reconstruction", "Semantics", "Shape", "Annotation tool", "semantic annotation", "3D segmentation", "3D reconstruction", "2D-3D interactive framework"], "referenced_by": ["IKEY:8941809", "IKEY:9008302", "IKEY:9090499"], "referencing": ["IKEY:6751312", "IKEY:6909459", "IKEY:7165673", "IKEY:6619113", "IKEY:6618864", "IKEY:5995448", "IKEY:5206848", "IKEY:5539970", "IKEY:4531741", "IKEY:6618923", "IKEY:5995693", "IKEY:6751377", "IKEY:6618852", "IKEY:6751286", "IKEY:6751287", "IKEY:6619247", "IKEY:6619022", "IKEY:993558", "IKEY:7354011", "IKEY:6162880", "IKEY:7299077", "IKEY:868688", "IKEY:6630857", "IKEY:6205760", "IKEY:1467360", "IKEY:4767851", "IKEY:1211346", "IKEY:7299067", "IKEY:6619221", "IKEY:937655", "IKEY:6751312", "IKEY:6909459", "IKEY:7165673", "IKEY:6619113", "IKEY:6618864", "IKEY:5995448", "IKEY:5206848", "IKEY:5539970", "IKEY:4531741", "IKEY:6618923", "IKEY:5995693", "IKEY:6751377", "IKEY:6618852", "IKEY:6751286", "IKEY:6751287", "IKEY:6619247", "IKEY:6619022", "IKEY:993558", "IKEY:7354011", "IKEY:6162880", "IKEY:7299077", "IKEY:868688", "IKEY:6630857", "IKEY:6205760", "IKEY:1467360", "IKEY:4767851", "IKEY:1211346", "IKEY:7299067", "IKEY:6619221", "IKEY:937655", "IKEY:6751312", "IKEY:6909459", "IKEY:7165673", "IKEY:6619113", "IKEY:6618864", "IKEY:5995448", "IKEY:5206848", "IKEY:5539970", "IKEY:4531741", "IKEY:6618923", "IKEY:5995693", "IKEY:6751377", "IKEY:6618852", "IKEY:6751286", "IKEY:6751287", "IKEY:6619247", "IKEY:6619022", "IKEY:993558", "IKEY:7354011", "IKEY:6162880", "IKEY:7299077", "IKEY:868688", "IKEY:6630857", "IKEY:6205760", "IKEY:1467360", "IKEY:4767851", "IKEY:1211346", "IKEY:7299067", "IKEY:6619221", "IKEY:937655", "10.1145/2751556", "10.1145/2702123.2702222", "10.1145/2366145.2366157", "10.1145/2366145.2366156", "10.1145/2366145.2366155", "10.1145/2661229.2661239", "10.1145/2768821", "10.1145/2461912.2461919", "10.1145/3054739", "10.1145/37401.37422", "10.1145/2751556", "10.1145/2702123.2702222", "10.1145/2366145.2366157", "10.1145/2366145.2366156", "10.1145/2366145.2366155", "10.1145/2661229.2661239", "10.1145/2768821", "10.1145/2461912.2461919", "10.1145/3054739", "10.1145/37401.37422", "10.1145/2751556", "10.1145/2702123.2702222", "10.1145/2366145.2366157", "10.1145/2366145.2366156", "10.1145/2366145.2366155", "10.1145/2661229.2661239", "10.1145/2768821", "10.1145/2461912.2461919", "10.1145/3054739", "10.1145/37401.37422", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1007/s11263-007-0090-8", "10.1007/s11263-009-0275-4", "10.1007/978-3-642-15555-0_6", "10.1111/cgf.12574", "10.1007/978-3-319-46484-8_30", "10.1023/B:VISI.0000022288.19776.77", "10.1016/S0031-3203(99)00074-6", "10.1007/BF02278710", "10.1364/JOSAA.4.000629", "10.1016/j.cviu.2011.07.002", "10.1016/j.imavis.2008.09.008", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1007/s11263-007-0090-8", "10.1007/s11263-009-0275-4", "10.1007/978-3-642-15555-0_6", "10.1111/cgf.12574", "10.1007/978-3-319-46484-8_30", "10.1023/B:VISI.0000022288.19776.77", "10.1016/S0031-3203(99)00074-6", "10.1007/BF02278710", "10.1364/JOSAA.4.000629", "10.1016/j.cviu.2011.07.002", "10.1016/j.imavis.2008.09.008", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1007/s11263-007-0090-8", "10.1007/s11263-009-0275-4", "10.1007/978-3-642-15555-0_6", "10.1111/cgf.12574", "10.1007/978-3-319-46484-8_30", "10.1023/B:VISI.0000022288.19776.77", "10.1016/S0031-3203(99)00074-6", "10.1007/BF02278710", "10.1364/JOSAA.4.000629", "10.1016/j.cviu.2011.07.002", "10.1016/j.imavis.2008.09.008"]}, "10.1109/TVCG.2017.2774292": {"doi": "10.1109/TVCG.2017.2774292", "author": ["F. Tang", "W. Dong", "Y. Meng", "X. Mei", "F. Huang", "X. Zhang", "O. Deussen"], "title": "Animated Construction of Chinese Brush Paintings", "year": "2018", "abstract": "In this paper, we present a method for reconstructing the drawing process of Chinese brush paintings. We demonstrate the possibility of computing an artistically reasonable drawing order from a static brush painting that is consistent with the rules of art. We map the key principles of drawing composition to our computational framework, which first organizes the strokes in three stages and then optimizes stroke ordering with natural evolution strategies. Our system produces reasonable animated constructions of Chinese brush paintings with minimal or no user intervention. We test our algorithm on a range of input paintings with varying degrees of complexity and structure and then evaluate the results via a user study. We discuss the applications of the proposed system to painting instruction, painting animation, and image stylization, especially in the context of art teaching.", "keywords": ["art", "computer animation", "animated construction", "Chinese brush paintings", "artistically reasonable drawing order", "static brush painting", "painting instruction", "painting animation", "Painting", "Brushes", "Ink", "Art", "Animation", "Videos", "Education", "Chinese brush painting", "animation", "drawing analysis", "art teaching"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2785807": {"doi": "10.1109/TVCG.2017.2785807", "author": ["D. Park", "S. M. Drucker", "R. Fernandez", "N. Elmqvist"], "title": "Atom: A Grammar for Unit Visualizations", "year": "2018", "abstract": "Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark-a visual unit-during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called Atom, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.", "keywords": ["data visualisation", "grammars", "unit visualizations", "visualization grammars", "visual mark", "Atom", "visual encoding", "user mental model", "Data visualization", "Visualization", "Grammar", "Scalability", "Layout", "Clutter", "Visualization grammar", "unit visualizations", "declarative specification", "Algorithms", "Computer Graphics", "Databases, Factual", "Humans", "Image Processing, Computer-Assisted", "User-Computer Interface"], "referenced_by": ["IKEY:8440803", "IKEY:8640251", "IKEY:8967136", "IKEY:9117123"], "referencing": ["IKEY:5184827", "IKEY:1207445", "IKEY:6634152", "IKEY:6875946", "IKEY:545307", "IKEY:4376143", "IKEY:4658123", "IKEY:7539404", "IKEY:6064996", "IKEY:5290720", "IKEY:5613453", "IKEY:7192704", "IKEY:7539624", "IKEY:6064987", "IKEY:6327265", "IKEY:963283", "IKEY:1173156", "IKEY:1249018", "IKEY:5473227", "IKEY:6876010", "IKEY:4376146", "IKEY:5184827", "IKEY:1207445", "IKEY:6634152", "IKEY:6875946", "IKEY:545307", "IKEY:4376143", "IKEY:4658123", "IKEY:7539404", "IKEY:6064996", "IKEY:5290720", "IKEY:5613453", "IKEY:7192704", "IKEY:7539624", "IKEY:6064987", "IKEY:6327265", "IKEY:963283", "IKEY:1173156", "IKEY:1249018", "IKEY:5473227", "IKEY:6876010", "IKEY:4376146", "IKEY:5184827", "IKEY:1207445", "IKEY:6634152", "IKEY:6875946", "IKEY:545307", "IKEY:4376143", "IKEY:4658123", "IKEY:7539404", "IKEY:6064996", "IKEY:5290720", "IKEY:5613453", "IKEY:7192704", "IKEY:7539624", "IKEY:6064987", "IKEY:6327265", "IKEY:963283", "IKEY:1173156", "IKEY:1249018", "IKEY:5473227", "IKEY:6876010", "IKEY:4376146", "10.1145/1978942.1979233", "10.1145/2598510.2598566", "10.1145/1124772.1124851", "10.1145/2468356.2479625", "10.1145/2556288.2557231", "10.1145/336597.336637", "10.1145/2642918.2647360", "10.1145/502356.502359", "10.1145/2380116.2380152", "10.1145/355588.365140", "10.1145/2702123.2702476", "10.1145/1978942.1979233", "10.1145/2598510.2598566", "10.1145/1124772.1124851", "10.1145/2468356.2479625", "10.1145/2556288.2557231", "10.1145/336597.336637", "10.1145/2642918.2647360", "10.1145/502356.502359", "10.1145/2380116.2380152", "10.1145/355588.365140", "10.1145/2702123.2702476", "10.1145/1978942.1979233", "10.1145/2598510.2598566", "10.1145/1124772.1124851", "10.1145/2468356.2479625", "10.1145/2556288.2557231", "10.1145/336597.336637", "10.1145/2642918.2647360", "10.1145/502356.502359", "10.1145/2380116.2380152", "10.1145/355588.365140", "10.1145/2702123.2702476", "10.1016/j.tics.2005.05.009", "10.1163/156856888X00122", "10.2307/2686111", "10.1057/palgrave.ivs.9500003", "10.1198/106186002317375631", "10.1111/cgf.12013", "10.1007/978-1-4613-8476-2", "10.1016/j.socscimed.2013.01.034", "10.2307/2284382", "10.1016/j.tics.2005.05.009", "10.1163/156856888X00122", "10.2307/2686111", "10.1057/palgrave.ivs.9500003", "10.1198/106186002317375631", "10.1111/cgf.12013", "10.1007/978-1-4613-8476-2", "10.1016/j.socscimed.2013.01.034", "10.2307/2284382", "10.1016/j.tics.2005.05.009", "10.1163/156856888X00122", "10.2307/2686111", "10.1057/palgrave.ivs.9500003", "10.1198/106186002317375631", "10.1111/cgf.12013", "10.1007/978-1-4613-8476-2", "10.1016/j.socscimed.2013.01.034", "10.2307/2284382"]}, "10.1109/TVCG.2017.2784441": {"doi": "10.1109/TVCG.2017.2784441", "author": ["R. Schmidtke", "K. Erleben"], "title": "Chunked Bounding Volume Hierarchies for Fast Digital Prototyping Using Volumetric Meshes", "year": "2018", "abstract": "We present a novel approach to using Bounding Volume Hierarchies (BVHs) for collision detection of volumetric meshes for digital prototyping based on accurate simulation. In general, volumetric meshes contain more primitives than surface meshes, which in turn means larger BVHs. To manage these larger BVHs, we propose an algorithm for splitting meshes into smaller chunks with a limited-size BVH each. Limited-height BVHs make guided, all-pairs testing of two chunked meshes well-suited for GPU implementation. This is because the dynamically generated work during BVH traversal becomes bounded. Chunking is simple to implement compared to dynamic load balancing methods and can result in an overall two orders of magnitude speedup on GPUs. This indicates that dynamic load balancing may not be a well suited scheme for the GPU. The overall application timings showed that data transfers were not the bottleneck. Instead, the conversion to and from OpenCL friendly data structures was causing serious performance impediments. Still, a simple OpenMP acceleration of the conversion allowed the GPU solution to beat the CPU solution by 20 percent. We demonstrate our results using rigid and deformable body scenes of varying complexities on a variety of GPUs.", "keywords": ["application program interfaces", "computational geometry", "data structures", "graphics processing units", "mesh generation", "multiprocessing programs", "volumetric meshes", "collision detection", "general meshes", "surface meshes", "splitting meshes", "chunked meshes well-suited", "GPU implementation", "BVH traversal", "chunking", "digital prototyping", "chunked bounding volume hierarchies", "GPU", "data transfers", "OpenCL friendly data structures", "OpenMP acceleration", "Collision avoidance", "Graphics processing units", "Load management", "Jamming", "Computational modeling", "Kernel", "Heuristic algorithms", "Collision detection", "bounding volume hierarchies", "parallel tandem traversal", "scheduling algorithm"], "referenced_by": [], "referencing": ["IKEY:4057175", "IKEY:982387", "IKEY:4745636", "IKEY:658428", "IKEY:4057175", "IKEY:982387", "IKEY:4745636", "IKEY:658428", "IKEY:4057175", "IKEY:982387", "IKEY:4745636", "IKEY:658428", "10.1145/231731.231732", "10.1145/1015706.1015735", "10.1145/1778765.1778819", "10.1145/2822013.2822015", "10.1145/311535.311550", "10.1145/882262.882358", "10.1145/2185520.2185602", "10.1145/1342250.1342260", "10.1145/2601097.2601114", "10.1145/2661229.2661237", "10.1145/2699276.2699286", "10.1145/1944745.1944756", "10.1145/1198555.1198788", "10.1145/1111411.1111432", "10.1145/2601097.2601199", "10.1145/3105762.3105776", "10.1145/3099564.3099575", "10.1145/231731.231732", "10.1145/1015706.1015735", "10.1145/1778765.1778819", "10.1145/2822013.2822015", "10.1145/311535.311550", "10.1145/882262.882358", "10.1145/2185520.2185602", "10.1145/1342250.1342260", "10.1145/2601097.2601114", "10.1145/2661229.2661237", "10.1145/2699276.2699286", "10.1145/1944745.1944756", "10.1145/1198555.1198788", "10.1145/1111411.1111432", "10.1145/2601097.2601199", "10.1145/3105762.3105776", "10.1145/3099564.3099575", "10.1145/231731.231732", "10.1145/1015706.1015735", "10.1145/1778765.1778819", "10.1145/2822013.2822015", "10.1145/311535.311550", "10.1145/882262.882358", "10.1145/2185520.2185602", "10.1145/1342250.1342260", "10.1145/2601097.2601114", "10.1145/2661229.2661237", "10.1145/2699276.2699286", "10.1145/1944745.1944756", "10.1145/1198555.1198788", "10.1145/1111411.1111432", "10.1145/2601097.2601199", "10.1145/3105762.3105776", "10.1145/3099564.3099575", "10.1111/j.1467-8659.2005.00829.x", "10.1007/1-4020-7775-0_5", "10.1111/1467-8659.t01-1-00587", "10.1080/10867651.1997.10487480", "10.1007/978-3-7091-6240-8_10", "10.1111/j.1467-8659.2009.01611.x", "10.1007/978-3-642-17452-0_13", "10.1111/j.1467-8659.2009.01556.x", "10.1111/j.1467-8659.2010.01769.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12582", "10.1111/cgf.13143", "10.1111/cgf.12272", "10.1111/j.1467-8659.2007.01012.x", "10.1111/j.1467-8659.2005.00829.x", "10.1111/j.1467-8659.2005.00829.x", "10.1007/1-4020-7775-0_5", "10.1111/1467-8659.t01-1-00587", "10.1080/10867651.1997.10487480", "10.1007/978-3-7091-6240-8_10", "10.1111/j.1467-8659.2009.01611.x", "10.1007/978-3-642-17452-0_13", "10.1111/j.1467-8659.2009.01556.x", "10.1111/j.1467-8659.2010.01769.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12582", "10.1111/cgf.13143", "10.1111/cgf.12272", "10.1111/j.1467-8659.2007.01012.x", "10.1111/j.1467-8659.2005.00829.x", "10.1111/j.1467-8659.2005.00829.x", "10.1007/1-4020-7775-0_5", "10.1111/1467-8659.t01-1-00587", "10.1080/10867651.1997.10487480", "10.1007/978-3-7091-6240-8_10", "10.1111/j.1467-8659.2009.01611.x", "10.1007/978-3-642-17452-0_13", "10.1111/j.1467-8659.2009.01556.x", "10.1111/j.1467-8659.2010.01769.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12582", "10.1111/cgf.13143", "10.1111/cgf.12272", "10.1111/j.1467-8659.2007.01012.x", "10.1111/j.1467-8659.2005.00829.x"]}, "10.1109/TVCG.2017.2779799": {"doi": "10.1109/TVCG.2017.2779799", "author": ["M. Zhang", "H. Lin", "K. Zhang", "J. Yu"], "title": "Computer Simulation and Generation of Moving Sand Pictures", "year": "2018", "abstract": "Moving sand pictures are interesting devices that can be used to generate an infinite number of unique scenes when repeatedly being flipped over. However, little work has been done on attempting to simulate the process of picture formulation. In this paper, we present an approach capable of generating images in the style of moving sand pictures. Our system defines moving sand pictures in a few steps, such as initialization, segmentation and physical simulation, so that a variety of moving sand pictures including mountain ridges, desert, clouds and even regular patterns can be generated by either automatic or semi-automatic via interaction during initialization and segmentation. Potential applications of our approach range from advertisements, posters, post cards, packaging, to digital arts.", "keywords": ["computer graphics", "digital simulation", "image motion analysis", "image segmentation", "computer simulation", "moving sand pictures", "physical simulation", "segmentation", "picture formulation", "Image color analysis", "Windows", "Computational modeling", "Image segmentation", "Adaptation models", "Mathematical model", "IP networks", "Moving sand picture", "tessellation", "segmentation", "color arrangement", "physical simulation"], "referenced_by": [], "referencing": ["IKEY:540485", "IKEY:1238267", "IKEY:540485", "IKEY:1238267", "IKEY:540485", "IKEY:1238267", "10.1145/1073204.1073298", "10.1145/1882261.1866195", "10.1145/2645703", "10.1145/2897824.2925897", "10.1145/1073368.1073379", "10.1145/2019406.2019410", "10.1145/2601097.2601152", "10.1145/2461912.2461948", "10.1145/2766996", "10.1145/2897824.2925906", "10.1145/2897824.2925877", "10.1145/166117.166162", "10.1145/35039.35040", "10.1145/1073204.1073298", "10.1145/1882261.1866195", "10.1145/2645703", "10.1145/2897824.2925897", "10.1145/1073368.1073379", "10.1145/2019406.2019410", "10.1145/2601097.2601152", "10.1145/2461912.2461948", "10.1145/2766996", "10.1145/2897824.2925906", "10.1145/2897824.2925877", "10.1145/166117.166162", "10.1145/35039.35040", "10.1145/1073204.1073298", "10.1145/1882261.1866195", "10.1145/2645703", "10.1145/2897824.2925897", "10.1145/1073368.1073379", "10.1145/2019406.2019410", "10.1145/2601097.2601152", "10.1145/2461912.2461948", "10.1145/2766996", "10.1145/2897824.2925906", "10.1145/2897824.2925877", "10.1145/166117.166162", "10.1145/35039.35040", "10.1088/0034-4885/68/8/R01", "10.1111/j.1467-8659.2009.01360.x", "10.1088/0305-4470/16/14/028", "10.2316/P.2011.756-036", "10.1016/0010-4655(94)00170-7", "10.1111/1467-8659.00299", "10.1051/jp1:1994195", "10.1016/S1631-0705(02)01304-X", "10.1103/PhysRev.148.375", "10.1088/0034-4885/68/8/R01", "10.1111/j.1467-8659.2009.01360.x", "10.1088/0305-4470/16/14/028", "10.2316/P.2011.756-036", "10.1016/0010-4655(94)00170-7", "10.1111/1467-8659.00299", "10.1051/jp1:1994195", "10.1016/S1631-0705(02)01304-X", "10.1103/PhysRev.148.375", "10.1088/0034-4885/68/8/R01", "10.1111/j.1467-8659.2009.01360.x", "10.1088/0305-4470/16/14/028", "10.2316/P.2011.756-036", "10.1016/0010-4655(94)00170-7", "10.1111/1467-8659.00299", "10.1051/jp1:1994195", "10.1016/S1631-0705(02)01304-X", "10.1103/PhysRev.148.375"]}, "10.1109/TVCG.2017.2778249": {"doi": "10.1109/TVCG.2017.2778249", "author": ["M. Wu", "V. Popescu"], "title": "Efficient VR and AR Navigation Through Multiperspective Occlusion Management", "year": "2018", "abstract": "Immersive navigation in virtual reality (VR) and augmented reality (AR) leverages physical locomotion through pose tracking of the head-mounted display. While this navigation modality is intuitive, regions of interest in the scene may suffer from occlusion and require significant viewpoint translation. Moreover, limited physical space and user mobility need to be taken into consideration. Some regions of interest may require viewpoints that are physically unreachable without less intuitive methods such as walking in-place or redirected walking. We propose a novel approach for increasing navigation efficiency in VR and AR using multiperspective visualization. Our approach samples occluded regions of interest from additional perspectives, which are integrated seamlessly into the user's perspective. This approach improves navigation efficiency by bringing simultaneously into view multiple regions of interest, allowing the user to explore more while moving less. We have conducted a user study that shows that our method brings significant performance improvement in VR and AR environments, on tasks that include tracking, matching, searching, and ambushing objects of interest.", "keywords": ["augmented reality", "data visualisation", "helmet mounted displays", "user interfaces", "head-mounted display", "multiperspective visualization", "multiperspective occlusion management", "immersive navigation", "pose tracking", "VR navigation", "virtual reality navigation", "augmented reality navigation", "AR navigation", "Visualization", "Navigation", "Cameras", "Legged locomotion", "Three-dimensional displays", "Explosions", "Buildings", "Augmented reality", "virtual reality", "navigation", "occlusion management", "multiperspective visualization", "depth cues"], "referenced_by": ["IKEY:8901214", "IKEY:9284661", "10.1007/978-3-030-01790-3_15"], "referencing": ["IKEY:5072212", "IKEY:1383052", "IKEY:4811002", "IKEY:5643547", "IKEY:5643546", "IKEY:4015467", "IKEY:4811001", "IKEY:7208899", "IKEY:7120994", "IKEY:6165137", "IKEY:5444815", "IKEY:5072212", "IKEY:1383052", "IKEY:4811002", "IKEY:5643547", "IKEY:5643546", "IKEY:4015467", "IKEY:4811001", "IKEY:7208899", "IKEY:7120994", "IKEY:6165137", "IKEY:5444815", "IKEY:5072212", "IKEY:1383052", "IKEY:4811002", "IKEY:5643547", "IKEY:5643546", "IKEY:4015467", "IKEY:4811001", "IKEY:7208899", "IKEY:7120994", "IKEY:6165137", "IKEY:5444815", "10.1145/2858036.2858084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/364338.364339", "10.1145/1836248.1836260", "10.1145/1272582.1272590", "10.1145/2897824.2925883", "10.1145/1450579.1450611", "10.1145/2686612.2686642", "10.1145/1457515.1409107", "10.1145/1276377.1276416", "10.1145/1360612.1360700", "10.1145/280814.280871", "10.1145/1618452.1618504", "10.1145/1518701.1518724", "10.1145/2858036.2858084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/364338.364339", "10.1145/1836248.1836260", "10.1145/1272582.1272590", "10.1145/2897824.2925883", "10.1145/1450579.1450611", "10.1145/2686612.2686642", "10.1145/1457515.1409107", "10.1145/1276377.1276416", "10.1145/1360612.1360700", "10.1145/280814.280871", "10.1145/1618452.1618504", "10.1145/1518701.1518724", "10.1145/2858036.2858084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/364338.364339", "10.1145/1836248.1836260", "10.1145/1272582.1272590", "10.1145/2897824.2925883", "10.1145/1450579.1450611", "10.1145/2686612.2686642", "10.1145/1457515.1409107", "10.1145/1276377.1276416", "10.1145/1360612.1360700", "10.1145/280814.280871", "10.1145/1618452.1618504", "10.1145/1518701.1518724", "10.1111/j.1467-8659.2005.00858.x", "10.22237/jmasm/1257035100", "10.1111/j.1467-8659.2005.00858.x", "10.22237/jmasm/1257035100", "10.1111/j.1467-8659.2005.00858.x", "10.22237/jmasm/1257035100"]}, "10.1109/TVCG.2018.2790961": {"doi": "10.1109/TVCG.2018.2790961", "author": ["V. Yoghourdjian", "T. Dwyer", "K. Klein", "K. Marriott", "M. Wybrow"], "title": "Graph Thumbnails: Identifying and Comparing Multiple Graphs at a Glance", "year": "2018", "abstract": "We propose Graph Thumbnails, small icon-like visualisations of the high-level structure of network data. Graph Thumbnails are designed to be legible in small multiples to support rapid browsing within large graph corpora. Compared to existing graph-visualisation techniques our representation has several advantages: (1) the visualisation can be computed in linear time; (2) it is canonical in the sense that isomorphic graphs will always have identical thumbnails; and (3) it provides precise information about the graph structure. We report the results of two user studies. The first study compares Graph Thumbnails to node-link and matrix views for identifying similar graphs. The second study investigates the comprehensibility of the different representations. We demonstrate the usefulness of this representation for summarising the evolution of protein-protein interaction networks across a range of species.", "keywords": ["data visualisation", "graph theory", "network theory (graphs)", "graph corpora", "isomorphic graphs", "graph structure", "Graph Thumbnails", "similar graph identification", "graph-visualisation techniques", "small icon-like visualisations", "high-level network data structure", "node-link", "matrix views", "protein-protein interaction networks", "Visualization", "Layout", "Data visualization", "Proteins", "Measurement", "Network visualisation", "circle packing", "k-core decomposition", "k-connected", "network identification", "large networks"], "referenced_by": ["IKEY:8809833", "IKEY:8812988", "IKEY:8807233", "IKEY:8809850", "IKEY:8933773", "IKEY:9308624"], "referencing": ["IKEY:4069239", "IKEY:7539373", "IKEY:63515", "IKEY:6634098", "IKEY:4658142", "IKEY:5429590", "IKEY:5992567", "IKEY:7965211", "IKEY:994702", "IKEY:4475481", "IKEY:7864456", "IKEY:7192733", "IKEY:6509098", "IKEY:4069239", "IKEY:7539373", "IKEY:63515", "IKEY:6634098", "IKEY:4658142", "IKEY:5429590", "IKEY:5992567", "IKEY:7965211", "IKEY:994702", "IKEY:4475481", "IKEY:7864456", "IKEY:7192733", "IKEY:6509098", "IKEY:4069239", "IKEY:7539373", "IKEY:63515", "IKEY:6634098", "IKEY:4658142", "IKEY:5429590", "IKEY:5992567", "IKEY:7965211", "IKEY:994702", "IKEY:4475481", "IKEY:7864456", "IKEY:7192733", "IKEY:6509098", "10.1145/1056018.1056041", "10.1145/2636240.2636839", "10.1145/2470654.2466444", "10.1145/1753326.1753358", "10.1145/2254556.2254651", "10.1145/1168149.1168168", "10.1145/2736277.2741640", "10.1145/2851613.2851650", "10.1145/1124772.1124851", "10.1145/3097983.3098130", "10.1145/1056018.1056041", "10.1145/2636240.2636839", "10.1145/2470654.2466444", "10.1145/1753326.1753358", "10.1145/2254556.2254651", "10.1145/1168149.1168168", "10.1145/2736277.2741640", "10.1145/2851613.2851650", "10.1145/1124772.1124851", "10.1145/3097983.3098130", "10.1145/1056018.1056041", "10.1145/2636240.2636839", "10.1145/2470654.2466444", "10.1145/1753326.1753358", "10.1145/2254556.2254651", "10.1145/1168149.1168168", "10.1145/2736277.2741640", "10.1145/2851613.2851650", "10.1145/1124772.1124851", "10.1145/3097983.3098130", "10.1111/cgf.12615", "10.1007/978-3-540-31843-9_6", "10.1137/130923646", "10.1137/070710111", "10.2307/2288400", "10.1007/s004539900017", "10.1016/j.ins.2013.09.048", "10.1073/pnas.122653799", "10.1002/wics.1343", "10.2307/2685881", "10.1093/bib/bbr069", "10.1057/ivs.2009.4", "10.1007/11569596_31", "10.1371/journal.pcbi.1000108", "10.1093/nar/gkh086", "10.1016/0378-8733(83)90028-X", "10.1037/h0046162", "10.1042/BST0311491", "10.1038/nature750", "10.1086/jar.33.4.3629752", "10.1111/cgf.12615", "10.1007/978-3-540-31843-9_6", "10.1137/130923646", "10.1137/070710111", "10.2307/2288400", "10.1007/s004539900017", "10.1016/j.ins.2013.09.048", "10.1073/pnas.122653799", "10.1002/wics.1343", "10.2307/2685881", "10.1093/bib/bbr069", "10.1057/ivs.2009.4", "10.1007/11569596_31", "10.1371/journal.pcbi.1000108", "10.1093/nar/gkh086", "10.1016/0378-8733(83)90028-X", "10.1037/h0046162", "10.1042/BST0311491", "10.1038/nature750", "10.1086/jar.33.4.3629752", "10.1111/cgf.12615", "10.1007/978-3-540-31843-9_6", "10.1137/130923646", "10.1137/070710111", "10.2307/2288400", "10.1007/s004539900017", "10.1016/j.ins.2013.09.048", "10.1073/pnas.122653799", "10.1002/wics.1343", "10.2307/2685881", "10.1093/bib/bbr069", "10.1057/ivs.2009.4", "10.1007/11569596_31", "10.1371/journal.pcbi.1000108", "10.1093/nar/gkh086", "10.1016/0378-8733(83)90028-X", "10.1037/h0046162", "10.1042/BST0311491", "10.1038/nature750", "10.1086/jar.33.4.3629752"]}, "10.1109/TVCG.2017.2787113": {"doi": "10.1109/TVCG.2017.2787113", "author": ["Y. Wang", "Z. Wang", "L. Zhu", "J. Zhang", "C. Fu", "Z. Cheng", "C. Tu", "B. Chen"], "title": "Is There a Robust Technique for Selecting Aspect Ratios in Line Charts?", "year": "2018", "abstract": "The aspect ratio of a line chart heavily influences the perception of the underlying data. Different methods explore different criteria in choosing aspect ratios, but so far, it was still unclear how to select aspect ratios appropriately for any given data. This paper provides a guideline for the user to choose aspect ratios for any input 1D curves by conducting an in-depth analysis of aspect ratio selection methods both theoretically and experimentally. By formulating several existing methods as line integrals, we explain their parameterization invariance. Moreover, we derive a new and improved aspect ratio selection method, namely the $L_1$ -LOR (local orientation resolution), with a certain degree of parameterization invariance. Furthermore, we connect different methods, including AL (arc length based method), the banking to 45 $^\\circ$ principle, RV (resultant vector) and AS (average absolute slope), as well as $L_1$ -LOR and AO (average absolute orientation). We verify these connections by a comparative evaluation involving various data sets, and show that the selections by RV and $L_1$ -LOR are complementary to each other for most data. Accordingly, we propose the dual-scale banking technique that combines the strengths of RV and $L_1$ -LOR, and demonstrate its practicability using multiple real-world data sets.", "keywords": ["Banking", "Robustness", "Guidelines", "Visual perception", "Data visualization", "Aspect ratio", "parameterization invariance", "line integral", "banking to 45 $^\\circ$ ", "orientation resolution"], "referenced_by": ["IKEY:8440843"], "referencing": ["IKEY:4015420", "IKEY:6064993", "IKEY:7465245", "IKEY:6634178", "IKEY:6327267", "IKEY:5613436", "IKEY:6875978", "IKEY:7192661", "IKEY:176285", "IKEY:5613429", "IKEY:4015420", "IKEY:6064993", "IKEY:7465245", "IKEY:6634178", "IKEY:6327267", "IKEY:5613436", "IKEY:6875978", "IKEY:7192661", "IKEY:176285", "IKEY:5613429", "IKEY:4015420", "IKEY:6064993", "IKEY:7465245", "IKEY:6634178", "IKEY:6327267", "IKEY:5613436", "IKEY:6875978", "IKEY:7192661", "IKEY:176285", "IKEY:5613429", "10.1145/1753326.1753357", "10.1145/1753326.1753357", "10.1145/1753326.1753357", "10.1080/01621459.1988.10478598", "10.1137/S1052623499362822", "10.1137/1.9780898718768", "10.1007/978-1-4899-1231-2_3", "10.1080/01621459.1988.10478598", "10.1137/S1052623499362822", "10.1137/1.9780898718768", "10.1007/978-1-4899-1231-2_3", "10.1080/01621459.1988.10478598", "10.1137/S1052623499362822", "10.1137/1.9780898718768", "10.1007/978-1-4899-1231-2_3"]}, "10.1109/TVCG.2017.2772237": {"doi": "10.1109/TVCG.2017.2772237", "author": ["S. Nadeem", "X. Gu", "A. E. Kaufman"], "title": "LMap: Shape-Preserving Local Mappings for Biomedical Visualization", "year": "2018", "abstract": "Visualization of medical organs and biological structures is a challenging task because of their complex geometry and the resultant occlusions. Global spherical and planar mapping techniques simplify the complex geometry and resolve the occlusions to aid in visualization. However, while resolving the occlusions these techniques do not preserve the geometric context, making them less suitable for mission-critical biomedical visualization tasks. In this paper, we present a shape-preserving local mapping technique for resolving occlusions locally while preserving the overall geometric context. More specifically, we present a novel visualization algorithm, LMap, for conformally parameterizing and deforming a selected local region-of-interest (ROI) on an arbitrary surface. The resultant shape-preserving local mappings help to visualize complex surfaces while preserving the overall geometric context. The algorithm is based on the robust and efficient extrinsic Ricci flow technique, and uses the dynamic Ricci flow algorithm to guarantee the existence of a local map for a selected ROI on an arbitrary surface. We show the effectiveness and efficacy of our method in three challenging use cases: (1) multimodal brain visualization, (2) optimal coverage of virtual colonoscopy centerline flythrough, and (3) molecular surface visualization.", "keywords": ["biological organs", "brain", "computational geometry", "computer vision", "data visualisation", "geometry", "medical image processing", "LMap", "medical organs", "biological structures", "complex geometry", "resultant occlusions", "global spherical mapping techniques", "planar mapping techniques", "geometric context", "mission-critical biomedical visualization tasks", "shape-preserving local mapping technique", "resultant shape-preserving local mappings", "complex surfaces", "robust Ricci flow technique", "dynamic Ricci flow algorithm", "extrinsic Ricci flow technique", "molecular surface visualization", "multimodal brain visualization", "Visualization", "Colon", "Heuristic algorithms", "Geometry", "Distortion", "Cavity resonators", "Biomedical visualization", "virtual colonoscopy", "multimodal brain visualization", "molecular surface visualization", "shape-preserving mapping", "Algorithms", "Brain", "Colonography, Computed Tomographic", "Computer Graphics", "Humans", "Imaging, Three-Dimensional", "Multimodal Imaging", "Surface Properties"], "referenced_by": [], "referencing": ["10.1145/2816795.2818099", "10.1145/2010324.1964999", "10.1145/1360612.1360676", "10.1145/2816795.2818099", "10.1145/2010324.1964999", "10.1145/1360612.1360676", "10.1145/2816795.2818099", "10.1145/2010324.1964999", "10.1145/1360612.1360676", "10.1007/10704282_30", "10.1093/nar/28.1.235", "10.1137/130950008", "10.1007/BFb0056277", "10.3389/fmolb.2014.00026", "10.1016/j.neuroimage.2004.07.018", "10.1016/j.neuroimage.2005.06.055", "10.1097/00004728-200003000-00001", "10.1093/bioinformatics/btt645", "10.1142/S0218195907002355", "10.1002/(SICI)1097-0282(199603)38:3&lt;305::AID-BIP4&gt;3.0.CO;2-Y", "10.1016/j.neuroimage.2013.05.041", "10.1007/11566489_83", "10.1007/10704282_30", "10.1093/nar/28.1.235", "10.1137/130950008", "10.1007/BFb0056277", "10.3389/fmolb.2014.00026", "10.1016/j.neuroimage.2004.07.018", "10.1016/j.neuroimage.2005.06.055", "10.1097/00004728-200003000-00001", "10.1093/bioinformatics/btt645", "10.1142/S0218195907002355", "10.1002/(SICI)1097-0282(199603)38:3&lt;305::AID-BIP4&gt;3.0.CO;2-Y", "10.1016/j.neuroimage.2013.05.041", "10.1007/11566489_83", "10.1007/10704282_30", "10.1093/nar/28.1.235", "10.1137/130950008", "10.1007/BFb0056277", "10.3389/fmolb.2014.00026", "10.1016/j.neuroimage.2004.07.018", "10.1016/j.neuroimage.2005.06.055", "10.1097/00004728-200003000-00001", "10.1093/bioinformatics/btt645", "10.1142/S0218195907002355", "10.1002/(SICI)1097-0282(199603)38:3&lt;305::AID-BIP4&gt;3.0.CO;2-Y", "10.1016/j.neuroimage.2013.05.041", "10.1007/11566489_83"]}, "10.1109/TVCG.2017.2772236": {"doi": "10.1109/TVCG.2017.2772236", "author": ["Q. Tong", "Z. Yuan", "X. Liao", "M. Zheng", "T. Yuan", "J. Zhao"], "title": "Magnetic Levitation Haptic Augmentation for Virtual Tissue Stiffness Perception", "year": "2018", "abstract": "Haptic-based tissue stiffness perception is essential for palpation training system, which can provide the surgeon haptic cues for improving the diagnostic abilities. However, current haptic devices, such as Geomagic Touch, fail to provide immersive and natural haptic interaction in virtual surgery due to the inherent mechanical friction, inertia, limited workspace and flawed haptic feedback. To tackle this issue, we design a novel magnetic levitation haptic device based on electromagnetic principles to augment the tissue stiffness perception in virtual environment. Users can naturally interact with the virtual tissue by tracking the motion of magnetic stylus using stereoscopic vision so that they can accurately sense the stiffness by the magnetic stylus, which moves in the magnetic field generated by our device. We propose the idea that the effective magnetic field (EMF) is closely related to the coil attitude for the first time. To fully harness the magnetic field and flexibly generate the specific magnetic field for obtaining required haptic perception, we adopt probability clouds to describe the requirement of interactive applications and put forward an algorithm to calculate the best coil attitude. Moreover, we design a control interface circuit and present a self-adaptive fuzzy proportion integration differentiation (PID) algorithm to precisely control the coil current. We evaluate our haptic device via a series of quantitative experiments which show the high consistency of the experimental and simulated magnetic flux density, the high accuracy (0.28 mm) of real-time 3D positioning and tracking of the magnetic stylus, the low power consumption of the adjustable coil configuration, and the tissue stiffness perception accuracy improvement by 2.38 percent with the self-adaptive fuzzy PID algorithm. We conduct a user study with 22 participants, and the results suggest most of the users can clearly and immersively perceive different tissue stiffness and easily detect the tissue abnormality. Experimental results demonstrate that our magnetic levitation haptic device can provide accurate tissue stiffness perception augmentation with natural and immersive haptic interaction.", "keywords": ["adaptive control", "biological tissues", "fuzzy control", "haptic interfaces", "magnetic flux", "magnetic levitation", "medical computing", "medical control systems", "self-adjusting systems", "surgery", "three-term control", "tissue abnormality", "tissue stiffness perception augmentation", "tissue stiffness perception", "magnetic levitation haptic device", "haptic-based tissue stiffness perception", "control interface circuit", "probability clouds", "self-adaptive fuzzy PID algorithm", "simulated magnetic flux density", "experimental flux density", "coil current", "self-adaptive fuzzy proportion integration differentiation algorithm", "interactive applications", "haptic perception", "specific magnetic field", "coil attitude", "effective magnetic field", "magnetic stylus", "virtual environment", "flawed haptic feedback", "inherent mechanical friction", "virtual surgery", "natural haptic interaction", "immersive interaction", "current haptic devices", "surgeon haptic cues", "palpation training system", "virtual tissue stiffness perception", "magnetic levitation haptic augmentation", "immersive haptic interaction", "natural interaction", "Haptic interfaces", "Magnetic levitation", "Surgery", "Friction", "Training", "Magnetic devices", "Algorithm design and analysis", "Virtual tissue stiffness perception", "magnetic levitation haptic feedback", "surgical simulation", "effective magnetic field", "Adult", "Algorithms", "Biomechanical Phenomena", "Elasticity", "Equipment Design", "Feedback", "Female", "Humans", "Kidney", "Magnetic Fields", "Male", "Models, Biological", "Palpation", "Phantoms, Imaging", "Signal Processing, Computer-Assisted", "Surgeons", "Virtual Reality"], "referenced_by": [], "referencing": ["IKEY:7833030", "IKEY:5453367", "IKEY:6165143", "IKEY:4543310", "IKEY:5746537", "IKEY:6225143", "IKEY:7457685", "IKEY:5759774", "IKEY:7359447", "IKEY:5945548", "IKEY:5710914", "IKEY:4810897", "IKEY:5152804", "IKEY:6183773", "IKEY:5985533", "IKEY:5977266", "IKEY:5326299", "IKEY:5945454", "IKEY:6894140", "IKEY:6907468", "IKEY:88141", "IKEY:5663692", "IKEY:4399165", "IKEY:888718", "IKEY:6183844", "IKEY:7833030", "IKEY:5453367", "IKEY:6165143", "IKEY:4543310", "IKEY:5746537", "IKEY:6225143", "IKEY:7457685", "IKEY:5759774", "IKEY:7359447", "IKEY:5945548", "IKEY:5710914", "IKEY:4810897", "IKEY:5152804", "IKEY:6183773", "IKEY:5985533", "IKEY:5977266", "IKEY:5326299", "IKEY:5945454", "IKEY:6894140", "IKEY:6907468", "IKEY:88141", "IKEY:5663692", "IKEY:4399165", "IKEY:888718", "IKEY:6183844", "IKEY:7833030", "IKEY:5453367", "IKEY:6165143", "IKEY:4543310", "IKEY:5746537", "IKEY:6225143", "IKEY:7457685", "IKEY:5759774", "IKEY:7359447", "IKEY:5945548", "IKEY:5710914", "IKEY:4810897", "IKEY:5152804", "IKEY:6183773", "IKEY:5985533", "IKEY:5977266", "IKEY:5326299", "IKEY:5945454", "IKEY:6894140", "IKEY:6907468", "IKEY:88141", "IKEY:5663692", "IKEY:4399165", "IKEY:888718", "IKEY:6183844", "10.1145/2897826.2927307", "10.1145/2461912.2462007", "10.1145/2501988.2502018", "10.1145/1401615.1401638", "10.1145/2993369.2993377", "10.1145/2661229.2661257", "10.1145/2407336.2407350", "10.1145/2897826.2927307", "10.1145/2461912.2462007", "10.1145/2501988.2502018", "10.1145/1401615.1401638", "10.1145/2993369.2993377", "10.1145/2661229.2661257", "10.1145/2407336.2407350", "10.1145/2897826.2927307", "10.1145/2461912.2462007", "10.1145/2501988.2502018", "10.1145/1401615.1401638", "10.1145/2993369.2993377", "10.1145/2661229.2661257", "10.1145/2407336.2407350", "10.1177/0278364905057861", "10.1007/s10916-016-0459-8", "10.1007/978-3-642-21504-9_9", "10.1177/027836490001900703", "10.1115/DSCC2008-2229", "10.1007/978-3-642-15567-3_21", "10.1111/j.1467-8659.2012.03230.x", "10.1177/0278364905057861", "10.1007/s10916-016-0459-8", "10.1007/978-3-642-21504-9_9", "10.1177/027836490001900703", "10.1115/DSCC2008-2229", "10.1007/978-3-642-15567-3_21", "10.1111/j.1467-8659.2012.03230.x", "10.1177/0278364905057861", "10.1007/s10916-016-0459-8", "10.1007/978-3-642-21504-9_9", "10.1177/027836490001900703", "10.1115/DSCC2008-2229", "10.1007/978-3-642-15567-3_21", "10.1111/j.1467-8659.2012.03230.x"]}, "10.1109/TVCG.2017.2786233": {"doi": "10.1109/TVCG.2017.2786233", "author": ["H. Zhang", "F. Xu"], "title": "MixedFusion: Real-Time Reconstruction of an Indoor Scene with Dynamic Objects", "year": "2018", "abstract": "Real-time indoor scene reconstruction aims to recover the 3D geometry of an indoor scene in real time with a sensor scanning the scene. Previous works of this topic consider pure static scenes, but in this paper, we focus on more challenging cases that the scene contains dynamic objects, for example, moving people and floating curtains, which are quite common in reality and thus are eagerly required to be handled. We develop an end-to-end system using a depth sensor to scan a scene on the fly. By proposing a Sigmoid-based Iterative Closest Point (S-ICP) method, we decouple the camera motion and the scene motion from the input sequence and segment the scene into static and dynamic parts accordingly. The static part is used to estimate the camera rigid motion, while for the dynamic part, graph node-based motion representation and model-to-depth fitting are applied to reconstruct the scene motions. With the camera and scene motions reconstructed, we further propose a novel mixed voxel allocation scheme to handle static and dynamic scene parts with different mechanisms, which helps to gradually fuse a large scene with both static and dynamic objects. Experiments show that our technique successfully fuses the geometry of both the static and dynamic objects in a scene in real time, which extends the usage of the current techniques for indoor scene reconstruction.", "keywords": ["cameras", "computer vision", "graph theory", "image fusion", "image reconstruction", "image representation", "image segmentation", "image sensors", "image sequences", "iterative methods", "motion estimation", "stereo image processing", "dynamic objects", "scene motion", "static scene parts", "dynamic scene parts", "static objects", "real-time reconstruction", "real-time indoor scene reconstruction", "3D geometry recovery", "moving people", "floating curtains", "depth sensor", "scene scanning", "sigmoid-based iterative closest point method", "camera motion", "scene segmentation", "camera rigid motion estimation", "graph node-based motion representation", "model-to-depth fitting", "mixed voxel allocation", "MixedFusion", "Dynamics", "Three-dimensional displays", "Cameras", "Real-time systems", "Image reconstruction", "Geometry", "Solid modeling", "Scene reconstruction", "dynamic reconstruction", "single view"], "referenced_by": ["IKEY:9022128", "IKEY:9089612"], "referencing": []}, "10.1109/TVCG.2017.2784830": {"doi": "10.1109/TVCG.2017.2784830", "author": ["O. Igouchkine", "Y. Zhang", "K. Ma"], "title": "Multi-Material Volume Rendering with a Physically-Based Surface Reflection Model", "year": "2018", "abstract": "Rendering techniques that increase realism in volume visualization help enhance perception of the 3D features in the volume data. While techniques focusing on high-quality global illumination have been extensively studied, few works handle the interaction of light with materials in the volume. Existing techniques for light-material interaction are limited in their ability to handle high-frequency real-world material data, and the current treatment of volume data poorly supports the correct integration of surface materials. In this paper, we introduce an alternative definition for the transfer function which supports surface-like behavior at the boundaries between volume components and volume-like behavior within. We show that this definition enables multi-material rendering with high-quality, real-world material data. We also show that this approach offers an efficient alternative to pre-integrated rendering through isosurface techniques. We introduce arbitrary spatially-varying materials to achieve better multi-material support for scanned volume data. Finally, we show that it is possible to map an arbitrary set of parameters directly to a material representation for the more intuitive creation of novel materials.", "keywords": ["data visualisation", "light reflection", "rendering (computer graphics)", "solid modelling", "transfer functions", "arbitrary spatially-varying materials", "volume-like behavior", "transfer function", "3D feature perception", "multimaterial volume rendering", "volume visualization", "realism", "surface materials", "light-material interaction", "high-quality global illumination", "rendering techniques", "physically-based surface reflection model", "material representation", "scanned volume data", "isosurface techniques", "multimaterial rendering", "volume components", "surface-like behavior", "Rendering (computer graphics)", "Transfer functions", "Lighting", "Scattering", "Isosurfaces", "Three-dimensional displays", "Volume visualization", "volume rendering", "global illumination", "BRDF", "hardware acceleration"], "referenced_by": ["IKEY:8933605"], "referencing": ["IKEY:6064955", "IKEY:5429594", "IKEY:6949566", "IKEY:6596129", "IKEY:4069241", "IKEY:5620899", "IKEY:6876035", "IKEY:745713", "IKEY:5290775", "IKEY:626170", "IKEY:6064955", "IKEY:5429594", "IKEY:6949566", "IKEY:6596129", "IKEY:4069241", "IKEY:5620899", "IKEY:6876035", "IKEY:745713", "IKEY:5290775", "IKEY:626170", "IKEY:6064955", "IKEY:5429594", "IKEY:6949566", "IKEY:6596129", "IKEY:4069241", "IKEY:5620899", "IKEY:6876035", "IKEY:745713", "IKEY:5290775", "IKEY:626170", "10.1145/54852.378484", "10.1145/197938.197971", "10.1145/37401.37422", "10.1145/882262.882343", "10.1145/344779.344814", "10.1145/360825.360839", "10.1145/357290.357293", "10.1145/2897824.2925917", "10.1145/2815618", "10.1145/2010324.1964939", "10.1145/2907941", "10.1145/1833351.1778772", "10.1145/965141.563893", "10.1145/1275808.1276473", "10.1145/1179352.1141987", "10.1145/54852.378484", "10.1145/197938.197971", "10.1145/37401.37422", "10.1145/882262.882343", "10.1145/344779.344814", "10.1145/360825.360839", "10.1145/357290.357293", "10.1145/2897824.2925917", "10.1145/2815618", "10.1145/2010324.1964939", "10.1145/2907941", "10.1145/1833351.1778772", "10.1145/965141.563893", "10.1145/1275808.1276473", "10.1145/1179352.1141987", "10.1145/54852.378484", "10.1145/197938.197971", "10.1145/37401.37422", "10.1145/882262.882343", "10.1145/344779.344814", "10.1145/360825.360839", "10.1145/357290.357293", "10.1145/2897824.2925917", "10.1145/2815618", "10.1145/2010324.1964939", "10.1145/2907941", "10.1145/1833351.1778772", "10.1145/965141.563893", "10.1145/1275808.1276473", "10.1145/1179352.1141987", "10.1111/cgf.12934", "10.1111/j.1467-8659.2007.01095.x", "10.1111/cgf.12916", "10.1111/j.1467-8659.2005.00855.x", "10.1371/journal.pone.0038586", "10.1364/AO.39.002592", "10.1016/j.jcde.2014.11.001", "10.1086/144246", "10.1111/cgf.12934", "10.1111/j.1467-8659.2007.01095.x", "10.1111/cgf.12916", "10.1111/j.1467-8659.2005.00855.x", "10.1371/journal.pone.0038586", "10.1364/AO.39.002592", "10.1016/j.jcde.2014.11.001", "10.1086/144246", "10.1111/cgf.12934", "10.1111/j.1467-8659.2007.01095.x", "10.1111/cgf.12916", "10.1111/j.1467-8659.2005.00855.x", "10.1371/journal.pone.0038586", "10.1364/AO.39.002592", "10.1016/j.jcde.2014.11.001", "10.1086/144246"]}, "10.1109/TVCG.2018.2796591": {"doi": "10.1109/TVCG.2018.2796591", "author": ["E. Cuenca", "A. Sallaberry", "F. Y. Wang", "P. Poncelet"], "title": "MultiStream: A Multiresolution Streamgraph Approach to Explore Hierarchical Time Series", "year": "2018", "abstract": "Multiple time series are a set of multiple quantitative variables occurring at the same interval. They are present in many domains such as medicine, finance, and manufacturing for analytical purposes. In recent years, streamgraph visualization (evolved from ThemeRiver) has been widely used for representing temporal evolution patterns in multiple time series. However, streamgraph as well as ThemeRiver suffer from scalability problems when dealing with several time series. To solve this problem, multiple time series can be organized into a hierarchical structure where individual time series are grouped hierarchically according to their proximity. In this paper, we present a new streamgraph-based approach to convey the hierarchical structure of multiple time series to facilitate the exploration and comparisons of temporal evolution. Based on a focus+context technique, our method allows time series exploration at different granularities (e.g., from overview to details). To illustrate our approach, two usage examples are presented.", "keywords": ["data structures", "data visualisation", "graph theory", "image resolution", "time series", "multiresolution visualization", "multiresolution streamgraph", "MultiStream", "stacked graph representation", "time series exploration", "Time series analysis", "Metals", "Rocks", "Data visualization", "Scalability", "Visualization", "Navigation", "Streamgraph", "stacked graph", "time series", "aggregation", "multiresolution visualization", "overview+detail", "focus+context", "fisheye"], "referenced_by": ["IKEY:8456856", "IKEY:8443124", "IKEY:8781573", "IKEY:8807213", "IKEY:8805466", "IKEY:8933582", "IKEY:8974327"], "referencing": ["IKEY:885098", "IKEY:981848", "IKEY:4658136", "IKEY:981847", "IKEY:6876032", "IKEY:6875992", "IKEY:6065008", "IKEY:7465276", "IKEY:6634152", "IKEY:7465277", "IKEY:1382913", "IKEY:1634320", "IKEY:4376131", "IKEY:4389005", "IKEY:6875938", "IKEY:545307", "IKEY:885098", "IKEY:981848", "IKEY:4658136", "IKEY:981847", "IKEY:6876032", "IKEY:6875992", "IKEY:6065008", "IKEY:7465276", "IKEY:6634152", "IKEY:7465277", "IKEY:1382913", "IKEY:1634320", "IKEY:4376131", "IKEY:4389005", "IKEY:6875938", "IKEY:545307", "IKEY:885098", "IKEY:981848", "IKEY:4658136", "IKEY:981847", "IKEY:6876032", "IKEY:6875992", "IKEY:6065008", "IKEY:7465276", "IKEY:6634152", "IKEY:7465277", "IKEY:1382913", "IKEY:1634320", "IKEY:4376131", "IKEY:4389005", "IKEY:6875938", "IKEY:545307", "10.1145/198366.198384", "10.1145/2470654.2466441", "10.1145/2598153.2598172", "10.1145/2089094.2089101", "10.1145/2396636.2396675", "10.1145/22627.22342", "10.1145/985692.985706", "10.1145/355637.355643", "10.1145/198366.198384", "10.1145/2470654.2466441", "10.1145/2598153.2598172", "10.1145/2089094.2089101", "10.1145/2396636.2396675", "10.1145/22627.22342", "10.1145/985692.985706", "10.1145/355637.355643", "10.1145/198366.198384", "10.1145/2470654.2466441", "10.1145/2598153.2598172", "10.1145/2089094.2089101", "10.1145/2396636.2396675", "10.1145/22627.22342", "10.1145/985692.985706", "10.1145/355637.355643", "10.1007/978-0-85729-079-3", "10.1080/00401706.1987.10488204", "10.1037/0022-3514.74.4.967", "10.1007/978-0-85729-079-3", "10.1080/00401706.1987.10488204", "10.1037/0022-3514.74.4.967", "10.1007/978-0-85729-079-3", "10.1080/00401706.1987.10488204", "10.1037/0022-3514.74.4.967"]}, "10.1109/TVCG.2017.2762691": {"doi": "10.1109/TVCG.2017.2762691", "author": ["M. Sra", "S. Garrido-Jurado", "P. Maes"], "title": "Oasis: Procedurally Generated Social Virtual Spaces from 3D Scanned Real Spaces", "year": "2018", "abstract": "We present Oasis, a novel system for automatically generating immersive and interactive virtual reality environments for single and multiuser experiences. Oasis enables real-walking in the generated virtual environment by capturing indoor scenes in 3D and mapping walkable areas. It makes use of available depth information for recognizing objects in the real environment which are paired with virtual counterparts to leverage the physicality of the real world, for a more immersive virtual experience. Oasis allows co-located and remotely located users to interact seamlessly and walk naturally in a shared virtual environment. Experiencing virtual reality with currently available devices can be cumbersome due to presence of objects and furniture which need to be removed every time the user wishes to use VR. Our approach is new, in that it allows casual users to easily create virtual reality environments in any indoor space without rearranging furniture or requiring specialized equipment, skill or training. We demonstrate our approach to overlay a virtual environment over an existing physical space through fully working single and multiuser systems implemented on a Tango tablet device.", "keywords": ["mobile computing", "object recognition", "virtual reality", "Oasis", "immersive reality environments", "interactive virtual reality environments", "multiuser experiences", "indoor scenes", "immersive virtual experience", "shared virtual environment", "procedurally generated social virtual spaces", "3D scanned real spaces", "Tango tablet device", "Three-dimensional displays", "Haptic interfaces", "Legged locomotion", "Tracking", "Object detection", "Virtual environments", "Virtual reality", "procedural generation", "multiuser interaction"], "referenced_by": ["IKEY:8798074", "IKEY:9199565", "IKEY:9262696"], "referencing": ["IKEY:6790640", "IKEY:658423", "IKEY:6162880", "IKEY:6630640", "IKEY:7504742", "IKEY:5759455", "IKEY:121791", "IKEY:583043", "IKEY:6790640", "IKEY:658423", "IKEY:6162880", "IKEY:6630640", "IKEY:7504742", "IKEY:5759455", "IKEY:121791", "IKEY:583043", "IKEY:6790640", "IKEY:658423", "IKEY:6162880", "IKEY:6630640", "IKEY:7504742", "IKEY:5759455", "IKEY:121791", "IKEY:583043", "10.1145/1476589.1476686", "10.1145/129888.129892", "10.1145/311535.311589", "10.1145/2818427.2818438", "10.1145/2815585.2817802", "10.1145/2807442.2807463", "10.1145/2702123.2702389", "10.1145/2858036.2858134", "10.1145/2858036.2858250", "10.1145/2984751.2984779", "10.1145/1190036.1190041", "10.1145/1476589.1476686", "10.1145/129888.129892", "10.1145/311535.311589", "10.1145/2818427.2818438", "10.1145/2815585.2817802", "10.1145/2807442.2807463", "10.1145/2702123.2702389", "10.1145/2858036.2858134", "10.1145/2858036.2858250", "10.1145/2984751.2984779", "10.1145/1190036.1190041", "10.1145/1476589.1476686", "10.1145/129888.129892", "10.1145/311535.311589", "10.1145/2818427.2818438", "10.1145/2815585.2817802", "10.1145/2807442.2807463", "10.1145/2702123.2702389", "10.1145/2858036.2858134", "10.1145/2858036.2858250", "10.1145/2984751.2984779", "10.1145/1190036.1190041", "10.1007/978-1-4471-4640-7_9", "10.1007/978-3-319-10599-4_41", "10.1162/pres.1997.6.4.461", "10.1007/BF00175354", "10.1016/j.patcog.2014.01.005", "10.1037/h0031545", "10.1007/BF01434994", "10.1177/0146167203029007002", "10.1007/978-1-4471-4640-7_9", "10.1007/978-3-319-10599-4_41", "10.1162/pres.1997.6.4.461", "10.1007/BF00175354", "10.1016/j.patcog.2014.01.005", "10.1037/h0031545", "10.1007/BF01434994", "10.1177/0146167203029007002", "10.1007/978-1-4471-4640-7_9", "10.1007/978-3-319-10599-4_41", "10.1162/pres.1997.6.4.461", "10.1007/BF00175354", "10.1016/j.patcog.2014.01.005", "10.1037/h0031545", "10.1007/BF01434994", "10.1177/0146167203029007002"]}, "10.1109/TVCG.2017.2783335": {"doi": "10.1109/TVCG.2017.2783335", "author": ["R. Luo", "W. Xu", "H. Wang", "K. Zhou", "Y. Yang"], "title": "Physics-Based Quadratic Deformation Using Elastic Weighting", "year": "2018", "abstract": "This paper presents a spatial reduction framework for simulating nonlinear deformable objects interactively. This reduced model is built using a small number of overlapping quadratic domains as we notice that incorporating high-order degrees of freedom (DOFs) is important for the simulation quality. Departing from existing multi-domain methods in graphics, our method interprets deformed shapes as blended quadratic transformations from nearby domains. Doing so avoids expensive safeguards against the domain coupling and improves the numerical robustness under large deformations. We present an algorithm that efficiently computes weight functions for reduced DOFs in a physics-aware manner. Inspired by the well-known multi-weight enveloping technique, our framework also allows subspace tweaking based on a few representative deformation poses. Such elastic weighting mechanism significantly extends the expressivity of the reduced model with light-weight computational efforts. Our simulator is versatile and can be well interfaced with many existing techniques. It also supports local DOF adaption to incorporate novel deformations (i.e., induced by the collision). The proposed algorithm complements state-of-the-art model reduction and domain decomposition methods by seeking for good trade-offs among animation quality, numerical robustness, pre-computation complexity and simulation efficiency from an alternative perspective.", "keywords": ["computational complexity", "computer animation", "deformation", "reduced order systems", "elastic weighting mechanism", "light-weight computational efforts", "local DOF adaption", "algorithm complements state-of-the-art model reduction", "domain decomposition methods", "animation quality", "numerical robustness", "pre-computation complexity", "physics-based quadratic deformation", "spatial reduction framework", "nonlinear deformable objects", "overlapping quadratic domains", "high-order degrees", "blended quadratic transformations", "domain coupling", "weight functions", "reduced DOFs", "physics-aware manner", "multiweight enveloping technique", "representative deformation", "Strain", "Deformable models", "Shape", "Computational modeling", "Numerical models", "Couplings", "Adaptation models", "Quadratic deformation", "FEM", "model reduction", "domain decomposition", "weight function"], "referenced_by": [], "referencing": ["IKEY:1359737", "IKEY:6461878", "IKEY:1056489", "IKEY:1359737", "IKEY:6461878", "IKEY:1056489", "IKEY:1359737", "IKEY:6461878", "IKEY:1056489", "10.1145/1964921.1964986", "10.1145/2816795.2818065", "10.1145/74333.74355", "10.1145/1186822.1073300", "10.1145/2508363.2508392", "10.1145/2816795.2818089", "10.1145/2019406.2019415", "10.1145/2487228.2487232", "10.1145/15922.15903", "10.1145/1964921.1964968", "10.1145/1944846.1944855", "10.1145/1778765.1778776", "10.1145/545261.545268", "10.1145/566570.566578", "10.1145/545261.545283", "10.1145/2343483.2343501", "10.1145/1731047.1731054", "10.1145/2816795.2818081", "10.1145/2231816.2231821", "10.1145/2980179.2982437", "10.1145/2980179.2980236", "10.1145/2897824.2925916", "10.1145/2461912.2461922", "10.1145/2766904", "10.1145/2567943", "10.1145/1073204.1073216", "10.1145/2614028.2615427", "10.1145/1276377.1276466", "10.1145/383259.383266", "10.1145/1073204.1073229", "10.1145/1576246.1531358", "10.1145/2766952", "10.1145/1028523.1028541", "10.1145/1457515.1409118", "10.1145/2601097.2601181", "10.1145/2010324.1964967", "10.1145/2010324.1964973", "10.1145/1964921.1964986", "10.1145/2816795.2818065", "10.1145/74333.74355", "10.1145/1186822.1073300", "10.1145/2508363.2508392", "10.1145/2816795.2818089", "10.1145/2019406.2019415", "10.1145/2487228.2487232", "10.1145/15922.15903", "10.1145/1964921.1964968", "10.1145/1944846.1944855", "10.1145/1778765.1778776", "10.1145/545261.545268", "10.1145/566570.566578", "10.1145/545261.545283", "10.1145/2343483.2343501", "10.1145/1731047.1731054", "10.1145/2816795.2818081", "10.1145/2231816.2231821", "10.1145/2980179.2982437", "10.1145/2980179.2980236", "10.1145/2897824.2925916", "10.1145/2461912.2461922", "10.1145/2766904", "10.1145/2567943", "10.1145/1073204.1073216", "10.1145/2614028.2615427", "10.1145/1276377.1276466", "10.1145/383259.383266", "10.1145/1073204.1073229", "10.1145/1576246.1531358", "10.1145/2766952", "10.1145/1028523.1028541", "10.1145/1457515.1409118", "10.1145/2601097.2601181", "10.1145/2010324.1964967", "10.1145/2010324.1964973", "10.1145/1964921.1964986", "10.1145/2816795.2818065", "10.1145/74333.74355", "10.1145/1186822.1073300", "10.1145/2508363.2508392", "10.1145/2816795.2818089", "10.1145/2019406.2019415", "10.1145/2487228.2487232", "10.1145/15922.15903", "10.1145/1964921.1964968", "10.1145/1944846.1944855", "10.1145/1778765.1778776", "10.1145/545261.545268", "10.1145/566570.566578", "10.1145/545261.545283", "10.1145/2343483.2343501", "10.1145/1731047.1731054", "10.1145/2816795.2818081", "10.1145/2231816.2231821", "10.1145/2980179.2982437", "10.1145/2980179.2980236", "10.1145/2897824.2925916", "10.1145/2461912.2461922", "10.1145/2766904", "10.1145/2567943", "10.1145/1073204.1073216", "10.1145/2614028.2615427", "10.1145/1276377.1276466", "10.1145/383259.383266", "10.1145/1073204.1073229", "10.1145/1576246.1531358", "10.1145/2766952", "10.1145/1028523.1028541", "10.1145/1457515.1409118", "10.1145/2601097.2601181", "10.1145/2010324.1964967", "10.1145/2010324.1964973", "10.1111/j.1467-8659.2006.01000.x", "10.1016/j.cagd.2008.09.009", "10.1016/S0167-8396(03)00002-5", "10.1111/j.1467-8659.2007.01103.x", "10.1017/CBO9780511804441", "10.1016/j.cag.2014.08.004", "10.1111/j.1467-8659.2006.01000.x", "10.1016/j.cagd.2008.09.009", "10.1016/S0167-8396(03)00002-5", "10.1111/j.1467-8659.2007.01103.x", "10.1017/CBO9780511804441", "10.1016/j.cag.2014.08.004", "10.1111/j.1467-8659.2006.01000.x", "10.1016/j.cagd.2008.09.009", "10.1016/S0167-8396(03)00002-5", "10.1111/j.1467-8659.2007.01103.x", "10.1017/CBO9780511804441", "10.1016/j.cag.2014.08.004"]}, "10.1109/TVCG.2017.2773071": {"doi": "10.1109/TVCG.2017.2773071", "author": ["J. Tao", "C. Wang", "N. V. Chawla", "L. Shi", "S. H. Kim"], "title": "Semantic Flow Graph: A Framework for Discovering Object Relationships in Flow Fields", "year": "2018", "abstract": "Visual exploration of flow fields is important for studying dynamic systems. We introduce semantic flow graph (SFG), a novel graph representation and interaction framework that enables users to explore the relationships among key objects (i.e., field lines, features, and spatiotemporal regions) of both steady and unsteady flow fields. The objects and their relationships are organized as a heterogeneous graph. We assign each object a set of attributes, based on which a semantic abstraction of the heterogeneous graph is generated. This semantic abstraction is SFG. We design a suite of operations to explore the underlying flow fields based on this graph representation and abstraction mechanism. Users can flexibly reconfigure SFG to examine the relationships among groups of objects at different abstraction levels. Three linked views are developed to display SFG, its node split criteria and history, and the objects in the spatial volume. For simplicity, we introduce SFG construction and exploration for steady flow fields with critical points being the only features. Then we demonstrate that SFG can be naturally extended to deal with unsteady flow fields and multiple types of features. We experiment with multiple data sets and conduct an expert evaluation to demonstrate the effectiveness of our approach.", "keywords": ["data visualisation", "graph theory", "SFG", "unsteady flow fields", "semantic flow graph", "object relationships", "visual exploration", "field lines", "heterogeneous graph", "semantic abstraction", "abstraction mechanism", "steady flow fields", "graph representation", "spatiotemporal regions", "Semantics", "Visualization", "History", "Two dimensional displays", "Clutter", "Three-dimensional displays", "Aerodynamics", "Flow visualization", "heterogeneous graph", "semantic abstraction", "critical points", "vortex cores", "FTLE", "field lines"], "referenced_by": ["IKEY:8291793"], "referencing": ["IKEY:1250376", "IKEY:8291793", "IKEY:1207439", "IKEY:4376210", "IKEY:7156350", "IKEY:5753894", "IKEY:7117453", "IKEY:5429603", "IKEY:1703379", "IKEY:663858", "IKEY:1183786", "IKEY:1703364", "IKEY:4015424", "IKEY:6596150", "IKEY:6613495", "IKEY:6183585", "IKEY:1382908", "IKEY:4376132", "IKEY:7192679", "IKEY:7192721", "IKEY:1250376", "IKEY:8291793", "IKEY:1207439", "IKEY:4376210", "IKEY:7156350", "IKEY:5753894", "IKEY:7117453", "IKEY:5429603", "IKEY:1703379", "IKEY:663858", "IKEY:1183786", "IKEY:1703364", "IKEY:4015424", "IKEY:6596150", "IKEY:6613495", "IKEY:6183585", "IKEY:1382908", "IKEY:4376132", "IKEY:7192679", "IKEY:7192721", "IKEY:1250376", "IKEY:8291793", "IKEY:1207439", "IKEY:4376210", "IKEY:7156350", "IKEY:5753894", "IKEY:7117453", "IKEY:5429603", "IKEY:1703379", "IKEY:663858", "IKEY:1183786", "IKEY:1703364", "IKEY:4015424", "IKEY:6596150", "IKEY:6613495", "IKEY:6183585", "IKEY:1382908", "IKEY:4376132", "IKEY:7192679", "IKEY:7192721", "10.1145/1124772.1124891", "10.1145/142750.142763", "10.1145/1124772.1124891", "10.1145/142750.142763", "10.1145/1124772.1124891", "10.1145/142750.142763", "10.1111/cgf.12800", "10.2200/S00433ED1V01Y201207DMK005", "10.1007/978-3-540-70823-0_3", "10.1080/0022250X.1971.9989788", "10.1007/BFb0021824", "10.1016/j.proci.2016.07.119", "10.1111/cgf.12800", "10.2200/S00433ED1V01Y201207DMK005", "10.1007/978-3-540-70823-0_3", "10.1080/0022250X.1971.9989788", "10.1007/BFb0021824", "10.1016/j.proci.2016.07.119", "10.1111/cgf.12800", "10.2200/S00433ED1V01Y201207DMK005", "10.1007/978-3-540-70823-0_3", "10.1080/0022250X.1971.9989788", "10.1007/BFb0021824", "10.1016/j.proci.2016.07.119"]}, "10.1109/TVCG.2017.2789203": {"doi": "10.1109/TVCG.2017.2789203", "author": ["X. Wang", "S. Liu", "Y. Tong"], "title": "Stain Formation on Deforming Inelastic Cloth", "year": "2018", "abstract": "We propose a novel approach to simulating the formation and evolution of stains on cloths in motion. We accurately capture the diffusion of a pigmented solution over a complex knitted or woven fabric through homogenization of its inhomogeneous and/or anisotropic properties into bulk anisotropic diffusion tensors. Secondary effects such as absorption, adsorption and evaporation are also accounted for through physically-based modeling. Finally, the influence of the cloth motion on the shape and evolution of the stain is captured by evaluating the inertial (e.g., centrifugal and Coriolis) forces experienced by the solution. The governing equations of motion are integrated in time directly on a deforming triangle mesh discretizing the inelastic cloth for efficiency and robustness. The deformation of the cloth can be precomputed or integrated through simplified two-way coupling, by using off-the-shell cloth simulations. Finally, numerical experiments demonstrate the plausibility of our results in practical applications by reproducing the usual shape and behavior of stains on various fabrics.", "keywords": ["adsorption", "clothing", "deformation", "elasticity", "fabrics", "mesh generation", "tensors", "anisotropic properties", "bulk anisotropic diffusion tensors", "adsorption", "evaporation", "cloth motion", "deforming triangle mesh", "off-the-shell cloth simulations", "stain formation", "pigmented solution", "inhomogeneous properties", "inelastic cloth deformation", "knitted woven fabric", "inertial force", "Mathematical model", "Weaving", "Fabrics", "Nonhomogeneous media", "Computational modeling", "Surface treatment", "Anisotropic magnetoresistance", "Cloth animation", "inhomogeneous and anisotropic cloth material", "stain formation", "deforming surfaces"], "referenced_by": [], "referencing": ["IKEY:393542", "IKEY:1309281", "IKEY:4392716", "IKEY:5999663", "IKEY:885721", "IKEY:7964760", "IKEY:393542", "IKEY:1309281", "IKEY:4392716", "IKEY:5999663", "IKEY:885721", "IKEY:7964760", "IKEY:393542", "IKEY:1309281", "IKEY:4392716", "IKEY:5999663", "IKEY:885721", "IKEY:7964760", "10.1145/311535.311548", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/882262.882338", "10.1145/1189762.1189766", "10.1145/2786784.2786793", "10.1145/2816795.2818130", "10.1145/1073204.1073284", "10.1145/1559764.1559772", "10.1145/2723158", "10.1145/2870629", "10.1145/1531326.1531357", "10.1145/1531326.1531358", "10.1145/2010324.1964985", "10.1145/311535.311548", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/882262.882338", "10.1145/1189762.1189766", "10.1145/2786784.2786793", "10.1145/2816795.2818130", "10.1145/1073204.1073284", "10.1145/1559764.1559772", "10.1145/2723158", "10.1145/2870629", "10.1145/1531326.1531357", "10.1145/1531326.1531358", "10.1145/2010324.1964985", "10.1145/311535.311548", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/882262.882338", "10.1145/1189762.1189766", "10.1145/2786784.2786793", "10.1145/2816795.2818130", "10.1145/1073204.1073284", "10.1145/1559764.1559772", "10.1145/2723158", "10.1145/2870629", "10.1145/1531326.1531357", "10.1145/1531326.1531358", "10.1145/2010324.1964985", "10.1142/S0218654301000047", "10.1002/cav.95", "10.1016/S0097-8493(99)00141-7", "10.1016/S0097-8493(00)00132-1", "10.1111/1467-8659.00505", "10.1007/s11042-015-2492-x", "10.1002/cav.19", "10.1111/cgf.12449", "10.1111/j.1467-8659.2012.03071.x", "10.1002/cav.1446", "10.1002/cpa.21395", "10.1111/cgf.12426", "10.1111/j.1467-8659.2008.01337.x", "10.1111/cgf.12230", "10.1007/978-3-642-84659-5", "10.1002/cpa.20163", "10.1016/S0143-7208(96)00080-0", "10.1111/j.1467-8659.2007.00940.x", "10.1107/S0567739478001680", "10.1007/BF01461107", "10.1021/ja02268a002", "10.1142/S0218654301000047", "10.1002/cav.95", "10.1016/S0097-8493(99)00141-7", "10.1016/S0097-8493(00)00132-1", "10.1111/1467-8659.00505", "10.1007/s11042-015-2492-x", "10.1002/cav.19", "10.1111/cgf.12449", "10.1111/j.1467-8659.2012.03071.x", "10.1002/cav.1446", "10.1002/cpa.21395", "10.1111/cgf.12426", "10.1111/j.1467-8659.2008.01337.x", "10.1111/cgf.12230", "10.1007/978-3-642-84659-5", "10.1002/cpa.20163", "10.1016/S0143-7208(96)00080-0", "10.1111/j.1467-8659.2007.00940.x", "10.1107/S0567739478001680", "10.1007/BF01461107", "10.1021/ja02268a002", "10.1142/S0218654301000047", "10.1002/cav.95", "10.1016/S0097-8493(99)00141-7", "10.1016/S0097-8493(00)00132-1", "10.1111/1467-8659.00505", "10.1007/s11042-015-2492-x", "10.1002/cav.19", "10.1111/cgf.12449", "10.1111/j.1467-8659.2012.03071.x", "10.1002/cav.1446", "10.1002/cpa.21395", "10.1111/cgf.12426", "10.1111/j.1467-8659.2008.01337.x", "10.1111/cgf.12230", "10.1007/978-3-642-84659-5", "10.1002/cpa.20163", "10.1016/S0143-7208(96)00080-0", "10.1111/j.1467-8659.2007.00940.x", "10.1107/S0567739478001680", "10.1007/BF01461107", "10.1021/ja02268a002"]}, "10.1109/TVCG.2017.2775241": {"doi": "10.1109/TVCG.2017.2775241", "author": ["K. Shkurko", "C. Yuksel", "D. Kopta", "I. Mallett", "E. Brunvand"], "title": "Time Interval Ray Tracing for Motion Blur", "year": "2018", "abstract": "We introduce a new motion blur computation method for ray tracing that provides an analytical approximation of motion blurred visibility per ray. Rather than relying on timestamped rays and Monte Carlo sampling to resolve the motion blur, we associate a time interval with rays and directly evaluate when and where each ray intersects with animated object faces. Based on our simplifications, the volume swept by each animated face is represented using a triangulation of the surface of this volume. Thus, we can resolve motion blur through ray intersections with stationary triangles, and we can use any standard ray tracing acceleration structure without modifications to account for the time dimension. Rays are intersected with these triangles to analytically determine the time interval and positions of the intersections with the moving objects. Furthermore, we explain an adaptive strategy to efficiently shade the intersection intervals. As a result, we can produce noise-free motion blur for both primary and secondary rays. We also provide a general framework for emulating various camera shutter mechanisms and an artistic modification that amplifies the visibility of moving objects for emphasizing the motion in videos or static images.", "keywords": ["computational geometry", "computer animation", "image motion analysis", "image restoration", "Monte Carlo methods", "ray tracing", "time interval ray tracing", "motion blurred visibility", "timestamped rays", "animated object faces", "ray intersections", "noise-free motion blur", "ray tracing acceleration structure", "motion blur computation", "Monte Carlo sampling", "Ray tracing", "Image reconstruction", "Cameras", "Geometry", "Dynamics", "Acceleration", "Strain", "Motion blur", "ray tracing", "sampling"], "referenced_by": [], "referencing": ["IKEY:998667", "IKEY:504", "IKEY:4061544", "IKEY:1500358", "IKEY:1284395", "IKEY:998667", "IKEY:504", "IKEY:4061544", "IKEY:1500358", "IKEY:1284395", "IKEY:998667", "IKEY:504", "IKEY:4061544", "IKEY:1500358", "IKEY:1284395", "10.1145/800031.808590", "10.1145/325165.325184", "10.1145/964967.801169", "10.1145/2159616.2159639", "10.1145/2018323.2018341", "10.1145/2492045.2492046", "10.1145/2010324.1964949", "10.1145/1179352.1141913", "10.1145/1833349.1778801", "10.1145/1399504.1360632", "10.1145/1964921.1964950", "10.1145/2185520.2185547", "10.1145/2167076.2167083", "10.1145/1661412.1618486", "10.1145/1576246.1531399", "10.1145/2487228.2487239", "10.1145/2461912.2462023", "10.1145/1833349.1778794", "10.1145/964967.801168", "10.1145/1572769.1572771", "10.1145/97879.97920", "10.1145/1944745.1944753", "10.1145/54852.378490", "10.1145/800031.808590", "10.1145/325165.325184", "10.1145/964967.801169", "10.1145/2159616.2159639", "10.1145/2018323.2018341", "10.1145/2492045.2492046", "10.1145/2010324.1964949", "10.1145/1179352.1141913", "10.1145/1833349.1778801", "10.1145/1399504.1360632", "10.1145/1964921.1964950", "10.1145/2185520.2185547", "10.1145/2167076.2167083", "10.1145/1661412.1618486", "10.1145/1576246.1531399", "10.1145/2487228.2487239", "10.1145/2461912.2462023", "10.1145/1833349.1778794", "10.1145/964967.801168", "10.1145/1572769.1572771", "10.1145/97879.97920", "10.1145/1944745.1944753", "10.1145/54852.378490", "10.1145/800031.808590", "10.1145/325165.325184", "10.1145/964967.801169", "10.1145/2159616.2159639", "10.1145/2018323.2018341", "10.1145/2492045.2492046", "10.1145/2010324.1964949", "10.1145/1179352.1141913", "10.1145/1833349.1778801", "10.1145/1399504.1360632", "10.1145/1964921.1964950", "10.1145/2185520.2185547", "10.1145/2167076.2167083", "10.1145/1661412.1618486", "10.1145/1576246.1531399", "10.1145/2487228.2487239", "10.1145/2461912.2462023", "10.1145/1833349.1778794", "10.1145/964967.801168", "10.1145/1572769.1572771", "10.1145/97879.97920", "10.1145/1944745.1944753", "10.1145/54852.378490", "10.1111/j.1467-8659.2010.01840.x", "10.1007/978-3-7091-6303-0_18", "10.1111/j.1467-8659.2012.03219.x", "10.1111/j.1467-8659.2006.00971.x", "10.1111/cgf.12415", "10.1111/j.1467-8659.2010.01731.x", "10.1080/10867651.2004.10504896", "10.1016/S0167-8396(03)00002-5", "10.1111/cgf.12158", "10.1111/j.1467-8659.2010.01840.x", "10.1007/978-3-7091-6303-0_18", "10.1111/j.1467-8659.2012.03219.x", "10.1111/j.1467-8659.2006.00971.x", "10.1111/cgf.12415", "10.1111/j.1467-8659.2010.01731.x", "10.1080/10867651.2004.10504896", "10.1016/S0167-8396(03)00002-5", "10.1111/cgf.12158", "10.1111/j.1467-8659.2010.01840.x", "10.1007/978-3-7091-6303-0_18", "10.1111/j.1467-8659.2012.03219.x", "10.1111/j.1467-8659.2006.00971.x", "10.1111/cgf.12415", "10.1111/j.1467-8659.2010.01731.x", "10.1080/10867651.2004.10504896", "10.1016/S0167-8396(03)00002-5", "10.1111/cgf.12158"]}, "10.1109/TVCG.2018.2794526": {"doi": "10.1109/TVCG.2018.2794526", "author": ["I. Mu\u00f1oz-Pandiella", "C. Bosch", "N. M\u00e9rillou", "G. Patow", "S. M\u00e9rillou", "X. Pueyo"], "title": "Urban Weathering: Interactive Rendering of Polluted Cities", "year": "2018", "abstract": "Weathering effects are ubiquitous phenomena in cities. Buildings age and deteriorate over time as they interact with the environment. Pollution accumulating on facades is a particularly visible consequence of this. Even though relevant work has been done to produce impressive images of virtual urban environments including weathering effects, so far, no technique using a global approach has been proposed to deal with weathering effects. Here, we propose a technique based on a fast physically-inspired approach, that focuses on modeling the changes in appearance due to pollution soiling on an urban scale. We consider pollution effects to depend on three main factors: wind, rain and sun exposure, and we take into account three intervening steps: deposition, reaction and washing. Using a low-cost pre-computation, we evaluate the pollution distribution throughout the city. Based on this and the use of screen-space operators, our method results in an efficient approach able to generate realistic images of urban scenes by combining the intervening factors at interactive rates. In addition, the pre-computation demands a reduced amount of memory to store the resulting pollution map and, as it is independent from scene complexity, it can suit large and complex models by adapting the map resolution.", "keywords": ["air pollution measurement", "data visualisation", "geophysical image processing", "realistic images", "rendering (computer graphics)", "urban weathering", "interactive rendering", "polluted cities", "weathering effects", "ubiquitous phenomena", "particularly visible consequence", "virtual urban environments", "pollution soiling", "urban scale", "pollution effects", "pollution distribution", "urban scenes", "interactive rates", "pollution map", "wind exposure", "rain exposure", "sun exposure", "screen-space operators", "realistic images", "Pollution", "Urban areas", "Computational modeling", "Buildings", "Surface contamination", "Two dimensional displays", "Surface treatment", "Weathering", "appearance", "interactive rendering", "urban scenes", "screen-space techniques", "shading and texture", "picture/image generation"], "referenced_by": [], "referencing": ["10.1145/1073204.1073321", "10.1145/1966394.1966399", "10.1145/237170.237280", "10.1145/1141911.1141951", "10.1145/2070781.2024181", "10.1145/1401032.1401061", "10.1145/1073204.1073321", "10.1145/1966394.1966399", "10.1145/237170.237280", "10.1145/1141911.1141951", "10.1145/2070781.2024181", "10.1145/1401032.1401061", "10.1145/1073204.1073321", "10.1145/1966394.1966399", "10.1145/237170.237280", "10.1145/1141911.1141951", "10.1145/2070781.2024181", "10.1145/1401032.1401061", "10.1111/cgf.12315", "10.1016/j.cag.2008.01.003", "10.1007/978-3-7091-6858-5_13", "10.1111/j.1467-8659.2008.01159.x", "10.1111/j.1467-8659.2011.01977.x", "10.1111/cgf.12850", "10.1023/B:JOSS.0000015179.12689.e4", "10.1080/02786829208959538", "10.1007/s13762-015-0898-7", "10.1111/cgf.13152", "10.1016/S1352-2310(97)00507-4", "10.1007/978-3-642-13544-6_16", "10.1111/cgf.12315", "10.1016/j.cag.2008.01.003", "10.1007/978-3-7091-6858-5_13", "10.1111/j.1467-8659.2008.01159.x", "10.1111/j.1467-8659.2011.01977.x", "10.1111/cgf.12850", "10.1023/B:JOSS.0000015179.12689.e4", "10.1080/02786829208959538", "10.1007/s13762-015-0898-7", "10.1111/cgf.13152", "10.1016/S1352-2310(97)00507-4", "10.1007/978-3-642-13544-6_16", "10.1111/cgf.12315", "10.1016/j.cag.2008.01.003", "10.1007/978-3-7091-6858-5_13", "10.1111/j.1467-8659.2008.01159.x", "10.1111/j.1467-8659.2011.01977.x", "10.1111/cgf.12850", "10.1023/B:JOSS.0000015179.12689.e4", "10.1080/02786829208959538", "10.1007/s13762-015-0898-7", "10.1111/cgf.13152", "10.1016/S1352-2310(97)00507-4", "10.1007/978-3-642-13544-6_16"]}, "10.1109/TVCG.2017.2776935": {"doi": "10.1109/TVCG.2017.2776935", "author": ["B. Ma", "A. Entezari"], "title": "Volumetric Feature-Based Classification and Visibility Analysis for Transfer Function Design", "year": "2018", "abstract": "Transfer function (TF) design is a central topic in direct volume rendering. The TF fundamentally translates data values into optical properties to reveal relevant features present in the volumetric data. We propose a semi-automatic TF design scheme which consists of two steps: First, we present a clustering process within 1D/2D TF domain based on the proximities of the respective volumetric features in the spatial domain. The presented approach provides an interactive tool that aids users in exploring clusters and identifying features of interest (FOI). Second, our method automatically generates a TF by iteratively refining the optical properties for the selected features using a novel feature visibility measurement. The proposed visibility measurement leverages the similarities of features to enhance their visibilities in DVR images. Compared to the conventional visibility measurement, the proposed feature visibility is able to efficiently sense opacity changes and precisely evaluate the impact of selected features on resulting visualizations. Our experiments validate the effectiveness of the proposed approach by demonstrating the advantages of integrating feature similarity into the visibility computations. We examine a number of datasets to establish the utility of our approach for semi-automatic TF design.", "keywords": ["data visualisation", "image classification", "image enhancement", "pattern clustering", "rendering (computer graphics)", "transfer functions", "volumetric feature-based classification", "visibility analysis", "transfer function design", "direct volume rendering", "data values", "optical properties", "relevant features", "volumetric data", "semiautomatic TF design scheme", "clustering process", "spatial domain", "novel feature visibility measurement", "visibility measurement leverages", "visibility computations", "feature similarity", "DVR images", "features of interest", "FOI", "Isosurfaces", "Histograms", "Rendering (computer graphics)", "Two dimensional displays", "Optical imaging", "Optical variables measurement", "Volume visualization", "transfer function (TF)", "direct volume rendering (DVR)", "classification", "feature visibility", "multidimensional TFs", "feature similarity", "isovalue clustering", "isosurfaces"], "referenced_by": [], "referencing": ["10.1145/37401.37422", "10.1145/258734.258887", "10.1145/37401.37422", "10.1145/258734.258887", "10.1145/37401.37422", "10.1145/258734.258887", "10.1111/cgf.12934", "10.1111/cgf.12128", "10.1111/j.1467-8659.2009.01689.x", "10.1007/978-3-319-14249-4_30", "10.1016/j.cag.2017.01.006", "10.1111/j.1467-8659.2012.03122.x", "10.1111/j.1467-8659.2011.02045.x", "10.1007/978-0-387-92920-0_32", "10.1111/cgf.12001", "10.1111/1467-8659.00236", "10.1111/cgf.12934", "10.1111/cgf.12128", "10.1111/j.1467-8659.2009.01689.x", "10.1007/978-3-319-14249-4_30", "10.1016/j.cag.2017.01.006", "10.1111/j.1467-8659.2012.03122.x", "10.1111/j.1467-8659.2011.02045.x", "10.1007/978-0-387-92920-0_32", "10.1111/cgf.12001", "10.1111/1467-8659.00236", "10.1111/cgf.12934", "10.1111/cgf.12128", "10.1111/j.1467-8659.2009.01689.x", "10.1007/978-3-319-14249-4_30", "10.1016/j.cag.2017.01.006", "10.1111/j.1467-8659.2012.03122.x", "10.1111/j.1467-8659.2011.02045.x", "10.1007/978-0-387-92920-0_32", "10.1111/cgf.12001", "10.1111/1467-8659.00236"]}, "10.1109/TVCG.2017.2779501": {"doi": "10.1109/TVCG.2017.2779501", "author": ["M. Rautenhaus", "M. B\u00f6ttinger", "S. Siemen", "R. Hoffman", "R. M. Kirby", "M. Mirzargar", "N. R\u00f6ber", "R. Westermann"], "title": "Visualization in Meteorology\u2014A Survey of Techniques and Tools for Data Analysis Tasks", "year": "2018", "abstract": "This article surveys the history and current state of the art of visualization in meteorology, focusing on visualization techniques and tools used for meteorological data analysis. We examine characteristics of meteorological data and analysis tasks, describe the development of computer graphics methods for visualization in meteorology from the 1960s to today, and visit the state of the art of visualization techniques and tools in operational weather forecasting and atmospheric research. We approach the topic from both the visualization and the meteorological side, showing visualization techniques commonly used in meteorological practice, and surveying recent studies in visualization research aimed at meteorological applications. Our overview covers visualization techniques from the fields of display design, 3D visualization, flow dynamics, feature-based visualization, comparative visualization and data fusion, uncertainty and ensemble visualization, interactive visual analysis, efficient rendering, and scalability and reproducibility. We discuss demands and challenges for visualization research targeting meteorological data analysis, highlighting aspects in demonstration of benefit, interactive visual analysis, seamless visualization, ensemble visualization, 3D visualization, and technical issues.", "keywords": ["data analysis", "data visualisation", "geophysics computing", "interactive systems", "meteorology", "rendering (computer graphics)", "sensor fusion", "weather forecasting", "data analysis tasks", "visualization techniques", "meteorological data analysis", "meteorological practice", "visualization research", "meteorological applications", "feature-based visualization", "comparative visualization", "interactive visual analysis", "meteorology visualization", "data fusion", "3D visualization", "flow dynamics", "operational weather forecasting", "atmospheric research", "Data visualization", "Atmospheric modeling", "Weather forecasting", "Numerical models", "Predictive models", "Forecasting", "Visualization", "meteorology", "atmospheric science", "weather forecasting", "climatology", "spatiotemporal data", "survey"], "referenced_by": [], "referencing": []}}