{"10.1109/TVCG.2016.2603558": {"doi": "10.1109/TVCG.2016.2603558", "author": ["L. De Floriani", "D. Schmalstieg"], "title": "Message from the Editor-in-Chief and from the Associate Editor-in-Chief", "year": "2016", "abstract": "Presents the introductory editorial for this issue of the publication.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2016.2593757": {"doi": "10.1109/TVCG.2016.2593757", "author": ["A. Bapat", "E. Dunn", "J. Frahm"], "title": "Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster of Rolling Shutter Cameras", "year": "2016", "abstract": "To maintain a reliable registration of the virtual world with the real world, augmented reality (AR) applications require highly accurate, low-latency tracking of the device. In this paper, we propose a novel method for performing this fast 6-DOF head pose tracking using a cluster of rolling shutter cameras. The key idea is that a rolling shutter camera works by capturing the rows of an image in rapid succession, essentially acting as a high-frequency 1D image sensor. By integrating multiple rolling shutter cameras on the AR device, our tracker is able to perform 6-DOF markerless tracking in a static indoor environment with minimal latency. Compared to state-of-the-art tracking systems, this tracking approach performs at significantly higher frequency, and it works in generalized environments. To demonstrate the feasibility of our system, we present thorough evaluations on synthetically generated data with tracking frequencies reaching 56.7 kHz. We further validate the method's accuracy on real-world images collected from a prototype of our tracking system against ground truth data using standard commodity GoPro cameras capturing at 120 Hz frame rate.", "keywords": ["augmented reality", "image registration", "image sensors", "object tracking", "pose estimation", "kilo-hertz 6-DoF visual tracking", "egocentric cluster", "multiple rolling shutter camera integration", "virtual world", "augmented reality applications", "low-latency device tracking", "6-DOF head pose tracking", "high-frequency 1D image sensor", "6-DOF markerless tracking", "standard commodity GoPro cameras", "frame rate", "frequency 56.7 kHz", "frequency 120 Hz", "Tracking", "Cameras", "Visualization", "Performance evaluation", "Simultaneous localization and mapping", "Light emitting diodes", "High frequency", "Visual inside-out tracking", "Rolling shutter"], "referenced_by": ["10.1109/TVCG.2018.2868527", "10.1109/TVCG.2018.2868591", "10.1109/CVPR.2018.00507", "10.1109/ICCE.2019.8662054", "10.1109/TVCG.2019.2899233", "10.1007/978-3-319-92198-3_25", "10.1117/1.OE.57.4.043104", "10.1111/cgf.13654", "10.1007/978-3-662-52956-0_2", "10.3233/JIFS-191674"], "referencing": ["10.1109/IROS.2015.7353366", "10.1109/ROBOT.2008.4543194", "10.1109/CVPR.1991.139759", "10.1109/70.326576", "10.1109/TPAMI.2008.275", "10.1109/ISMAR.2014.6948420", "10.1109/TVCG.2014.27", "10.1109/34.888718", "10.1109/IROS.2015.7353366", "10.1109/ROBOT.2008.4543194", "10.1109/CVPR.1991.139759", "10.1109/70.326576", "10.1109/TPAMI.2008.275", "10.1109/ISMAR.2014.6948420", "10.1109/TVCG.2014.27", "10.1109/34.888718", "10.1109/IROS.2015.7353366", "10.1109/ROBOT.2008.4543194", "10.1109/CVPR.1991.139759", "10.1109/70.326576", "10.1109/TPAMI.2008.275", "10.1109/ISMAR.2014.6948420", "10.1109/TVCG.2014.27", "10.1109/34.888718", "10.1162/pres.1997.6.4.355", "10.1007/978-3-642-37484-5_9", "10.1016/j.imavis.2004.02.007", "10.1007/s11263-011-0465-8", "10.1023/A:1014573219977", "10.1162/105474601750182289", "10.1162/pres.1997.6.4.355", "10.1007/978-3-642-37484-5_9", "10.1016/j.imavis.2004.02.007", "10.1007/s11263-011-0465-8", "10.1023/A:1014573219977", "10.1162/105474601750182289", "10.1162/pres.1997.6.4.355", "10.1007/978-3-642-37484-5_9", "10.1016/j.imavis.2004.02.007", "10.1007/s11263-011-0465-8", "10.1023/A:1014573219977", "10.1162/105474601750182289"]}, "10.1109/TVCG.2016.2593779": {"doi": "10.1109/TVCG.2016.2593779", "author": ["Y. Itoh", "T. Amano", "D. Iwai", "G. Klinker"], "title": "Gaussian Light Field: Estimation of Viewpoint-Dependent Blur for Optical See-Through Head-Mounted Displays", "year": "2016", "abstract": "We propose a method to calibrate viewpoint-dependent, channel-wise image blur of near-eye displays, especially of Optical See-Through Head-Mounted Displays (OST-HMDs). Imperfections in HMD optics cause channel-wise image shift and blur that degrade the image quality of the display at a user's viewpoint. If we can estimate such characteristics perfectly, we could mitigate the effect by applying correction techniques from the computational photography in computer vision as analogous to cameras. Unfortunately, directly applying existing calibration techniques of cameras to OST-HMDs is not a straightforward task. Unlike ordinary imaging systems, image blur in OST-HMDs is viewpoint-dependent, i.e., the optical characteristic of a display dynamically changes depending on the current viewpoint of the user. This constraint makes the problem challenging since we must measure image blur of an HMD, ideally, over the entire 3D eyebox in which a user can see an image. To overcome this problem, we model the viewpoint-dependent blur as a Gaussian Light Field (GLF) that stores spatial information of the display screen as a (4D) light field with depth information and the blur as point-spread functions in the form of Gaussian kernels, respectively. We first describe both our GLF model and a calibration procedure to learn a GLF for a given OST-HMD. We then apply our calibration method to two HMDs that use different optics: a cubic prism or holographic gratings. The results show that our method achieves significantly better accuracy in Point-Spread Function (PSF) estimations with an accuracy about 2 to 7 dB in Peak SNR.", "keywords": ["calibration", "computer vision", "Gaussian processes", "helmet mounted displays", "holographic displays", "holographic gratings", "image restoration", "optical prisms", "optical transfer function", "screens (display)", "Gaussian light field", "viewpoint-dependent blur estimation", "optical see-through head-mounted displays", "viewpoint dependent channel-wise image blur calibration", "near-eye displays", "OST-HMD", "HMD optics", "channel-wise image shift", "image quality degradation", "correction techniques", "computational photography", "computer vision", "optical characteristics", "3D eyebox", "spatial information", "display screen", "4D light field", "depth information", "point-spread functions", "GLF model", "cubic prism", "holographic gratings", "PSF estimations", "Optical imaging", "Adaptive optics", "Optical distortion", "Calibration", "Cameras", "Holography", "OST-HMD", "calibration", "optical see-through", "chromatic aberration", "point-spread functions", "light field"], "referenced_by": ["10.1109/JSTSP.2017.2747126", "10.1109/TVCG.2018.2793659", "10.1109/TVCG.2017.2754257", "10.1109/TVCG.2018.2868591", "10.1109/ISMAR-Adjunct.2019.00050", "10.1109/TVCG.2019.2894627", "10.1145/3414685.3417820", "10.1007/s11548-017-1564-y", "10.1111/cgf.13654", "10.1364/OE.391447"], "referencing": ["10.1109/CVPR.2006.145", "10.1109/TIP.2015.2478388", "10.1109/TVCG.2015.2459892", "10.1109/ISMAR.2015.14", "10.1109/TVCG.2015.2391859", "10.1109/VRAIS.1993.380772", "10.1109/ICME.2015.7177483", "10.1109/TVCG.2015.2391857", "10.1109/TIP.2006.888330", "10.1109/70.34770", "10.1109/TIP.2003.819861", "10.1109/ISMAR.2014.6948427", "10.1109/CVPR.2006.145", "10.1109/TIP.2015.2478388", "10.1109/TVCG.2015.2459892", "10.1109/ISMAR.2015.14", "10.1109/TVCG.2015.2391859", "10.1109/VRAIS.1993.380772", "10.1109/ICME.2015.7177483", "10.1109/TVCG.2015.2391857", "10.1109/TIP.2006.888330", "10.1109/70.34770", "10.1109/TIP.2003.819861", "10.1109/ISMAR.2014.6948427", "10.1109/CVPR.2006.145", "10.1109/TIP.2015.2478388", "10.1109/TVCG.2015.2459892", "10.1109/ISMAR.2015.14", "10.1109/TVCG.2015.2391859", "10.1109/VRAIS.1993.380772", "10.1109/ICME.2015.7177483", "10.1109/TVCG.2015.2391857", "10.1109/TIP.2006.888330", "10.1109/70.34770", "10.1109/TIP.2003.819861", "10.1109/ISMAR.2014.6948427", "10.1145/192161.192199", "10.1145/2508363.2508416", "10.1145/1805964.1805966", "10.1145/2516971.2516974", "10.1145/2601097.2601122", "10.1145/2735711.2735787", "10.1145/1882262.1866164", "10.1145/2185520.2185577", "10.1145/2503713.2503752", "10.1145/1141911.1141974", "10.1145/192161.192199", "10.1145/2508363.2508416", "10.1145/1805964.1805966", "10.1145/2516971.2516974", "10.1145/2601097.2601122", "10.1145/2735711.2735787", "10.1145/1882262.1866164", "10.1145/2185520.2185577", "10.1145/2503713.2503752", "10.1145/1141911.1141974", "10.1145/192161.192199", "10.1145/2508363.2508416", "10.1145/1805964.1805966", "10.1145/2516971.2516974", "10.1145/2601097.2601122", "10.1145/2735711.2735787", "10.1145/1882262.1866164", "10.1145/2185520.2185577", "10.1145/2503713.2503752", "10.1145/1141911.1141974", "10.1364/OE.22.020705", "10.1364/AO.41.005282", "10.1364/OL.39.000127", "10.1117/12.237443", "10.1364/AO.52.000C88", "10.1364/OE.22.020705", "10.1364/AO.41.005282", "10.1364/OL.39.000127", "10.1117/12.237443", "10.1364/AO.52.000C88", "10.1364/OE.22.020705", "10.1364/AO.41.005282", "10.1364/OL.39.000127", "10.1117/12.237443", "10.1364/AO.52.000C88"]}, "10.1109/TVCG.2016.2593766": {"doi": "10.1109/TVCG.2016.2593766", "author": ["S. Willi", "A. Grundh\u00f6fer"], "title": "Spatio-Temporal Point Path Analysis and Optimization of a Galvanoscopic Scanning Laser Projector", "year": "2016", "abstract": "Galvanoscopic scanning laser projectors are powerful vector graphic devices offering a tremendous local brightness advantage compared to standard video projection systems. However, such devices have inherent problems, such as temporal flicker and spatially inaccurate rendering. We propose a method to generate an accurate point-based projection with such devices. To overcome the mentioned problems, we present a camera-based method to automatically analyze the laser projector's motion behavior. With this information, a model database is generated that is used to optimize the scanning path of projected point sequences. The optimization considers the overall path length, its angular shape, acceleration behavior, and the spatio-temporal point neighborhood. The method minimizes perceived visual flickering while guaranteeing an accurate spatial point projection at the same time. Comparisons and timing measurements prove the effectiveness of our method. An informal user evaluation shows substantial visual quality improvement as well.", "keywords": ["brightness", "computer graphic equipment", "image motion analysis", "laser beam applications", "minimisation", "optical projectors", "optical scanners", "spatiotemporal phenomena", "spatiotemporal point path analysis", "spatiotemporal point path optimization", "galvanoscopic scanning laser projector", "vector graphic devices", "local brightness", "point-based projection", "camera-based method", "automatic motion behavior analysis", "model database", "scanning path optimization", "point sequences", "overall path length", "angular shape", "acceleration behavior", "spatiotemporal point neighborhood", "perceived visual flickering minimization", "timing measurements", "informal user evaluation", "visual quality improvement", "Laser modes", "Calibration", "Cameras", "Mirrors", "Optimization", "Analytical models", "Projector-camera systems", "Calibration and registration of sensing systems", "Display hardware", "including 3D", "stereoscopic and multi-user Entertainment", "broadcast"], "referenced_by": ["10.1109/TVCG.2017.2734598", "10.1109/TVCG.2018.2868591", "10.1109/ISMAR50242.2020.00069", "10.1109/ISMAR-Adjunct51615.2020.00058", "10.1145/2980179.2980243"], "referencing": ["10.1109/CVPR.2011.5995416", "10.1109/TPAMI.2013.137", "10.1109/CVPR.2011.5995416", "10.1109/TPAMI.2013.137", "10.1109/CVPR.2011.5995416", "10.1109/TPAMI.2013.137", "10.1145/2782782.2792487", "10.1145/2782782.2792487", "10.1145/2782782.2792487", "10.1201/b10624", "10.1080/2151237X.2007.10129236", "10.1007/3-540-47977-5_2", "10.4086/toc.2012.v008a019", "10.1016/j.displa.2009.03.003", "10.1002/j.1538-7305.1965.tb04146.x", "10.1287/opre.21.2.498", "10.1007/BF01469346", "10.1016/0304-3975(77)90012-3", "10.3756/artsci.7.155", "10.1016/j.jcrs.2006.10.042", "10.1201/b10624", "10.1080/2151237X.2007.10129236", "10.1007/3-540-47977-5_2", "10.4086/toc.2012.v008a019", "10.1016/j.displa.2009.03.003", "10.1002/j.1538-7305.1965.tb04146.x", "10.1287/opre.21.2.498", "10.1007/BF01469346", "10.1016/0304-3975(77)90012-3", "10.3756/artsci.7.155", "10.1016/j.jcrs.2006.10.042", "10.1201/b10624", "10.1080/2151237X.2007.10129236", "10.1007/3-540-47977-5_2", "10.4086/toc.2012.v008a019", "10.1016/j.displa.2009.03.003", "10.1002/j.1538-7305.1965.tb04146.x", "10.1287/opre.21.2.498", "10.1007/BF01469346", "10.1016/0304-3975(77)90012-3", "10.3756/artsci.7.155", "10.1016/j.jcrs.2006.10.042"]}, "10.1109/TVCG.2016.2593781": {"doi": "10.1109/TVCG.2016.2593781", "author": ["T. Langlotz", "M. Cook", "H. Regenbrecht"], "title": "Real-Time Radiometric Compensation for Optical See-Through Head-Mounted Displays", "year": "2016", "abstract": "Optical see-through head-mounted displays are currently seeing a transition out of research labs towards the consumer-oriented market. However, whilst availability has improved and prices have decreased, the technology has not matured much. Most commercially available optical see-through head mounted displays follow a similar principle and use an optical combiner blending the physical environment with digital information. This approach yields problems as the colors for the overlaid digital information can not be correctly reproduced. The perceived pixel colors are always a result of the displayed pixel color and the color of the current physical environment seen through the head-mounted display. In this paper we present an initial approach for mitigating the effect of color-blending in optical see-through head-mounted displays by introducing a real-time radiometric compensation. Our approach is based on a novel prototype for an optical see-through head-mounted display that allows the capture of the current environment as seen by the user's eye. We present three different algorithms using this prototype to compensate color blending in real-time and with pixel-accuracy. We demonstrate the benefits and performance as well as the results of a user study. We see application for all common Augmented Reality scenarios but also for other areas such as Diminished Reality or supporting color-blind people.", "keywords": ["augmented reality", "helmet mounted displays", "image colour analysis", "radiometry", "optical see-through head-mounted displays", "consumer-oriented market", "optical combiner blending", "physical environment", "digital information", "color problem", "overlaid digital information", "perceived pixel colors", "color-blending", "real-time radiometric compensation", "pixel-accuracy", "augmented reality scenario", "diminished reality", "color-blind people", "Image color analysis", "Adaptive optics", "Optical imaging", "Prototypes", "Radiometry", "Cameras", "Real-time systems", "Radiosity", "global illumination", "constant time"], "referenced_by": ["10.1109/ISMAR-Adjunct.2017.40", "10.1109/TMM.2017.2733461", "10.1109/TVCG.2017.2734427", "10.1109/TVCG.2017.2754257", "10.1109/VR.2018.8446441", "10.1109/TVCG.2019.2899229", "10.1109/TVCG.2019.2933120", "10.1109/TVCG.2020.2973443", "10.1109/TVCG.2020.3023637", "10.1109/TVCG.2020.3023569", "10.1109/ISMAR50242.2020.00069", "10.1162/PRES_a_00284", "10.1007/978-981-13-6252-1_4", "10.3390/app10072381", "10.1364/OE.391447", "10.1364/BOE.405026"], "referencing": ["10.1109/MMUL.2005.9", "10.1109/MC.2005.17", "10.1109/ISMAR.2014.6948410", "10.1109/VR.2013.6549410", "10.1109/TVCG.2007.1052", "10.1109/TVCG.2015.2459892", "10.1109/ISAR.2000.880924", "10.1109/TVCG.2015.2391857", "10.1109/MMUL.2005.9", "10.1109/MC.2005.17", "10.1109/ISMAR.2014.6948410", "10.1109/VR.2013.6549410", "10.1109/TVCG.2007.1052", "10.1109/TVCG.2015.2459892", "10.1109/ISAR.2000.880924", "10.1109/TVCG.2015.2391857", "10.1109/MMUL.2005.9", "10.1109/MC.2005.17", "10.1109/ISMAR.2014.6948410", "10.1109/VR.2013.6549410", "10.1109/TVCG.2007.1052", "10.1109/TVCG.2015.2459892", "10.1109/ISAR.2000.880924", "10.1109/TVCG.2015.2391857", "10.1145/311535.311543", "10.1145/2503713.2503716", "10.1145/1476589.1476686", "10.1145/311535.311543", "10.1145/2503713.2503716", "10.1145/1476589.1476686", "10.1145/311535.311543", "10.1145/2503713.2503716", "10.1145/1476589.1476686", "10.1007/s007790200007", "10.1007/s007790200007", "10.1007/s007790200007"]}, "10.1109/TVCG.2016.2593768": {"doi": "10.1109/TVCG.2016.2593768", "author": ["F. Rameau", "H. Ha", "K. Joo", "J. Choi", "K. Park", "I. S. Kweon"], "title": "A Real-Time Augmented Reality System to See-Through Cars", "year": "2016", "abstract": "One of the most hazardous driving scenario is the overtaking of a slower vehicle, indeed, in this case the front vehicle (being overtaken) can occlude an important part of the field of view of the rear vehicle's driver. This lack of visibility is the most probable cause of accidents in this context. Recent research works tend to prove that augmented reality applied to assisted driving can significantly reduce the risk of accidents. In this paper, we present a real-time marker-less system to see through cars. For this purpose, two cars are equipped with cameras and an appropriate wireless communication system. The stereo vision system mounted on the front car allows to create a sparse 3D map of the environment where the rear car can be localized. Using this inter-car pose estimation, a synthetic image is generated to overcome the occlusion and to create a seamless see-through effect which preserves the structure of the scene.", "keywords": ["augmented reality", "driver information systems", "pose estimation", "stereo image processing", "real-time augmented reality system", "see-through cars", "assisted driving", "real-time marker-less system", "wireless communication system", "sparse 3D map", "inter-car pose estimation", "synthetic image", "stereo vision system", "Automobiles", "Three-dimensional displays", "Cameras", "Accidents", "Delays", "Real-time systems", "Augmented reality", "collaborative vehicle", "see-through"], "referenced_by": ["10.1109/ICOIN.2018.8343121", "10.1109/ICPADS.2017.00026", "10.1109/ACCESS.2019.2901280", "10.1109/IIAI-AAI.2018.00174", "10.1109/ISMAR-Adjunct.2018.00020", "10.1109/ISMAR-Adjunct.2018.00103", "10.1109/APMAR.2019.8709269", "10.1109/ICVRV.2017.00110", "10.1109/AIVR46125.2019.00039", "10.1109/CVPR42600.2020.00469", "10.1109/TVCG.2020.3003768", "10.1109/ISMAR50242.2020.00069", "10.1186/s41074-017-0028-1", "10.4236/ce.2017.89101", "10.1007/978-3-319-08234-9_274-1", "10.1080/1059924X.2018.1470051", "10.1080/17538947.2018.1564379", "10.1007/978-981-15-0184-5_71", "10.1088/1757-899X/767/1/012062", "10.1016/j.comcom.2020.05.007", "10.3390/jsan9020024", "10.1007/978-3-030-50523-3_27", "10.1007/s00779-020-01421-3", "10.3390/su12166469", "10.7746/jkros.2020.15.3.293", "10.1007/s11554-020-01045-z"], "referencing": ["10.1109/ICCV.2015.356", "10.1109/ICCV.2015.356", "10.1109/ICCV.2015.356", "10.1023/B:VISI.0000011205.11775.fd", "10.1016/S0001-4575(97)00105-X", "10.1007/978-3-642-19315-6_3", "10.1007/978-1-4419-9563-6", "10.1117/12.585481", "10.1007/978-3-319-10602-1_49", "10.1023/B:VISI.0000011205.11775.fd", "10.1016/S0001-4575(97)00105-X", "10.1007/978-3-642-19315-6_3", "10.1007/978-1-4419-9563-6", "10.1117/12.585481", "10.1007/978-3-319-10602-1_49", "10.1023/B:VISI.0000011205.11775.fd", "10.1016/S0001-4575(97)00105-X", "10.1007/978-3-642-19315-6_3", "10.1007/978-1-4419-9563-6", "10.1117/12.585481", "10.1007/978-3-319-10602-1_49"]}, "10.1109/TVCG.2016.2593780": {"doi": "10.1109/TVCG.2016.2593780", "author": ["Y. Kim", "H. Park", "S. Bang", "S. Lee"], "title": "Retargeting Human-Object Interaction to Virtual Avatars", "year": "2016", "abstract": "In augmented reality (AR) applications, a virtual avatar serves as a useful medium to represent a human in a different place. This paper deals with the problem of retargeting a human motion to an avatar. In particular, we present a novel method that retargets a human motion with respect to an object to that of an avatar with respect to a different object of a similar shape. To achieve this, we developed a spatial map that defines the correspondences between any points in the 3D spaces around the respective objects. The key advantage of the spatial map is that it identifies the desired locations of the avatar's body parts for any input motion of a human. Once the spatial map is created offline, the motion retargeting can be performed in real-time. The retargeted motion preserves important features of the original motion such as the human pose and the spatial relation with the object. We report the results of a number of experiments that demonstrate the effectiveness of the proposed method.", "keywords": ["augmented reality", "avatars", "human-object interaction retargeting", "virtual avatars", "augmented reality", "AR applications", "human motion retargeting", "Avatars", "Shape", "Three-dimensional displays", "Trajectory", "Mesh generation", "Real-time systems", "Geometry", "Motion retargeting", "human-object interaction", "augmented reality", "telepresence", "avatar animation"], "referenced_by": ["10.1109/TVCG.2018.2868591", "10.1109/ANIVAE.2018.8587272", "10.1109/VR46266.2020.00017", "10.1007/s11390-017-1742-y", "10.1111/cgf.13363", "10.1007/978-981-13-9917-6_61", "10.1111/cgf.12999"], "referencing": ["10.1109/TVCG.2013.33", "10.1109/TVCG.2007.1054", "10.1109/ISMAR.2011.6092379", "10.1109/VR.2013.6549352", "10.1109/HUMANOIDS.2013.7029978", "10.1109/TVCG.2013.33", "10.1109/TVCG.2007.1054", "10.1109/ISMAR.2011.6092379", "10.1109/VR.2013.6549352", "10.1109/HUMANOIDS.2013.7029978", "10.1109/TVCG.2013.33", "10.1109/TVCG.2007.1054", "10.1109/ISMAR.2011.6092379", "10.1109/VR.2013.6549352", "10.1109/HUMANOIDS.2013.7029978", "10.1145/2461912.2461931", "10.1145/2485895.2485905", "10.1145/344779.344859", "10.1145/1833349.1778775", "10.1145/280814.280820", "10.1145/1833349.1778770", "10.1145/1186822.1073323", "10.1145/2461912.2461916", "10.1145/571878.571895", "10.1145/1576246.1531385", "10.1145/2185520.2335459", "10.1145/37401.37422", "10.1145/2915926.2915947", "10.1145/2629697", "10.1145/1037957.1037963", "10.1145/2503177", "10.1145/2461912.2461931", "10.1145/2485895.2485905", "10.1145/344779.344859", "10.1145/1833349.1778775", "10.1145/280814.280820", "10.1145/1833349.1778770", "10.1145/1186822.1073323", "10.1145/2461912.2461916", "10.1145/571878.571895", "10.1145/1576246.1531385", "10.1145/2185520.2335459", "10.1145/37401.37422", "10.1145/2915926.2915947", "10.1145/2629697", "10.1145/1037957.1037963", "10.1145/2503177", "10.1145/2461912.2461931", "10.1145/2485895.2485905", "10.1145/344779.344859", "10.1145/1833349.1778775", "10.1145/280814.280820", "10.1145/1833349.1778770", "10.1145/1186822.1073323", "10.1145/2461912.2461916", "10.1145/571878.571895", "10.1145/1576246.1531385", "10.1145/2185520.2335459", "10.1145/37401.37422", "10.1145/2915926.2915947", "10.1145/2629697", "10.1145/1037957.1037963", "10.1145/2503177", "10.1007/3-540-26808-1_9", "10.1002/cav.1645", "10.1111/j.1467-8659.2008.01282.x", "10.1111/j.1467-8659.2008.01290.x", "10.1080/10586458.1993.10504266", "10.1111/cgf.12179", "10.1561/0600000011", "10.1007/3-540-26808-1_9", "10.1002/cav.1645", "10.1111/j.1467-8659.2008.01282.x", "10.1111/j.1467-8659.2008.01290.x", "10.1080/10586458.1993.10504266", "10.1111/cgf.12179", "10.1561/0600000011", "10.1007/3-540-26808-1_9", "10.1002/cav.1645", "10.1111/j.1467-8659.2008.01282.x", "10.1111/j.1467-8659.2008.01290.x", "10.1080/10586458.1993.10504266", "10.1111/cgf.12179", "10.1561/0600000011"]}, "10.1109/TVCG.2016.2593778": {"doi": "10.1109/TVCG.2016.2593778", "author": ["K. Gupta", "G. A. Lee", "M. Billinghurst"], "title": "Do You See What I See? The Effect of Gaze Tracking on Task Space Remote Collaboration", "year": "2016", "abstract": "We present results from research exploring the effect of sharing virtual gaze and pointing cues in a wearable interface for remote collaboration. A local worker wears a Head-mounted Camera, Eye-tracking camera and a Head-Mounted Display and shares video and virtual gaze information with a remote helper. The remote helper can provide feedback using a virtual pointer on the live video view. The prototype system was evaluated with a formal user study. Comparing four conditions, (1) NONE (no cue), (2) POINTER, (3) EYE-TRACKER and (4) BOTH (both pointer and eye-tracker cues), we observed that the task completion performance was best in the BOTH condition with a significant difference of POINTER and EYETRACKER individually. The use of eye-tracking and a pointer also significantly improved the co-presence felt between the users. We discuss the implications of this research and the limitations of the developed system that could be improved in further work.", "keywords": ["cameras", "gaze tracking", "helmet mounted displays", "gaze tracking", "task space remote collaboration", "wearable interface", "remote collaboration", "head-mounted camera", "eye-tracking camera", "head-mounted display", "virtual gaze information", "virtual pointer", "Collaboration", "Cameras", "Teleconferencing", "Prototypes", "Head", "Gaze tracking", "Computers", "Computer-supported collaborative work", "Computer conferencing", "teleconferencing", "videoconferencing"], "referenced_by": ["10.1109/ISMAR-Adjunct.2017.36", "10.1109/ISUVR.2017.13", "10.1109/ISUVR.2017.20", "10.1109/IECON.2017.8217457", "10.1109/TVCG.2018.2868591", "10.1109/ISMAR.2018.00051", "10.1109/ISMAR-Adjunct.2018.00071", "10.1109/ISMAR-Adjunct.2018.00038", "10.1109/VR.2019.8798024", "10.1109/IV-2.2019.00026", "10.1109/ISMAR.2019.00021", "10.1109/ISMAR-Adjunct.2019.00019", "10.1109/NEWCAS44328.2019.8961304", "10.1109/TVCG.2020.2973054", "10.1109/ISMAR50242.2020.00069", "10.1109/ISMAR50242.2020.00078", "10.1109/ACCESS.2020.3043783", "10.1145/3361127", "10.1007/s12193-017-0250-2", "10.1093/iwc/iwx019", "10.4018/978-1-5225-3290-3.ch001", "10.4018/978-1-5225-5469-1.ch033", "10.1007/978-3-319-95270-3_25", "10.1007/978-3-030-01388-2_8", "10.1016/j.jvcir.2018.12.010", "10.1007/s00170-018-03237-1", "10.1093/iwc/iwy026", "10.3389/frobt.2019.00005", "10.1016/j.ijhcs.2019.05.011", "10.1007/s00366-019-00792-3", "10.1007/978-3-030-29384-0_16", "10.1371/journal.pone.0189078", "10.1007/s00170-019-04434-2", "10.1007/s12193-020-00331-1", "10.1007/s12193-020-00330-2", "10.1007/s12193-020-00335-x", "10.1093/iwcomp/iwaa012", "10.1007/s11042-020-09731-7", "10.1007/s12193-020-00346-8", "10.1515/icom-2020-0012", "10.3390/informatics7040055"], "referencing": ["10.1109/IWAR.1999.803809", "10.1109/TMM.2008.2001363", "10.1109/ISWC.2003.1241393", "10.1109/IWAR.1999.803809", "10.1109/TMM.2008.2001363", "10.1109/ISWC.2003.1241393", "10.1109/IWAR.1999.803809", "10.1109/TMM.2008.2001363", "10.1109/ISWC.2003.1241393", "10.1145/514236.514265", "10.1145/358916.358947", "10.1145/642611.642701", "10.1145/765891.765980", "10.1145/2671015.2671016", "10.1145/169059.169268", "10.1145/179606.179687", "10.1145/2638728.2641695", "10.1145/192844.192866", "10.1145/142750.142980", "10.1145/2818052.2869097", "10.1145/2851581.2892370", "10.1145/1054972.1055005", "10.1145/223355.223690", "10.1145/1027933.1027936", "10.1145/1460563.1460593", "10.1145/1228175.1228262", "10.1145/1978942.1978963", "10.1145/1240624.1240668", "10.1145/514236.514265", "10.1145/358916.358947", "10.1145/642611.642701", "10.1145/765891.765980", "10.1145/2671015.2671016", "10.1145/169059.169268", "10.1145/179606.179687", "10.1145/2638728.2641695", "10.1145/192844.192866", "10.1145/142750.142980", "10.1145/2818052.2869097", "10.1145/2851581.2892370", "10.1145/1054972.1055005", "10.1145/223355.223690", "10.1145/1027933.1027936", "10.1145/1460563.1460593", "10.1145/1228175.1228262", "10.1145/1978942.1978963", "10.1145/1240624.1240668", "10.1145/514236.514265", "10.1145/358916.358947", "10.1145/642611.642701", "10.1145/765891.765980", "10.1145/2671015.2671016", "10.1145/169059.169268", "10.1145/179606.179687", "10.1145/2638728.2641695", "10.1145/192844.192866", "10.1145/142750.142980", "10.1145/2818052.2869097", "10.1145/2851581.2892370", "10.1145/1054972.1055005", "10.1145/223355.223690", "10.1145/1027933.1027936", "10.1145/1460563.1460593", "10.1145/1228175.1228262", "10.1145/1978942.1978963", "10.1145/1240624.1240668", "10.1177/1932296814535561", "10.1016/j.cognition.2007.05.012", "10.1080/17470218.2012.737813", "10.3758/PBR.17.5.718", "10.1177/1932296814535561", "10.1016/j.cognition.2007.05.012", "10.1080/17470218.2012.737813", "10.3758/PBR.17.5.718", "10.1177/1932296814535561", "10.1016/j.cognition.2007.05.012", "10.1080/17470218.2012.737813", "10.3758/PBR.17.5.718"]}, "10.1109/TVCG.2015.2507578": {"doi": "10.1109/TVCG.2015.2507578", "author": ["G. Nader", "K. Wang", "F. H\u00e9troy-Wheeler", "F. Dupont"], "title": "Just Noticeable Distortion Profile for Flat-Shaded 3D Mesh Surfaces", "year": "2016", "abstract": "It is common that a 3D mesh undergoes some lossy operations (e.g., compression, watermarking and transmission through noisy channels), which can introduce geometric distortions as a change in vertex position. In most cases the end users of 3D meshes are human beings; therefore, it is important to evaluate the visibility of introduced vertex displacement. In this paper we present a model for computing a Just Noticeable Distortion (JND) profile for flat-shaded 3D meshes. The proposed model is based on an experimental study of the properties of the human visual system while observing a flat-shaded 3D mesh surface, in particular the contrast sensitivity function and contrast masking. We first define appropriate local perceptual properties on 3D meshes. We then detail the results of a series of psychophysical experiments where we have measured the threshold needed for a human observer to detect the change in vertex position. These results allow us to compute the JND profile for flat-shaded 3D meshes. The proposed JND model has been evaluated via a subjective experiment, and applied to guide 3D mesh simplification as well as to determine the optimal vertex coordinates quantization level for a 3D model.", "keywords": ["computational geometry", "geometry", "mesh generation", "observers", "just noticeable distortion profile", "flat-shaded 3D mesh surfaces", "compression", "watermarking", "transmission", "noisy channels", "lossy operations", "geometric distortions", "vertex position", "vertex displacement", "JND profile", "human visual system", "contrast sensitivity function", "contrast masking", "psychophysical experiments", "optimal vertex coordinate quantization level", "3D model", "flat-shaded 3D meshes", "optimal vertex coordinates", "Three-dimensional displays", "Computational modeling", "Visual systems", "Distortion", "Sensitivity", "Visualization", "Solid modeling", "Just noticeable distortion", "human visual system", "psychophysical experiments", "contrast sensitivity function", "contrast masking", "3D mesh"], "referenced_by": ["10.1109/ICALIP.2018.8455210", "10.1109/ACCESS.2019.2939569", "10.1109/LSP.2019.2963793", "10.1145/2996296", "10.1145/3129505", "10.1111/cgf.13046", "10.1111/cgf.13150", "10.1186/s13640-016-0157-y", "10.1007/978-3-030-22335-9_2", "10.1177/1473871619862791", "10.1002/9781119681069.refs"], "referencing": ["10.1109/ICME.2002.1035879", "10.1109/TCSVT.2005.848345", "10.1109/76.475889", "10.1109/TIP.2006.873460", "10.1109/TMM.2010.2060475", "10.1109/TMM.2006.886261", "10.1109/QoMEX.2014.6982277", "10.1109/ISM.2006.124", "10.1109/TVCG.2012.106", "10.1109/TIT.1974.1055250", "10.1109/LSP.2010.2090041", "10.1109/TMM.2013.2268053", "10.1109/VISUAL.1998.745314", "10.1109/ICME.2002.1035879", "10.1109/TCSVT.2005.848345", "10.1109/76.475889", "10.1109/TIP.2006.873460", "10.1109/TMM.2010.2060475", "10.1109/TMM.2006.886261", "10.1109/QoMEX.2014.6982277", "10.1109/ISM.2006.124", "10.1109/TVCG.2012.106", "10.1109/TIT.1974.1055250", "10.1109/LSP.2010.2090041", "10.1109/TMM.2013.2268053", "10.1109/VISUAL.1998.745314", "10.1109/ICME.2002.1035879", "10.1109/TCSVT.2005.848345", "10.1109/76.475889", "10.1109/TIP.2006.873460", "10.1109/TMM.2010.2060475", "10.1109/TMM.2006.886261", "10.1109/QoMEX.2014.6982277", "10.1109/ISM.2006.124", "10.1109/TVCG.2012.106", "10.1109/TIT.1974.1055250", "10.1109/LSP.2010.2090041", "10.1109/TMM.2013.2268053", "10.1109/VISUAL.1998.745314", "10.1145/1667239.1667263", "10.1145/1462048.1462052", "10.1145/1073204.1073244", "10.1145/2185520.2185525", "10.1145/2530691", "10.1145/2010324.1964935", "10.1145/311535.311543", "10.1145/1276377.1276472", "10.1145/641480.641503", "10.1145/1667239.1667263", "10.1145/1462048.1462052", "10.1145/1073204.1073244", "10.1145/2185520.2185525", "10.1145/2530691", "10.1145/2010324.1964935", "10.1145/311535.311543", "10.1145/1276377.1276472", "10.1145/641480.641503", "10.1145/1667239.1667263", "10.1145/1462048.1462052", "10.1145/1073204.1073244", "10.1145/2185520.2185525", "10.1145/2530691", "10.1145/2010324.1964935", "10.1145/311535.311543", "10.1145/1276377.1276472", "10.1145/641480.641503", "10.1111/1467-8659.00236", "10.1111/cgf.12001", "10.1016/j.image.2013.06.003", "10.1016/j.jvcir.2011.01.005", "10.1113/jphysiol.1969.sp008862", "10.1016/j.visres.2013.04.015", "10.1167/5.9.6", "10.1364/JOSA.70.001458", "10.1016/j.cag.2012.06.004", "10.1111/j.1467-8659.2011.02017.x", "10.1016/j.gmod.2013.05.002", "10.1016/j.cad.2014.08.002", "10.1111/j.1467-8659.2010.01815.x", "10.3758/BF03202828", "10.1364/JOSAA.3.001166", "10.1016/j.image.2015.02.001", "10.1038/nrn2787", "10.1016/j.image.2014.12.008", "10.1111/1467-8659.00236", "10.1111/cgf.12001", "10.1016/j.image.2013.06.003", "10.1016/j.jvcir.2011.01.005", "10.1113/jphysiol.1969.sp008862", "10.1016/j.visres.2013.04.015", "10.1167/5.9.6", "10.1364/JOSA.70.001458", "10.1016/j.cag.2012.06.004", "10.1111/j.1467-8659.2011.02017.x", "10.1016/j.gmod.2013.05.002", "10.1016/j.cad.2014.08.002", "10.1111/j.1467-8659.2010.01815.x", "10.3758/BF03202828", "10.1364/JOSAA.3.001166", "10.1016/j.image.2015.02.001", "10.1038/nrn2787", "10.1016/j.image.2014.12.008", "10.1111/1467-8659.00236", "10.1111/cgf.12001", "10.1016/j.image.2013.06.003", "10.1016/j.jvcir.2011.01.005", "10.1113/jphysiol.1969.sp008862", "10.1016/j.visres.2013.04.015", "10.1167/5.9.6", "10.1364/JOSA.70.001458", "10.1016/j.cag.2012.06.004", "10.1111/j.1467-8659.2011.02017.x", "10.1016/j.gmod.2013.05.002", "10.1016/j.cad.2014.08.002", "10.1111/j.1467-8659.2010.01815.x", "10.3758/BF03202828", "10.1364/JOSAA.3.001166", "10.1016/j.image.2015.02.001", "10.1038/nrn2787", "10.1016/j.image.2014.12.008"]}, "10.1109/TVCG.2015.2510000": {"doi": "10.1109/TVCG.2015.2510000", "author": ["Z. Liu", "L. Zhou", "H. Leung", "H. P. H. Shum"], "title": "Kinect Posture Reconstruction Based on a Local Mixture of Gaussian Process Models", "year": "2016", "abstract": "Depth sensor based 3D human motion estimation hardware such as Kinect has made interactive applications more popular recently. However, it is still challenging to accurately recognize postures from a single depth camera due to the inherently noisy data derived from depth images and self-occluding action performed by the user. In this paper, we propose a new real-time probabilistic framework to enhance the accuracy of live captured postures that belong to one of the action classes in the database. We adopt the Gaussian Process model as a prior to leverage the position data obtained from Kinect and marker-based motion capture system. We also incorporate a temporal consistency term into the optimization framework to constrain the velocity variations between successive frames. To ensure that the reconstructed posture resembles the accurate parts of the observed posture, we embed a set of joint reliability measurements into the optimization framework. A major drawback of Gaussian Process is its cubic learning complexity when dealing with a large database due to the inverse of a covariance matrix. To solve the problem, we propose a new method based on a local mixture of Gaussian Processes, in which Gaussian Processes are defined in local regions of the state space. Due to the significantly decreased sample size in each local Gaussian Process, the learning time is greatly reduced. At the same time, the prediction speed is enhanced as the weighted mean prediction for a given sample is determined by the nearby local models only. Our system also allows incrementally updating a specific local Gaussian Process in real time, which enhances the likelihood of adapting to run-time postures that are different from those in the database. Experimental results demonstrate that our system can generate high quality postures even under severe self-occlusion situations, which is beneficial for real-time applications such as motion-based gaming and sport training.", "keywords": ["computational complexity", "covariance matrices", "Gaussian processes", "image capture", "image denoising", "image enhancement", "image motion analysis", "image reconstruction", "learning (artificial intelligence)", "optimisation", "pose estimation", "severe self-occlusion situations", "run-time postures", "covariance matrix", "cubic learning complexity", "marker-based motion capture system", "action classes", "live captured postures", "depth images", "noisy data", "posture recognition", "3D human motion estimation", "depth sensor", "depth camera", "Gaussian process models", "kinect posture reconstruction", "Gaussian processes", "Databases", "Image reconstruction", "Three-dimensional displays", "Adaptation models", "Real-time systems", "Reliability", "Gaussian process", "incremental learning", "kinect", "posture reconstruction"], "referenced_by": ["10.1109/ICSP.2016.7877975", "10.1109/ITNEC.2017.8284904", "10.1109/JSEN.2018.2810449", "10.1109/SKIMA.2017.8294096", "10.1109/ICRAS.2018.8443267", "10.1109/JTEHM.2018.2859992", "10.1109/IROS.2018.8593816", "10.1109/TVCG.2018.2812879", "10.1109/TMM.2019.2905692", "10.1109/ACCESS.2020.2980316", "10.1109/MCE.2019.2956205", "10.1109/ISITIA49792.2020.9163662", "10.1109/TNNLS.2019.2957109", "10.1109/TVCG.2019.2936810", "10.1145/2994258.2994266", "10.1007/978-3-319-30808-1_7-1", "10.1007/978-3-319-76270-8_46", "10.1007/s11390-017-1742-y", "10.1016/j.cag.2017.09.007", "10.3390/s17061261", "10.1016/j.cag.2017.10.008", "10.1007/s10462-018-9651-1", "10.1016/j.cag.2018.07.002", "10.1007/s00371-019-01678-7", "10.3390/s20041119", "10.1016/j.isatra.2020.02.023"], "referencing": ["10.1109/TLT.2010.27", "10.1109/CVPR.2011.5995316", "10.1109/TCYB.2013.2275945", "10.1109/MSP.2013.2266959", "10.1109/TIP.2010.2076820", "10.1109/TLT.2010.27", "10.1109/CVPR.2011.5995316", "10.1109/TCYB.2013.2275945", "10.1109/MSP.2013.2266959", "10.1109/TIP.2010.2076820", "10.1109/TLT.2010.27", "10.1109/CVPR.2011.5995316", "10.1109/TCYB.2013.2275945", "10.1109/MSP.2013.2266959", "10.1109/TIP.2010.2076820", "10.1145/1186822.1073248", "10.1145/1944745.1944768", "10.1145/2671015.2671021", "10.1145/2047196.2047270", "10.1145/2338676.2338703", "10.1145/2407336.2407340", "10.1145/2466715.2466722", "10.1145/2366145.2366207", "10.1145/2629500", "10.1145/2185520.2185586", "10.1145/1186822.1073248", "10.1145/1944745.1944768", "10.1145/2671015.2671021", "10.1145/2047196.2047270", "10.1145/2338676.2338703", "10.1145/2407336.2407340", "10.1145/2466715.2466722", "10.1145/2366145.2366207", "10.1145/2629500", "10.1145/2185520.2185586", "10.1145/1186822.1073248", "10.1145/1944745.1944768", "10.1145/2671015.2671021", "10.1145/2047196.2047270", "10.1145/2338676.2338703", "10.1145/2407336.2407340", "10.1145/2466715.2466722", "10.1145/2366145.2366207", "10.1145/2629500", "10.1145/2185520.2185586", "10.1002/cav.1557", "10.1007/s11263-008-0204-y", "10.1007/978-3-319-10605-2_3", "10.1002/cav.1557", "10.1007/s11263-008-0204-y", "10.1007/978-3-319-10605-2_3", "10.1002/cav.1557", "10.1007/s11263-008-0204-y", "10.1007/978-3-319-10605-2_3"]}, "10.1109/TVCG.2015.2509990": {"doi": "10.1109/TVCG.2015.2509990", "author": ["S. Liu", "J. Yin", "X. Wang", "W. Cui", "K. Cao", "J. Pei"], "title": "Online Visual Analytics of Text Streams", "year": "2016", "abstract": "We present an online visual analytics approach to helping users explore and understand hierarchical topic evolution in high-volume text streams. The key idea behind this approach is to identify representative topics in incoming documents and align them with the existing representative topics that they immediately follow (in time). To this end, we learn a set of streaming tree cuts from topic trees based on user-selected focus nodes. A dynamic Bayesian network model has been developed to derive the tree cuts in the incoming topic trees to balance the fitness of each tree cut and the smoothness between adjacent tree cuts. By connecting the corresponding topics at different times, we are able to provide an overview of the evolving hierarchical topics. A sedimentation-based visualization has been designed to enable the interactive analysis of streaming text data from global patterns to local details. We evaluated our method on real-world datasets and the results are generally favorable.", "keywords": ["belief networks", "data analysis", "data visualisation", "text analysis", "online visual analytics", "hierarchical topic evolution", "high-volume text streams", "streaming tree cuts", "topic trees", "user-selected focus nodes", "dynamic Bayesian network model", "adjacent tree cuts", "sedimentation-based visualization", "interactive analysis", "streaming text data", "global patterns", "Visualization", "Bayes methods", "Heuristic algorithms", "Data visualization", "Electronic mail", "Algorithm design and analysis", "Clustering algorithms", "Streaming text data", "evolutionary tree clustering", "streaming tree cut", "streaming topic visualization"], "referenced_by": ["10.1109/MCG.2016.73", "10.1109/TMM.2016.2614220", "10.1109/TVCG.2017.2745141", "10.1109/TVCG.2017.2744378", "10.1109/DASC-PICom-DataCom-CyberSciTec.2017.176", "10.1109/YAC.2018.8406403", "10.1109/TVCG.2017.2764459", "10.1109/BDVA.2018.8533897", "10.1109/TVCG.2018.2865041", "10.1109/TVCG.2018.2864899", "10.1109/TVCG.2018.2834341", "10.1109/ACCESS.2019.2923736", "10.1109/PacificVis.2019.00021", "10.1109/ACCESS.2019.2935471", "10.1109/TVCG.2018.2859973", "10.1109/TVCG.2019.2934433", "10.1109/TVCG.2018.2873011", "10.1109/TVCG.2018.2882449", "10.1145/3200766", "10.1111/cgf.13211", "10.1111/cgf.13425", "10.1111/cgf.13446", "10.1016/j.jvlc.2018.08.008", "10.1007/s12650-018-0531-1", "10.1007/s12650-018-0518-y", "10.1177/1473871618757228", "10.1016/j.visinf.2019.06.002", "10.1111/cgf.13714", "10.1016/j.neucom.2019.10.072", "10.3390/data5010020", "10.1007/s12650-020-00644-z", "10.1007/978-3-030-49418-6_12", "10.1007/978-981-15-5788-0_54", "10.1007/s41095-020-0197-1", "10.1007/s41095-020-0191-7"], "referencing": ["10.1109/34.1000236", "10.1109/TVCG.2011.239", "10.1109/TVCG.2014.2346433", "10.1109/TVCG.2008.135", "10.1109/2945.981848", "10.1109/TSA.2003.814379", "10.1109/TVCG.2013.227", "10.1109/TSMC.1981.4308636", "10.1109/TVCG.2014.2346919", "10.1109/TKDE.2014.2373384", "10.1109/TVCG.2012.225", "10.1109/TVCG.2014.2346922", "10.1109/34.1000236", "10.1109/TVCG.2011.239", "10.1109/TVCG.2014.2346433", "10.1109/TVCG.2008.135", "10.1109/2945.981848", "10.1109/TSA.2003.814379", "10.1109/TVCG.2013.227", "10.1109/TSMC.1981.4308636", "10.1109/TVCG.2014.2346919", "10.1109/TKDE.2014.2373384", "10.1109/TVCG.2012.225", "10.1109/TVCG.2014.2346922", "10.1109/34.1000236", "10.1109/TVCG.2011.239", "10.1109/TVCG.2014.2346433", "10.1109/TVCG.2008.135", "10.1109/2945.981848", "10.1109/TSA.2003.814379", "10.1109/TVCG.2013.227", "10.1109/TSMC.1981.4308636", "10.1109/TVCG.2014.2346919", "10.1109/TKDE.2014.2373384", "10.1109/TVCG.2012.225", "10.1109/TVCG.2014.2346922", "10.1145/1143844.1143859", "10.1145/1150402.1150467", "10.1145/22627.22342", "10.1145/1557019.1557077", "10.1145/1645953.1646023", "10.1145/2089094.2089101", "10.1145/2339530.2339754", "10.1145/1124772.1124851", "10.1145/1281192.1281276", "10.1145/1498759.1498826", "10.1145/1978942.1979196", "10.1145/1835804.1835940", "10.1145/1143844.1143859", "10.1145/1150402.1150467", "10.1145/22627.22342", "10.1145/1557019.1557077", "10.1145/1645953.1646023", "10.1145/2089094.2089101", "10.1145/2339530.2339754", "10.1145/1124772.1124851", "10.1145/1281192.1281276", "10.1145/1498759.1498826", "10.1145/1978942.1979196", "10.1145/1835804.1835940", "10.1145/1143844.1143859", "10.1145/1150402.1150467", "10.1145/22627.22342", "10.1145/1557019.1557077", "10.1145/1645953.1646023", "10.1145/2089094.2089101", "10.1145/2339530.2339754", "10.1145/1124772.1124851", "10.1145/1281192.1281276", "10.1145/1498759.1498826", "10.1145/1978942.1979196", "10.1145/1835804.1835940", "10.1137/1.9781611972788.20", "10.1002/9781119995678.ch8", "10.1177/1473871613493996", "10.1007/s00371-013-0892-3", "10.1086/267990", "10.1007/s11390-013-1383-8", "10.3115/v1/P14-1146", "10.1016/S0020-7373(84)80043-7", "10.1002/sam.10059", "10.1137/1.9781611972788.20", "10.1002/9781119995678.ch8", "10.1177/1473871613493996", "10.1007/s00371-013-0892-3", "10.1086/267990", "10.1007/s11390-013-1383-8", "10.3115/v1/P14-1146", "10.1016/S0020-7373(84)80043-7", "10.1002/sam.10059", "10.1137/1.9781611972788.20", "10.1002/9781119995678.ch8", "10.1177/1473871613493996", "10.1007/s00371-013-0892-3", "10.1086/267990", "10.1007/s11390-013-1383-8", "10.3115/v1/P14-1146", "10.1016/S0020-7373(84)80043-7", "10.1002/sam.10059"]}, "10.1109/TVCG.2015.2511751": {"doi": "10.1109/TVCG.2015.2511751", "author": ["K. Cheng", "R. Tong", "M. Tang", "J. Qian", "M. Sarkis"], "title": "Parametric Human Body Reconstruction Based on Sparse Key Points", "year": "2016", "abstract": "We propose an automatic parametric human body reconstruction algorithm which can efficiently construct a model using a single Kinect sensor. A user needs to stand still in front of the sensor for a couple of seconds to measure the range data. The user's body shape and pose will then be automatically constructed in several seconds. Traditional methods optimize dense correspondences between range data and meshes. In contrast, our proposed scheme relies on sparse key points for the reconstruction. It employs regression to find the corresponding key points between the scanned range data and some annotated training data. We design two kinds of feature descriptors as well as corresponding regression stages to make the regression robust and accurate. Our scheme follows with dense refinement where a pre-factorization method is applied to improve the computational efficiency. Compared with other methods, our scheme achieves similar reconstruction accuracy but significantly reduces runtime.", "keywords": ["feature extraction", "image reconstruction", "regression analysis", "solid modelling", "computer graphics", "3D human body model reconstruction", "computational efficiency improvement", "prefactorization method", "regression stages", "feature descriptors", "dense correspondence optimization", "range data measurement", "Kinect sensor", "sparse key points", "automatic parametric human body reconstruction algorithm", "Shape", "Image reconstruction", "Biological system modeling", "Optimization", "Computational modeling", "Three-dimensional displays", "Parametric statistics", "Human body reconstruction", "range data", "regression", "SCAPE modeling", "Algorithms", "Computer Graphics", "Human Body", "Humans"], "referenced_by": ["10.1109/ACCESS.2018.2837147", "10.1109/ICCSE.2018.8468823", "10.1109/TMM.2018.2844087", "10.1109/ICCV.2019.00553", "10.1109/ICACI49185.2020.9177788", "10.1007/978-3-319-69487-0_7", "10.1007/s11390-017-1742-y", "10.1016/j.cag.2017.11.008", "10.1111/cgf.13012", "10.1007/s00530-018-0594-9", "10.1088/1742-6596/1098/1/012024", "10.1007/s11390-019-1909-9", "10.1016/B978-0-12-816713-7.00045-3"], "referencing": ["10.1109/34.784284", "10.1109/TVCG.2013.249", "10.1109/TVCG.2012.56", "10.1109/34.784284", "10.1109/TVCG.2013.249", "10.1109/TVCG.2012.56", "10.1109/34.784284", "10.1109/TVCG.2013.249", "10.1109/TVCG.2012.56", "10.1145/1073204.1073207", "10.1145/2461912.2462012", "10.1145/1778765.1778863", "10.1145/2508363.2508407", "10.1145/2047196.2047270", "10.1145/2185520.2185531", "10.1145/2461912.2462019", "10.1145/1073204.1073207", "10.1145/2461912.2462012", "10.1145/1778765.1778863", "10.1145/2508363.2508407", "10.1145/2047196.2047270", "10.1145/2185520.2185531", "10.1145/2461912.2462019", "10.1145/1073204.1073207", "10.1145/2461912.2462012", "10.1145/1778765.1778863", "10.1145/2508363.2508407", "10.1145/2047196.2047270", "10.1145/2185520.2185531", "10.1145/2461912.2462019", "10.1016/j.cag.2009.03.026", "10.1007/s11263-013-0667-3", "10.1111/j.1467-8659.2009.01373.x", "10.1007/978-3-540-88688-4_2", "10.1007/s00371-013-0775-7", "10.1007/978-3-642-33783-3_18", "10.1007/978-1-4471-4640-7_5", "10.1111/j.1467-8659.2011.01879.x", "10.1007/s41095-015-0019-z", "10.1016/j.cag.2009.03.026", "10.1007/s11263-013-0667-3", "10.1111/j.1467-8659.2009.01373.x", "10.1007/978-3-540-88688-4_2", "10.1007/s00371-013-0775-7", "10.1007/978-3-642-33783-3_18", "10.1007/978-1-4471-4640-7_5", "10.1111/j.1467-8659.2011.01879.x", "10.1007/s41095-015-0019-z", "10.1016/j.cag.2009.03.026", "10.1007/s11263-013-0667-3", "10.1111/j.1467-8659.2009.01373.x", "10.1007/978-3-540-88688-4_2", "10.1007/s00371-013-0775-7", "10.1007/978-3-642-33783-3_18", "10.1007/978-1-4471-4640-7_5", "10.1111/j.1467-8659.2011.01879.x", "10.1007/s41095-015-0019-z"]}, "10.1109/TVCG.2015.2511734": {"doi": "10.1109/TVCG.2015.2511734", "author": ["C. Dick", "M. Rogowsky", "R. Westermann"], "title": "Solving the Fluid Pressure Poisson Equation Using Multigrid\u2014Evaluation and Improvements", "year": "2016", "abstract": "In many numerical simulations of fluids governed by the incompressible Navier-Stokes equations, the pressure Poisson equation needs to be solved to enforce mass conservation. Multigrid solvers show excellent convergence in simple scenarios, yet they can converge slowly in domains where physically separated regions are combined at coarser scales. Moreover, existing multigrid solvers are tailored to specific discretizations of the pressure Poisson equation, and they cannot easily be adapted to other discretizations. In this paper we analyze the convergence properties of existing multigrid solvers for the pressure Poisson equation in different simulation domains, and we show how to further improve the multigrid convergence rate by using a graph-based extension to determine the coarse grid hierarchy. The proposed multigrid solver is generic in that it can be applied to different kinds of discretizations of the pressure Poisson equation, by using solely the specification of the simulation domain and pre-assembled computational stencils. We analyze the proposed solver in combination with finite difference and finite volume discretizations of the pressure Poisson equation. Our evaluations show that, despite the common assumption, multigrid schemes can exploit their potential even in the most complicated simulation scenarios, yet this behavior is obtained at the price of higher memory consumption.", "keywords": ["computational fluid dynamics", "convergence of numerical methods", "finite difference methods", "finite volume methods", "flow simulation", "graph theory", "Navier-Stokes equations", "Poisson equation", "fluid pressure Poisson equation", "numerical simulations", "incompressible Navier-Stokes equations", "mass conservation", "simulation domains", "multigrid convergence rate improvement", "graph-based extension", "coarse grid hierarchy", "generic multigrid solver", "preassembled computational stencils", "finite difference discretizations", "finite volume discretizations", "Mathematical model", "Convergence", "Computational modeling", "Multigrid methods", "Interpolation", "Poisson equations", "Topology", "Fluid simulation", "multigrid", "convergence analysis"], "referenced_by": ["10.1145/2980179.2982430", "10.1145/3072959.3073625", "10.1145/3340255", "10.1145/2897824.2925919", "10.1145/3092818", "10.1145/3386569.3392455", "10.1002/cav.1836", "10.1111/cgf.13048", "10.1111/cgf.13909"], "referencing": ["10.1109/TVCG.2014.2307873", "10.1109/TVCG.2014.2307873", "10.1109/TVCG.2014.2307873", "10.1145/383259.383260", "10.1145/2019406.2019418", "10.1145/1276377.1276502", "10.1145/311535.311548", "10.1145/1073204.1073298", "10.1145/2661229.2661269", "10.1145/1198555.1198784", "10.1145/1964921.1964977", "10.1145/383259.383260", "10.1145/2019406.2019418", "10.1145/1276377.1276502", "10.1145/311535.311548", "10.1145/1073204.1073298", "10.1145/2661229.2661269", "10.1145/1198555.1198784", "10.1145/1964921.1964977", "10.1145/383259.383260", "10.1145/2019406.2019418", "10.1145/1276377.1276502", "10.1145/311535.311548", "10.1145/1073204.1073298", "10.1145/2661229.2661269", "10.1145/1198555.1198784", "10.1145/1964921.1964977", "10.1006/gmip.1996.0039", "10.1016/j.jcp.2009.08.032", "10.1201/b10635", "10.1063/1.1761178", "10.1006/jcph.2001.6977", "10.1137/1.9781611970753", "10.1137/1.9780898719505", "10.1016/j.jcp.2011.11.023", "10.1006/jcph.1998.5965", "10.1111/cgf.12577", "10.1006/gmip.1996.0039", "10.1016/j.jcp.2009.08.032", "10.1201/b10635", "10.1063/1.1761178", "10.1006/jcph.2001.6977", "10.1137/1.9781611970753", "10.1137/1.9780898719505", "10.1016/j.jcp.2011.11.023", "10.1006/jcph.1998.5965", "10.1111/cgf.12577", "10.1006/gmip.1996.0039", "10.1016/j.jcp.2009.08.032", "10.1201/b10635", "10.1063/1.1761178", "10.1006/jcph.2001.6977", "10.1137/1.9781611970753", "10.1137/1.9780898719505", "10.1016/j.jcp.2011.11.023", "10.1006/jcph.1998.5965", "10.1111/cgf.12577"]}, "10.1109/TVCG.2015.2509996": {"doi": "10.1109/TVCG.2015.2509996", "author": ["Q. Mo", "H. Yeh", "D. Manocha"], "title": "Tracing Analytic Ray Curves for Light and Sound Propagation in Non-Linear Media", "year": "2016", "abstract": "The physical world consists of spatially varying media, such as the atmosphere and the ocean, in which light and sound propagates along non-linear trajectories. This presents a challenge to existing ray-tracing based methods, which are widely adopted to simulate propagation due to their efficiency and flexibility, but assume linear rays. We present a novel algorithm that traces analytic ray curves computed from local media gradients, and utilizes the closed-form solutions of both the intersections of the ray curves with planar surfaces, and the travel distance. By constructing an adaptive unstructured mesh, our algorithm is able to model general media profiles that vary in three dimensions with complex boundaries consisting of terrains and other scene objects such as buildings. Our analytic ray curve tracer with the adaptive mesh improves the efficiency considerably over prior methods. We highlight the algorithm's application on simulation of visual and sound propagation in outdoor scenes.", "keywords": ["mesh generation", "ray tracing", "analytic ray curve tracing", "nonlinear media", "ray-tracing based methods", "closed-form solutions", "adaptive unstructured mesh", "general media profiles", "Media", "Atmospheric modeling", "Ray tracing", "Mathematical model", "Trajectory", "Computational modeling", "Acoustics"], "referenced_by": ["10.1109/VR46266.2020.00111", "10.1121/1.4977005", "10.1121/10.0002966"], "referencing": ["10.1109/38.55151", "10.1109/38.62692", "10.1109/TVCG.2007.24", "10.1109/MCG.2011.106", "10.1109/VISUAL.2000.885728", "10.1109/TVCG.2007.70566", "10.1109/TVCG.2011.216", "10.1109/TVCG.2005.79", "10.1109/2945.646238", "10.1109/TVCG.2009.105", "10.1109/TVCG.2013.115", "10.1109/38.55151", "10.1109/38.62692", "10.1109/TVCG.2007.24", "10.1109/MCG.2011.106", "10.1109/VISUAL.2000.885728", "10.1109/TVCG.2007.70566", "10.1109/TVCG.2011.216", "10.1109/TVCG.2005.79", "10.1109/2945.646238", "10.1109/TVCG.2009.105", "10.1109/TVCG.2013.115", "10.1109/38.55151", "10.1109/38.62692", "10.1109/TVCG.2007.24", "10.1109/MCG.2011.106", "10.1109/VISUAL.2000.885728", "10.1109/TVCG.2007.70566", "10.1109/TVCG.2011.216", "10.1109/TVCG.2005.79", "10.1109/2945.646238", "10.1109/TVCG.2009.105", "10.1109/TVCG.2013.115", "10.1145/1508044.1508109", "10.1145/1276377.1276451", "10.1145/1360612.1360634", "10.1145/1882262.1866199", "10.1145/2487228.2487235", "10.1145/197938.197952", "10.1145/2557605", "10.1145/1508044.1508109", "10.1145/1276377.1276451", "10.1145/1360612.1360634", "10.1145/1882262.1866199", "10.1145/2487228.2487235", "10.1145/197938.197952", "10.1145/2557605", "10.1145/1508044.1508109", "10.1145/1276377.1276451", "10.1145/1360612.1360634", "10.1145/1882262.1866199", "10.1145/2487228.2487235", "10.1145/197938.197952", "10.1145/2557605", "10.1175/1520-0469(1971)028&lt;0181:FPRITA&gt;2.0.CO;2", "10.1007/978-1-4419-8678-8", "10.1007/978-94-010-0660-6", "10.1029/GD025p0133", "10.1080/01431168608954706", "10.1364/AO.16.002427", "10.1007/978-3-540-77455-6", "10.1111/j.1467-8659.2010.01733.x", "10.1007/978-3-7091-7484-5_23", "10.1016/j.cag.2006.05.002", "10.1007/BF01901044", "10.1111/j.1467-8659.2004.00794.x", "10.1111/j.1467-8659.2008.01269.x", "10.1007/s00371-005-0287-1", "10.1111/j.1467-8659.2010.01831.x", "10.1364/BOE.1.000165", "10.1007/s00371-012-0742-8", "10.1190/1.2159061", "10.1175/1520-0493(2000)128&lt;2044:ADAWAD&gt;2.0.CO;2", "10.2458/azu_uapress_9780816530595-ch009", "10.1017/CBO9781139644181", "10.1007/978-3-642-84031-9", "10.1111/j.1467-8659.2010.01742.x", "10.1111/j.1467-8659.2009.01470.x", "10.1119/1.10919", "10.1016/0003-682X(93)90092-K", "10.1121/1.406737", "10.1002/qj.49710143015", "10.2514/6.2003-3986", "10.1007/3-540-29090-7_9", "10.1175/1520-0469(1971)028&lt;0181:FPRITA&gt;2.0.CO;2", "10.1007/978-1-4419-8678-8", "10.1007/978-94-010-0660-6", "10.1029/GD025p0133", "10.1080/01431168608954706", "10.1364/AO.16.002427", "10.1007/978-3-540-77455-6", "10.1111/j.1467-8659.2010.01733.x", "10.1007/978-3-7091-7484-5_23", "10.1016/j.cag.2006.05.002", "10.1007/BF01901044", "10.1111/j.1467-8659.2004.00794.x", "10.1111/j.1467-8659.2008.01269.x", "10.1007/s00371-005-0287-1", "10.1111/j.1467-8659.2010.01831.x", "10.1364/BOE.1.000165", "10.1007/s00371-012-0742-8", "10.1190/1.2159061", "10.1175/1520-0493(2000)128&lt;2044:ADAWAD&gt;2.0.CO;2", "10.2458/azu_uapress_9780816530595-ch009", "10.1017/CBO9781139644181", "10.1007/978-3-642-84031-9", "10.1111/j.1467-8659.2010.01742.x", "10.1111/j.1467-8659.2009.01470.x", "10.1119/1.10919", "10.1016/0003-682X(93)90092-K", "10.1121/1.406737", "10.1002/qj.49710143015", "10.2514/6.2003-3986", "10.1007/3-540-29090-7_9", "10.1175/1520-0469(1971)028&lt;0181:FPRITA&gt;2.0.CO;2", "10.1007/978-1-4419-8678-8", "10.1007/978-94-010-0660-6", "10.1029/GD025p0133", "10.1080/01431168608954706", "10.1364/AO.16.002427", "10.1007/978-3-540-77455-6", "10.1111/j.1467-8659.2010.01733.x", "10.1007/978-3-7091-7484-5_23", "10.1016/j.cag.2006.05.002", "10.1007/BF01901044", "10.1111/j.1467-8659.2004.00794.x", "10.1111/j.1467-8659.2008.01269.x", "10.1007/s00371-005-0287-1", "10.1111/j.1467-8659.2010.01831.x", "10.1364/BOE.1.000165", "10.1007/s00371-012-0742-8", "10.1190/1.2159061", "10.1175/1520-0493(2000)128&lt;2044:ADAWAD&gt;2.0.CO;2", "10.2458/azu_uapress_9780816530595-ch009", "10.1017/CBO9781139644181", "10.1007/978-3-642-84031-9", "10.1111/j.1467-8659.2010.01742.x", "10.1111/j.1467-8659.2009.01470.x", "10.1119/1.10919", "10.1016/0003-682X(93)90092-K", "10.1121/1.406737", "10.1002/qj.49710143015", "10.2514/6.2003-3986", "10.1007/3-540-29090-7_9"]}}