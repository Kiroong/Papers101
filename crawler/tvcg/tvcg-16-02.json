{"10.1109/TVCG.2015.2440259": {"doi": "10.1109/TVCG.2015.2440259", "author": ["G. Di Lorenzo", "M. Sbodio", "F. Calabrese", "M. Berlingerio", "F. Pinelli", "R. Nair"], "title": "AllAboard: Visual Exploration of Cellphone Mobility Data to Optimise Public Transport", "year": "2016", "abstract": "The deep penetration of mobile phones offers cities the ability to opportunistically monitor citizens' mobility and use data-driven insights to better plan and manage services. With large scale data on mobility patterns, operators can move away from the costly, mostly survey based, transportation planning processes, to a more data-centric view, that places the instrumented user at the center of development. In this framework, using mobile phone data to perform transit analysis and optimization represents a new frontier with significant societal impact, especially in developing countries. In this paper we present AllAboard, an intelligent tool that analyses cellphone data to help city authorities in visually exploring urban mobility and optimizing public transport. This is performed within a self contained tool, as opposed to the current solutions which rely on a combination of several distinct tools for analysis, reporting, optimisation and planning. An interactive user interface allows transit operators to visually explore the travel demand in both space and time, correlate it with the transit network, and evaluate the quality of service that a transit network provides to the citizens at very fine grain. Operators can visually test scenarios for transit network improvements, and compare the expected impact on the travellers' experience. The system has been tested using real telecommunication data for the city of Abidjan, Ivory Coast, and evaluated from a data mining, optimisation and user prospective.", "keywords": ["data mining", "data visualisation", "interactive systems", "mobile handsets", "optimisation", "planning", "public transport", "user interfaces", "deep penetration", "mobile phones", "citizen mobility monitoring", "data-driven insights", "service management", "service planning", "large scale data", "mobility patterns", "survey based transportation planning processes", "data-centric view", "instrumented user", "mobile phone data", "transit analysis", "AllAboard", "intelligent tool", "cellphone data analysis", "visually exploring urban mobility", "public transport optimization", "self contained tool", "interactive user interface", "travel demand", "traveller experience", "telecommunication data", "city of Abidjan", "Ivory Coast", "data mining", "user prospective", "Optimization", "Urban areas", "Data mining", "Mobile handsets", "Planning", "Visualization", "Antennas", "Spatio-temporal mining", "cellphone data", "visual exploration", "urban data mobility", "transit network", "Cell Phone", "Cities", "Computer Graphics", "Cote d'Ivoire", "Data Mining", "Humans", "Software", "Spatio-Temporal Analysis", "Transportation"], "referenced_by": ["10.1109/BigData.2016.7840677", "10.1109/PACIFICVIS.2017.8031583", "10.1109/SIBGRAPI.2017.21", "10.1109/TBDATA.2016.2586447", "10.1109/TITS.2017.2711644", "10.1109/TITS.2017.2727281", "10.1109/TITS.2017.2771746", "10.1109/ICDE.2019.00238", "10.1109/ACTEA.2019.8851091", "10.1109/MILTECHS.2019.8870096", "10.1109/AICT47866.2019.8981782", "10.1109/TVCG.2019.2922597", "10.1007/978-3-319-59105-6_27", "10.1007/s11390-016-1663-1", "10.1007/s41060-017-0061-2", "10.15701/kcgs.2016.22.5.27", "10.1007/s12650-018-0531-1", "10.1016/j.trc.2019.02.013", "10.3390/su11102944", "10.1007/978-3-030-21952-9_19", "10.3390/ijgi8100434", "10.1111/coin.12387", "10.1002/cpe.6105", "10.1007/s41095-020-0191-7", "10.1007/s11116-020-10151-7"], "referencing": ["10.1109/TVCG.2010.44", "10.1109/TVCG.2011.181", "10.1109/TVCG.2013.228", "10.1109/TVCG.2013.226", "10.1109/TITS.2013.2263225", "10.1109/MPRV.2011.41", "10.1109/TVCG.2003.1196007", "10.1109/TVCG.2012.279", "10.1109/TVCG.2013.179", "10.1109/TVCG.2010.44", "10.1109/TVCG.2011.181", "10.1109/TVCG.2013.228", "10.1109/TVCG.2013.226", "10.1109/TITS.2013.2263225", "10.1109/MPRV.2011.41", "10.1109/TVCG.2003.1196007", "10.1109/TVCG.2012.279", "10.1109/TVCG.2013.179", "10.1109/TVCG.2010.44", "10.1109/TVCG.2011.181", "10.1109/TVCG.2013.228", "10.1109/TVCG.2013.226", "10.1109/TITS.2013.2263225", "10.1109/MPRV.2011.41", "10.1109/TVCG.2003.1196007", "10.1109/TVCG.2012.279", "10.1109/TVCG.2013.179", "10.1145/2655691", "10.1145/2557500.2557532", "10.1145/1133265.1133348", "10.1145/2655691", "10.1145/2557500.2557532", "10.1145/1133265.1133348", "10.1145/2655691", "10.1145/2557500.2557532", "10.1145/1133265.1133348", "10.1177/1473871612457601", "10.1007/s00778-011-0244-8", "10.1038/nature06958", "10.1016/0191-2615(89)90034-9", "10.1016/j.tra.2008.03.011", "10.1287/trsc.1060.0161", "10.1559/152304087783875273", "10.1111/j.1467-8659.2011.01946.x", "10.1111/j.1467-8659.2009.01450.x", "10.1179/000870410X12658023467367", "10.1098/rsif.2012.0986", "10.1111/cgf.12129", "10.1177/1473871612457601", "10.1007/s00778-011-0244-8", "10.1038/nature06958", "10.1016/0191-2615(89)90034-9", "10.1016/j.tra.2008.03.011", "10.1287/trsc.1060.0161", "10.1559/152304087783875273", "10.1111/j.1467-8659.2011.01946.x", "10.1111/j.1467-8659.2009.01450.x", "10.1179/000870410X12658023467367", "10.1098/rsif.2012.0986", "10.1111/cgf.12129", "10.1177/1473871612457601", "10.1007/s00778-011-0244-8", "10.1038/nature06958", "10.1016/0191-2615(89)90034-9", "10.1016/j.tra.2008.03.011", "10.1287/trsc.1060.0161", "10.1559/152304087783875273", "10.1111/j.1467-8659.2011.01946.x", "10.1111/j.1467-8659.2009.01450.x", "10.1179/000870410X12658023467367", "10.1098/rsif.2012.0986", "10.1111/cgf.12129"]}, "10.1109/TVCG.2015.2430322": {"doi": "10.1109/TVCG.2015.2430322", "author": ["R. Zhang", "S. Wang", "X. Chen", "C. Ding", "L. Jiang", "J. Zhou", "L. Liu"], "title": "Designing Planar Deployable Objects via Scissor Structures", "year": "2016", "abstract": "Scissor structure is used to generate deployable objects for space-saving in a variety of applications, from architecture to aerospace science. While deployment from a small, regular shape to a larger one is easy to design, we focus on a more challenging task: designing a planar scissor structure that deploys from a given source shape into a specific target shape. We propose a two-step constructive method to generate a scissor structure from a high-dimensional parameter space. Topology construction of the scissor structure is first performed to approximate the two given shapes, as well as to guarantee the deployment. Then the geometry of the scissor structure is optimized in order to minimize the connection deflections and maximize the shape approximation. With the optimized parameters, the deployment can be simulated by controlling an anchor scissor unit. Physical deployable objects are fabricated according to the designed scissor structures by using 3D printing or manual assembly. We show a number of results for different shapes to demonstrate that even with fabrication errors, our designed structures can deform fluently between the source and target shapes.", "keywords": ["computational geometry", "deployable structures", "structural engineering computing", "topology", "planar deployable object design", "planar scissor structure design", "source shape", "target shape", "two-step constructive method", "high-dimensional parameter space", "topology construction", "connection deflection minimization", "geometry optimization", "shape approximation", "anchor scissor unit", "physical deployable objects", "3D printing", "manual assembly", "fabrication errors", "Shape", "Topology", "Geometry", "Joints", "Three-dimensional displays", "Fabrication", "Approximation methods", "Computer graphics", "Digital fabrication", "Computer graphics", "digital fabrication"], "referenced_by": ["10.1109/REMAR.2018.8449872", "10.1109/ICVRV.2017.00022", "10.1145/3072959.3073710", "10.1002/cav.1848", "10.1007/978-3-319-98020-1_49", "10.1016/j.ijmecsci.2019.02.005", "10.1016/j.cagd.2019.04.018", "10.1007/s00707-019-02449-0", "10.1016/j.mechmachtheory.2020.104169"], "referencing": ["10.1109/TVCG.2014.2307885", "10.1109/TVCG.2013.82", "10.1109/TPAMI.2003.1159951", "10.1109/TVCG.2014.2307885", "10.1109/TVCG.2013.82", "10.1109/TPAMI.2003.1159951", "10.1109/TVCG.2014.2307885", "10.1109/TVCG.2013.82", "10.1109/TPAMI.2003.1159951", "10.1145/2185520.2185583", "10.1145/2461912.2461958", "10.1145/2461912.2461915", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/2461912.2461953", "10.1145/2601097.2601143", "10.1145/2508363.2508400", "10.1145/2601097.2601127", "10.1145/2508363.2508382", "10.1145/1778765.1778800", "10.1145/2461912.2461979", "10.1145/2185520.2185543", "10.1145/2185520.2185583", "10.1145/2461912.2461958", "10.1145/2461912.2461915", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/2461912.2461953", "10.1145/2601097.2601143", "10.1145/2508363.2508400", "10.1145/2601097.2601127", "10.1145/2508363.2508382", "10.1145/1778765.1778800", "10.1145/2461912.2461979", "10.1145/2185520.2185543", "10.1145/2185520.2185583", "10.1145/2461912.2461958", "10.1145/2461912.2461915", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/2461912.2461953", "10.1145/2601097.2601143", "10.1145/2508363.2508400", "10.1145/2601097.2601127", "10.1145/2508363.2508382", "10.1145/1778765.1778800", "10.1145/2461912.2461979", "10.1145/2185520.2185543", "10.1007/978-3-7091-2584-7", "10.1080/00038628.2011.590054", "10.1260/026635101760832172", "10.1080/15397734.2012.687296", "10.1007/978-3-7091-1251-9_21", "10.1007/978-3-7091-2584-7", "10.1080/00038628.2011.590054", "10.1260/026635101760832172", "10.1080/15397734.2012.687296", "10.1007/978-3-7091-1251-9_21", "10.1007/978-3-7091-2584-7", "10.1080/00038628.2011.590054", "10.1260/026635101760832172", "10.1080/15397734.2012.687296", "10.1007/978-3-7091-1251-9_21"]}, "10.1109/TVCG.2015.2440273": {"doi": "10.1109/TVCG.2015.2440273", "author": ["M. Yang", "H. Chao", "C. Zhang", "J. Guo", "L. Yuan", "J. Sun"], "title": "Effective Clipart Image Vectorization through Direct Optimization of Bezigons", "year": "2016", "abstract": "Bezigons, i.e., closed paths composed of B\u00e9zier curves, have been widely employed to describe shapes in image vectorization results. However, most existing vectorization techniques infer the bezigons by simply approximating an intermediate vector representation (such as polygons). Consequently, the resultant bezigons are sometimes imperfect due to accumulated errors, fitting ambiguities, and a lack of curve priors, especially for low-resolution images. In this paper, we describe a novel method for vectorizing clipart images. In contrast to previous methods, we directly optimize the bezigons rather than using other intermediate representations; therefore, the resultant bezigons are not only of higher fidelity compared with the original raster image but also more reasonable because they were traced by a proficient expert. To enable such optimization, we have overcome several challenges and have devised a differentiable data energy as well as several curve-based prior terms. To improve the efficiency of the optimization, we also take advantage of the local control property of bezigons and adopt an overlapped piecewise optimization strategy. The experimental results show that our method outperforms both the current state-of-the-art method and commonly used commercial software in terms of bezigon quality.", "keywords": ["curve fitting", "image representation", "image resolution", "shape recognition", "effective clipart image vectorization technique", "direct bezigon optimization", "Bezier curves", "intermediate vector representation", "fitting ambiguities", "low-resolution images", "differentiable data energy", "curve-based prior terms", "local control property", "piecewise optimization strategy", "bezigon quality", "Optimization", "Image color analysis", "Image segmentation", "Sun", "Shape", "Software", "Visualization", "clipart vectorization", "clipart tracing", "bezigon optimization", "Clipart vectorization", "clipart tracing", "bezigon optimization"], "referenced_by": ["10.1145/3197517.3201312", "10.1145/3414685.3417871", "10.15587/1729-4061.2019.177817", "10.21303/2461-4262.2020.001108", "10.1155/2020/8871588"], "referencing": ["10.1109/TVCG.2009.9", "10.1109/ICDAR.1993.395716", "10.1109/34.754586", "10.1109/TPAMI.2006.127", "10.1109/AIPR.2006.30", "10.1109/TVCG.2009.9", "10.1109/ICDAR.1993.395716", "10.1109/34.754586", "10.1109/TPAMI.2006.127", "10.1109/AIPR.2006.30", "10.1109/TVCG.2009.9", "10.1109/ICDAR.1993.395716", "10.1109/34.754586", "10.1109/TPAMI.2006.127", "10.1109/AIPR.2006.30", "10.1145/2421636.2421640", "10.1145/1276377.1276391", "10.1145/2483852.2483873", "10.1145/1618452.1618463", "10.1145/2024676.2024687", "10.1145/1964921.1964994", "10.1145/965139.807360", "10.1145/2421636.2421640", "10.1145/1276377.1276391", "10.1145/2483852.2483873", "10.1145/1618452.1618463", "10.1145/2024676.2024687", "10.1145/1964921.1964994", "10.1145/965139.807360", "10.1145/2421636.2421640", "10.1145/1276377.1276391", "10.1145/2483852.2483873", "10.1145/1618452.1618463", "10.1145/2024676.2024687", "10.1145/1964921.1964994", "10.1145/965139.807360", "10.1016/S0734-189X(88)80001-X", "10.1016/0734-189X(90)90111-8", "10.1007/s00371-006-0051-1", "10.1023/B:VISI.0000022288.19776.77", "10.1016/B978-0-08-050753-8.50132-7", "10.1111/j.1467-8659.2011.01887.x", "10.1111/cgf.12070", "10.1137/S0363012992238369", "10.1137/0802028", "10.1007/0-387-30065-1_16", "10.1007/BF01589116", "10.6028/jres.049.044", "10.1016/S0734-189X(88)80001-X", "10.1016/0734-189X(90)90111-8", "10.1007/s00371-006-0051-1", "10.1023/B:VISI.0000022288.19776.77", "10.1016/B978-0-08-050753-8.50132-7", "10.1111/j.1467-8659.2011.01887.x", "10.1111/cgf.12070", "10.1137/S0363012992238369", "10.1137/0802028", "10.1007/0-387-30065-1_16", "10.1007/BF01589116", "10.6028/jres.049.044", "10.1016/S0734-189X(88)80001-X", "10.1016/0734-189X(90)90111-8", "10.1007/s00371-006-0051-1", "10.1023/B:VISI.0000022288.19776.77", "10.1016/B978-0-08-050753-8.50132-7", "10.1111/j.1467-8659.2011.01887.x", "10.1111/cgf.12070", "10.1137/S0363012992238369", "10.1137/0802028", "10.1007/0-387-30065-1_16", "10.1007/BF01589116", "10.6028/jres.049.044"]}, "10.1109/TVCG.2015.2440262": {"doi": "10.1109/TVCG.2015.2440262", "author": ["K. Petkov", "A. E. Kaufman"], "title": "Frameless Volume Visualization", "year": "2016", "abstract": "We have developed a novel visualization system based on the reconstruction of high resolution and high frame rate images from a multi-tiered stream of samples that are rendered framelessly. This decoupling of the rendering system from the display system is particularly suitable when dealing with very high resolution displays or expensive rendering algorithms, where the latency of generating complete frames may be prohibitively high for interactive applications. In contrast to the traditional frameless rendering technique, we generate the lowest latency samples on the optimal sampling lattice in the 3D domain. This approach avoids many of the artifacts associated with existing sample caching and reprojection methods during interaction that may not be acceptable in many visualization applications. Advanced visualization effects are generated remotely and streamed into the reconstruction system using tiered samples with varying latencies and quality levels. We demonstrate the use of our visualization system for the exploration of volumetric data at stable guaranteed frame rates on high resolution displays, including a 470 megapixel tiled display as part of the Reality Deck immersive visualization facility.", "keywords": ["data visualisation", "image reconstruction", "image resolution", "image sampling", "rendering (computer graphics)", "virtual reality", "frameless volume visualization system", "high resolution image reconstruction", "high frame rate image reconstruction", "multitiered stream", "rendering system", "interactive applications", "optimal sampling lattice", "sample caching", "reprojection methods", "advanced visualization effects", "quality levels", "volumetric data exploration", "high resolution displays", "reality deck immersive visualization facility", "Lattices", "Image reconstruction", "Rendering (computer graphics)", "Graphics processing units", "Image resolution", "Three-dimensional displays", "Streaming media", "Frameless rendering", "GPU", "volume rendering", "Reality Deck", "Frameless rendering", "GPU", "volume rendering", "Reality Deck"], "referenced_by": ["10.1080/1206212X.2017.1397341", "10.1111/cgf.13166"], "referencing": ["10.1109/TVCG.2008.201", "10.1109/TVCG.2009.32", "10.1109/TVCG.2007.70429", "10.1109/VISUAL.1994.346331", "10.1109/TVCG.2008.201", "10.1109/TVCG.2009.32", "10.1109/TVCG.2007.70429", "10.1109/VISUAL.1994.346331", "10.1109/TVCG.2008.201", "10.1109/TVCG.2009.32", "10.1109/TVCG.2007.70429", "10.1109/VISUAL.1994.346331", "10.1145/1198555.1198763", "10.1145/1101616.1101650", "10.1145/1730804.1730819", "10.1145/1198555.1198763", "10.1145/1101616.1101650", "10.1145/1730804.1730819", "10.1145/1198555.1198763", "10.1145/1101616.1101650", "10.1145/1730804.1730819", "10.1007/978-3-7091-6809-7_3", "10.1111/cgf.12364", "10.1111/j.1467-8659.2009.01445.x", "10.1016/j.cag.2010.02.002", "10.1111/j.1467-8659.2009.01641.x", "10.1007/978-3-7091-6809-7_3", "10.1111/cgf.12364", "10.1111/j.1467-8659.2009.01445.x", "10.1016/j.cag.2010.02.002", "10.1111/j.1467-8659.2009.01641.x", "10.1007/978-3-7091-6809-7_3", "10.1111/cgf.12364", "10.1111/j.1467-8659.2009.01445.x", "10.1016/j.cag.2010.02.002", "10.1111/j.1467-8659.2009.01641.x"]}, "10.1109/TVCG.2015.2440255": {"doi": "10.1109/TVCG.2015.2440255", "author": ["W. Dong", "F. Wu", "Y. Kong", "X. Mei", "T. Lee", "X. Zhang"], "title": "Image Retargeting by Texture-Aware Synthesis", "year": "2016", "abstract": "Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on exampled-based texture synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural & non-textural regions with different strategies since they have different natures. We propose to retarget the textural regions by example-based synthesis and non-textural regions by fast multi-operator. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image retargeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method.", "keywords": ["image texture", "real-world images", "exampled-based texture synthesis", "content-aware image retargeting enhancement", "textural image content synthesis", "textural region manipulation", "nontextural region manipulation", "multioperator", "multiple disjoint textural region detection", "Image color analysis", "Visualization", "Shape", "Benchmark testing", "Reliability", "Image segmentation", "Feature extraction", "Natural image", "texture detection", "texture-based significance map", "texture-aware synthesis", "Natural image", "texture detection", "texture-based significance map", "texture-aware synthesis"], "referenced_by": ["10.1109/AICCSA.2017.209", "10.1109/TIP.2017.2681840", "10.1109/TMM.2016.2631123", "10.1109/TVCG.2016.2532329", "10.1109/ICCV.2017.488", "10.1109/ICIP.2019.8802946", "10.1109/TVCG.2018.2866106", "10.1109/TMM.2019.2920613", "10.1109/i-PACT44901.2019.8960083", "10.1109/TMM.2019.2932620", "10.1109/TMM.2019.2932566", "10.1109/TMM.2019.2959925", "10.1109/ICIP40778.2020.9191354", "10.1145/3231598", "10.1007/s11042-017-4674-1", "10.1007/s11704-016-6084-3", "10.1016/j.neucom.2016.12.028", "10.1016/j.neucom.2018.01.058", "10.1111/j.1467-8659.2007.01046.x", "10.1049/iet-ipr.2018.5283", "10.1007/978-981-10-7898-9_8"], "referencing": ["10.1109/TMM.2012.2228475", "10.1109/TMM.2013.2286817", "10.1109/MCG.2012.123", "10.1109/TPAMI.2013.46", "10.1109/TCSVT.2013.2291282", "10.1109/LSP.2011.2165337", "10.1109/TPAMI.2012.120", "10.1109/CVPR.2011.5995344", "10.1109/TPAMI.2011.272", "10.1109/TMM.2012.2228475", "10.1109/TMM.2013.2286817", "10.1109/MCG.2012.123", "10.1109/TPAMI.2013.46", "10.1109/TCSVT.2013.2291282", "10.1109/LSP.2011.2165337", "10.1109/TPAMI.2012.120", "10.1109/CVPR.2011.5995344", "10.1109/TPAMI.2011.272", "10.1109/TMM.2012.2228475", "10.1109/TMM.2013.2286817", "10.1109/MCG.2012.123", "10.1109/TPAMI.2013.46", "10.1109/TCSVT.2013.2291282", "10.1109/LSP.2011.2165337", "10.1109/TPAMI.2012.120", "10.1109/CVPR.2011.5995344", "10.1109/TPAMI.2011.272", "10.1145/1882261.1866186", "10.1145/1882261.1866185", "10.1145/964696.964707", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1661412.1618472", "10.1145/1360612.1360651", "10.1145/2508363.2508403", "10.1145/1857907.1857910", "10.1145/1073204.1073263", "10.1145/1141911.1141921", "10.1145/1882261.1866186", "10.1145/1882261.1866185", "10.1145/964696.964707", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1661412.1618472", "10.1145/1360612.1360651", "10.1145/2508363.2508403", "10.1145/1857907.1857910", "10.1145/1073204.1073263", "10.1145/1141911.1141921", "10.1145/1882261.1866186", "10.1145/1882261.1866185", "10.1145/964696.964707", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1661412.1618472", "10.1145/1360612.1360651", "10.1145/2508363.2508403", "10.1145/1857907.1857910", "10.1145/1073204.1073263", "10.1145/1141911.1141921", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11390-012-1211-6", "10.1006/cgip.1993.1016", "10.1111/j.1467-8659.2008.01325.x", "10.1016/j.patcog.2011.03.005", "10.1007/s00371-008-0232-1", "10.1007/978-3-540-69736-7_72", "10.1007/s11554-013-0373-y", "10.1007/s00371-006-0078-3", "10.1111/j.1467-8659.2011.01881.x", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11390-012-1211-6", "10.1006/cgip.1993.1016", "10.1111/j.1467-8659.2008.01325.x", "10.1016/j.patcog.2011.03.005", "10.1007/s00371-008-0232-1", "10.1007/978-3-540-69736-7_72", "10.1007/s11554-013-0373-y", "10.1007/s00371-006-0078-3", "10.1111/j.1467-8659.2011.01881.x", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11390-012-1211-6", "10.1006/cgip.1993.1016", "10.1111/j.1467-8659.2008.01325.x", "10.1016/j.patcog.2011.03.005", "10.1007/s00371-008-0232-1", "10.1007/978-3-540-69736-7_72", "10.1007/s11554-013-0373-y", "10.1007/s00371-006-0078-3", "10.1111/j.1467-8659.2011.01881.x"]}, "10.1109/TVCG.2015.2430337": {"doi": "10.1109/TVCG.2015.2430337", "author": ["F. G. Fuchs", "J. M. Hjelmervik"], "title": "Interactive Isogeometric Volume Visualization with Pixel-Accurate Geometry", "year": "2016", "abstract": "A recent development, called isogeometric analysis, provides a unified approach for design, analysis and optimization of functional products in industry. Traditional volume rendering methods for inspecting the results from the numerical simulations cannot be applied directly to isogeometric models. We present a novel approach for interactive visualization of isogeometric analysis results, ensuring correct, i.e., pixel-accurate geometry of the volume including its bounding surfaces. The entire OpenGL pipeline is used in a multi-stage algorithm leveraging techniques from surface rendering, order-independent transparency, as well as theory and numerical methods for ordinary differential equations. We showcase the efficiency of our approach on different models relevant to industry, ranging from quality inspection of the parametrization of the geometry, to stress analysis in linear elasticity, to visualization of computational fluid dynamics results.", "keywords": ["CAD", "computational fluid dynamics", "computational geometry", "data visualisation", "differential equations", "elasticity", "pipeline processing", "product design", "rendering (computer graphics)", "stress analysis", "interactive isogeometric volume visualization", "functional product optimization", "functional product design", "functional product analysis", "interactive visualization", "isogeometric analysis", "pixel-accurate geometry", "bounding surfaces", "OpenGL pipeline", "multistage algorithm leveraging techniques", "surface rendering", "order-independent transparency", "numerical methods", "ordinary differential equations", "quality inspection", "computational fluid dynamic visualization", "stress analysis", "linear elasticity", "Geometry", "Rendering (computer graphics)", "Splines (mathematics)", "Visualization", "Approximation methods", "Solid modeling", "Mathematical model", "Volume visualization", "Isogeometric analysis", "Splines", "Roots of Nonlinear Equations", "Ordinary Differential Equations", "GPU", "Rendering", "Volume visualization", "isogeometric analysis", "splines", "roots of nonlinear equations", "ordinary differential equations", "GPU", "rendering"], "referenced_by": ["10.1109/MCG.2016.53"], "referencing": ["10.1109/38.511", "10.1109/TVCG.2011.103", "10.1109/TVCG.2014.2327977", "10.1109/38.511", "10.1109/TVCG.2011.103", "10.1109/TVCG.2014.2327977", "10.1109/38.511", "10.1109/TVCG.2011.103", "10.1109/TVCG.2014.2327977", "10.1145/800031.808585", "10.1145/383507.383515", "10.1145/965145.801287", "10.1145/1141911.1141939", "10.1145/37401.37422", "10.1145/376957.376984", "10.1145/2159616.2159644", "10.1145/800031.808585", "10.1145/383507.383515", "10.1145/965145.801287", "10.1145/1141911.1141939", "10.1145/37401.37422", "10.1145/376957.376984", "10.1145/2159616.2159644", "10.1145/800031.808585", "10.1145/383507.383515", "10.1145/965145.801287", "10.1145/1141911.1141939", "10.1145/37401.37422", "10.1145/376957.376984", "10.1145/2159616.2159644", "10.1016/0010-4485(94)00006-Y", "10.1007/978-1-4684-0364-0", "10.1002/9780470749081", "10.1007/978-0-387-21582-2", "10.1111/j.1467-8659.2008.01182.x", "10.1201/b10629", "10.1007/978-3-642-54382-1_14", "10.1201/b10685", "10.1007/978-1-4613-0003-8", "10.1111/j.1467-8659.2010.01725.x", "10.1016/0010-4485(94)00006-Y", "10.1007/978-1-4684-0364-0", "10.1002/9780470749081", "10.1007/978-0-387-21582-2", "10.1111/j.1467-8659.2008.01182.x", "10.1201/b10629", "10.1007/978-3-642-54382-1_14", "10.1201/b10685", "10.1007/978-1-4613-0003-8", "10.1111/j.1467-8659.2010.01725.x", "10.1016/0010-4485(94)00006-Y", "10.1007/978-1-4684-0364-0", "10.1002/9780470749081", "10.1007/978-0-387-21582-2", "10.1111/j.1467-8659.2008.01182.x", "10.1201/b10629", "10.1007/978-3-642-54382-1_14", "10.1201/b10685", "10.1007/978-1-4613-0003-8", "10.1111/j.1467-8659.2010.01725.x"]}, "10.1109/TVCG.2015.2430290": {"doi": "10.1109/TVCG.2015.2430290", "author": ["Y. Wang", "W. Peng"], "title": "Interactive Metro Map Editing", "year": "2016", "abstract": "Manual editing of a metro map is essential because many aesthetic and readability demands in map generation cannot be achieved by using a fully automatic method. In addition, a metro map should be updated when new metro lines are developed in a city. Considering that manually designing a metro map is time-consuming and requires expert skills, we present an interactive editing system that considers human knowledge and adjusts the layout to make it consistent with user expectations. In other words, only a few stations are controlled and the remaining stations are relocated by our system. Our system supports both curvilinear and octilinear layouts when creating metro maps. It solves an optimization problem, in which even spaces, route straightness, and maximum included angles at junctions are considered to obtain a curvilinear result. The system then rotates each edge to extend either vertically, horizontally, or diagonally while approximating the station positions provided by users to generate an octilinear layout. Experimental results, quantitative and qualitative evaluations, and user studies show that our editing system is easy to use and allows even non-professionals to design a metro map.", "keywords": ["cartography", "graph theory", "least squares approximations", "optimisation", "railways", "interactive metro map editing", "manual metro map editing", "map generation", "expert skills", "interactive editing system", "human knowledge", "curvilinear layouts", "octilinear layouts", "route straightness", "Optimization", "Junctions", "Visualization", "Linear programming", "Topology", "Urban areas", "Metro Map", "Interactive editing", "octilinear", "least squares optimization", "Metro map", "interactive editing", "octilinear", "least squares optimization"], "referenced_by": ["10.1109/TVCG.2016.2535234"], "referencing": ["10.1109/TVCG.2010.24", "10.1109/TVCG.2010.81", "10.1109/TVCG.2011.205", "10.1109/TVCG.2014.2312010", "10.1109/TVCG.2010.24", "10.1109/TVCG.2010.81", "10.1109/TVCG.2011.205", "10.1109/TVCG.2014.2312010", "10.1109/TVCG.2010.24", "10.1109/TVCG.2010.81", "10.1109/TVCG.2011.205", "10.1109/TVCG.2014.2312010", "10.1016/j.comgeo.2013.10.002", "10.1007/s00450-007-0036-y", "10.1007/978-3-540-70904-6_26", "10.1016/j.jvlc.2005.09.001", "10.1016/j.ijhcs.2012.09.004", "10.1111/j.1467-8659.2012.03085.x", "10.1111/cgf.12113", "10.1111/cgf.12383", "10.1080/13658816.2014.887718", "10.1016/j.comgeo.2013.10.002", "10.1007/s00450-007-0036-y", "10.1007/978-3-540-70904-6_26", "10.1016/j.jvlc.2005.09.001", "10.1016/j.ijhcs.2012.09.004", "10.1111/j.1467-8659.2012.03085.x", "10.1111/cgf.12113", "10.1111/cgf.12383", "10.1080/13658816.2014.887718", "10.1016/j.comgeo.2013.10.002", "10.1007/s00450-007-0036-y", "10.1007/978-3-540-70904-6_26", "10.1016/j.jvlc.2005.09.001", "10.1016/j.ijhcs.2012.09.004", "10.1111/j.1467-8659.2012.03085.x", "10.1111/cgf.12113", "10.1111/cgf.12383", "10.1080/13658816.2014.887718"]}, "10.1109/TVCG.2015.2430333": {"doi": "10.1109/TVCG.2015.2430333", "author": ["M. Arikan", "R. Preiner", "M. Wimmer"], "title": "Multi-Depth-Map Raytracing for Efficient Large-Scene Reconstruction", "year": "2016", "abstract": "With the enormous advances of the acquisition technology over the last years, fast processing and high-quality visualization of large point clouds have gained increasing attention. Commonly, a mesh surface is reconstructed from the point cloud and a high-resolution texture is generated over the mesh from the images taken at the site to represent surface materials. However, this global reconstruction and texturing approach becomes impractical with increasing data sizes. Recently, due to its potential for scalability and extensibility, a method for texturing a set of depth maps in a preprocessing and stitching them at runtime has been proposed to represent large scenes. However, the rendering performance of this method is strongly dependent on the number of depth maps and their resolution. Moreover, for the proposed scene representation, every single depth map has to be textured by the images, which in practice heavily increases processing costs. In this paper, we present a novel method to break these dependencies by introducing an efficient raytracing of multiple depth maps. In a preprocessing phase, we first generate high-resolution textured depth maps by rendering the input points from image cameras and then perform a graph-cut based optimization to assign a small subset of these points to the images. At runtime, we use the resulting point-to-image assignments (1) to identify for each view ray which depth map contains the closest ray-surface intersection and (2) to efficiently compute this intersection point. The resulting algorithm accelerates both the texturing and the rendering of the depth maps by an order of magnitude.", "keywords": ["cameras", "data visualisation", "graph theory", "image reconstruction", "image resolution", "image texture", "mesh generation", "optimisation", "ray tracing", "rendering (computer graphics)", "multidepth-map raytracing", "large-scene reconstruction", "high-quality visualization", "mesh surface", "high-resolution texture", "image texture", "image representation", "surface material representation", "rendering performance", "high-resolution textured depth map", "graph-cut based optimization", "closest ray-surface intersection", "Rendering (computer graphics)", "Labeling", "Three-dimensional displays", "Image reconstruction", "Surface texture", "Surface reconstruction", "Runtime", "Point-based rendering", "raytracing depth maps", "large-scale models", "Point-based rendering", "raytracing depth maps", "large-scale models"], "referenced_by": ["10.1109/TVCG.2018.2794071", "10.1109/IVS.2018.8500494", "10.1145/2980179.2982420", "10.1145/3272127.3275084", "10.4236/am.2016.77064"], "referencing": ["10.1109/TVCG.2014.2312011", "10.1109/TVCG.2014.2312011", "10.1109/TVCG.2014.2312011", "10.1145/2487228.2487237", "10.1145/2487228.2487237", "10.1145/2487228.2487237", "10.1111/j.1467-8659.2009.01617.x", "10.1016/j.cag.2011.01.004", "10.1111/j.1467-8659.2009.01617.x", "10.1016/j.cag.2011.01.004", "10.1111/j.1467-8659.2009.01617.x", "10.1016/j.cag.2011.01.004"]}, "10.1109/TVCG.2015.2417575": {"doi": "10.1109/TVCG.2015.2417575", "author": ["L. Yu", "S. Yeung", "D. Terzopoulos"], "title": "The Clutterpalette: An Interactive Tool for Detailing Indoor Scenes", "year": "2016", "abstract": "We introduce the Clutterpalette, an interactive tool for detailing indoor scenes with small-scale items. When the user points to a location in the scene, the Clutterpalette suggests detail items for that location. In order to present appropriate suggestions, the Clutterpalette is trained on a dataset of images of real-world scenes, annotated with support relations. Our experiments demonstrate that the adaptive suggestions presented by the Clutterpalette increase modeling speed and enhance the realism of indoor scenes.", "keywords": ["indoor environment", "interactive systems", "natural scenes", "Clutterpalette", "interactive tool", "indoor scenes", "small-scale items", "real-world scenes", "Clutter", "Three-dimensional displays", "Training", "Computational modeling", "Computer graphics", "Solid modeling", "Training data", "interactive 3D modeling,", "scene modeling", "scene understanding", "indoor scenes", "modeling tools", "suggestive user interfaces", "Interactive 3D modeling", "scene modeling", "scene understanding", "indoor scenes", "modeling tools", "suggestive user interfaces"], "referenced_by": ["10.1109/TVCG.2017.2656958", "10.1109/CW.2019.00014", "10.1145/3272127.3275035", "10.1145/3013971.3014002", "10.1007/978-3-319-69487-0_10", "10.1007/s00371-017-1394-5", "10.1007/s41095-018-0110-3", "10.1007/s11263-018-1103-5", "10.1007/s11390-019-1929-5", "10.1111/cgf.14020", "10.1016/j.cag.2020.08.002", "10.1111/cgf.14166"], "referencing": ["10.1145/800179.810236", "10.1145/199404.199427", "10.1145/1882261.1866204", "10.1145/2010324.1964929", "10.1145/2185520.2185552", "10.1145/2461912.2461968", "10.1145/1882261.1866205", "10.1145/1730804.1730833", "10.1145/2366145.2366162", "10.1145/2185520.2185582", "10.1145/2366145.2366153", "10.1145/800179.810236", "10.1145/199404.199427", "10.1145/1882261.1866204", "10.1145/2010324.1964929", "10.1145/2185520.2185552", "10.1145/2461912.2461968", "10.1145/1882261.1866205", "10.1145/1730804.1730833", "10.1145/2366145.2366162", "10.1145/2185520.2185582", "10.1145/2366145.2366153", "10.1145/800179.810236", "10.1145/199404.199427", "10.1145/1882261.1866204", "10.1145/2010324.1964929", "10.1145/2185520.2185552", "10.1145/2461912.2461968", "10.1145/1882261.1866205", "10.1145/1730804.1730833", "10.1145/2366145.2366162", "10.1145/2185520.2185582", "10.1145/2366145.2366153"]}, "10.1109/TVCG.2015.2424878": {"doi": "10.1109/TVCG.2015.2424878", "author": ["N. Cao", "Y. Lin", "D. Gotz"], "title": "UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data", "year": "2016", "abstract": "Data with multiple probabilistic labels are common in many situations. For example, a movie may be associated with multiple genres with different levels of confidence. Despite their ubiquity, the problem of visualizing probabilistic labels has not been adequately addressed. Existing approaches often either discard the probabilistic information, or map the data to a low-dimensional subspace where their associations with original labels are obscured. In this paper, we propose a novel visual technique, UnTangle Map, for visualizing probabilistic multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle Map provides both (a) an automatic label placement algorithm, and (b) adaptive interactions that allow users to control the label positioning for different information needs. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their probabilistic labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of probabilistic information in the data labels.", "keywords": ["data visualisation", "probability", "UnTangle Map", "visual analysis", "data mapping", "low-dimensional subspace", "visual technique", "probabilistic multilabel data visualization", "data items", "triangle vertices", "automatic label placement algorithm", "adaptive interactions", "label positioning control", "Data visualization", "Probabilistic logic", "Visualization", "Layout", "Distributed databases", "Motion pictures", "Data models", "Visualization", "Multidimensional Visualization", "Probability Vector", "Visualization", "multidimensional visualization", "probability vector"], "referenced_by": ["10.1109/TVCG.2017.2744683", "10.1109/VAST.2016.7883510", "10.1109/TVCG.2015.2467196", "10.1109/TVCG.2018.2865020", "10.1109/ACCESS.2019.2942844", "10.1145/3183347", "10.1371/journal.pone.0194136", "10.3390/ijgi7060225", "10.1007/s12650-018-0518-y", "10.1111/cgf.13722", "10.1007/s12650-019-00577-2", "10.1007/s11390-020-0271-2"], "referencing": ["10.1109/VISUAL.1990.146402", "10.1109/IV.2009.74", "10.1109/TVCG.2008.153", "10.1109/TFUZZ.2003.812696", "10.1109/2945.841119", "10.1109/INFVIS.2004.1", "10.1109/TVCG.2011.186", "10.1109/TVCG.2009.122", "10.1109/TVCG.2010.210", "10.1109/TVCG.2010.28", "10.1109/TVCG.2011.239", "10.1109/TVCG.2009.140", "10.1109/INFVIS.1995.528686", "10.1109/TSMCB.2005.850151", "10.1109/IV.2009.45", "10.1109/TVCG.2014.2346660", "10.1109/VISUAL.1990.146402", "10.1109/IV.2009.74", "10.1109/TVCG.2008.153", "10.1109/TFUZZ.2003.812696", "10.1109/2945.841119", "10.1109/INFVIS.2004.1", "10.1109/TVCG.2011.186", "10.1109/TVCG.2009.122", "10.1109/TVCG.2010.210", "10.1109/TVCG.2010.28", "10.1109/TVCG.2011.239", "10.1109/TVCG.2009.140", "10.1109/INFVIS.1995.528686", "10.1109/TSMCB.2005.850151", "10.1109/IV.2009.45", "10.1109/TVCG.2014.2346660", "10.1109/VISUAL.1990.146402", "10.1109/IV.2009.74", "10.1109/TVCG.2008.153", "10.1109/TFUZZ.2003.812696", "10.1109/2945.841119", "10.1109/INFVIS.2004.1", "10.1109/TVCG.2011.186", "10.1109/TVCG.2009.122", "10.1109/TVCG.2010.210", "10.1109/TVCG.2010.28", "10.1109/TVCG.2011.239", "10.1109/TVCG.2009.140", "10.1109/INFVIS.1995.528686", "10.1109/TSMCB.2005.850151", "10.1109/IV.2009.45", "10.1109/TVCG.2014.2346660", "10.1145/2133806.2133826", "10.1145/1401890.1401937", "10.1145/775047.775143", "10.1145/2133806.2133826", "10.1145/1401890.1401937", "10.1145/775047.775143", "10.1145/2133806.2133826", "10.1145/1401890.1401937", "10.1145/775047.775143", "10.1016/0895-7177(93)90202-A", "10.1086/227352", "10.2307/2289444", "10.1111/j.1467-8659.2008.01241.x", "10.1111/j.1467-8659.2009.01666.x", "10.1002/wics.101", "10.1016/0377-0427(87)90125-7", "10.1016/j.fss.2004.07.009", "10.1016/j.jda.2011.12.009", "10.1111/cgf.12093", "10.1186/1471-2105-15-S6-S4", "10.1016/S0148-9062(98)00011-4", "10.1137/1.9781611972801.37", "10.1111/j.1467-8659.2009.01452.x", "10.1002/aris.1440380105", "10.1057/palgrave.ivs.9500023", "10.1038/44565", "10.1016/0895-7177(93)90202-A", "10.1086/227352", "10.2307/2289444", "10.1111/j.1467-8659.2008.01241.x", "10.1111/j.1467-8659.2009.01666.x", "10.1002/wics.101", "10.1016/0377-0427(87)90125-7", "10.1016/j.fss.2004.07.009", "10.1016/j.jda.2011.12.009", "10.1111/cgf.12093", "10.1186/1471-2105-15-S6-S4", "10.1016/S0148-9062(98)00011-4", "10.1137/1.9781611972801.37", "10.1111/j.1467-8659.2009.01452.x", "10.1002/aris.1440380105", "10.1057/palgrave.ivs.9500023", "10.1038/44565", "10.1016/0895-7177(93)90202-A", "10.1086/227352", "10.2307/2289444", "10.1111/j.1467-8659.2008.01241.x", "10.1111/j.1467-8659.2009.01666.x", "10.1002/wics.101", "10.1016/0377-0427(87)90125-7", "10.1016/j.fss.2004.07.009", "10.1016/j.jda.2011.12.009", "10.1111/cgf.12093", "10.1186/1471-2105-15-S6-S4", "10.1016/S0148-9062(98)00011-4", "10.1137/1.9781611972801.37", "10.1111/j.1467-8659.2009.01452.x", "10.1002/aris.1440380105", "10.1057/palgrave.ivs.9500023", "10.1038/44565"]}, "10.1109/TVCG.2015.2430343": {"doi": "10.1109/TVCG.2015.2430343", "author": ["J. M. Noguera", "J. R. Jim\u00e9nez"], "title": "Mobile Volume Rendering: Past, Present and Future", "year": "2016", "abstract": "Volume rendering has been a relevant topic in scientific visualization for the last decades. However, the exploration of reasonably big volume datasets requires considerable computing power, which has limited this field to the desktop scenario. But the recent advances in mobile graphics hardware have motivated the research community to overcome these restrictions and to bring volume graphics to these ubiquitous handheld platforms. This survey presents the past and present work on mobile volume rendering, and is meant to serve as an overview and introduction to the field. It proposes a classification of the current efforts and covers aspects such as advantages and issues of the mobile platforms, rendering strategies, performance and user interfaces. The paper ends by highlighting promising research directions to motivate the development of new and interesting mobile volume solutions.", "keywords": ["data visualisation", "mobile computing", "rendering (computer graphics)", "scientific visualization", "big volume datasets", "mobile graphics hardware", "mobile volume rendering", "volume graphics", "ubiquitous handheld platforms", "mobile volume solutions", "Rendering (computer graphics)", "Mobile communication", "Mobile handsets", "Servers", "Three-dimensional displays", "Graphics processing units", "Mobile computing", "Computer graphics", "Volume rendering", "Medical imaging", "Data compression", "Cloud computing", "Mobile computing", "computer graphics", "volume rendering", "medical imaging", "data compression", "cloud computing"], "referenced_by": ["10.1109/CISP-BMEI.2016.7852799", "10.1109/MCG.2016.99", "10.1109/TMM.2017.2757759", "10.1109/SEARIS41720.2017.9183509", "10.1109/CCECE47787.2020.9255742", "10.1145/3083187.3084015", "10.1016/j.cag.2018.02.007", "10.1080/21681163.2016.1253505", "10.1016/j.imu.2018.10.010", "10.1111/cgf.13671", "10.1016/j.imu.2019.100253"], "referencing": ["10.1109/JPROC.2008.917719", "10.1109/MCG.2008.83", "10.1109/TVCG.2007.29", "10.1109/TVCG.2012.92", "10.1109/38.511", "10.1109/38.920623", "10.1109/TVCG.2002.1021579", "10.1109/JPROC.2008.917719", "10.1109/MCG.2008.83", "10.1109/TVCG.2007.29", "10.1109/TVCG.2012.92", "10.1109/38.511", "10.1109/38.920623", "10.1109/TVCG.2002.1021579", "10.1109/JPROC.2008.917719", "10.1109/MCG.2008.83", "10.1109/TVCG.2007.29", "10.1109/TVCG.2012.92", "10.1109/38.511", "10.1109/38.920623", "10.1109/TVCG.2002.1021579", "10.1145/2010425.2010449", "10.1145/197938.197961", "10.1145/346876.348238", "10.1145/2543581.2543594", "10.1145/378456.378497", "10.1145/1152399.1152430", "10.1145/2010425.2010449", "10.1145/197938.197961", "10.1145/346876.348238", "10.1145/2543581.2543594", "10.1145/378456.378497", "10.1145/1152399.1152430", "10.1145/2010425.2010449", "10.1145/197938.197961", "10.1145/346876.348238", "10.1145/2543581.2543594", "10.1145/378456.378497", "10.1145/1152399.1152430", "10.1111/cgf.12280", "10.1007/s00779-012-0595-1", "10.1007/s10916-009-9386-2", "10.1007/s00330-006-0153-1", "10.1007/978-1-4471-6497-5_23", "10.1007/s00371-007-0117-8", "10.1016/j.cmpb.2007.11.012", "10.1007/s13319-015-0040-0", "10.1371/journal.pone.0007974", "10.1007/s00779-012-0596-0", "10.1016/j.ijinfomgt.2013.11.005", "10.1016/j.compedu.2013.07.007", "10.1007/s11277-013-1354-y", "10.1016/j.cag.2014.02.002", "10.1007/s10140-012-1037-0", "10.1111/1467-8659.00298", "10.1016/j.compedu.2012.03.016", "10.1002/ase.58", "10.1007/s00779-012-0594-2", "10.1007/PL00013406", "10.1007/s13244-013-0274-4", "10.1016/j.cag.2012.01.002", "10.1111/cgf.12280", "10.1007/s00779-012-0595-1", "10.1007/s10916-009-9386-2", "10.1007/s00330-006-0153-1", "10.1007/978-1-4471-6497-5_23", "10.1007/s00371-007-0117-8", "10.1016/j.cmpb.2007.11.012", "10.1007/s13319-015-0040-0", "10.1371/journal.pone.0007974", "10.1007/s00779-012-0596-0", "10.1016/j.ijinfomgt.2013.11.005", "10.1016/j.compedu.2013.07.007", "10.1007/s11277-013-1354-y", "10.1016/j.cag.2014.02.002", "10.1007/s10140-012-1037-0", "10.1111/1467-8659.00298", "10.1016/j.compedu.2012.03.016", "10.1002/ase.58", "10.1007/s00779-012-0594-2", "10.1007/PL00013406", "10.1007/s13244-013-0274-4", "10.1016/j.cag.2012.01.002", "10.1111/cgf.12280", "10.1007/s00779-012-0595-1", "10.1007/s10916-009-9386-2", "10.1007/s00330-006-0153-1", "10.1007/978-1-4471-6497-5_23", "10.1007/s00371-007-0117-8", "10.1016/j.cmpb.2007.11.012", "10.1007/s13319-015-0040-0", "10.1371/journal.pone.0007974", "10.1007/s00779-012-0596-0", "10.1016/j.ijinfomgt.2013.11.005", "10.1016/j.compedu.2013.07.007", "10.1007/s11277-013-1354-y", "10.1016/j.cag.2014.02.002", "10.1007/s10140-012-1037-0", "10.1111/1467-8659.00298", "10.1016/j.compedu.2012.03.016", "10.1002/ase.58", "10.1007/s00779-012-0594-2", "10.1007/PL00013406", "10.1007/s13244-013-0274-4", "10.1016/j.cag.2012.01.002"]}, "10.1109/TVCG.2015.2498798": {"doi": "10.1109/TVCG.2015.2498798", "author": [""], "title": "2015 Index IEEE Transactions on Visualization and Computer Graphics Vol. 21", "year": "2016", "abstract": "This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.", "keywords": ["Indexes"], "referenced_by": [], "referencing": []}}