{"10.1109/TVCG.2018.2805110": {"doi": "10.1109/TVCG.2018.2805110", "author": [""], "title": "IEEE Transactions on Visualization and Computer Graphics", "year": "2018", "abstract": "Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805138": {"doi": "10.1109/TVCG.2018.2805138", "author": [""], "title": "Contents", "year": "2018", "abstract": "Presents the table of contents for this issue of the publication.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805123": {"doi": "10.1109/TVCG.2018.2805123", "author": ["L. De Floriani", "D. Schmalstieg"], "title": "Introducing the IEEE Virtual Reality 2018 Special Issue", "year": "2018", "abstract": "This special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG) contains the 29 full papers selected for the IEEE Virtual Reality and 3D User Interfaces (IEEE VR 2018) Conference held in Reutlingen, Germany, March 18-22, 2017. Since its inception in 1993, IEEE VR has been the premier venue to present new research results in the field of Virtual Reality (VR). The strong current trends toward VR systems for consumer audiences heightens the importance of this event. This fact is reflected in the cooperation between TVCG and IEEE VR, which is in its seventh year and is one cornerstone of the strategy of TVCG to combine computer graphics and data visualization in its scope with virtual and augmented reality. The special issue format combines speed of publication with all the established advantages of an archival journal. To that end, a rigorous and competitive two-round review process was performed to ensure the highest quality.", "keywords": ["Special issues and sections", "Meetings", "Virtual reality"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805171": {"doi": "10.1109/TVCG.2018.2805171", "author": ["K. Kiyokawa", "F. Steinicke", "B. Thomas", "G. Welch"], "title": "Preface", "year": "2018", "abstract": "We are pleased to present the technical papers for IEEE VR 2018: the 25th IEEE Conference on Virtual Reality and 3D User Interfaces, held March 18-22, 2018 in Reutlingen, Germany. IEEE VR 2018 features two categories of submissions: (i) VR Journal Papers, and (ii) VR Conference Papers. Both categories have their own program committees, submission processes, and review processes. This year, 178 submissions were submitted to the Journal track from which 29 were accepted as articles to IEEE TVCG (16.3%). All of these will be presented at the IEEE VR 2018 conference, along with 6 additional papers in the VR area that were published in IEEE TVCG during the past year. Furthermore, 6 submission (3.4%) were recommended for a regular issue of TVCG with major revisions with reviewer continuity. Each of the papers in this special issue went through a rigorous two-round review process. All accepted journal papers are published in a special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG). The Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces contains all of the accepted conference papers and the poster abstracts.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805140": {"doi": "10.1109/TVCG.2018.2805140", "author": ["C. T. Silva"], "title": "IEEE Visualization and Graphics Technical Committee (VGTC)", "year": "2018", "abstract": "Presents a listing of the IEEE Visualization and Graphics Technical Committee (VGTC).", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805143": {"doi": "10.1109/TVCG.2018.2805143", "author": [""], "title": "Conference Committee", "year": "2018", "abstract": "Presents a listing of the 2018 Virtual Reality Conference committee.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805144": {"doi": "10.1109/TVCG.2018.2805144", "author": [""], "title": "International Program Committee for Journal Papers", "year": "2018", "abstract": "Preesnts information on the 2018 Virtual RealtyConference international program committee.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2805145": {"doi": "10.1109/TVCG.2018.2805145", "author": [""], "title": "Papers Reviewers", "year": "2018", "abstract": "Presents paper reviewers for the 2018 Virtual Realty Conference.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2794072": {"doi": "10.1109/TVCG.2018.2794072", "author": ["H. Adams", "G. Narasimham", "J. Rieser", "S. Creem-Regehr", "J. Stefanucci", "B. Bodenheimer"], "title": "Locomotive Recalibration and Prism Adaptation of Children and Teens in Immersive Virtual Environments", "year": "2018", "abstract": "As virtual reality expands in popularity, an increasingly diverse audience is gaining exposure to immersive virtual environments (IVEs). A significant body of research has demonstrated how perception and action work in such environments, but most of this work has been done studying adults. Less is known about how physical and cognitive development affect perception and action in IVEs, particularly as applied to preteen and teenage children. Accordingly, in the current study we assess how preteens (children aged 8-12 years) and teenagers (children aged 15-18 years) respond to mismatches between their motor behavior and the visual information presented by an IVE. Over two experiments, we evaluate how these individuals recalibrate their actions across functionally distinct systems of movement. The first experiment analyzed forward walking recalibration after exposure to an IVE with either increased or decreased visual flow. Visual flow during normal bipedal locomotion was manipulated to be either twice or half as fast as the physical gait. The second experiment leveraged a prism throwing adaptation paradigm to test the effect of recalibration on throwing movement. In the first experiment, our results show no differences across age groups, although subjects generally experienced a post-exposure effect of shortened distance estimation after experiencing visually faster flow and longer distance estimation after experiencing visually slower flow. In the second experiment, subjects generally showed the typical prism adaptation behavior of a throwing after-effect error. The error lasted longer for preteens than older children. Our results have implications for the design of virtual systems with children as a target audience.", "keywords": ["gait analysis", "virtual reality", "visual perception", "teenage children", "visual information", "normal bipedal locomotion", "physical gait", "adaptation paradigm", "throwing movement", "post-exposure effect", "visually faster flow", "visually slower flow", "throwing after-effect error", "virtual systems", "locomotive recalibration", "immersive virtual environments", "action work", "physical development", "cognitive development", "visual flow", "virtual reality", "prism adaptation behavior", "time 8 year to 12 year", "time 15 year to 18 year", "Legged locomotion", "Visualization", "Virtual environments", "Biomechanics", "Calibration", "Electronic mail", "Virtual environments", "perceptual-motor recalibration", "perception", "children", "Adaptation, Physiological", "Adolescent", "Child", "Female", "Humans", "Male", "Perception", "Psychomotor Performance", "User-Computer Interface", "Virtual Reality", "Walking"], "referenced_by": ["IKEY:9089559", "IKEY:9089552"], "referencing": ["IKEY:5374395", "IKEY:4811208", "IKEY:7131732", "IKEY:4161000", "IKEY:6777445", "IKEY:5072212", "IKEY:5759455", "IKEY:5374395", "IKEY:4811208", "IKEY:7131732", "IKEY:4161000", "IKEY:6777445", "IKEY:5072212", "IKEY:5759455", "IKEY:5374395", "IKEY:4811208", "IKEY:7131732", "IKEY:4161000", "IKEY:6777445", "IKEY:5072212", "IKEY:5759455", "10.1145/1272582.1272589", "10.1145/2628257.2628268", "10.1145/1823738.1823744", "10.1145/2043603.2043604", "10.1145/2983631", "10.1145/2338676.2338701", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/1402236.1402241", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1643928.1643943", "10.1145/973801.973818", "10.1145/1140491.1140495", "10.1145/2628257.2628261", "10.1145/1272582.1272589", "10.1145/2628257.2628268", "10.1145/1823738.1823744", "10.1145/2043603.2043604", "10.1145/2983631", "10.1145/2338676.2338701", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/1402236.1402241", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1643928.1643943", "10.1145/973801.973818", "10.1145/1140491.1140495", "10.1145/2628257.2628261", "10.1145/1272582.1272589", "10.1145/2628257.2628268", "10.1145/1823738.1823744", "10.1145/2043603.2043604", "10.1145/2983631", "10.1145/2338676.2338701", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/1402236.1402241", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1643928.1643943", "10.1145/973801.973818", "10.1145/1140491.1140495", "10.1145/2628257.2628261", "10.1037/0096-1523.26.3.1148", "10.1073/pnas.1306779110", "10.1177/0956797610372635", "10.1111/j.1442-9071.1997.tb01743.x", "10.1016/j.jecp.2013.12.006", "10.1016/j.jecp.2013.12.006", "10.1017/S1355617703940045", "10.3758/BF03209249", "10.1007/PL00005680", "10.1046/j.0956-7976.2003.psci_1466.x", "10.1037/0096-1523.31.3.398", "10.1016/j.visres.2010.09.019", "10.1037/a0029716", "10.1162/105474698565839", "10.1037/h0022611", "10.1016/j.jenvp.2004.08.003", "10.3758/s13414-013-0503-4", "10.1162/1054746042545238", "10.1068/p7545", "10.1371/journal.pone.0054446", "10.1068/p7929", "10.1016/B978-012156651-7/50013-7", "10.1037/0096-1523.18.4.906", "10.1093/brain/119.4.1183", "10.1162/pres.19.3.230", "10.1136/injuryprev-2014-041508", "10.1093/jpepsy/jsv078", "10.1136/injuryprev-2014-041357", "10.1093/jpepsy/jsv019", "10.1016/j.cobeha.2016.04.010", "10.1038/srep29163", "10.1111/j.1467-8721.2007.00515.x", "10.1016/j.jecp.2010.07.005", "10.1037//0096-1523.22.2.379", "10.1080/00222890109601923", "10.1080/00222890209601935", "10.1068/p190675", "10.1037//0096-1523.21.3.480", "10.1152/jn.1995.74.1.457", "10.1162/105474699566215", "10.1006/ijhc.2001.0473", "10.1080/09541440802426465", "10.1016/j.aap.2008.03.005", "10.1098/rstb.2009.0138", "10.1093/jpepsy/jss116", "10.1037/0003-066X.50.2.79", "10.1162/1054746042545292", "10.1518/001872098779591340", "10.3758/s13414-013-0473-6", "10.1037/0096-1523.26.3.1148", "10.1073/pnas.1306779110", "10.1177/0956797610372635", "10.1111/j.1442-9071.1997.tb01743.x", "10.1016/j.jecp.2013.12.006", "10.1016/j.jecp.2013.12.006", "10.1017/S1355617703940045", "10.3758/BF03209249", "10.1007/PL00005680", "10.1046/j.0956-7976.2003.psci_1466.x", "10.1037/0096-1523.31.3.398", "10.1016/j.visres.2010.09.019", "10.1037/a0029716", "10.1162/105474698565839", "10.1037/h0022611", "10.1016/j.jenvp.2004.08.003", "10.3758/s13414-013-0503-4", "10.1162/1054746042545238", "10.1068/p7545", "10.1371/journal.pone.0054446", "10.1068/p7929", "10.1016/B978-012156651-7/50013-7", "10.1037/0096-1523.18.4.906", "10.1093/brain/119.4.1183", "10.1162/pres.19.3.230", "10.1136/injuryprev-2014-041508", "10.1093/jpepsy/jsv078", "10.1136/injuryprev-2014-041357", "10.1093/jpepsy/jsv019", "10.1016/j.cobeha.2016.04.010", "10.1038/srep29163", "10.1111/j.1467-8721.2007.00515.x", "10.1016/j.jecp.2010.07.005", "10.1037//0096-1523.22.2.379", "10.1080/00222890109601923", "10.1080/00222890209601935", "10.1068/p190675", "10.1037//0096-1523.21.3.480", "10.1152/jn.1995.74.1.457", "10.1162/105474699566215", "10.1006/ijhc.2001.0473", "10.1080/09541440802426465", "10.1016/j.aap.2008.03.005", "10.1098/rstb.2009.0138", "10.1093/jpepsy/jss116", "10.1037/0003-066X.50.2.79", "10.1162/1054746042545292", "10.1518/001872098779591340", "10.3758/s13414-013-0473-6", "10.1037/0096-1523.26.3.1148", "10.1073/pnas.1306779110", "10.1177/0956797610372635", "10.1111/j.1442-9071.1997.tb01743.x", "10.1016/j.jecp.2013.12.006", "10.1016/j.jecp.2013.12.006", "10.1017/S1355617703940045", "10.3758/BF03209249", "10.1007/PL00005680", "10.1046/j.0956-7976.2003.psci_1466.x", "10.1037/0096-1523.31.3.398", "10.1016/j.visres.2010.09.019", "10.1037/a0029716", "10.1162/105474698565839", "10.1037/h0022611", "10.1016/j.jenvp.2004.08.003", "10.3758/s13414-013-0503-4", "10.1162/1054746042545238", "10.1068/p7545", "10.1371/journal.pone.0054446", "10.1068/p7929", "10.1016/B978-012156651-7/50013-7", "10.1037/0096-1523.18.4.906", "10.1093/brain/119.4.1183", "10.1162/pres.19.3.230", "10.1136/injuryprev-2014-041508", "10.1093/jpepsy/jsv078", "10.1136/injuryprev-2014-041357", "10.1093/jpepsy/jsv019", "10.1016/j.cobeha.2016.04.010", "10.1038/srep29163", "10.1111/j.1467-8721.2007.00515.x", "10.1016/j.jecp.2010.07.005", "10.1037//0096-1523.22.2.379", "10.1080/00222890109601923", "10.1080/00222890209601935", "10.1068/p190675", "10.1037//0096-1523.21.3.480", "10.1152/jn.1995.74.1.457", "10.1162/105474699566215", "10.1006/ijhc.2001.0473", "10.1080/09541440802426465", "10.1016/j.aap.2008.03.005", "10.1098/rstb.2009.0138", "10.1093/jpepsy/jss116", "10.1037/0003-066X.50.2.79", "10.1162/1054746042545292", "10.1518/001872098779591340", "10.3758/s13414-013-0473-6"]}, "10.1109/TVCG.2018.2794639": {"doi": "10.1109/TVCG.2018.2794639", "author": ["A. Bhargava", "J. W. Bertrand", "A. K. Gramopadhye", "K. C. Madathil", "S. V. Babu"], "title": "Evaluating Multiple Levels of an Interaction Fidelity Continuum on Performance and Learning in Near-Field Training Simulations", "year": "2018", "abstract": "With costs of head-mounted displays (HMDs) and tracking technology decreasing rapidly, various virtual reality applications are being widely adopted for education and training. Hardware advancements have enabled replication of real-world interactions in virtual environments to a large extent, paving the way for commercial grade applications that provide a safe and risk-free training environment at a fraction of the cost. But this also mandates the need to develop more intrinsic interaction techniques and to empirically evaluate them in a more comprehensive manner. Although there exists a body of previous research that examines the benefits of selected levels of interaction fidelity on performance, few studies have investigated the constituent components of fidelity in a Interaction Fidelity Continuum (IFC) with several system instances and their respective effects on performance and learning in the context of a real-world skills training application. Our work describes a large between-subjects investigation conducted over several years that utilizes bimanual interaction metaphors at six discrete levels of interaction fidelity to teach basic precision metrology concepts in a near-field spatial interaction task in VR. A combined analysis performed on the data compares and contrasts the six different conditions and their overall effects on performance and learning outcomes, eliciting patterns in the results between the discrete application points on the IFC. With respect to some performance variables, results indicate that simpler restrictive interaction metaphors and highest fidelity metaphors perform better than medium fidelity interaction metaphors. In light of these results, a set of general guidelines are created for developers of spatial interaction metaphors in immersive virtual environments for precise fine-motor skills training simulations.", "keywords": ["computer based training", "helmet mounted displays", "virtual reality", "Interaction Fidelity Continuum", "near-field training simulations", "virtual reality applications", "education", "real-world interactions", "safe risk-free training environment", "intrinsic interaction techniques", "near-field spatial interaction task", "learning outcomes", "discrete application points", "performance variables", "medium fidelity interaction metaphors", "spatial interaction metaphors", "immersive virtual environments", "fine-motor skills training simulations", "bimanual interaction metaphors", "restrictive interaction metaphors", "head-mounted displays", "tracking technology", "Training", "Aerospace electronics", "Solid modeling", "Metrology", "Mice", "Virtual environments", "Bimanual Interaction", "Interaction Fidelity", "Empirical Evaluation", "Educational Virtual Reality", "Adolescent", "Adult", "Cognition", "Female", "High Fidelity Simulation Training", "Humans", "Male", "Surveys and Questionnaires", "Task Performance and Analysis", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8978263", "IKEY:9090673"], "referencing": ["IKEY:7223317", "IKEY:1364592", "IKEY:7223386", "IKEY:6184195", "IKEY:7223317", "IKEY:1364592", "IKEY:7223386", "IKEY:6184195", "IKEY:7223317", "IKEY:1364592", "IKEY:7223386", "IKEY:6184195", "10.1145/22339.22390", "10.1145/2047196.2047248", "10.1145/286498.286720", "10.1145/192426.192501", "10.1145/253284.253318", "10.1145/292834.292849", "10.1145/300520.300522", "10.1145/237091.237102", "10.1145/22339.22390", "10.1145/2047196.2047248", "10.1145/286498.286720", "10.1145/192426.192501", "10.1145/253284.253318", "10.1145/292834.292849", "10.1145/300520.300522", "10.1145/237091.237102", "10.1145/22339.22390", "10.1145/2047196.2047248", "10.1145/286498.286720", "10.1145/192426.192501", "10.1145/253284.253318", "10.1145/292834.292849", "10.1145/300520.300522", "10.1145/237091.237102", "10.1016/j.cag.2012.12.003", "10.1006/jvlc.1998.0111", "10.1080/07370020802278163", "10.1080/00222895.1987.10735426", "10.1097/01.sla.0000154552.89886.91", "10.1080/10447319509526110", "10.1007/978-3-319-39907-2_6", "10.1111/j.1365-2923.2012.04243.x", "10.21236/ADA280011", "10.1162/105474698565631", "10.1162/105474698565686", "10.1016/j.cag.2012.12.003", "10.1006/jvlc.1998.0111", "10.1080/07370020802278163", "10.1080/00222895.1987.10735426", "10.1097/01.sla.0000154552.89886.91", "10.1080/10447319509526110", "10.1007/978-3-319-39907-2_6", "10.1111/j.1365-2923.2012.04243.x", "10.21236/ADA280011", "10.1162/105474698565631", "10.1162/105474698565686", "10.1016/j.cag.2012.12.003", "10.1006/jvlc.1998.0111", "10.1080/07370020802278163", "10.1080/00222895.1987.10735426", "10.1097/01.sla.0000154552.89886.91", "10.1080/10447319509526110", "10.1007/978-3-319-39907-2_6", "10.1111/j.1365-2923.2012.04243.x", "10.21236/ADA280011", "10.1162/105474698565631", "10.1162/105474698565686"]}, "10.1109/TVCG.2018.2794658": {"doi": "10.1109/TVCG.2018.2794658", "author": ["S. Bovet", "H. G. Debarba", "B. Herbelin", "E. Molla", "R. Boulic"], "title": "The Critical Role of Self-Contact for Embodiment in Virtual Reality", "year": "2018", "abstract": "With the broad range of motion capture devices available on the market, it is now commonplace to directly control the limb movement of an avatar during immersion in a virtual environment. Here, we study how the subjective experience of embodying a full-body controlled avatar is influenced by motor alteration and self-contact mismatches. Self-contact is in particular a strong source of passive haptic feedback and we assume it to bring a clear benefit in terms of embodiment. For evaluating this hypothesis, we experimentally manipulate self-contacts and the virtual hand displacement relatively to the body. We introduce these body posture transformations to experimentally reproduce the imperfect or incorrect mapping between real and virtual bodies, with the goal of quantifying the limits of acceptance for distorted mapping on the reported body ownership and agency. We first describe how we exploit egocentric coordinate representations to perform a motion capture ensuring that real and virtual hands coincide whenever the real hand is in contact with the body. Then, we present a pilot study that focuses on quantifying our sensitivity to visuo-tactile mismatches. The results are then used to design our main study with two factors, offset (for self-contact) and amplitude (for movement amplification). Our main result shows that subjects' embodiment remains important, even when an artificially amplified movement of the hand was performed, but provided that correct self-contacts are ensured.", "keywords": ["avatars", "haptic interfaces", "human computer interaction", "virtual reality", "motion capture devices", "full-body controlled avatar", "motor alteration", "self-contact mismatches", "passive haptic feedback", "virtual hand displacement", "body posture transformations", "virtual reality embodiment", "visuo-tactile mismatches", "Avatars", "Sensitivity", "Haptic interfaces", "Visualization", "Virtual environments", "Task analysis", "Virtual Reality", "Avatar", "Embodiment", "Agency", "Body Ownership", "Self-contact", "Adult", "Awareness", "Computer Graphics", "Female", "Hand", "Humans", "Male", "Movement", "Pilot Projects", "Signal Processing, Computer-Assisted", "Touch", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8446173", "IKEY:8448285", "IKEY:8797716"], "referencing": ["IKEY:5072212", "IKEY:5072212", "IKEY:5072212", "10.1145/2485895.2485905", "10.1145/1180495.1180499", "10.1145/2818998", "10.1145/1402236.1402238", "10.1145/237091.237102", "10.1145/1778765.1778829", "10.1145/2485895.2485905", "10.1145/1180495.1180499", "10.1145/2818998", "10.1145/1402236.1402238", "10.1145/237091.237102", "10.1145/1778765.1778829", "10.1145/2485895.2485905", "10.1145/1180495.1180499", "10.1145/2818998", "10.1145/1402236.1402238", "10.1145/237091.237102", "10.1145/1778765.1778829", "10.1016/j.gmod.2011.05.003", "10.1177/2041669517690163", "10.1038/nrn3292", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1038/35784", "10.1162/pres.2006.15.1.1", "10.1016/j.concog.2010.09.004", "10.1016/S0028-3932(98)00006-2", "10.3389/fpsyg.2015.01509", "10.1007/s10339-004-0013-3", "10.1179/1945511915Y.0000000007", "10.1016/S0166-4328(02)00384-4", "10.1162/PRES_a_00124", "10.1007/s11097-005-9015-6", "10.2522/ptj.20130579", "10.3758/BF03213053", "10.1111/j.1467-9450.1963.tb01326.x", "10.1186/s12984-017-0328-9", "10.1371/journal.pone.0010381", "10.1098/rstb.2009.0138", "10.1007/s00221-015-4202-3", "10.1016/j.gmod.2011.05.003", "10.1177/2041669517690163", "10.1038/nrn3292", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1038/35784", "10.1162/pres.2006.15.1.1", "10.1016/j.concog.2010.09.004", "10.1016/S0028-3932(98)00006-2", "10.3389/fpsyg.2015.01509", "10.1007/s10339-004-0013-3", "10.1179/1945511915Y.0000000007", "10.1016/S0166-4328(02)00384-4", "10.1162/PRES_a_00124", "10.1007/s11097-005-9015-6", "10.2522/ptj.20130579", "10.3758/BF03213053", "10.1111/j.1467-9450.1963.tb01326.x", "10.1186/s12984-017-0328-9", "10.1371/journal.pone.0010381", "10.1098/rstb.2009.0138", "10.1007/s00221-015-4202-3", "10.1016/j.gmod.2011.05.003", "10.1177/2041669517690163", "10.1038/nrn3292", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1038/35784", "10.1162/pres.2006.15.1.1", "10.1016/j.concog.2010.09.004", "10.1016/S0028-3932(98)00006-2", "10.3389/fpsyg.2015.01509", "10.1007/s10339-004-0013-3", "10.1179/1945511915Y.0000000007", "10.1016/S0166-4328(02)00384-4", "10.1162/PRES_a_00124", "10.1007/s11097-005-9015-6", "10.2522/ptj.20130579", "10.3758/BF03213053", "10.1111/j.1467-9450.1963.tb01326.x", "10.1186/s12984-017-0328-9", "10.1371/journal.pone.0010381", "10.1098/rstb.2009.0138", "10.1007/s00221-015-4202-3"]}, "10.1109/TVCG.2018.2794058": {"doi": "10.1109/TVCG.2018.2794058", "author": ["O. Erat", "W. A. Isop", "D. Kalkofen", "D. Schmalstieg"], "title": "Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas", "year": "2018", "abstract": "Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.", "keywords": ["augmented reality", "interactive systems", "mobile robots", "remotely operated vehicles", "rendering (computer graphics)", "robot vision", "user interfaces", "virtual reality", "drone", "dangerous areas", "impassable areas", "egocentric view", "narrow environments", "constrained environments", "exocentric view", "intuitive flight control", "exocentric viewpoint", "occluded environment", "indoor environment", "human vision", "exocentric control", "indoor environments", "drone-augmented human vision", "X-ray vision", "Drones", "Cameras", "Streaming media", "Three-dimensional displays", "Visualization", "Robot vision systems", "X-ray", "mixed reality", "hololens", "drone", "pick-and-place", "Adult", "Aircraft", "Computer Graphics", "Data Display", "Humans", "Imaging, Three-Dimensional", "Male", "Man-Machine Systems", "Video Recording", "Virtual Reality", "X-Rays", "Young Adult"], "referenced_by": ["IKEY:8468622", "IKEY:8626140", "IKEY:8642365", "IKEY:8709269", "IKEY:8794200", "IKEY:8782102", "IKEY:8793587", "IKEY:8798367", "IKEY:8794622", "IKEY:8943627", "IKEY:8943686", "IKEY:9074937", "IKEY:9196654", "IKEY:9223601", "IKEY:9288399", "10.1049/iet-smc.2020.0043"], "referencing": ["10.1145/2578153.2578156", "10.1145/2468356.2468721", "10.1145/3055031.3055084", "10.1145/2460625.2460661", "10.1145/2582051.2582097", "10.1145/258734.258747", "10.1145/223904.223938", "10.1145/2578153.2578156", "10.1145/2468356.2468721", "10.1145/3055031.3055084", "10.1145/2460625.2460661", "10.1145/2582051.2582097", "10.1145/258734.258747", "10.1145/223904.223938", "10.1145/2578153.2578156", "10.1145/2468356.2468721", "10.1145/3055031.3055084", "10.1145/2460625.2460661", "10.1145/2582051.2582097", "10.1145/258734.258747", "10.1145/223904.223938", "10.1093/iwc/iww027", "10.1162/pres.1995.4.3.318", "10.1016/S0166-4115(08)62386-9", "10.1007/s10846-009-9356-4", "10.1093/iwc/iww027", "10.1162/pres.1995.4.3.318", "10.1016/S0166-4115(08)62386-9", "10.1007/s10846-009-9356-4", "10.1093/iwc/iww027", "10.1162/pres.1995.4.3.318", "10.1016/S0166-4115(08)62386-9", "10.1007/s10846-009-9356-4"]}, "10.1109/TVCG.2018.2793698": {"doi": "10.1109/TVCG.2018.2793698", "author": ["S. Ghosh", "L. Winston", "N. Panchal", "P. Kimura-Thollander", "J. Hotnog", "D. Cheong", "G. Reyes", "G. D. Abowd"], "title": "NotifiVR: Exploring Interruptions and Notifications in Virtual Reality", "year": "2018", "abstract": "The proliferation of high resolution and affordable virtual reality (VR) headsets is quickly making room-scale VR experiences available in our homes. Most VR experiences strive to achieve complete immersion by creating a disconnect from the real world. However, due to the lack of a standardized notification management system and minimal context awareness in VR, an immersed user may face certain situations such as missing an important phone call (digital scenario), tripping over wandering pets (physical scenario), or losing track of time (temporal scenario). In this paper, we present the results of 1) a survey across 61 VR users to understand common interruptions and scenarios that would benefit from some form of notifications; 2) a design exercise with VR professionals to explore possible notification methods; and 3) an empirical study on the noticeability and perception of 5 different VR interruption scenarios across 6 modality combinations (e.g., audio, visual, haptic, audio + haptic, visual + haptic, and audio + visual) implemented in Unity and presented using the HTC Vive headset. Finally, we combine key learnings from each of these steps along with participant feedback to present a set of observations and recommendations for notification design in VR.", "keywords": ["virtual reality", "minimal context awareness", "VR professionals", "notification design", "room-scale VR experiences", "standardized notification management system", "virtual reality headsets", "VR interruption scenarios", "NotifiVR", "notification method", "HTC Vive headset", "Visualization", "Haptic interfaces", "Vibrations", "Headphones", "Virtual environments", "Legged locomotion", "Virtual Reality", "Notifications", "Interruptibility", "Multi-Modal", "Feedback", "Context Awareness", "Adult", "Feedback", "Female", "Humans", "Male", "Middle Aged", "Surveys and Questionnaires", "User-Computer Interface", "Video Games", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8572543", "IKEY:8798240", "IKEY:8938049", "IKEY:8961433", "IKEY:9199565", "IKEY:9284807", "IKEY:9284807"], "referencing": ["IKEY:1241415", "IKEY:7383328", "IKEY:6165136", "IKEY:1241415", "IKEY:7383328", "IKEY:6165136", "IKEY:1241415", "IKEY:7383328", "IKEY:6165136", "10.1145/2858036.2858226", "10.1145/2927929.2927945", "10.1145/2875194.2875204", "10.1145/1152215.1152261", "10.1145/1168987.1169042", "10.1145/2799250.2799281", "10.1145/642611.642657", "10.1145/985692.985701", "10.1145/2663806.2663824", "10.1145/1095034.1095073", "10.1145/1142405.1142457", "10.1145/966930.966933", "10.1145/108844.108896", "10.1145/2639189.2639231", "10.1145/1133265.1133277", "10.1145/2702123.2702350", "10.1145/2702123.2702389", "10.1145/2802083.2802092", "10.1145/2897824.2925883", "10.1145/985921.986138", "10.1145/2799250.2799267", "10.1145/2957265.2962661", "10.1145/2991561.2998472", "10.1145/2932206.2932212", "10.1145/2858036.2858226", "10.1145/2927929.2927945", "10.1145/2875194.2875204", "10.1145/1152215.1152261", "10.1145/1168987.1169042", "10.1145/2799250.2799281", "10.1145/642611.642657", "10.1145/985692.985701", "10.1145/2663806.2663824", "10.1145/1095034.1095073", "10.1145/1142405.1142457", "10.1145/966930.966933", "10.1145/108844.108896", "10.1145/2639189.2639231", "10.1145/1133265.1133277", "10.1145/2702123.2702350", "10.1145/2702123.2702389", "10.1145/2802083.2802092", "10.1145/2897824.2925883", "10.1145/985921.986138", "10.1145/2799250.2799267", "10.1145/2957265.2962661", "10.1145/2991561.2998472", "10.1145/2932206.2932212", "10.1145/2858036.2858226", "10.1145/2927929.2927945", "10.1145/2875194.2875204", "10.1145/1152215.1152261", "10.1145/1168987.1169042", "10.1145/2799250.2799281", "10.1145/642611.642657", "10.1145/985692.985701", "10.1145/2663806.2663824", "10.1145/1095034.1095073", "10.1145/1142405.1142457", "10.1145/966930.966933", "10.1145/108844.108896", "10.1145/2639189.2639231", "10.1145/1133265.1133277", "10.1145/2702123.2702350", "10.1145/2702123.2702389", "10.1145/2802083.2802092", "10.1145/2897824.2925883", "10.1145/985921.986138", "10.1145/2799250.2799267", "10.1145/2957265.2962661", "10.1145/2991561.2998472", "10.1145/2932206.2932212", "10.1207/S15327051HCI1701_1", "10.1162/105474605323384654", "10.1207/S15327051HCI1701_1", "10.1162/105474605323384654", "10.1207/S15327051HCI1701_1", "10.1162/105474605323384654"]}, "10.1109/TVCG.2018.2793659": {"doi": "10.1109/TVCG.2018.2793659", "author": ["T. Hamasaki", "Y. Itoh", "Y. Hiroi", "D. Iwai", "M. Sugimoto"], "title": "HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection", "year": "2018", "abstract": "Spatial augmented reality (SAR) pursues realism in rendering materials and objects. To advance this goal, we propose a hybrid SAR (HySAR) that combines a projector with optical see-through head-mounted displays (OST-HMD). In an ordinary SAR scenario with co-located viewers, the viewers perceive the same virtual material on physical surfaces. In general, the material consists of two components: a view-independent (VI) component such as diffuse reflection, and a view-dependent (VD) component such as specular reflection. The VI component is static over viewpoints, whereas the VD should change for each viewpoint even if a projector can simulate only one viewpoint at one time. In HySAR, a projector only renders the static VI components. In addition, the OST-HMD renders the dynamic VD components according to the viewer's current viewpoint. Unlike conventional SAR, the HySAR concept theoretically allows an unlimited number of co-located viewers to see the correct material over different viewpoints. Furthermore, the combination enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With proof-of-concept systems, we demonstrate HySAR both qualitatively and quantitatively with real objects. First, we demonstrate HySAR by rendering synthetic material properties on a real object from different viewpoints. Our quantitative evaluation shows that our system increases the dynamic range by 2.24 times and the maximum intensity by 2.12 times compared to an ordinary SAR system. Second, we replicate the material properties of a real object by SAR and HySAR, and show that HySAR outperforms SAR in rendering VD specular components.", "keywords": ["augmented reality", "helmet mounted displays", "rendering (computer graphics)", "optical see-through head-mounted display", "VD specular components", "static VI components", "view-dependent component", "view-independent component", "virtual material", "hybrid SAR", "spatial augmented reality projection", "hybrid material rendering", "Augmented realtiy", "Head-mounted displays", "Rendering (computer graphics)", "Optical reflection", "Adaptive optics", "Optical see-through displays", "hybrid material rendering", "spatial augmented reality"], "referenced_by": ["IKEY:8943682"], "referencing": ["IKEY:6671826", "IKEY:6549352", "IKEY:7165643", "IKEY:7265057", "IKEY:7523375", "IKEY:6671826", "IKEY:6549352", "IKEY:7165643", "IKEY:7265057", "IKEY:7523375", "IKEY:6671826", "IKEY:6549352", "IKEY:7165643", "IKEY:7265057", "IKEY:7523375", "10.1145/258734.258875", "10.1145/1185657.1185804", "10.1145/2642918.2647402", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/2816795.2818111", "10.1145/1180495.1180546", "10.1145/2207676.2207704", "10.1145/2897824.2925949", "10.1145/2807442.2807493", "10.1145/2897824.2925925", "10.1145/2816795.2818070", "10.1145/2642918.2647383", "10.1145/563858.563893", "10.1145/258734.258875", "10.1145/1185657.1185804", "10.1145/2642918.2647402", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/2816795.2818111", "10.1145/1180495.1180546", "10.1145/2207676.2207704", "10.1145/2897824.2925949", "10.1145/2807442.2807493", "10.1145/2897824.2925925", "10.1145/2816795.2818070", "10.1145/2642918.2647383", "10.1145/563858.563893", "10.1145/258734.258875", "10.1145/1185657.1185804", "10.1145/2642918.2647402", "10.1145/2159516.2159518", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/2816795.2818111", "10.1145/1180495.1180546", "10.1145/2207676.2207704", "10.1145/2897824.2925949", "10.1145/2807442.2807493", "10.1145/2897824.2925925", "10.1145/2816795.2818070", "10.1145/2642918.2647383", "10.1145/563858.563893", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.2197/ipsjtcva.2.71", "10.1002/col.5080100409", "10.1007/978-3-7091-6221-7_21", "10.1111/j.1467-8659.2011.01891.x", "10.1364/AO.47.000317", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.2197/ipsjtcva.2.71", "10.1002/col.5080100409", "10.1007/978-3-7091-6221-7_21", "10.1111/j.1467-8659.2011.01891.x", "10.1364/AO.47.000317", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.2197/ipsjtcva.2.71", "10.1002/col.5080100409", "10.1007/978-3-7091-6221-7_21", "10.1111/j.1467-8659.2011.01891.x", "10.1364/AO.47.000317"]}, "10.1109/TVCG.2018.2794659": {"doi": "10.1109/TVCG.2018.2794659", "author": ["D. T. Han", "M. Suhail", "E. D. Ragan"], "title": "Evaluating Remapped Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality", "year": "2018", "abstract": "Virtual reality often uses motion tracking to incorporate physical hand movements into interaction techniques for selection and manipulation of virtual objects. To increase realism and allow direct hand interaction, real-world physical objects can be aligned with virtual objects to provide tactile feedback and physical grasping. However, unless a physical space is custom configured to match a specific virtual reality experience, the ability to perfectly match the physical and virtual objects is limited. Our research addresses this challenge by studying methods that allow one physical object to be mapped to multiple virtual objects that can exist at different virtual locations in an egocentric reference frame. We study two such techniques: one that introduces a static translational offset between the virtual and physical hand before a reaching action, and one that dynamically interpolates the position of the virtual hand during a reaching motion. We conducted two experiments to assess how the two methods affect reaching effectiveness, comfort, and ability to adapt to the remapping techniques when reaching for objects with different types of mismatches between physical and virtual locations. We also present a case study to demonstrate how the hand remapping techniques could be used in an immersive game application to support realistic hand interaction while optimizing usability. Overall, the translational technique performed better than the interpolated reach technique and was more robust for situations with larger mismatches between virtual and physical objects.", "keywords": ["haptic interfaces", "human computer interaction", "image capture", "motion estimation", "object tracking", "solid modelling", "virtual reality", "remapped physical reach", "real-world physical objects", "physical grasping", "reaching action", "reaching motion", "remapping techniques", "realistic hand interaction", "virtual reality", "physical hand movements", "passive haptics", "motion tracking", "tactile feedback", "3D interaction", "3D object selection", "OptiTrack capture system", "Haptic interfaces", "Visualization", "Tactile sensors", "Virtual environments", "Virtual reality", "3D interaction", "passive haptics", "hand interaction", "remapped reach", "3D object selection", "Adult", "Female", "Hand", "Humans", "Imaging, Three-Dimensional", "Male", "Task Performance and Analysis", "Touch", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8798155", "IKEY:8797974", "IKEY:8798143", "IKEY:8798205", "IKEY:8809587", "IKEY:8816164", "IKEY:8943608", "IKEY:8961433", "IKEY:9089480", "IKEY:9284804"], "referencing": ["IKEY:6184193", "IKEY:6550201", "IKEY:5759455", "IKEY:6165136", "IKEY:6184193", "IKEY:6550201", "IKEY:5759455", "IKEY:6165136", "IKEY:6184193", "IKEY:6550201", "IKEY:5759455", "IKEY:6165136", "10.1145/2858036.2858226", "10.1145/253284.253301", "10.1145/2330667.2330687", "10.1145/3025453.3025753", "10.1145/2628257.2628268", "10.1145/237091.237102", "10.1145/2858036.2858226", "10.1145/253284.253301", "10.1145/2330667.2330687", "10.1145/3025453.3025753", "10.1145/2628257.2628268", "10.1145/237091.237102", "10.1145/2858036.2858226", "10.1145/253284.253301", "10.1145/2330667.2330687", "10.1145/3025453.3025753", "10.1145/2628257.2628268", "10.1145/237091.237102", "10.1162/pres.18.1.39", "10.1111/1467-8659.00252", "10.1027/1614-2241/a000016", "10.1007/3-540-44589-7_1", "10.1111/j.1469-8986.1987.tb00324.x", "10.1162/pres.18.1.39", "10.1111/1467-8659.00252", "10.1027/1614-2241/a000016", "10.1007/3-540-44589-7_1", "10.1111/j.1469-8986.1987.tb00324.x", "10.1162/pres.18.1.39", "10.1111/1467-8659.00252", "10.1027/1614-2241/a000016", "10.1007/3-540-44589-7_1", "10.1111/j.1469-8986.1987.tb00324.x"]}, "10.1109/TVCG.2018.2794118": {"doi": "10.1109/TVCG.2018.2794118", "author": ["K. Hasegawa", "L. Qiu", "H. Shinoda"], "title": "Midair Ultrasound Fragrance Rendering", "year": "2018", "abstract": "We propose a system that controls the spatial distribution of odors in an environment by generating electronically steerable ultrasound-driven narrow air flows. The proposed system is designed not only to remotely present a preset fragrance to a user, but also to provide applications that would be conventionally inconceivable, such as: 1) fetching the odor of a generic object placed at a location remote from the user and guiding it to his or her nostrils, or 2) nullifying the odor of an object near a user by carrying it away before it reaches his or her nostrils (Fig. 1). These are all accomplished with an ultrasound-driven air stream serving as an airborne carrier of fragrant substances. The flow originates from a point in midair located away from the ultrasound source and travels while accelerating and maintaining its narrow cross-sectional area. These properties differentiate the flow from conventional jet- or fan-driven flows and contribute to achieving a midair flow. In our system, we employed a phased array of ultrasound transducers so that the traveling direction of the flow could be electronically and instantaneously controlled. In this paper, we describe the physical principle of odor control, the system construction, and experiments conducted to evaluate remote fragrance presentation and fragrance tracking.", "keywords": ["electronic noses", "flow visualisation", "ultrasonic transducers", "midair ultrasound fragrance rendering", "ultrasound-driven air stream", "fragrant substances", "ultrasound source", "midair flow", "ultrasound transducers", "odor control", "fragrance tracking", "electronically steerable ultrasound-driven narrow air flows", "Ultrasonic imaging", "Phased arrays", "Transducers", "Olfactory", "Acceleration", "Acoustics", "Acoustic beams", "Olfactory display", "ultrasound", "nonlinear acoustics", "acoustic streaming"], "referenced_by": ["IKEY:8859852", "IKEY:8956347", "IKEY:9089617"], "referencing": ["IKEY:756955", "IKEY:6548380", "IKEY:7447720", "IKEY:6548460", "IKEY:5406524", "IKEY:903313", "IKEY:6479189", "IKEY:1667646", "IKEY:4161021", "IKEY:1667645", "IKEY:1310054", "IKEY:756955", "IKEY:6548380", "IKEY:7447720", "IKEY:6548460", "IKEY:5406524", "IKEY:903313", "IKEY:6479189", "IKEY:1667646", "IKEY:4161021", "IKEY:1667645", "IKEY:1310054", "IKEY:756955", "IKEY:6548380", "IKEY:7447720", "IKEY:6548460", "IKEY:5406524", "IKEY:903313", "IKEY:6479189", "IKEY:1667646", "IKEY:4161021", "IKEY:1667645", "IKEY:1310054", "10.1145/3025453.3026004", "10.1145/3123021.3123035", "10.1145/3132818.3132833", "10.1145/2661229.2661257", "10.1145/1873951.1873994", "10.1145/3025453.3026004", "10.1145/3123021.3123035", "10.1145/3132818.3132833", "10.1145/2661229.2661257", "10.1145/1873951.1873994", "10.1145/3025453.3026004", "10.1145/3123021.3123035", "10.1145/3132818.3132833", "10.1145/2661229.2661257", "10.1145/1873951.1873994", "10.1063/1.4985159", "10.1007/978-3-540-69057-3_64", "10.1002/(SICI)1520-6440(199902)82:2&lt;76::AID-ECJC9&gt;3.0.CO;2-Q", "10.1038/ncomms5316", "10.1063/1.4985159", "10.1007/978-3-540-69057-3_64", "10.1002/(SICI)1520-6440(199902)82:2&lt;76::AID-ECJC9&gt;3.0.CO;2-Q", "10.1038/ncomms5316", "10.1063/1.4985159", "10.1007/978-3-540-69057-3_64", "10.1002/(SICI)1520-6440(199902)82:2&lt;76::AID-ECJC9&gt;3.0.CO;2-Q", "10.1038/ncomms5316"]}, "10.1109/TVCG.2018.2794598": {"doi": "10.1109/TVCG.2018.2794598", "author": ["C. Jeunet", "L. Albert", "F. Argelaguet", "A. L\u00e9cuyer"], "title": "\u201cDo You Feel in Control?\u201d: Towards Novel Approaches to Characterise, Manipulate and Measure the Sense of Agency in Virtual Environments", "year": "2018", "abstract": "While the Sense of Agency (SoA) has so far been predominantly characterised in VR as a component of the Sense of Embodiment, other communities (e.g., in psychology or neurosciences) have investigated the SoA from a different perspective proposing complementary theories. Yet, despite the acknowledged potential benefits of catching up with these theories a gap remains. This paper first aims to contribute to fill this gap by introducing a theory according to which the SoA can be divided into two components, the feeling and the judgment of agency, and relies on three principles, namely the principles of priority, exclusivity and consistency. We argue that this theory could provide insights on the factors influencing the SoA in VR systems. Second, we propose novel approaches to manipulate the SoA in controlled VR experiments (based on these three principles) as well as to measure the SoA, and more specifically its two components based on neurophysiological markers, using ElectroEncephaloGraphy (EEG). We claim that these approaches would enable us to deepen our understanding of the SoA in VR contexts. Finally, we validate these approaches in an experiment. Our results (N=24) suggest that our approach was successful in manipulating the SoA as the modulation of each of the three principles induced significant decreases of the SoA (measured using questionnaires). In addition, we recorded participants' EEG signals during the VR experiment, and neurophysiological markers of the SoA, potentially reflecting the feeling and judgment of agency specifically, were revealed. Our results also suggest that users' profile, more precisely their Locus of Control (LoC), influences their level of immersion and SoA.", "keywords": ["electroencephalography", "neurophysiology", "psychology", "virtual reality", "SoA", "virtual environments", "Sense of Embodiment", "Sense of Agency", "VR systems", "electroencephalography", "EEG signals", "neurophysiological markers", "Locus of Control", "Electroencephalography", "Virtual environments", "Modulation", "Psychology", "Biology", "Sense of Agency", "Priority Principle", "Consistency Principle", "Exclusivity Principle", "Feeling of Agency", "Judgment of Agency", "EEG", "Neurophysiological Marker", "Pre-Motor Cortex", "Right Posterior Parietal Cortex", "Locus of Control", "Adult", "Algorithms", "Brain", "Electroencephalography", "Female", "Humans", "Internal-External Control", "Male", "Signal Processing, Computer-Assisted", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": [], "referencing": ["10.1145/2818998", "10.1145/2492494.2501895", "10.1145/2818998", "10.1145/2492494.2501895", "10.1145/2818998", "10.1145/2492494.2501895", "10.1523/JNEUROSCI.0745-06.2006", "10.1073/pnas.1414936111", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1176/jnp.2009.21.3.279", "10.1038/35784", "10.1093/cercor/bhs059", "10.1162/jocn.2006.18.6.898", "10.1016/j.concog.2008.03.004", "10.1016/j.concog.2010.09.004", "10.1016/j.jneumeth.2003.10.009", "10.1126/science.1097011", "10.1006/nimg.2001.1009", "10.1016/S1364-6613(99)01417-5", "10.1016/j.newideapsych.2010.03.003", "10.3389/fnhum.2016.00392", "10.1038/nrn.2017.14", "10.1016/bs.pbr.2016.04.002", "10.3389/fnhum.2012.00040", "10.1371/journal.pone.0135261", "10.1162/PRES_a_00124", "10.1371/journal.pone.0040867", "10.1038/srep28879", "10.1016/j.cortex.2012.09.002", "10.1111/j.1460-9568.2011.07647.x", "10.1080/00223891.1974.10119988", "10.1371/journal.pone.0068594", "10.1016/j.chb.2004.12.010", "10.1093/cercor/bhq059", "10.1016/j.neuroimage.2015.08.022", "10.1162/pres.19.1.35", "10.1038/nrn1651", "10.1007/s00429-010-0298-1", "10.1016/j.concog.2007.03.010", "10.1007/s00221-015-4202-3", "10.1016/j.concog.2005.09.004", "10.1162/089892904970799", "10.1017/S0140525X04000159", "10.1037/0022-3514.86.6.838", "10.1037/0003-066X.54.7.480", "10.1523/JNEUROSCI.0745-06.2006", "10.1073/pnas.1414936111", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1176/jnp.2009.21.3.279", "10.1038/35784", "10.1093/cercor/bhs059", "10.1162/jocn.2006.18.6.898", "10.1016/j.concog.2008.03.004", "10.1016/j.concog.2010.09.004", "10.1016/j.jneumeth.2003.10.009", "10.1126/science.1097011", "10.1006/nimg.2001.1009", "10.1016/S1364-6613(99)01417-5", "10.1016/j.newideapsych.2010.03.003", "10.3389/fnhum.2016.00392", "10.1038/nrn.2017.14", "10.1016/bs.pbr.2016.04.002", "10.3389/fnhum.2012.00040", "10.1371/journal.pone.0135261", "10.1162/PRES_a_00124", "10.1371/journal.pone.0040867", "10.1038/srep28879", "10.1016/j.cortex.2012.09.002", "10.1111/j.1460-9568.2011.07647.x", "10.1080/00223891.1974.10119988", "10.1371/journal.pone.0068594", "10.1016/j.chb.2004.12.010", "10.1093/cercor/bhq059", "10.1016/j.neuroimage.2015.08.022", "10.1162/pres.19.1.35", "10.1038/nrn1651", "10.1007/s00429-010-0298-1", "10.1016/j.concog.2007.03.010", "10.1007/s00221-015-4202-3", "10.1016/j.concog.2005.09.004", "10.1162/089892904970799", "10.1017/S0140525X04000159", "10.1037/0022-3514.86.6.838", "10.1037/0003-066X.54.7.480", "10.1523/JNEUROSCI.0745-06.2006", "10.1073/pnas.1414936111", "10.1016/j.tics.2008.10.003", "10.1016/j.neuron.2015.09.029", "10.1176/jnp.2009.21.3.279", "10.1038/35784", "10.1093/cercor/bhs059", "10.1162/jocn.2006.18.6.898", "10.1016/j.concog.2008.03.004", "10.1016/j.concog.2010.09.004", "10.1016/j.jneumeth.2003.10.009", "10.1126/science.1097011", "10.1006/nimg.2001.1009", "10.1016/S1364-6613(99)01417-5", "10.1016/j.newideapsych.2010.03.003", "10.3389/fnhum.2016.00392", "10.1038/nrn.2017.14", "10.1016/bs.pbr.2016.04.002", "10.3389/fnhum.2012.00040", "10.1371/journal.pone.0135261", "10.1162/PRES_a_00124", "10.1371/journal.pone.0040867", "10.1038/srep28879", "10.1016/j.cortex.2012.09.002", "10.1111/j.1460-9568.2011.07647.x", "10.1080/00223891.1974.10119988", "10.1371/journal.pone.0068594", "10.1016/j.chb.2004.12.010", "10.1093/cercor/bhq059", "10.1016/j.neuroimage.2015.08.022", "10.1162/pres.19.1.35", "10.1038/nrn1651", "10.1007/s00429-010-0298-1", "10.1016/j.concog.2007.03.010", "10.1007/s00221-015-4202-3", "10.1016/j.concog.2005.09.004", "10.1162/089892904970799", "10.1017/S0140525X04000159", "10.1037/0022-3514.86.6.838", "10.1037/0003-066X.54.7.480"]}, "10.1109/TVCG.2018.2794073": {"doi": "10.1109/TVCG.2018.2794073", "author": ["K. Karunanayaka", "N. Johari", "S. Hariri", "H. Camelia", "K. S. Bielawski", "A. D. Cheok"], "title": "New Thermal Taste Actuation Technology for Future Multisensory Virtual Reality and Internet", "year": "2018", "abstract": "Today's virtual reality (VR) applications such as gaming, multisensory entertainment, remote dining, and online shopping are mainly based on audio, visual, and touch interactions between humans and virtual worlds. Integrating the sense of taste into VR is difficult since humans are dependent on chemical-based taste delivery systems. This paper presents the `Thermal Taste Machine', a new digital taste actuation technology that can effectively produce and modify thermal taste sensations on the tongue. It modifies the temperature of the surface of the tongue within a short period of time (from 25\u00b0C to 40 \u00b0C while heating, and from 25\u00b0C to 10 \u00b0C while cooling). We tested this device on human subjects and described the experience of thermal taste using 20 known (taste and non-taste) sensations. Our results suggested that rapidly heating the tongue produces sweetness, fatty/oiliness, electric taste, warmness, and reduces the sensibility for metallic taste. Similarly, cooling the tongue produced mint taste, pleasantness, and coldness. By conducting another user study on the perceived sweetness of sucrose solutions after the thermal stimulation, we found that heating the tongue significantly enhances the intensity of sweetness for both thermal tasters and non-thermal tasters. Also, we found that faster temperature rises on the tongue produce more intense sweet sensations for thermal tasters. This technology will be useful in two ways: First, it can produce taste sensations without using chemicals for the individuals who are sensitive to thermal taste. Second, the temperature rise of the device can be used as a way to enhance the intensity of sweetness. We believe that this technology can be used to digitally produce and enhance taste sensations in future virtual reality applications. The key novelties of this paper are as follows: 1. Development of a thermal taste actuation technology for stimulating the human taste receptors, 2. Characterization of the thermal taste produced by the device using taste-related sensations and non-taste related sensations, 3. Research on enhancing the intensity for sucrose solutions using thermal stimulation, 4. Research on how different speeds of heating affect the intensity of sweetness produced by thermal stimulation.", "keywords": ["chemical technology", "chemioception", "Internet", "virtual reality", "future multisensory virtual reality", "chemical-based taste delivery systems", "Thermal Taste Machine", "digital taste actuation technology", "thermal taste sensations", "electric taste", "metallic taste", "mint taste", "thermal stimulation", "thermal tasters", "nonthermal tasters", "intense sweet sensations", "human taste receptors", "Thermal Taste actuation technology", "Internet", "Tongue", "Temperature sensors", "Silver", "Heating systems", "Chemicals", "Cooling", "Virtual reality", "Thermal Taste", "Multisensory VR", "Digitizing Taste", "Characterization of Thermal Taste", "TRPM5", "Adult", "Equipment Design", "Female", "Humans", "Internet", "Male", "Signal Processing, Computer-Assisted", "Taste", "Temperature", "Tongue", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8732156"], "referencing": ["10.1145/2832932.2856225", "10.1145/2984751.2985729", "10.1145/2663806.2663825", "10.1145/2832932.2856225", "10.1145/2984751.2985729", "10.1145/2663806.2663825", "10.1145/2832932.2856225", "10.1145/2984751.2985729", "10.1145/2663806.2663825", "10.1016/j.concog.2007.06.005", "10.1016/j.physbeh.2008.08.009", "10.1126/science.1174601", "10.1038/35002581", "10.1037/0003-066X.63.7.591", "10.1159/000113337", "10.1152/ajpcell.1997.272.4.C1203", "10.1093/chemse/bjh065", "10.1093/chemse/bjv021", "10.1007/978-3-540-34891-7_17", "10.1073/pnas.2334159100", "10.1523/JNEUROSCI.6273-10.2011", "10.1007/978-3-642-22021-0_29", "10.1038/nature726", "10.1142/p575", "10.4108/icst.bodynets.2011.247067", "10.1007/s00018-006-6384-0", "10.1038/nature04248", "10.1016/j.foodqual.2014.05.013", "10.1016/j.physbeh.2007.03.011", "10.1016/S0092-8674(03)00071-0", "10.1016/j.concog.2007.06.005", "10.1016/j.physbeh.2008.08.009", "10.1126/science.1174601", "10.1038/35002581", "10.1037/0003-066X.63.7.591", "10.1159/000113337", "10.1152/ajpcell.1997.272.4.C1203", "10.1093/chemse/bjh065", "10.1093/chemse/bjv021", "10.1007/978-3-540-34891-7_17", "10.1073/pnas.2334159100", "10.1523/JNEUROSCI.6273-10.2011", "10.1007/978-3-642-22021-0_29", "10.1038/nature726", "10.1142/p575", "10.4108/icst.bodynets.2011.247067", "10.1007/s00018-006-6384-0", "10.1038/nature04248", "10.1016/j.foodqual.2014.05.013", "10.1016/j.physbeh.2007.03.011", "10.1016/S0092-8674(03)00071-0", "10.1016/j.concog.2007.06.005", "10.1016/j.physbeh.2008.08.009", "10.1126/science.1174601", "10.1038/35002581", "10.1037/0003-066X.63.7.591", "10.1159/000113337", "10.1152/ajpcell.1997.272.4.C1203", "10.1093/chemse/bjh065", "10.1093/chemse/bjv021", "10.1007/978-3-540-34891-7_17", "10.1073/pnas.2334159100", "10.1523/JNEUROSCI.6273-10.2011", "10.1007/978-3-642-22021-0_29", "10.1038/nature726", "10.1142/p575", "10.4108/icst.bodynets.2011.247067", "10.1007/s00018-006-6384-0", "10.1038/nature04248", "10.1016/j.foodqual.2014.05.013", "10.1016/j.physbeh.2007.03.011", "10.1016/S0092-8674(03)00071-0"]}, "10.1109/TVCG.2018.2793641": {"doi": "10.1109/TVCG.2018.2793641", "author": ["G. Kato", "Y. Kuroda", "K. Kiyokawa", "H. Takemura"], "title": "Force Rendering and its Evaluation of a Friction-Based Walking Sensation Display for a Seated User", "year": "2018", "abstract": "Most existing locomotion devices that represent the sensation of walking target a user who is actually performing a walking motion. Here, we attempted to represent the walking sensation, especially a kinesthetic sensation and advancing feeling (the sense of moving forward) while the user remains seated. To represent the walking sensation using a relatively simple device, we focused on the force rendering and its evaluation of the longitudinal friction force applied on the sole during walking. Based on the measurement of the friction force applied on the sole during actual walking, we developed a novel friction force display that can present the friction force without the influence of body weight. Using performance evaluation testing, we found that the proposed method can stably and rapidly display friction force. Also, we developed a virtual reality (VR) walk-through system that is able to present the friction force through the proposed device according to the avatar's walking motion in a virtual world. By evaluating the realism, we found that the proposed device can represent a more realistic advancing feeling than vibration feedback.", "keywords": ["avatars", "friction", "haptic interfaces", "rendering (computer graphics)", "force rendering", "walking sensation display", "seated user", "walking target", "walking motion", "kinesthetic sensation", "longitudinal friction force", "actual walking", "performance evaluation testing", "virtual reality walk-through system", "avatar", "locomotion devices", "friction force display", "friction-based walking sensation display", "Legged locomotion", "Force", "Friction", "Force measurement", "Foot", "Actuators", "Performance evaluation", "Virtual Reality", "Locomotion Interface", "Seated Position", "Walking Sensation", "Friction Force", "Adult", "Equipment Design", "Friction", "Humans", "Kinesthesis", "Sitting Position", "User-Computer Interface", "Virtual Reality", "Walking", "Young Adult"], "referenced_by": ["IKEY:8960448"], "referencing": ["IKEY:5759455", "IKEY:913779", "IKEY:756964", "IKEY:6183826", "IKEY:6269027", "IKEY:5759455", "IKEY:913779", "IKEY:756964", "IKEY:6183826", "IKEY:6269027", "IKEY:5759455", "IKEY:913779", "IKEY:756964", "IKEY:6183826", "IKEY:6269027", "10.1145/2983310.2989204", "10.1145/3005274.3005302", "10.1145/2522628.2522655", "10.1145/2983310.2989204", "10.1145/3005274.3005302", "10.1145/2522628.2522655", "10.1145/2983310.2989204", "10.1145/3005274.3005302", "10.1145/2522628.2522655", "10.1162/105474602317473204", "10.1007/978-1-4419-8432-6", "10.1111/j.1748-1716.1989.tb08655.x", "10.1299/jsmesports.2009.0_277", "10.1007/978-3-7091-9433-1_12", "10.1016/S0966-6362(03)00031-6", "10.1121/1.1912271", "10.1016/j.apacoust.2017.08.021", "10.1162/105474602317473204", "10.1007/978-1-4419-8432-6", "10.1111/j.1748-1716.1989.tb08655.x", "10.1299/jsmesports.2009.0_277", "10.1007/978-3-7091-9433-1_12", "10.1016/S0966-6362(03)00031-6", "10.1121/1.1912271", "10.1016/j.apacoust.2017.08.021", "10.1162/105474602317473204", "10.1007/978-1-4419-8432-6", "10.1111/j.1748-1716.1989.tb08655.x", "10.1299/jsmesports.2009.0_277", "10.1007/978-3-7091-9433-1_12", "10.1016/S0966-6362(03)00031-6", "10.1121/1.1912271", "10.1016/j.apacoust.2017.08.021"]}, "10.1109/TVCG.2018.2793680": {"doi": "10.1109/TVCG.2018.2793680", "author": ["H. Kim", "J. L. Gabbard", "A. M. Anon", "T. Misu"], "title": "Driver Behavior and Performance with Augmented Reality Pedestrian Collision Warning: An Outdoor User Study", "year": "2018", "abstract": "This article investigates the effects of visual warning presentation methods on human performance in augmented reality (AR) driving. An experimental user study was conducted in a parking lot where participants drove a test vehicle while braking for any cross traffic with assistance from AR visual warnings presented on a monoscopic and volumetric head-up display (HUD). Results showed that monoscopic displays can be as effective as volumetric displays for human performance in AR braking tasks. The experiment also demonstrated the benefits of conformal graphics, which are tightly integrated into the real world, such as their ability to guide drivers' attention and their positive consequences on driver behavior and performance. These findings suggest that conformal graphics presented via monoscopic HUDs can enhance driver performance by leveraging the effectiveness of monocular depth cues. The proposed approaches and methods can be used and further developed by future researchers and practitioners to better understand driver performance in AR as well as inform usability evaluation of future automotive AR applications.", "keywords": ["augmented reality", "computer graphics", "driver information systems", "head-up displays", "pedestrians", "road safety", "road traffic", "road vehicles", "augmented reality pedestrian collision warning", "outdoor user study", "visual warning presentation methods", "human performance", "test vehicle", "cross traffic", "AR visual warnings", "volumetric head-up display", "AR braking tasks", "conformal graphics", "driver behavior", "monoscopic HUDs", "driver performance", "augmented reality driving", "monoscopic head-up display", "monocular depth cues", "Three-dimensional displays", "Visualization", "Vehicles", "Task analysis", "Stereo image processing", "Observers", "Augmented reality", "human performance", "automotive", "depth cues", "three-dimensional displays", "Accidents, Traffic", "Adult", "Automobile Driving", "Depth Perception", "Humans", "Imaging, Three-Dimensional", "Middle Aged", "Pedestrians", "Task Performance and Analysis", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8466859", "IKEY:8798018", "IKEY:8809586", "IKEY:8968117", "IKEY:9024040"], "referencing": ["IKEY:6704805", "IKEY:7829412", "IKEY:4135650", "IKEY:4079251", "IKEY:7223465", "IKEY:7904661", "IKEY:6704805", "IKEY:7829412", "IKEY:4135650", "IKEY:4079251", "IKEY:7223465", "IKEY:7904661", "IKEY:6704805", "IKEY:7829412", "IKEY:4135650", "IKEY:4079251", "IKEY:7223465", "IKEY:7904661", "10.1145/2856767.2856815", "10.1145/2611009.2611011", "10.1145/2667317.2667329", "10.1145/2611009.2611011", "10.1145/2856767.2856815", "10.1145/2611009.2611011", "10.1145/2667317.2667329", "10.1145/2611009.2611011", "10.1145/2856767.2856815", "10.1145/2611009.2611011", "10.1145/2667317.2667329", "10.1145/2611009.2611011", "10.1162/105474600566808", "10.1007/978-3-540-79567-4", "10.1117/12.850094", "10.1002/9781118583593.ch18", "10.1007/978-1-4471-6651-1", "10.1016/B978-012240530-3/50005-5", "10.1007/978-3-540-79567-4_116", "10.1162/105474600566583", "10.1162/pres.1995.4.1.24", "10.1364/OE.19.000704", "10.1016/j.displa.2013.10.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1177/1541931213601474", "10.3141/1701-02", "10.1016/j.aap.2004.09.003", "10.1177/0018720812448474", "10.1080/0014013021000039574", "10.1007/978-3-319-09903-3_20", "10.1167/11.5.13", "10.1177/0956797613504966", "10.3758/BF03208759", "10.1163/156856893X00036", "10.1068/p030063", "10.3758/BF03202998", "10.1016/0042-6989(79)90205-0", "10.1177/1541931214581351", "10.1162/105474600566808", "10.1007/978-3-540-79567-4", "10.1117/12.850094", "10.1002/9781118583593.ch18", "10.1007/978-1-4471-6651-1", "10.1016/B978-012240530-3/50005-5", "10.1007/978-3-540-79567-4_116", "10.1162/105474600566583", "10.1162/pres.1995.4.1.24", "10.1364/OE.19.000704", "10.1016/j.displa.2013.10.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1177/1541931213601474", "10.3141/1701-02", "10.1016/j.aap.2004.09.003", "10.1177/0018720812448474", "10.1080/0014013021000039574", "10.1007/978-3-319-09903-3_20", "10.1167/11.5.13", "10.1177/0956797613504966", "10.3758/BF03208759", "10.1163/156856893X00036", "10.1068/p030063", "10.3758/BF03202998", "10.1016/0042-6989(79)90205-0", "10.1177/1541931214581351", "10.1162/105474600566808", "10.1007/978-3-540-79567-4", "10.1117/12.850094", "10.1002/9781118583593.ch18", "10.1007/978-1-4471-6651-1", "10.1016/B978-012240530-3/50005-5", "10.1007/978-3-540-79567-4_116", "10.1162/105474600566583", "10.1162/pres.1995.4.1.24", "10.1364/OE.19.000704", "10.1016/j.displa.2013.10.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1177/1541931213601474", "10.3141/1701-02", "10.1016/j.aap.2004.09.003", "10.1177/0018720812448474", "10.1080/0014013021000039574", "10.1007/978-3-319-09903-3_20", "10.1167/11.5.13", "10.1177/0956797613504966", "10.3758/BF03208759", "10.1163/156856893X00036", "10.1068/p030063", "10.3758/BF03202998", "10.1016/0042-6989(79)90205-0", "10.1177/1541931214581351"]}, "10.1109/TVCG.2018.2794074": {"doi": "10.1109/TVCG.2018.2794074", "author": ["M. Lee", "G. Bruder", "T. H\u00f6llerer", "G. Welch"], "title": "Effects of Unaugmented Periphery and Vibrotactile Feedback on Proxemics with Virtual Humans in AR", "year": "2018", "abstract": "In this paper, we investigate factors and issues related to human locomotion behavior and proxemics in the presence of a real or virtual human in augmented reality (AR). First, we discuss a unique issue with current-state optical see-through head-mounted displays, namely the mismatch between a small augmented visual field and a large unaugmented periphery, and its potential impact on locomotion behavior in close proximity of virtual content. We discuss a potential simple solution based on restricting the field of view to the central region, and we present the results of a controlled human-subject study. The study results show objective benefits for this approach in producing behaviors that more closely match those that occur when seeing a real human, but also some drawbacks in overall acceptance of the restricted field of view. Second, we discuss the limited multimodal feedback provided by virtual humans in AR, present a potential improvement based on vibrotactile feedback induced via the floor to compensate for the limited augmented visual field, and report results showing that benefits of such vibrations are less visible in objective locomotion behavior than in subjective estimates of co-presence. Third, we investigate and document significant differences in the effects that real and virtual humans have on locomotion behavior in AR with respect to clearance distances, walking speed, and head motions. We discuss potential explanations for these effects related to social expectations, and analyze effects of different types of behaviors including idle standing, jumping, and walking that such real or virtual humans may exhibit in the presence of an observer.", "keywords": ["augmented reality", "haptic interfaces", "helmet mounted displays", "human computer interaction", "virtual content", "potential simple solution", "human-subject study", "objective benefits", "multimodal feedback", "potential improvement", "vibrotactile feedback", "augmented visual field", "objective locomotion behavior", "potential explanations", "unaugmented periphery", "proxemics", "human locomotion behavior", "real human", "virtual human", "augmented reality", "head-mounted displays", "AR", "Visualization", "Legged locomotion", "Collision avoidance", "Optical feedback", "Task analysis", "Vibrations", "Augmented reality", "virtual humans", "locomotion", "proxemics", "vibrotactile feedback", "field of view", "Adult", "Feedback", "Female", "Head Movements", "Humans", "Male", "Middle Aged", "Touch", "User-Computer Interface", "Vibration", "Virtual Reality", "Visual Fields", "Walking", "Young Adult"], "referenced_by": ["IKEY:8456568", "IKEY:8943625", "IKEY:8998141", "IKEY:9288473"], "referencing": ["IKEY:5539766", "IKEY:5620905", "IKEY:6479211", "IKEY:6948425", "IKEY:4689554", "IKEY:7042312", "IKEY:5539766", "IKEY:5620905", "IKEY:6479211", "IKEY:6948425", "IKEY:4689554", "IKEY:7042312", "IKEY:5539766", "IKEY:5620905", "IKEY:6479211", "IKEY:6948425", "IKEY:4689554", "IKEY:7042312", "10.1145/365058.365082", "10.1145/1227134.1227136", "10.1145/2983631", "10.1145/1228716.1228736", "10.1145/2543581.2543590", "10.1145/365058.365082", "10.1145/1227134.1227136", "10.1145/2983631", "10.1145/1228716.1228736", "10.1145/2543581.2543590", "10.1145/365058.365082", "10.1145/1227134.1227136", "10.1145/2983631", "10.1145/1228716.1228736", "10.1145/2543581.2543590", "10.2466/PMS.70.1.35-45", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1016/j.gaitpost.2012.08.003", "10.1016/j.cub.2014.09.049", "10.1016/j.humov.2007.10.001", "10.3758/BF03334122", "10.1037/0033-295X.102.4.627", "10.1016/j.neulet.2005.06.052", "10.1037/0096-1523.29.2.343", "10.1016/j.neubiorev.2008.10.004", "10.1177/154193129804200103", "10.1007/s10919-014-0184-2", "10.1016/j.chb.2011.11.007", "10.1371/journal.pone.0089589", "10.1007/978-3-662-44193-0_32", "10.1007/s00221-011-2757-1", "10.1207/s15327108ijap0303_3", "10.1097/00006199-199003000-00019", "10.1097/00000637-199609000-00006", "10.1098/rspb.2009.0405", "10.1073/pnas.1016507108", "10.1016/j.gaitpost.2013.03.017", "10.1016/j.gaitpost.2012.03.021", "10.1016/j.cag.2009.01.003", "10.1007/978-3-642-14075-4_26", "10.1098/rstb.2009.0138", "10.2466/pms.105.4.1245-1256", "10.1117/12.771950", "10.2466/PMS.70.1.35-45", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1016/j.gaitpost.2012.08.003", "10.1016/j.cub.2014.09.049", "10.1016/j.humov.2007.10.001", "10.3758/BF03334122", "10.1037/0033-295X.102.4.627", "10.1016/j.neulet.2005.06.052", "10.1037/0096-1523.29.2.343", "10.1016/j.neubiorev.2008.10.004", "10.1177/154193129804200103", "10.1007/s10919-014-0184-2", "10.1016/j.chb.2011.11.007", "10.1371/journal.pone.0089589", "10.1007/978-3-662-44193-0_32", "10.1007/s00221-011-2757-1", "10.1207/s15327108ijap0303_3", "10.1097/00006199-199003000-00019", "10.1097/00000637-199609000-00006", "10.1098/rspb.2009.0405", "10.1073/pnas.1016507108", "10.1016/j.gaitpost.2013.03.017", "10.1016/j.gaitpost.2012.03.021", "10.1016/j.cag.2009.01.003", "10.1007/978-3-642-14075-4_26", "10.1098/rstb.2009.0138", "10.2466/pms.105.4.1245-1256", "10.1117/12.771950", "10.2466/PMS.70.1.35-45", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1016/j.gaitpost.2012.08.003", "10.1016/j.cub.2014.09.049", "10.1016/j.humov.2007.10.001", "10.3758/BF03334122", "10.1037/0033-295X.102.4.627", "10.1016/j.neulet.2005.06.052", "10.1037/0096-1523.29.2.343", "10.1016/j.neubiorev.2008.10.004", "10.1177/154193129804200103", "10.1007/s10919-014-0184-2", "10.1016/j.chb.2011.11.007", "10.1371/journal.pone.0089589", "10.1007/978-3-662-44193-0_32", "10.1007/s00221-011-2757-1", "10.1207/s15327108ijap0303_3", "10.1097/00006199-199003000-00019", "10.1097/00000637-199609000-00006", "10.1098/rspb.2009.0405", "10.1073/pnas.1016507108", "10.1016/j.gaitpost.2013.03.017", "10.1016/j.gaitpost.2012.03.021", "10.1016/j.cag.2009.01.003", "10.1007/978-3-642-14075-4_26", "10.1098/rstb.2009.0138", "10.2466/pms.105.4.1245-1256", "10.1117/12.771950"]}, "10.1109/TVCG.2018.2794119": {"doi": "10.1109/TVCG.2018.2794119", "author": ["P. Lungaro", "R. Sj\u00f6berg", "A. J. F. Valero", "A. Mittal", "K. Tollmar"], "title": "Gaze-Aware Streaming Solutions for the Next Generation of Mobile VR Experiences", "year": "2018", "abstract": "This paper presents a novel approach to content delivery for video streaming services. It exploits information from connected eye-trackers embedded in the next generation of VR Head Mounted Displays (HMDs). The proposed solution aims to deliver high visual quality, in real time, around the users' fixations points while lowering the quality everywhere else. The goal of the proposed approach is to substantially reduce the overall bandwidth requirements for supporting VR video experiences while delivering high levels of user perceived quality. The prerequisites to achieve these results are: (1) mechanisms that can cope with different degrees of latency in the system and (2) solutions that support fast adaptation of video quality in different parts of a frame, without requiring a large increase in bitrate. A novel codec configuration, capable of supporting near-instantaneous video quality adaptation in specific portions of a video frame, is presented. The proposed method exploits in-built properties of HEVC encoders and while it introduces a moderate amount of error, these errors are indetectable by users. Fast adaptation is the key to enable gaze-aware streaming and its reduction in bandwidth. A testbed implementing gaze-aware streaming, together with a prototype HMD with in-built eye tracker, is presented and was used for testing with real users. The studies quantified the bandwidth savings achievable by the proposed approach and characterize the relationships between Quality of Experience (QoE) and network latency. The results showed that up to 83% less bandwidth is required to deliver high QoE levels to the users, as compared to conventional solutions.", "keywords": ["helmet mounted displays", "quality of experience", "video codecs", "video coding", "video streaming", "virtual reality", "HEVC encoders", "quality of experience", "HMD", "VR head mounted displays", "mobile VR video experiences", "eye tracker", "codec configuration", "gaze-aware streaming solutions", "high QoE levels", "near-instantaneous video quality adaptation", "user perceived quality", "video streaming services", "content delivery", "Streaming media", "Bandwidth", "Quality of experience", "Visualization", "Bit rate", "Rendering (computer graphics)", "Servers", "Eye-tracking", "VR", "QoE", "video streaming", "content delivery"], "referenced_by": ["IKEY:8447551", "IKEY:8514957", "IKEY:8611777", "IKEY:8626197", "IKEY:8716694", "IKEY:8713498", "IKEY:8855916", "IKEY:8942328", "IKEY:9013251", "IKEY:9005240", "IKEY:8998133", "IKEY:8955928", "IKEY:9123080", "IKEY:9149357", "IKEY:9097455", "IKEY:9209747", "IKEY:9225004"], "referencing": ["IKEY:7593456", "IKEY:5167460", "IKEY:7475948", "IKEY:6557521", "IKEY:5202656", "IKEY:7823595", "IKEY:1218196", "IKEY:4472353", "IKEY:6316136", "IKEY:7593456", "IKEY:5167460", "IKEY:7475948", "IKEY:6557521", "IKEY:5202656", "IKEY:7823595", "IKEY:1218196", "IKEY:4472353", "IKEY:6316136", "IKEY:7593456", "IKEY:5167460", "IKEY:7475948", "IKEY:6557521", "IKEY:5202656", "IKEY:7823595", "IKEY:1218196", "IKEY:4472353", "IKEY:6316136", "10.1145/3072959.3073642", "10.1145/2366145.2366183", "10.1145/2980055.2980056", "10.1145/2964284.2973796", "10.1145/2964284.2967292", "10.1145/3072959.3073642", "10.1145/2366145.2366183", "10.1145/2980055.2980056", "10.1145/2964284.2973796", "10.1145/2964284.2967292", "10.1145/3072959.3073642", "10.1145/2366145.2366183", "10.1145/2980055.2980056", "10.1145/2964284.2973796", "10.1145/2964284.2967292", "10.1364/JOSAA.8.001775", "10.1016/0166-4328(96)00012-5", "10.1016/S0166-2236(00)01685-4", "10.1080/13506280444000409", "10.1016/0042-6989(78)90104-9", "10.1016/0013-4694(93)90068-7", "10.1364/JOSAA.8.001775", "10.1016/0166-4328(96)00012-5", "10.1016/S0166-2236(00)01685-4", "10.1080/13506280444000409", "10.1016/0042-6989(78)90104-9", "10.1016/0013-4694(93)90068-7", "10.1364/JOSAA.8.001775", "10.1016/0166-4328(96)00012-5", "10.1016/S0166-2236(00)01685-4", "10.1080/13506280444000409", "10.1016/0042-6989(78)90104-9", "10.1016/0013-4694(93)90068-7"]}, "10.1109/TVCG.2018.2794071": {"doi": "10.1109/TVCG.2018.2794071", "author": ["B. Luo", "F. Xu", "C. Richardt", "J. -H. Yong"], "title": "Parallax360: Stereoscopic 360\u00b0 Scene Representation for Head-Motion Parallax", "year": "2018", "abstract": "We propose a novel 360\u00b0 scene representation for converting real scenes into stereoscopic 3D virtual reality content with head-motion parallax. Our image-based scene representation enables efficient synthesis of novel views with six degrees-of-freedom (6-DoF) by fusing motion fields at two scales: (1) disparity motion fields carry implicit depth information and are robustly estimated from multiple laterally displaced auxiliary viewpoints, and (2) pairwise motion fields enable real-time flow-based blending, which improves the visual fidelity of results by minimizing ghosting and view transition artifacts. Based on our scene representation, we present an end-to-end system that captures real scenes with a robotic camera arm, processes the recorded data, and finally renders the scene in a head-mounted display in real time (more than 40 Hz). Our approach is the first to support head-motion parallax when viewing real 360\u00b0 scenes. We demonstrate compelling results that illustrate the enhanced visual experience - and hence sense of immersion-achieved with our approach compared to widely-used stereoscopic panoramas.", "keywords": ["cameras", "helmet mounted displays", "image fusion", "image motion analysis", "image representation", "image sequences", "manipulators", "rendering (computer graphics)", "robot vision", "stereo image processing", "virtual reality", "parallax360", "stereoscopic 360\u00b0 scene representation", "head-motion parallax", "stereoscopic 3D virtual reality content", "head-mounted display", "360\u00b0 scene representation", "disparity motion fields", "fusing motion fields", "robotic camera arm", "Three-dimensional displays", "Visualization", "Image reconstruction", "Rendering (computer graphics)", "Real-time systems", "Videos", "Robustness", "360\u00b0 scene capture", "scene representation", "head-motion parallax", "6 degrees-of-freedom (6-DoF)", "image-based rendering", "Depth Perception", "Head Movements", "Humans", "Imaging, Three-Dimensional", "User-Computer Interface", "Video Recording", "Virtual Reality"], "referenced_by": ["IKEY:8661657", "IKEY:8651483", "IKEY:8798281", "IKEY:8798226", "IKEY:9089526", "IKEY:9105244", "IKEY:9138937"], "referencing": ["IKEY:7102754", "IKEY:5226635", "IKEY:6162880", "IKEY:910880", "IKEY:6940309", "IKEY:6574844", "IKEY:7102754", "IKEY:5226635", "IKEY:6162880", "IKEY:910880", "IKEY:6940309", "IKEY:6574844", "IKEY:7102754", "IKEY:5226635", "IKEY:6162880", "IKEY:910880", "IKEY:6940309", "IKEY:6574844", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2766945", "10.1145/2897824.2925969", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3130800.3130828", "10.1145/2980179.2982420", "10.1145/2897824.2925983", "10.1145/237170.237199", "10.1145/2461912.2461914", "10.1145/3072959.3073645", "10.1145/2682631", "10.1145/311535.311573", "10.1145/2816795.2818075", "10.1145/2768821", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2766945", "10.1145/2897824.2925969", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3130800.3130828", "10.1145/2980179.2982420", "10.1145/2897824.2925983", "10.1145/237170.237199", "10.1145/2461912.2461914", "10.1145/3072959.3073645", "10.1145/2682631", "10.1145/311535.311573", "10.1145/2816795.2818075", "10.1145/2768821", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2766945", "10.1145/2897824.2925969", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3130800.3130828", "10.1145/2980179.2982420", "10.1145/2897824.2925983", "10.1145/237170.237199", "10.1145/2461912.2461914", "10.1145/3072959.3073645", "10.1145/2682631", "10.1145/311535.311573", "10.1145/2816795.2818075", "10.1145/2768821", "10.1111/cgf.12289", "10.1007/s11263-006-0002-3", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46484-8_22", "10.1111/j.1467-8659.2009.01485.x", "10.5244/C.28.67", "10.1111/cgf.12541", "10.1023/A:1008195814169", "10.1561/0600000009", "10.1111/cgf.12289", "10.1007/s11263-006-0002-3", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46484-8_22", "10.1111/j.1467-8659.2009.01485.x", "10.5244/C.28.67", "10.1111/cgf.12541", "10.1023/A:1008195814169", "10.1561/0600000009", "10.1111/cgf.12289", "10.1007/s11263-006-0002-3", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46484-8_22", "10.1111/j.1467-8659.2009.01485.x", "10.5244/C.28.67", "10.1111/cgf.12541", "10.1023/A:1008195814169", "10.1561/0600000009"]}, "10.1109/TVCG.2018.2794599": {"doi": "10.1109/TVCG.2018.2794599", "author": ["G. Lyu", "X. Shen", "T. Komura", "K. Subr", "L. Teng"], "title": "Widening Viewing Angles of Automultiscopic Displays Using Refractive Inserts", "year": "2018", "abstract": "Displays that can portray environments that are perceivable from multiple views are known as multiscopic displays. Some multiscopic displays enable realistic perception of 3D environments without the need for cumbersome mounts or fragile head-tracking algorithms. These automultiscopic displays carefully control the distribution of emitted light over space, direction (angle) and time so that even a static image displayed can encode parallax across viewing directions (Iightfield). This allows simultaneous observation by multiple viewers, each perceiving 3D from their own (correct) perspective. Currently, the illusion can only be effectively maintained over a narrow range of viewing angles. In this paper, we propose and analyze a simple solution to widen the range of viewing angles for automultiscopic displays that use parallax barriers. We propose the use of a refractive medium, with a high refractive index, between the display and parallax barriers. The inserted medium warps the exitant lightfield in a way that increases the potential viewing angle. We analyze the consequences of this warp and build a prototype with a 93% increase in the effective viewing angle.", "keywords": ["image resolution", "refractive index", "rendering (computer graphics)", "solid modelling", "stereo image processing", "three-dimensional displays", "static image", "viewing angle widening", "potential viewing angle", "rendering", "high-resolution views", "fragile head-tracking algorithms", "multiscopic displays", "refractive inserts", "parallax barriers", "automultiscopic displays", "Three-dimensional displays", "Two dimensional displays", "Rendering (computer graphics)", "Prototypes", "Spatial resolution", "Refractive index", "Electronic mail", "Automultiscopic displays", "lightfields"], "referenced_by": ["IKEY:8550627"], "referencing": ["IKEY:4015502", "IKEY:6678165", "IKEY:6574844", "IKEY:4359490", "IKEY:7223335", "IKEY:4015502", "IKEY:6678165", "IKEY:6574844", "IKEY:4359490", "IKEY:7223335", "IKEY:4015502", "IKEY:6678165", "IKEY:6574844", "IKEY:4359490", "IKEY:7223335", "10.1145/344779.344932", "10.1145/2661229.2661248", "10.1145/1186822.1073320", "10.1145/2897824.2925921", "10.1145/3072959.3073672", "10.1145/271283.271309", "10.1145/2782782.2792493", "10.1145/2980179.2980251", "10.1145/1833349.1778812", "10.1145/1882261.1866164", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/237170.237199", "10.1145/2665075", "10.1145/1015706.1015805", "10.1145/1198555.1198735", "10.1145/2461912.2462011", "10.1145/2807442.2807477", "10.1145/1964921.1964990", "10.1145/1179849.1179983", "10.1145/344779.344932", "10.1145/2661229.2661248", "10.1145/1186822.1073320", "10.1145/2897824.2925921", "10.1145/3072959.3073672", "10.1145/271283.271309", "10.1145/2782782.2792493", "10.1145/2980179.2980251", "10.1145/1833349.1778812", "10.1145/1882261.1866164", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/237170.237199", "10.1145/2665075", "10.1145/1015706.1015805", "10.1145/1198555.1198735", "10.1145/2461912.2462011", "10.1145/2807442.2807477", "10.1145/1964921.1964990", "10.1145/1179849.1179983", "10.1145/344779.344932", "10.1145/2661229.2661248", "10.1145/1186822.1073320", "10.1145/2897824.2925921", "10.1145/3072959.3073672", "10.1145/271283.271309", "10.1145/2782782.2792493", "10.1145/2980179.2980251", "10.1145/1833349.1778812", "10.1145/1882261.1866164", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/237170.237199", "10.1145/2665075", "10.1145/1015706.1015805", "10.1145/1198555.1198735", "10.1145/2461912.2462011", "10.1145/2807442.2807477", "10.1145/1964921.1964990", "10.1145/1179849.1179983", "10.1016/j.ijleo.2015.10.150", "10.1117/12.468040", "10.1038/nature11972", "10.1117/12.430821", "10.1002/sapm193918151", "10.1002/jsid.172", "10.1364/AO.42.001996", "10.1364/OL.27.000818", "10.1051/jphystap:019080070082100", "10.1364/AO.6.001739", "10.1364/OE.17.015716", "10.1016/j.cag.2013.10.003", "10.1889/1.1832576", "10.1007/s41095-016-0037-5", "10.1016/j.optcom.2017.01.051", "10.1016/j.ijleo.2015.10.150", "10.1117/12.468040", "10.1038/nature11972", "10.1117/12.430821", "10.1002/sapm193918151", "10.1002/jsid.172", "10.1364/AO.42.001996", "10.1364/OL.27.000818", "10.1051/jphystap:019080070082100", "10.1364/AO.6.001739", "10.1364/OE.17.015716", "10.1016/j.cag.2013.10.003", "10.1889/1.1832576", "10.1007/s41095-016-0037-5", "10.1016/j.optcom.2017.01.051", "10.1016/j.ijleo.2015.10.150", "10.1117/12.468040", "10.1038/nature11972", "10.1117/12.430821", "10.1002/sapm193918151", "10.1002/jsid.172", "10.1364/AO.42.001996", "10.1364/OL.27.000818", "10.1051/jphystap:019080070082100", "10.1364/AO.6.001739", "10.1364/OE.17.015716", "10.1016/j.cag.2013.10.003", "10.1889/1.1832576", "10.1007/s41095-016-0037-5", "10.1016/j.optcom.2017.01.051"]}, "10.1109/TVCG.2018.2793561": {"doi": "10.1109/TVCG.2018.2793561", "author": ["A. MacQuarrie", "A. Steed"], "title": "The Effect of Transition Type in Multi-View 360\u00b0 Media", "year": "2018", "abstract": "360\u00b0 images and video have become extremely popular formats for immersive displays, due in large part to the technical ease of content production. While many experiences use a single camera viewpoint, an increasing number of experiences use multiple camera locations. In such multi-view 360\u00b0 media (MV360M) systems, a visual effect is required when the user transitions from one camera location to another. This effect can take several forms, such as a cut or an image-based warp, and the choice of effect may impact many aspects of the experience, including issues related to enjoyment and scene understanding. To investigate the effect of transition types on immersive MV360M experiences, a repeated-measures experiment was conducted with 31 participants. Wearing a head-mounted display, participants explored four static scenes, for which multiple 360\u00b0 images and a reconstructed 3D model were available. Three transition types were examined: teleport, a linear move through a 3D model of the scene, and an image-based transition using a M\u00f6bius transformation. The metrics investigated included spatial awareness, users' movement profiles, transition preference and the subjective feeling of moving through the space. Results indicate that there was no significant difference between transition types in terms of spatial awareness, while significant differences were found for users' movement profiles, with participants taking 1.6 seconds longer to select their next location following a teleport transition. The model and M\u00f6bius transitions were significantly better in terms of creating the feeling of moving through the space. Preference was also significantly different, with model and teleport transitions being preferred over M\u00f6bius transitions. Our results indicate that trade-offs between transitions will require content creators to think carefully about what aspects they consider to be most important when producing MV360M experiences.", "keywords": ["cameras", "helmet mounted displays", "human computer interaction", "image reconstruction", "stereo image processing", "three-dimensional displays", "video signal processing", "transition type", "immersive displays", "single camera viewpoint", "multiple camera locations", "multiview 360\u00b0 media systems", "visual effect", "camera location", "repeated-measures experiment", "spatial awareness", "transition preference", "teleport transition", "M\u00f6bius transitions", "scene understanding", "immersive MV360M experiences", "reconstructed 3D model", "image-based transition", "M\u00f6bius transformation", "head-mounted display", "time 1.6 s", "Cameras", "Media", "Solid modeling", "Three-dimensional displays", "Navigation", "Measurement", "Resists", "H.5.1 [Information interfaces and presentation]: Multimedia Information Systems \u2014 Artificial, augmented and virtual realities"], "referenced_by": ["IKEY:8642365", "IKEY:8798025", "IKEY:8797777", "IKEY:8998141", "IKEY:8554159", "IKEY:9090548", "IKEY:9155956"], "referencing": ["IKEY:5481932", "IKEY:583043", "IKEY:5481932", "IKEY:583043", "IKEY:5481932", "IKEY:583043", "10.1145/2967934.2968105", "10.1145/1201775.882309", "10.1145/218380.218395", "10.1145/168642.168658", "10.1145/258734.258854", "10.1145/218380.218398", "10.1145/1609967.1609972", "10.1145/311535.311589", "10.1145/2967934.2968105", "10.1145/1201775.882309", "10.1145/218380.218395", "10.1145/168642.168658", "10.1145/258734.258854", "10.1145/218380.218398", "10.1145/1609967.1609972", "10.1145/311535.311589", "10.1145/2967934.2968105", "10.1145/1201775.882309", "10.1145/218380.218395", "10.1145/168642.168658", "10.1145/258734.258854", "10.1145/218380.218398", "10.1145/1609967.1609972", "10.1145/311535.311589", "10.1162/pres.17.3.283", "10.1162/105474699566521", "10.2307/2334029", "10.1177/154193129804202101", "10.1207/s15327876mp0203_4", "10.1207/s15327108ijap0303_3", "10.1177/0013916586186004", "10.3758/BF03211566", "10.1162/105474699566143", "10.1162/105474698565767", "10.1016/0010-0285(82)90019-6", "10.1162/pres.17.3.283", "10.1162/105474699566521", "10.2307/2334029", "10.1177/154193129804202101", "10.1207/s15327876mp0203_4", "10.1207/s15327108ijap0303_3", "10.1177/0013916586186004", "10.3758/BF03211566", "10.1162/105474699566143", "10.1162/105474698565767", "10.1016/0010-0285(82)90019-6", "10.1162/pres.17.3.283", "10.1162/105474699566521", "10.2307/2334029", "10.1177/154193129804202101", "10.1207/s15327876mp0203_4", "10.1207/s15327108ijap0303_3", "10.1177/0013916586186004", "10.3758/BF03211566", "10.1162/105474699566143", "10.1162/105474698565767", "10.1016/0010-0285(82)90019-6"]}, "10.1109/TVCG.2018.2793638": {"doi": "10.1109/TVCG.2018.2793638", "author": ["M. Murcia-L\u00f3pez", "A. Steed"], "title": "A Comparison of Virtual and Physical Training Transfer of Bimanual Assembly Tasks", "year": "2018", "abstract": "As we explore the use of consumer virtual reality technology for training applications, there is a need to evaluate its validity compared to more traditional training formats. In this paper, we present a study that compares the effectiveness of virtual training and physical training for teaching a bimanual assembly task. In a between-subjects experiment, 60 participants were trained to solve three 3D burr puzzles in one of six conditions comprised of virtual and physical training elements. In the four physical conditions, training was delivered via paper- and video-based instructions, with or without the physical puzzles to practice with. In the two virtual conditions, participants learnt to assemble the puzzles in an interactive virtual environment, with or without 3D animations showing the assembly process. After training, we conducted immediate tests in which participants were asked to solve a physical version of the puzzles. We measured performance through success rates and assembly completion testing times. We also measured training times as well as subjective ratings on several aspects of the experience. Our results show that the performance of virtually trained participants was promising. A statistically significant difference was not found between virtual training with animated instructions and the best performing physical condition (in which physical blocks were available during training) for the last and most complex puzzle in terms of success rates and testing times. Performance in retention tests two weeks after training was generally not as good as expected for all experimental conditions. We discuss the implications of the results and highlight the validity of virtual reality systems in training.", "keywords": ["assembling", "computer animation", "computer based training", "interactive systems", "virtual reality", "physical training transfer", "bimanual assembly task", "consumer virtual reality technology", "traditional training formats", "virtual training", "3D burr puzzles", "physical puzzles", "interactive virtual environment", "physical blocks", "virtual reality systems", "assembly completion testing", "Training", "Testing", "Three-dimensional displays", "Virtual environments", "Haptic interfaces", "Learning transfer", "virtual reality", "assembly", "training", "Adult", "Female", "Humans", "Male", "Task Performance and Analysis", "Transfer, Psychology", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8463411", "IKEY:8797917", "IKEY:8798155", "IKEY:8798112", "IKEY:8906037", "IKEY:9089455", "IKEY:9130859", "IKEY:9262701", "IKEY:9280182"], "referencing": ["IKEY:7014246", "IKEY:6180873", "IKEY:5759244", "IKEY:7014246", "IKEY:6180873", "IKEY:5759244", "IKEY:7014246", "IKEY:6180873", "IKEY:5759244", "10.1145/1394669.1394672", "10.1145/1394669.1394672", "10.1145/1394669.1394672", "10.1007/s00897970138a", "10.1080/00401706.1964.10490181", "10.1080/10494820.2013.815221", "10.3389/frobt.2017.00003", "10.3389/fict.2016.00024", "10.1177/154193120304702012", "10.1080/001401300184378", "10.1115/1.4001565", "10.1162/105474698565767", "10.2466/pms.1978.47.2.599", "10.1162/105474698565631", "10.1007/s00897970138a", "10.1080/00401706.1964.10490181", "10.1080/10494820.2013.815221", "10.3389/frobt.2017.00003", "10.3389/fict.2016.00024", "10.1177/154193120304702012", "10.1080/001401300184378", "10.1115/1.4001565", "10.1162/105474698565767", "10.2466/pms.1978.47.2.599", "10.1162/105474698565631", "10.1007/s00897970138a", "10.1080/00401706.1964.10490181", "10.1080/10494820.2013.815221", "10.3389/frobt.2017.00003", "10.3389/fict.2016.00024", "10.1177/154193120304702012", "10.1080/001401300184378", "10.1115/1.4001565", "10.1162/105474698565767", "10.2466/pms.1978.47.2.599", "10.1162/105474698565631"]}, "10.1109/TVCG.2018.2793038": {"doi": "10.1109/TVCG.2018.2793038", "author": ["R. Nagao", "K. Matsumoto", "T. Narumi", "T. Tanikawa", "M. Hirose"], "title": "Ascending and Descending in Virtual Reality: Simple and Safe System Using Passive Haptics", "year": "2018", "abstract": "This paper presents a novel interactive system that provides users with virtual reality (VR) experiences, wherein users feel as if they are ascending/descending stairs through passive haptic feedback. The passive haptic stimuli are provided by small bumps under the feet of users; these stimuli are provided to represent the edges of the stairs in the virtual environment. The visual stimuli of the stairs and shoes, provided by head-mounted displays, evoke a visuo-haptic interaction that modifies a user's perception of the floor shape. Our system enables users to experience all types of stairs, such as half-turn and spiral stairs, in a VR setting. We conducted a preliminary user study and two experiments to evaluate the proposed technique. The preliminary user study investigated the effectiveness of the basic idea associated with the proposed technique for the case of a user ascending stairs. The results demonstrated that the passive haptic feedback produced by the small bumps enhanced the user's feeling of presence and sense of ascending. We subsequently performed an experiment to investigate an improved viewpoint manipulation method and the interaction of the manipulation and haptics for both the ascending and descending cases. The experimental results demonstrated that the participants had a feeling of presence and felt a steep stair gradient under the condition of haptic feedback and viewpoint manipulation based on the characteristics of actual stair walking data. However, these results also indicated that the proposed system may not be as effective in providing a sense of descending stairs without an optimization of the haptic stimuli. We then redesigned the shape of the small bumps, and evaluated the design in a second experiment. The results indicated that the best shape to present haptic stimuli is a right triangle cross section in both the ascending and descending cases. Although it is necessary to install small protrusions in the determined direction, by using this optimized shape the users feeling of presence of the stairs and the sensation of walking up and down was enhanced.", "keywords": ["haptic interfaces", "virtual reality", "descending cases", "virtual reality", "safe system", "passive haptics", "interactive system", "passive haptic feedback", "passive haptic stimuli", "virtual environment", "visual stimuli", "visuo-haptic interaction", "spiral stairs", "user ascending stairs", "steep stair gradient", "viewpoint manipulation", "Haptic interfaces", "Legged locomotion", "Visualization", "Shape", "Foot", "Actuators", "Virtual reality", "Virtual reality", "locomotion", "haptic feedback", "perception", "stairs", "staircase", "Adult", "Computer Graphics", "Feedback", "Humans", "Male", "Pressure", "Touch", "User-Computer Interface", "Virtual Reality", "Walking", "Young Adult"], "referenced_by": ["IKEY:8643571", "IKEY:8797989", "IKEY:8797960", "IKEY:8943608", "IKEY:8961433", "IKEY:8998361", "IKEY:9089462", "IKEY:9090408", "IKEY:9090487", "IKEY:9089444", "IKEY:8809840", "IKEY:9284694"], "referencing": ["IKEY:6183793", "IKEY:1381227", "IKEY:913779", "IKEY:5446238", "IKEY:7460038", "IKEY:6229796", "IKEY:5072212", "IKEY:6184179", "IKEY:6183793", "IKEY:1381227", "IKEY:913779", "IKEY:5446238", "IKEY:7460038", "IKEY:6229796", "IKEY:5072212", "IKEY:6184179", "IKEY:6183793", "IKEY:1381227", "IKEY:913779", "IKEY:5446238", "IKEY:7460038", "IKEY:6229796", "IKEY:5072212", "IKEY:6184179", "10.1145/2858036.2858226", "10.1145/263407.263550", "10.1145/1152399.1152451", "10.1145/2983310.2985759", "10.1145/566570.566630", "10.1145/210079.210084", "10.1145/311535.311589", "10.1145/2858036.2858226", "10.1145/263407.263550", "10.1145/1152399.1152451", "10.1145/2983310.2985759", "10.1145/566570.566630", "10.1145/210079.210084", "10.1145/311535.311589", "10.1145/2858036.2858226", "10.1145/263407.263550", "10.1145/1152399.1152451", "10.1145/2983310.2985759", "10.1145/566570.566630", "10.1145/210079.210084", "10.1145/311535.311589", "10.1016/S0966-6362(03)00095-X", "10.1016/S0268-0033(01)00090-0", "10.1016/j.clinbiomech.2006.09.010", "10.2307/2281310", "10.1162/1054746041944849", "10.1037/0096-1523.10.5.683", "10.1016/S0966-6362(03)00095-X", "10.1016/S0268-0033(01)00090-0", "10.1016/j.clinbiomech.2006.09.010", "10.2307/2281310", "10.1162/1054746041944849", "10.1037/0096-1523.10.5.683", "10.1016/S0966-6362(03)00095-X", "10.1016/S0268-0033(01)00090-0", "10.1016/j.clinbiomech.2006.09.010", "10.2307/2281310", "10.1162/1054746041944849", "10.1037/0096-1523.10.5.683"]}, "10.1109/TVCG.2018.2793560": {"doi": "10.1109/TVCG.2018.2793560", "author": ["N. Padmanaban", "T. Ruban", "V. Sitzmann", "A. M. Norcia", "G. Wetzstein"], "title": "Towards a Machine-Learning Approach for Sickness Prediction in 360\u00b0 Stereoscopic Videos", "year": "2018", "abstract": "Virtual reality systems are widely believed to be the next major computing platform. There are, however, some barriers to adoption that must be addressed, such as that of motion sickness - which can lead to undesirable symptoms including postural instability, headaches, and nausea. Motion sickness in virtual reality occurs as a result of moving visual stimuli that cause users to perceive self-motion while they remain stationary in the real world. There are several contributing factors to both this perception of motion and the subsequent onset of sickness, including field of view, motion velocity, and stimulus depth. We verify first that differences in vection due to relative stimulus depth remain correlated with sickness. Then, we build a dataset of stereoscopic 3D videos and their corresponding sickness ratings in order to quantify their nauseogenicity, which we make available for future use. Using this dataset, we train a machine learning algorithm on hand-crafted features (quantifying speed, direction, and depth as functions of time) from each video, learning the contributions of these various features to the sickness ratings. Our predictor generally outperforms a na\u00efve estimate, but is ultimately limited by the size of the dataset. However, our result is promising and opens the door to future work with more extensive datasets. This and further advances in this space have the potential to alleviate developer and end user concerns about motion sickness in the increasingly commonplace virtual world.", "keywords": ["human factors", "image motion analysis", "learning (artificial intelligence)", "medical image processing", "stereo image processing", "video signal processing", "virtual reality", "visual perception", "sickness ratings", "commonplace virtual world", "virtual reality systems", "stereoscopic videos", "sickness prediction", "machine-learning approach", "machine learning algorithm", "stereoscopic 3D videos", "relative stimulus depth", "motion velocity", "motion sickness", "Videos", "Stereo image processing", "Three-dimensional displays", "Visualization", "Virtual environments", "Machine learning algorithms", "Trajectory", "Virtual reality", "simulator sickness", "vection", "machine learning", "Adult", "Algorithms", "Computer Graphics", "Databases, Factual", "Depth Perception", "Female", "Humans", "Machine Learning", "Male", "Middle Aged", "Motion Sickness", "User-Computer Interface", "Video Recording", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8516469", "IKEY:8613651", "IKEY:8642906", "IKEY:8798213", "IKEY:8798297", "IKEY:8943774", "IKEY:8977814", "IKEY:8960364", "IKEY:9010856", "IKEY:9090670", "IKEY:9090495", "IKEY:9090494", "IKEY:9105244", "IKEY:9163348", "IKEY:9201649", "IKEY:9284761", "IKEY:9284654", "IKEY:9288028"], "referencing": ["10.1145/2993369.2993391", "10.1145/365024.365051", "10.1145/1152215.1152263", "10.1145/2858036.2858212", "10.1145/2993369.2993391", "10.1145/365024.365051", "10.1145/1152215.1152263", "10.1145/2858036.2858212", "10.1145/2993369.2993391", "10.1145/365024.365051", "10.1145/1152215.1152263", "10.1145/2858036.2858212", "10.1068/p2891", "10.1007/BF00234916", "10.3357/ASEM.2394.2009", "10.1016/j.displa.2007.09.002", "10.1007/BF00234474", "10.3758/BF03203301", "10.1007/BF00058655", "10.1080/10447318.2017.1286767", "10.1111/j.1749-6632.2011.06147.x", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1068/p180657", "10.1207/s15327108ijap0303_3", "10.3389/fpsyg.2015.00472", "10.1177/154193120204602605", "10.2466/pms.102.3.871-877", "10.1111/j.1468-5884.2008.00363.x", "10.1068/p2939", "10.1068/p160017", "10.1007/PL00005624", "10.1016/j.visres.2009.09.017", "10.1167/11.8.11", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008", "10.1068/p2891", "10.1007/BF00234916", "10.3357/ASEM.2394.2009", "10.1016/j.displa.2007.09.002", "10.1007/BF00234474", "10.3758/BF03203301", "10.1007/BF00058655", "10.1080/10447318.2017.1286767", "10.1111/j.1749-6632.2011.06147.x", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1068/p180657", "10.1207/s15327108ijap0303_3", "10.3389/fpsyg.2015.00472", "10.1177/154193120204602605", "10.2466/pms.102.3.871-877", "10.1111/j.1468-5884.2008.00363.x", "10.1068/p2939", "10.1068/p160017", "10.1007/PL00005624", "10.1016/j.visres.2009.09.017", "10.1167/11.8.11", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008", "10.1068/p2891", "10.1007/BF00234916", "10.3357/ASEM.2394.2009", "10.1016/j.displa.2007.09.002", "10.1007/BF00234474", "10.3758/BF03203301", "10.1007/BF00058655", "10.1080/10447318.2017.1286767", "10.1111/j.1749-6632.2011.06147.x", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1068/p180657", "10.1207/s15327108ijap0303_3", "10.3389/fpsyg.2015.00472", "10.1177/154193120204602605", "10.2466/pms.102.3.871-877", "10.1111/j.1468-5884.2008.00363.x", "10.1068/p2939", "10.1068/p160017", "10.1007/PL00005624", "10.1016/j.visres.2009.09.017", "10.1167/11.8.11", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008"]}, "10.1109/TVCG.2018.2793598": {"doi": "10.1109/TVCG.2018.2793598", "author": ["T. C. Peck", "M. Doan", "K. A. Bourne", "J. J. Good"], "title": "The Effect of Gender Body-Swap Illusions on Working Memory and Stereotype Threat", "year": "2018", "abstract": "The underrepresentation of women in technical and STEM fields is a well-known problem, and stereotype threatening situations have been linked to the inability to recruit and retain women into these fields. Virtual reality enables the unique ability to perform body-swap illusions, and research has shown that these illusions can change participant behavior. Characteristically people take on the traits of the avatar they are embodying. We hypothesized that female participants embodying male avatars when a stereotype threat was made salient would demonstrate stereotype lift. We tested our hypothesis through a between-participants user study in an immersive virtual environment by measuring working memory. Our results support that stereotype threat can be induced in an immersive virtual environment, and that stereotype lift is possible with fully-immersive body-swap illusions. Additionally, our results suggest that participants in a gender-swapped avatar without an induced stereotype threat have significantly impaired working memory; however, this impairment is lifted when a threat is made salient. We discuss possible theories as to why a body-swap illusion from a female participant into a male avatar would only increase working memory impairment when not under threat, as well as applications and future research directions. Our results offer additional insight into understanding the cognitive effects of body-swap illusions, and provide evidence that virtual reality may be an applicable tool for decreasing the gender gap in technology.", "keywords": ["avatars", "cognition", "gender issues", "psychology", "working memory", "stereotype threat", "stereotype threatening situations", "STEM fields", "technical fields", "gender body-swap illusions", "working memory impairment", "induced stereotype threat", "fully-immersive body-swap illusions", "immersive virtual environment", "stereotype lift", "male avatar", "female participant", "participant behavior", "body-swap illusion", "virtual reality", "Avatars", "Atmospheric measurements", "Particle measurements", "Electronic mail", "Virtual environments", "Virtual reality", "body-swap illusions", "virtual embodiment", "avatars", "stereotype threat", "working memory", "Adolescent", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Memory, Short-Term", "Stereotyping", "Virtual Reality", "Virtual Reality Exposure Therapy", "Young Adult"], "referenced_by": ["IKEY:8797926", "IKEY:8998371"], "referencing": ["IKEY:6479188", "IKEY:6479188", "IKEY:6479188", "10.1145/1978942.1979052", "10.1145/1978942.1979052", "10.1145/1978942.1979052", "10.1080/15213269.2012.755877", "10.1016/B978-012679130-3/50039-9", "10.3758/BF03194059", "10.1080/17482798.2016.1268779", "10.1073/pnas.1306779110", "10.3389/fnhum.2016.00601", "10.1037/0096-3445.136.2.256", "10.1016/S0065-2601(08)60024-6", "10.1371/journal.pone.0148060", "10.1038/35784", "10.1002/ejsp.607", "10.1016/S0022-1031(03)00039-8", "10.1073/pnas.1422822112", "10.1007/BF03173012", "10.1016/j.lindif.2015.12.018", "10.1111/1467-8721.00160", "10.1192/bjpo.bp.115.002147", "10.1371/journal.pone.0111933", "10.1016/j.psychres.2013.12.014", "10.1016/j.appdev.2007.10.004", "10.1371/journal.pone.0174965", "10.1037/0022-3514.93.4.544", "10.1037/0096-3445.130.2.169", "10.3389/fnhum.2015.00141", "10.1037/h0036125", "10.1089/cyber.2013.0358", "10.1126/science.1143439", "10.1016/j.cognition.2013.04.002", "10.1016/j.tics.2014.11.001", "10.1016/j.appdev.2006.06.003", "10.2466/pms.109.1.76-78", "10.1007/s002210050473", "10.1177/0146167203029006010", "10.1177/0093650209346802", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0003832", "10.1016/j.chb.2015.04.010", "10.1037/0096-3445.123.4.374", "10.1177/0146167213513475", "10.1006/jesp.2001.1500", "10.1037/0022-3514.85.3.440", "10.1111/0022-4537.00203", "10.1080/17470910903205503", "10.1177/1088868306294790", "10.1371/journal.pone.0010564", "10.1006/jesp.1998.1373", "10.1037//0022-3514.69.5.797", "10.1177/014616702237648", "10.1037/0022-3514.77.6.1213", "10.1371/journal.pone.0040682", "10.1371/journal.pone.0004040", "10.1037/0278-7393.30.6.1302", "10.1016/S0022-1031(03)00019-2", "10.3389/fpsyg.2013.00433", "10.1111/j.1468-2958.2007.00299.x", "10.1080/15213269.2012.755877", "10.1016/B978-012679130-3/50039-9", "10.3758/BF03194059", "10.1080/17482798.2016.1268779", "10.1073/pnas.1306779110", "10.3389/fnhum.2016.00601", "10.1037/0096-3445.136.2.256", "10.1016/S0065-2601(08)60024-6", "10.1371/journal.pone.0148060", "10.1038/35784", "10.1002/ejsp.607", "10.1016/S0022-1031(03)00039-8", "10.1073/pnas.1422822112", "10.1007/BF03173012", "10.1016/j.lindif.2015.12.018", "10.1111/1467-8721.00160", "10.1192/bjpo.bp.115.002147", "10.1371/journal.pone.0111933", "10.1016/j.psychres.2013.12.014", "10.1016/j.appdev.2007.10.004", "10.1371/journal.pone.0174965", "10.1037/0022-3514.93.4.544", "10.1037/0096-3445.130.2.169", "10.3389/fnhum.2015.00141", "10.1037/h0036125", "10.1089/cyber.2013.0358", "10.1126/science.1143439", "10.1016/j.cognition.2013.04.002", "10.1016/j.tics.2014.11.001", "10.1016/j.appdev.2006.06.003", "10.2466/pms.109.1.76-78", "10.1007/s002210050473", "10.1177/0146167203029006010", "10.1177/0093650209346802", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0003832", "10.1016/j.chb.2015.04.010", "10.1037/0096-3445.123.4.374", "10.1177/0146167213513475", "10.1006/jesp.2001.1500", "10.1037/0022-3514.85.3.440", "10.1111/0022-4537.00203", "10.1080/17470910903205503", "10.1177/1088868306294790", "10.1371/journal.pone.0010564", "10.1006/jesp.1998.1373", "10.1037//0022-3514.69.5.797", "10.1177/014616702237648", "10.1037/0022-3514.77.6.1213", "10.1371/journal.pone.0040682", "10.1371/journal.pone.0004040", "10.1037/0278-7393.30.6.1302", "10.1016/S0022-1031(03)00019-2", "10.3389/fpsyg.2013.00433", "10.1111/j.1468-2958.2007.00299.x", "10.1080/15213269.2012.755877", "10.1016/B978-012679130-3/50039-9", "10.3758/BF03194059", "10.1080/17482798.2016.1268779", "10.1073/pnas.1306779110", "10.3389/fnhum.2016.00601", "10.1037/0096-3445.136.2.256", "10.1016/S0065-2601(08)60024-6", "10.1371/journal.pone.0148060", "10.1038/35784", "10.1002/ejsp.607", "10.1016/S0022-1031(03)00039-8", "10.1073/pnas.1422822112", "10.1007/BF03173012", "10.1016/j.lindif.2015.12.018", "10.1111/1467-8721.00160", "10.1192/bjpo.bp.115.002147", "10.1371/journal.pone.0111933", "10.1016/j.psychres.2013.12.014", "10.1016/j.appdev.2007.10.004", "10.1371/journal.pone.0174965", "10.1037/0022-3514.93.4.544", "10.1037/0096-3445.130.2.169", "10.3389/fnhum.2015.00141", "10.1037/h0036125", "10.1089/cyber.2013.0358", "10.1126/science.1143439", "10.1016/j.cognition.2013.04.002", "10.1016/j.tics.2014.11.001", "10.1016/j.appdev.2006.06.003", "10.2466/pms.109.1.76-78", "10.1007/s002210050473", "10.1177/0146167203029006010", "10.1177/0093650209346802", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0003832", "10.1016/j.chb.2015.04.010", "10.1037/0096-3445.123.4.374", "10.1177/0146167213513475", "10.1006/jesp.2001.1500", "10.1037/0022-3514.85.3.440", "10.1111/0022-4537.00203", "10.1080/17470910903205503", "10.1177/1088868306294790", "10.1371/journal.pone.0010564", "10.1006/jesp.1998.1373", "10.1037//0022-3514.69.5.797", "10.1177/014616702237648", "10.1037/0022-3514.77.6.1213", "10.1371/journal.pone.0040682", "10.1371/journal.pone.0004040", "10.1037/0278-7393.30.6.1302", "10.1016/S0022-1031(03)00019-2", "10.3389/fpsyg.2013.00433", "10.1111/j.1468-2958.2007.00299.x"]}, "10.1109/TVCG.2018.2794098": {"doi": "10.1109/TVCG.2018.2794098", "author": ["A. Rungta", "C. Schissler", "N. Rewkowski", "R. Mehra", "D. Manocha"], "title": "Diffraction Kernels for Interactive Sound Propagation in Dynamic Environments", "year": "2018", "abstract": "We present a novel method to generate plausible diffraction effects for interactive sound propagation in dynamic scenes. Our approach precomputes a diffraction kernel for each dynamic object in the scene and combines them with interactive ray tracing algorithms at runtime. A diffraction kernel encapsulates the sound interaction behavior of individual objects in the free field and we present a new source placement algorithm to significantly accelerate the precomputation. Our overall propagation algorithm can handle highly-tessellated or smooth objects undergoing rigid motion. We have evaluated our algorithm's performance on different scenarios with multiple moving objects and demonstrate the benefits over prior interactive geometric sound propagation methods. We also performed a user study to evaluate the perceived smoothness of the diffracted field and found that the auditory perception using our approach is comparable to that of a wave-based sound propagation method.", "keywords": ["acoustic wave propagation", "hearing", "interactive systems", "ray tracing", "diffraction kernel", "interactive sound propagation", "dynamic environments", "plausible diffraction effects", "dynamic scenes", "dynamic object", "interactive ray", "sound interaction behavior", "individual objects", "source placement algorithm", "propagation algorithm", "smooth objects", "multiple moving objects", "prior interactive geometric sound propagation methods", "diffracted field", "auditory perception", "wave-based sound propagation method", "Diffraction", "Heuristic algorithms", "Kernel", "Computational modeling", "Acoustics", "Ray tracing", "Runtime", "sound propagation", "diffraction", "dynamic environments", "spatial presence"], "referenced_by": ["IKEY:8794093", "IKEY:8998301", "IKEY:9089553"], "referencing": ["IKEY:7014276", "IKEY:6143937", "IKEY:855493", "IKEY:7014276", "IKEY:6143937", "IKEY:855493", "IKEY:7014276", "IKEY:6143937", "IKEY:855493", "10.1145/2980179.2982431", "10.1145/777792.777839", "10.1145/1179352.1141983", "10.1145/2451236.2451245", "10.1145/1179352.1141924", "10.1145/1833349.1778805", "10.1145/2931002.2963134", "10.1145/2943779", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2508363.2508420", "10.1145/2980179.2982431", "10.1145/777792.777839", "10.1145/1179352.1141983", "10.1145/2451236.2451245", "10.1145/1179352.1141924", "10.1145/1833349.1778805", "10.1145/2931002.2963134", "10.1145/2943779", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2508363.2508420", "10.1145/2980179.2982431", "10.1145/777792.777839", "10.1145/1179352.1141983", "10.1145/2451236.2451245", "10.1145/1179352.1141924", "10.1145/1833349.1778805", "10.1145/2931002.2963134", "10.1145/2943779", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2508363.2508420", "10.1016/S0045-7825(98)00051-6", "10.1121/1.418499", "10.1121/1.425644", "10.1007/978-1-84882-733-2", "10.1002/nme.2080", "10.1016/0022-460X(68)90198-3", "10.1250/ast.26.145", "10.1155/2007/70540", "10.1121/1.4949905", "10.1121/1.4950568", "10.1121/1.4981234", "10.1121/1.4926438", "10.3813/AAA.918304", "10.1121/1.428071", "10.1121/1.1340647", "10.1016/0032-5910(95)03049-2", "10.1016/S0045-7825(98)00051-6", "10.1121/1.418499", "10.1121/1.425644", "10.1007/978-1-84882-733-2", "10.1002/nme.2080", "10.1016/0022-460X(68)90198-3", "10.1250/ast.26.145", "10.1155/2007/70540", "10.1121/1.4949905", "10.1121/1.4950568", "10.1121/1.4981234", "10.1121/1.4926438", "10.3813/AAA.918304", "10.1121/1.428071", "10.1121/1.1340647", "10.1016/0032-5910(95)03049-2", "10.1016/S0045-7825(98)00051-6", "10.1121/1.418499", "10.1121/1.425644", "10.1007/978-1-84882-733-2", "10.1002/nme.2080", "10.1016/0022-460X(68)90198-3", "10.1250/ast.26.145", "10.1155/2007/70540", "10.1121/1.4949905", "10.1121/1.4950568", "10.1121/1.4981234", "10.1121/1.4926438", "10.3813/AAA.918304", "10.1121/1.428071", "10.1121/1.1340647", "10.1016/0032-5910(95)03049-2"]}, "10.1109/TVCG.2018.2793671": {"doi": "10.1109/TVCG.2018.2793671", "author": ["P. Schmitz", "J. Hildebrandt", "A. C. Valdez", "L. Kobbelt", "M. Ziefle"], "title": "You Spin my Head Right Round: Threshold of Limited Immersion for Rotation Gains in Redirected Walking", "year": "2018", "abstract": "In virtual environments, the space that can be explored by real walking is limited by the size of the tracked area. To enable unimpeded walking through large virtual spaces in small real-world surroundings, redirection techniques are used. These unnoticeably manipulate the user's virtual walking trajectory. It is important to know how strongly such techniques can be applied without the user noticing the manipulation - or getting cybersick. Previously, this was estimated by measuring a detection threshold (DT) in highly-controlled psychophysical studies, which experimentally isolate the effect but do not aim for perceived immersion in the context of VR applications. While these studies suggest that only relatively low degrees of manipulation are tolerable, we claim that, besides establishing detection thresholds, it is important to know when the user's immersion breaks. We hypothesize that the degree of unnoticed manipulation is significantly different from the detection threshold when the user is immersed in a task. We conducted three studies: a) to devise an experimental paradigm to measure the threshold of limited immersion (TLI), b) to measure the TLI for slowly decreasing and increasing rotation gains, and c) to establish a baseline of cybersickness for our experimental setup. For rotation gains greater than 1.0, we found that immersion breaks quite late after the gain is detectable. However, for gains lesser than 1.0, some users reported a break of immersion even before established detection thresholds were reached. Apparently, the developed metric measures an additional quality of user experience. This article contributes to the development of effective spatial compression methods by utilizing the break of immersion as a benchmark for redirection techniques.", "keywords": ["human factors", "user interfaces", "virtual reality", "user experience", "established detection thresholds", "experimental setup", "experimental paradigm", "unnoticed manipulation", "immersion breaks", "relatively low degrees", "perceived immersion", "highly-controlled psychophysical studies", "detection threshold", "virtual walking trajectory", "redirection techniques", "real-world surroundings", "virtual spaces", "unimpeded walking", "tracked area", "virtual environments", "redirected walking", "rotation gains", "Legged locomotion", "Visualization", "Gain measurement", "Rotation measurement", "Tracking", "Virtual reality", "redirected walking", "rotation gain", "perceptual threshold", "immersion", "cybersickness", "Adolescent", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Motion Sickness", "Rotation", "Virtual Reality", "Walking", "Young Adult"], "referenced_by": ["IKEY:8797989", "IKEY:8797994", "IKEY:9019652", "IKEY:8998141", "IKEY:8580399", "IKEY:9089480", "IKEY:9089561", "IKEY:9133071"], "referencing": ["IKEY:6802053", "IKEY:7010955", "IKEY:6479192", "IKEY:6200791", "IKEY:4663065", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6165136", "IKEY:6802053", "IKEY:7010955", "IKEY:6479192", "IKEY:6200791", "IKEY:4663065", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6165136", "IKEY:6802053", "IKEY:7010955", "IKEY:6479192", "IKEY:6200791", "IKEY:4663065", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6165136", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2983310.2989204", "10.1145/333329.333344", "10.1145/2207676.2207690", "10.1145/311535.311589", "10.1145/2993369.2996349", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2983310.2989204", "10.1145/333329.333344", "10.1145/2207676.2207690", "10.1145/311535.311589", "10.1145/2993369.2996349", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2983310.2989204", "10.1145/333329.333344", "10.1145/2207676.2207690", "10.1145/311535.311589", "10.1145/2993369.2996349", "10.1162/pres.1992.1.3.334", "10.1162/105474699566107", "10.1518/001872098779591386", "10.1207/s15327108ijap0303_3", "10.4028/www.scientific.net/AEF.10.34", "10.17011/ht/urn.201611174652", "10.1016/j.cedpsych.2010.10.002", "10.1207/s15326969eco0303_2", "10.1162/105474600566925", "10.1518/hfes.45.3.504.27254", "10.1006/ijhc.1996.0035", "10.1097/OPX.0b013e31825da430", "10.1162/pres.1992.1.3.334", "10.1162/105474699566107", "10.1518/001872098779591386", "10.1207/s15327108ijap0303_3", "10.4028/www.scientific.net/AEF.10.34", "10.17011/ht/urn.201611174652", "10.1016/j.cedpsych.2010.10.002", "10.1207/s15326969eco0303_2", "10.1162/105474600566925", "10.1518/hfes.45.3.504.27254", "10.1006/ijhc.1996.0035", "10.1097/OPX.0b013e31825da430", "10.1162/pres.1992.1.3.334", "10.1162/105474699566107", "10.1518/001872098779591386", "10.1207/s15327108ijap0303_3", "10.4028/www.scientific.net/AEF.10.34", "10.17011/ht/urn.201611174652", "10.1016/j.cedpsych.2010.10.002", "10.1207/s15326969eco0303_2", "10.1162/105474600566925", "10.1518/hfes.45.3.504.27254", "10.1006/ijhc.1996.0035", "10.1097/OPX.0b013e31825da430"]}, "10.1109/TVCG.2018.2793599": {"doi": "10.1109/TVCG.2018.2793599", "author": ["V. Sitzmann", "A. Serrano", "A. Pavel", "M. Agrawala", "D. Gutierrez", "B. Masia", "G. Wetzstein"], "title": "Saliency in VR: How Do People Explore Virtual Environments?", "year": "2018", "abstract": "Understanding how people explore immersive virtual environments is crucial for many applications, such as designing virtual reality (VR) content, developing new compression algorithms, or learning computational models of saliency or visual attention. Whereas a body of recent work has focused on modeling saliency in desktop viewing conditions, VR is very different from these conditions in that viewing behavior is governed by stereoscopic vision and by the complex interaction of head orientation, gaze, and other kinematic constraints. To further our understanding of viewing behavior and saliency in VR, we capture and analyze gaze and head orientation data of 169 users exploring stereoscopic, static omni-directional panoramas, for a total of 1980 head and gaze trajectories for three different viewing conditions. We provide a thorough analysis of our data, which leads to several important insights, such as the existence of a particular fixation bias, which we then use to adapt existing saliency predictors to immersive VR conditions. In addition, we explore other applications of our data and analysis, including automatic alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and saliency-basedcompression.", "keywords": ["gaze tracking", "image motion analysis", "stereo image processing", "virtual reality", "immersive virtual environments", "virtual reality content", "compression algorithms", "computational models", "visual attention", "modeling saliency", "desktop viewing conditions", "viewing behavior", "stereoscopic vision", "head orientation data", "stereoscopic omni-directional panoramas", "static omni-directional panoramas", "immersive VR conditions", "VR video cuts", "viewing conditions", "saliency predictors", "gaze orientation data", "saliency-based compression", "Head", "Visualization", "Magnetic heads", "Virtual environments", "Stereo image processing", "Predictive models", "Computational modeling", "Saliency", "omnidirectional stereoscopic panoramas", "Adolescent", "Adult", "Computer Graphics", "Computer Simulation", "Depth Perception", "Exploratory Behavior", "Female", "Fixation, Ocular", "Humans", "Male", "Middle Aged", "User-Computer Interface", "Video Recording", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8434258", "IKEY:8463369", "IKEY:8551577", "IKEY:8616354", "IKEY:8613647", "IKEY:8643434", "IKEY:8642906", "IKEY:8610044", "IKEY:8683854", "IKEY:8683318", "IKEY:8702654", "IKEY:8717893", "IKEY:8715472", "IKEY:8803479", "IKEY:8803296", "IKEY:8803637", "IKEY:8726371", "IKEY:8736985", "IKEY:8842556", "IKEY:8868478", "IKEY:8901701", "IKEY:8926406", "IKEY:8953510", "IKEY:8961433", "IKEY:8969778", "IKEY:8911454", "IKEY:8960364", "IKEY:8945366", "IKEY:8984718", "10.1049/trit.2018.1012"], "referencing": ["10.1145/2980179.2980257", "10.1145/1743666.1743696", "10.1145/3025453.3025675", "10.1145/2980179.2980246", "10.1145/355017.355028", "10.1145/1124772.1124886", "10.1145/3072959.3073668", "10.1145/2897824.2925883", "10.1145/332040.332443", "10.1145/3025453.3025521", "10.1145/2980179.2980257", "10.1145/1743666.1743696", "10.1145/3025453.3025675", "10.1145/2980179.2980246", "10.1145/355017.355028", "10.1145/1124772.1124886", "10.1145/3072959.3073668", "10.1145/2897824.2925883", "10.1145/332040.332443", "10.1145/3025453.3025521", "10.1145/2980179.2980257", "10.1145/1743666.1743696", "10.1145/3025453.3025675", "10.1145/2980179.2980246", "10.1145/355017.355028", "10.1145/1124772.1124886", "10.1145/3072959.3073668", "10.1145/2897824.2925883", "10.1145/332040.332443", "10.1145/3025453.3025521", "10.3758/APP.71.4.881", "10.1016/j.physa.2003.09.011", "10.1167/12.2.9", "10.1089/cpb.2004.7.621", "10.1007/s00221-008-1504-8", "10.1007/978-94-009-3833-5_5", "10.1167/14.3.14", "10.1113/jphysiol.1986.sp016043", "10.3758/s13428-012-0226-9", "10.1016/j.visres.2014.12.026", "10.1016/j.visres.2015.10.001", "10.1167/10.8.20", "10.1073/pnas.1617251114", "10.1111/cgf.12603", "10.1037/0033-295X.113.4.766", "10.1016/j.sigpro.2012.06.014", "10.3758/APP.71.4.881", "10.1016/j.physa.2003.09.011", "10.1167/12.2.9", "10.1089/cpb.2004.7.621", "10.1007/s00221-008-1504-8", "10.1007/978-94-009-3833-5_5", "10.1167/14.3.14", "10.1113/jphysiol.1986.sp016043", "10.3758/s13428-012-0226-9", "10.1016/j.visres.2014.12.026", "10.1016/j.visres.2015.10.001", "10.1167/10.8.20", "10.1073/pnas.1617251114", "10.1111/cgf.12603", "10.1037/0033-295X.113.4.766", "10.1016/j.sigpro.2012.06.014", "10.3758/APP.71.4.881", "10.1016/j.physa.2003.09.011", "10.1167/12.2.9", "10.1089/cpb.2004.7.621", "10.1007/s00221-008-1504-8", "10.1007/978-94-009-3833-5_5", "10.1167/14.3.14", "10.1113/jphysiol.1986.sp016043", "10.3758/s13428-012-0226-9", "10.1016/j.visres.2014.12.026", "10.1016/j.visres.2015.10.001", "10.1167/10.8.20", "10.1073/pnas.1617251114", "10.1111/cgf.12603", "10.1037/0033-295X.113.4.766", "10.1016/j.sigpro.2012.06.014"]}, "10.1109/TVCG.2018.2794629": {"doi": "10.1109/TVCG.2018.2794629", "author": ["T. Waltemate", "D. Gall", "D. Roth", "M. Botsch", "M. E. Latoschik"], "title": "The Impact of Avatar Personalization and Immersion on Virtual Body Ownership, Presence, and Emotional Response", "year": "2018", "abstract": "This article reports the impact of the degree of personalization and individualization of users' avatars as well as the impact of the degree of immersion on typical psychophysical factors in embodied Virtual Environments. We investigated if and how virtual body ownership (including agency), presence, and emotional response are influenced depending on the specific look of users' avatars, which varied between (1) a generic hand-modeled version, (2) a generic scanned version, and (3) an individualized scanned version. The latter two were created using a state-of-the-art photogrammetry method providing a fast 3D-scan and post-process workflow. Users encountered their avatars in a virtual mirror metaphor using two VR setups that provided a varying degree of immersion, (a) a large screen surround projection (L-shape part of a CAVE) and (b) a head-mounted display (HMD). We found several significant as well as a number of notable effects. First, personalized avatars significantly increase body ownership, presence, and dominance compared to their generic counterparts, even if the latter were generated by the same photogrammetry process and hence could be valued as equal in terms of the degree of realism and graphical quality. Second, the degree of immersion significantly increases the body ownership, agency, as well as the feeling of presence. These results substantiate the value of personalized avatars resembling users' real-world appearances as well as the value of the deployed scanning process to generate avatars for VR-setups where the effect strength might be substantial, e.g., in social Virtual Reality (VR) or in medical VR-based therapies relying on embodied interfaces. Additionally, our results also strengthen the value of fully immersive setups which, today, are accessible for a variety of applications due to the widely available consumer HMDs.", "keywords": ["avatars", "helmet mounted displays", "solid modelling", "VR-setups", "social Virtual Reality", "fully immersive setups", "avatar personalization", "virtual body ownership", "emotional response", "individualization", "embodied Virtual Environments", "generic hand-modeled version", "generic scanned version", "individualized scanned version", "post-process workflow", "virtual mirror metaphor", "screen surround projection", "personalized avatars", "photogrammetry process", "photogrammetry method", "avatar immersion", "psychophysical factors", "head-mounted display", "CAVE", "user real-world appearances", "medical VR-based therapies", "graphical quality", "realism degree", "Avatars", "Mirrors", "Rubber", "Three-dimensional displays", "Psychology", "Face", "Avatars", "presence", "virtual body ownership", "emotion", "personalization", "immersion", "Adult", "Clothing", "Computer Graphics", "Emotions", "Female", "Humans", "Male", "Personality", "Task Performance and Analysis", "User-Computer Interface", "Video Games", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8648222", "IKEY:8643417", "IKEY:8643340", "IKEY:8798139", "IKEY:8797719", "IKEY:8798208", "IKEY:8798040", "IKEY:8798021", "IKEY:8798264", "IKEY:8848005", "IKEY:8864579", "IKEY:8942371", "IKEY:8943738", "IKEY:8998305", "IKEY:9089482", "IKEY:9089654", "IKEY:9090662", "IKEY:9089660", "IKEY:9090591", "IKEY:9090671", "IKEY:9103109", "IKEY:9152989", "IKEY:9199571", "IKEY:9284723", "IKEY:9284711", "IKEY:9284776", "10.1049/iet-sen.2019.0038"], "referencing": ["IKEY:6479188", "IKEY:7223379", "IKEY:7504761", "IKEY:6479188", "IKEY:7223379", "IKEY:7504761", "IKEY:6479188", "IKEY:7223379", "IKEY:7504761", "10.1145/3139131.3139154", "10.1145/91394.91409", "10.1145/2699276.2721398", "10.1145/129888.129892", "10.1145/3084363.3085045", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/882262.882269", "10.1145/3027063.3053272", "10.1145/2821592.2821607", "10.1145/2993369.2993381", "10.1145/2470654.2466428", "10.1145/3139131.3139154", "10.1145/91394.91409", "10.1145/2699276.2721398", "10.1145/129888.129892", "10.1145/3084363.3085045", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/882262.882269", "10.1145/3027063.3053272", "10.1145/2821592.2821607", "10.1145/2993369.2993381", "10.1145/2470654.2466428", "10.1145/3139131.3139154", "10.1145/91394.91409", "10.1145/2699276.2721398", "10.1145/129888.129892", "10.1145/3084363.3085045", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/882262.882269", "10.1145/3027063.3053272", "10.1145/2821592.2821607", "10.1145/2993369.2993381", "10.1145/2470654.2466428", "10.1073/pnas.1306779110", "10.1089/cpb.2004.7.734", "10.1111/j.1468-2958.2008.00322.x", "10.1207/S15327965PLI1302_01", "10.1038/35784", "10.1162/pres.17.4.376", "10.1016/0005-7916(94)90063-9", "10.1162/PRES_a_00005", "10.1162/105474699566017", "10.1162/pres.15.4.455", "10.3389/fnhum.2012.00040", "10.1007/978-3-319-47665-0_31", "10.3389/fnhum.2013.00083", "10.1080/02699930802204677", "10.1126/science.1136930", "10.1371/journal.pone.0016128", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0010381", "10.1162/105474699566477", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010564", "10.1162/105474600566925", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3389/fict.2015.00008", "10.1037/0096-1523.31.1.80", "10.1080/02699930903498186", "10.1111/j.1468-2958.2007.00299.x", "10.1073/pnas.1306779110", "10.1089/cpb.2004.7.734", "10.1111/j.1468-2958.2008.00322.x", "10.1207/S15327965PLI1302_01", "10.1038/35784", "10.1162/pres.17.4.376", "10.1016/0005-7916(94)90063-9", "10.1162/PRES_a_00005", "10.1162/105474699566017", "10.1162/pres.15.4.455", "10.3389/fnhum.2012.00040", "10.1007/978-3-319-47665-0_31", "10.3389/fnhum.2013.00083", "10.1080/02699930802204677", "10.1126/science.1136930", "10.1371/journal.pone.0016128", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0010381", "10.1162/105474699566477", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010564", "10.1162/105474600566925", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3389/fict.2015.00008", "10.1037/0096-1523.31.1.80", "10.1080/02699930903498186", "10.1111/j.1468-2958.2007.00299.x", "10.1073/pnas.1306779110", "10.1089/cpb.2004.7.734", "10.1111/j.1468-2958.2008.00322.x", "10.1207/S15327965PLI1302_01", "10.1038/35784", "10.1162/pres.17.4.376", "10.1016/0005-7916(94)90063-9", "10.1162/PRES_a_00005", "10.1162/105474699566017", "10.1162/pres.15.4.455", "10.3389/fnhum.2012.00040", "10.1007/978-3-319-47665-0_31", "10.3389/fnhum.2013.00083", "10.1080/02699930802204677", "10.1126/science.1136930", "10.1371/journal.pone.0016128", "10.1016/j.concog.2013.04.016", "10.1371/journal.pone.0010381", "10.1162/105474699566477", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010564", "10.1162/105474600566925", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3389/fict.2015.00008", "10.1037/0096-1523.31.1.80", "10.1080/02699930903498186", "10.1111/j.1468-2958.2007.00299.x"]}, "10.1109/TVCG.2018.2794222": {"doi": "10.1109/TVCG.2018.2794222", "author": ["R. Xiao", "J. Schwarz", "N. Throm", "A. D. Wilson", "H. Benko"], "title": "MRTouch: Adding Touch Input to Head-Mounted Mixed Reality", "year": "2018", "abstract": "We present MRTouch, a novel multitouch input solution for head-mounted mixed reality systems. Our system enables users to reach out and directly manipulate virtual interfaces affixed to surfaces in their environment, as though they were touchscreens. Touch input offers precise, tactile and comfortable user input, and naturally complements existing popular modalities, such as voice and hand gesture. Our research prototype combines both depth and infrared camera streams together with real-time detection and tracking of surface planes to enable robust finger-tracking even when both the hand and head are in motion. Our technique is implemented on a commercial Microsoft HoloLens without requiring any additional hardware nor any user or environmental calibration. Through our performance evaluation, we demonstrate high input accuracy with an average positional error of 5.4 mm and 95% button size of 16 mm, across 17 participants, 2 surface orientations and 4 surface materials. Finally, we demonstrate the potential of our technique to enable on-world touch interactions through 5 example applications.", "keywords": ["augmented reality", "haptic interfaces", "helmet mounted displays", "human computer interaction", "touch sensitive screens", "tracking", "user interfaces", "MRTouch", "touch input", "head-mounted mixed reality", "mixed reality systems", "virtual interfaces", "infrared camera streams", "robust finger-tracking", "commercial Microsoft HoloLens", "high input accuracy", "on-world touch interactions", "multitouch input solution", "Microsoft HoloLens", "Virtual reality", "Thumb", "Cameras", "Tracking", "Engines", "Sensors", "Augmented reality", "touch interaction", "depth sensing", "sensor fusion", "on-world interaction", "Adult", "Algorithms", "Depth Perception", "Female", "Head", "Humans", "Image Processing, Computer-Assisted", "Male", "Touch", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8699311", "IKEY:8797993", "IKEY:8809589", "IKEY:9015075", "IKEY:9090487", "IKEY:9089598", "IKEY:9102253"], "referencing": ["IKEY:1492776", "IKEY:4767851", "IKEY:5387038", "IKEY:4637353", "IKEY:4384131", "IKEY:1492776", "IKEY:4767851", "IKEY:5387038", "IKEY:4637353", "IKEY:4384131", "IKEY:1492776", "IKEY:4767851", "IKEY:5387038", "IKEY:4637353", "IKEY:4384131", "10.1145/1186415.1186463", "10.1145/2839462.2839484", "10.1145/2858036.2858226", "10.1145/2999508.2999530", "10.1145/2984511.2984526", "10.1145/800250.807503", "10.1145/1111360.1111370", "10.1145/2501988.2502018", "10.1145/2818346.2820752", "10.1145/2642918.2647392", "10.1145/358669.358692", "10.1145/2984511.2984576", "10.1145/2047196.2047273", "10.1145/1095034.1095054", "10.1145/2047196.2047255", "10.1145/2858036.2858134", "10.1145/2556288.2557130", "10.1145/2858036.2858095", "10.1145/1753326.1753413", "10.1145/258549.258715", "10.1145/2047196.2047270", "10.1145/2470654.2481317", "10.1145/504704.504706", "10.1145/1029632.1029652", "10.1145/1077534.1077562", "10.1145/253284.253292", "10.1145/2534329.2534349", "10.1145/2380116.2380174", "10.1145/2461912.2462007", "10.1145/2470654.2466113", "10.1145/2992154.2992173", "10.1145/2628363.2628383", "10.1145/1186415.1186463", "10.1145/2839462.2839484", "10.1145/2858036.2858226", "10.1145/2999508.2999530", "10.1145/2984511.2984526", "10.1145/800250.807503", "10.1145/1111360.1111370", "10.1145/2501988.2502018", "10.1145/2818346.2820752", "10.1145/2642918.2647392", "10.1145/358669.358692", "10.1145/2984511.2984576", "10.1145/2047196.2047273", "10.1145/1095034.1095054", "10.1145/2047196.2047255", "10.1145/2858036.2858134", "10.1145/2556288.2557130", "10.1145/2858036.2858095", "10.1145/1753326.1753413", "10.1145/258549.258715", "10.1145/2047196.2047270", "10.1145/2470654.2481317", "10.1145/504704.504706", "10.1145/1029632.1029652", "10.1145/1077534.1077562", "10.1145/253284.253292", "10.1145/2534329.2534349", "10.1145/2380116.2380174", "10.1145/2461912.2462007", "10.1145/2470654.2466113", "10.1145/2992154.2992173", "10.1145/2628363.2628383", "10.1145/1186415.1186463", "10.1145/2839462.2839484", "10.1145/2858036.2858226", "10.1145/2999508.2999530", "10.1145/2984511.2984526", "10.1145/800250.807503", "10.1145/1111360.1111370", "10.1145/2501988.2502018", "10.1145/2818346.2820752", "10.1145/2642918.2647392", "10.1145/358669.358692", "10.1145/2984511.2984576", "10.1145/2047196.2047273", "10.1145/1095034.1095054", "10.1145/2047196.2047255", "10.1145/2858036.2858134", "10.1145/2556288.2557130", "10.1145/2858036.2858095", "10.1145/1753326.1753413", "10.1145/258549.258715", "10.1145/2047196.2047270", "10.1145/2470654.2481317", "10.1145/504704.504706", "10.1145/1029632.1029652", "10.1145/1077534.1077562", "10.1145/253284.253292", "10.1145/2534329.2534349", "10.1145/2380116.2380174", "10.1145/2461912.2462007", "10.1145/2470654.2466113", "10.1145/2992154.2992173", "10.1145/2628363.2628383", "10.1162/pres.1992.1.1.18", "10.1007/11424925_104", "10.1007/s11042-011-0983-y", "10.1162/pres.1992.1.1.18", "10.1007/11424925_104", "10.1007/s11042-011-0983-y", "10.1162/pres.1992.1.1.18", "10.1007/11424925_104", "10.1007/s11042-011-0983-y"]}, "10.1109/TVCG.2018.2793618": {"doi": "10.1109/TVCG.2018.2793618", "author": ["B. Xie", "Y. Zhang", "H. Huang", "E. Ogawa", "T. You", "L. -F. Yu"], "title": "Exercise Intensity-Driven Level Design", "year": "2018", "abstract": "Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player's physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge.", "keywords": ["augmented reality", "computer games", "human computer interaction", "Markov processes", "Monte Carlo methods", "motion estimation", "optimisation", "physical challenge", "exercise intensity levels", "game level synthesis", "exercise intensity-driven level design", "player physical experience", "augmented reality", "trans-dimensional optimization problem", "Reversible-jump Markov chain Monte Carlo technique", "motion-based virtual reality games", "Games", "Optimization", "Headphones", "Measurement", "Augmented reality", "Computational modeling", "Virtual Reality", "Level Design", "Procedural Modeling", "Exergaming", "Adolescent", "Adult", "Computer Graphics", "Exercise Therapy", "Female", "Humans", "Male", "User-Computer Interface", "Video Games", "Virtual Reality", "Virtual Reality Exposure Therapy", "Young Adult"], "referenced_by": ["IKEY:8448290", "IKEY:8794230", "IKEY:8942352", "IKEY:9089656", "IKEY:9101254", "IKEY:9148327", "IKEY:9284702"], "referencing": ["10.1145/1814256.1814263", "10.1145/2422956.2422957", "10.1145/1814256.1814267", "10.1145/2212908.2212942", "10.1145/1321261.1321313", "10.1145/2010324.1964981", "10.1145/1814256.1814263", "10.1145/2422956.2422957", "10.1145/1814256.1814267", "10.1145/2212908.2212942", "10.1145/1321261.1321313", "10.1145/2010324.1964981", "10.1145/1814256.1814263", "10.1145/2422956.2422957", "10.1145/1814256.1814267", "10.1145/2212908.2212942", "10.1145/1321261.1321313", "10.1145/2010324.1964981", "10.1519/JPT.0b013e3182191d98", "10.1249/00005768-198205000-00012", "10.1007/s40279-016-0485-1", "10.1162/pres_a_00036", "10.4017/gt.2013.12.2.001.00", "10.1093/biomet/82.4.711", "10.1589/jpts.28.3168", "10.2165/00007256-198805050-00002", "10.1080/02640410470001730089", "10.1016/j.apmr.2012.12.010", "10.1113/jphysiol.2007.147629", "10.1123/japa.2014-0267", "10.1016/j.entcom.2009.09.004", "10.1186/1471-2318-14-107", "10.1002/oby.20282", "10.1371/journal.pone.0069471", "10.1007/978-3-319-41316-7_9", "10.1089/g4h.2013.0093", "10.1519/JPT.0b013e3182191d98", "10.1249/00005768-198205000-00012", "10.1007/s40279-016-0485-1", "10.1162/pres_a_00036", "10.4017/gt.2013.12.2.001.00", "10.1093/biomet/82.4.711", "10.1589/jpts.28.3168", "10.2165/00007256-198805050-00002", "10.1080/02640410470001730089", "10.1016/j.apmr.2012.12.010", "10.1113/jphysiol.2007.147629", "10.1123/japa.2014-0267", "10.1016/j.entcom.2009.09.004", "10.1186/1471-2318-14-107", "10.1002/oby.20282", "10.1371/journal.pone.0069471", "10.1007/978-3-319-41316-7_9", "10.1089/g4h.2013.0093", "10.1519/JPT.0b013e3182191d98", "10.1249/00005768-198205000-00012", "10.1007/s40279-016-0485-1", "10.1162/pres_a_00036", "10.4017/gt.2013.12.2.001.00", "10.1093/biomet/82.4.711", "10.1589/jpts.28.3168", "10.2165/00007256-198805050-00002", "10.1080/02640410470001730089", "10.1016/j.apmr.2012.12.010", "10.1113/jphysiol.2007.147629", "10.1123/japa.2014-0267", "10.1016/j.entcom.2009.09.004", "10.1186/1471-2318-14-107", "10.1002/oby.20282", "10.1371/journal.pone.0069471", "10.1007/978-3-319-41316-7_9", "10.1089/g4h.2013.0093"]}, "10.1109/TVCG.2018.2793679": {"doi": "10.1109/TVCG.2018.2793679", "author": ["J. Zhang", "E. Langbehn", "D. Krupke", "N. Katzakis", "F. Steinicke"], "title": "Detection Thresholds for Rotation and Translation Gains in 360\u00b0 Video-Based Telepresence Systems", "year": "2018", "abstract": "Telepresence systems have the potential to overcome limits and distance constraints of the real-world by enabling people to remotely visit and interact with each other. However, current telepresence systems usually lack natural ways of supporting interaction and exploration of remote environments (REs). In particular, single webcams for capturing the RE provide only a limited illusion of spatial presence, and movement control of mobile platforms in today's telepresence systems are often restricted to simple interaction devices. One of the main challenges of telepresence systems is to allow users to explore a RE in an immersive, intuitive and natural way, e.g., by real walking in the user's local environment (LE), and thus controlling motions of the robot platform in the RE. However, the LE in which the user's motions are tracked usually provides a much smaller interaction space than the RE. In this context, redirected walking (RDW) is a very suitable approach to solve this problem. However, so far there is no previous work, which explored if and how RDW can be used in video-based 360\u00b0 telepresence systems. In this article, we conducted two psychophysical experiments in which we have quantified how much humans can be unknowingly redirected on virtual paths in the RE, which are different from the physical paths that they actually walk in the LE. Experiment 1 introduces a discrimination task between local and remote translations, and in Experiment 2 we analyzed the discrimination between local and remote rotations. In Experiment 1 participants performed straightforward translations in the LE that were mapped to straightforward translations in the RE shown as 360\u00b0 videos, which were manipulated by different gains. Then, participants had to estimate if the remotely perceived translation was faster or slower than the actual physically performed translation. Similarly, in Experiment 2 participants performed rotations in the LE that were mapped to the virtual rotations in a 360\u00b0 video-based RE to which we applied different gains. Again, participants had to estimate whether the remotely perceived rotation was smaller or larger than the actual physically performed rotation. Our results show that participants are not able to reliably discriminate the difference between physical motion in the LE and the virtual motion from the 360\u00b0 video RE when virtual translations are down-scaled by 5.8% and up-scaled by 9.7%, and virtual rotations are about 12.3% less or 9.2% more than the corresponding physical rotations in the LE.", "keywords": ["control engineering computing", "telerobotics", "virtual reality", "360\u00b0 video-based telepresence systems", "remote environments", "local environment", "RE", "LE", "remotely perceived rotation", "virtual rotations", "remotely perceived translation", "straightforward translations", "local rotations", "remote translations", "local translations", "Telepresence", "Legged locomotion", "Cameras", "Aerospace electronics", "Resists", "Task analysis", "Virtual reality", "telepresence", "360\u00b0 camera", "locomotion", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Rotation", "Video Recording", "Virtual Reality", "Walking", "Young Adult"], "referenced_by": ["IKEY:8797756", "IKEY:8797805", "IKEY:8998141", "IKEY:9089444", "IKEY:9162461"], "referencing": ["IKEY:6479190", "IKEY:6754845", "IKEY:7036075", "IKEY:6081857", "IKEY:1667620", "IKEY:799737", "IKEY:1381227", "IKEY:7792698", "IKEY:7833190", "IKEY:6162881", "IKEY:6200791", "IKEY:7504752", "IKEY:4663065", "IKEY:5072212", "IKEY:4741303", "IKEY:6180877", "IKEY:5759455", "IKEY:7383142", "IKEY:6549386", "IKEY:6479190", "IKEY:6754845", "IKEY:7036075", "IKEY:6081857", "IKEY:1667620", "IKEY:799737", "IKEY:1381227", "IKEY:7792698", "IKEY:7833190", "IKEY:6162881", "IKEY:6200791", "IKEY:7504752", "IKEY:4663065", "IKEY:5072212", "IKEY:4741303", "IKEY:6180877", "IKEY:5759455", "IKEY:7383142", "IKEY:6549386", "IKEY:6479190", "IKEY:6754845", "IKEY:7036075", "IKEY:6081857", "IKEY:1667620", "IKEY:799737", "IKEY:1381227", "IKEY:7792698", "IKEY:7833190", "IKEY:6162881", "IKEY:6200791", "IKEY:7504752", "IKEY:4663065", "IKEY:5072212", "IKEY:4741303", "IKEY:6180877", "IKEY:5759455", "IKEY:7383142", "IKEY:6549386", "10.1145/1556262.1556299", "10.1145/1450579.1450612", "10.1145/1227134.1227137", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3131277.3132181", "10.1145/1152399.1152451", "10.1145/333329.333344", "10.1145/2929464.2929482", "10.1145/2818048.2819922", "10.1145/1450579.1450611", "10.1145/1753326.1753482", "10.1145/1556262.1556299", "10.1145/1450579.1450612", "10.1145/1227134.1227137", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3131277.3132181", "10.1145/1152399.1152451", "10.1145/333329.333344", "10.1145/2929464.2929482", "10.1145/2818048.2819922", "10.1145/1450579.1450611", "10.1145/1753326.1753482", "10.1145/1556262.1556299", "10.1145/1450579.1450612", "10.1145/1227134.1227137", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3131277.3132181", "10.1145/1152399.1152451", "10.1145/333329.333344", "10.1145/2929464.2929482", "10.1145/2818048.2819922", "10.1145/1450579.1450611", "10.1145/1753326.1753482", "10.1007/978-3-319-51811-4_7", "10.1007/978-3-642-46354-9_25", "10.1038/415429a", "10.1162/pres.17.2.176", "10.1007/s10916-016-0481-x", "10.1016/j.cag.2012.04.011", "10.1007/978-3-540-73107-8_102", "10.1080/10668926.2016.1273152", "10.1007/978-1-4419-8432-6", "10.1016/j.ifacol.2015.06.321", "10.1007/978-3-319-05341-7_7", "10.1007/978-3-319-51811-4_7", "10.1007/978-3-642-46354-9_25", "10.1038/415429a", "10.1162/pres.17.2.176", "10.1007/s10916-016-0481-x", "10.1016/j.cag.2012.04.011", "10.1007/978-3-540-73107-8_102", "10.1080/10668926.2016.1273152", "10.1007/978-1-4419-8432-6", "10.1016/j.ifacol.2015.06.321", "10.1007/978-3-319-05341-7_7", "10.1007/978-3-319-51811-4_7", "10.1007/978-3-642-46354-9_25", "10.1038/415429a", "10.1162/pres.17.2.176", "10.1007/s10916-016-0481-x", "10.1016/j.cag.2012.04.011", "10.1007/978-3-540-73107-8_102", "10.1080/10668926.2016.1273152", "10.1007/978-1-4419-8432-6", "10.1016/j.ifacol.2015.06.321", "10.1007/978-3-319-05341-7_7"]}, "10.1109/TVCG.2018.2794638": {"doi": "10.1109/TVCG.2018.2794638", "author": ["K. Zibrek", "E. Kokkinara", "R. Mcdonnell"], "title": "The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character's Personality Play a Role?", "year": "2018", "abstract": "Virtual characters that appear almost photo-realistic have been shown to induce negative responses from viewers in traditional media, such as film and video games. This effect, described as the uncanny valley, is the reason why realism is often avoided when the aim is to create an appealing virtual character. In Virtual Reality, there have been few attempts to investigate this phenomenon and the implications of rendering virtual characters with high levels of realism on user enjoyment. In this paper, we conducted a large-scale experiment on over one thousand members of the public in order to gather information on how virtual characters are perceived in interactive virtual reality games. We were particularly interested in whether different render styles (realistic, cartoon, etc.) would directly influence appeal, or if a character's personality was the most important indicator of appeal. We used a number of perceptual metrics such as subjective ratings, proximity, and attribution bias in order to test our hypothesis. Our main result shows that affinity towards virtual characters is a complex interaction between the character's appearance and personality, and that realism is in fact a positive choice for virtual characters in virtual reality.", "keywords": ["computer games", "human computer interaction", "rendering (computer graphics)", "virtual reality", "interactive virtual reality games", "realistic virtual character appearance", "immersive environments", "virtual character rendering", "render styles", "Virtual reality", "Visualization", "Shape", "Solid modeling", "Skin", "Games", "Geometry", "Personality", "virtual characters", "virtual reality", "perception", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Personality", "Surveys and Questionnaires", "User-Computer Interface", "Video Games", "Virtual Reality"], "referenced_by": ["IKEY:8797719", "IKEY:8794519", "IKEY:8998352", "IKEY:9090662", "IKEY:9090599", "IKEY:9284778"], "referencing": ["IKEY:7014249", "IKEY:7383334", "IKEY:7014249", "IKEY:7383334", "IKEY:7014249", "IKEY:7383334", "10.1145/2492494.2502059", "10.1145/642611.642703", "10.1145/1823738.1823740", "10.1145/2185520.2185587", "10.1145/1278387.1278390", "10.1145/2816795.2818126", "10.1145/3119881.3119887", "10.1145/2628257.2628270", "10.1145/2492494.2502059", "10.1145/642611.642703", "10.1145/1823738.1823740", "10.1145/2185520.2185587", "10.1145/1278387.1278390", "10.1145/2816795.2818126", "10.1145/3119881.3119887", "10.1145/2628257.2628270", "10.1145/2492494.2502059", "10.1145/642611.642703", "10.1145/1823738.1823740", "10.1145/2185520.2185587", "10.1145/1278387.1278390", "10.1145/2816795.2818126", "10.1145/3119881.3119887", "10.1145/2628257.2628270", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1162/105474605774785235", "10.1111/j.1467-9280.2005.01619.x", "10.1007/978-1-4471-0277-9_8", "10.1093/scan/nsm017", "10.3791/4375", "10.1037/0022-3514.44.1.113", "10.1016/S0191-8869(99)00106-3", "10.1007/s11199-009-9599-3", "10.1037/0022-3514.54.5.733", "10.1007/s10803-005-0057-y", "10.1037//0022-3514.59.6.1216", "10.1080/15534510802643750", "10.1016/j.chb.2010.05.015", "10.3758/BF03212378", "10.1177/014616727800400131", "10.1016/j.chb.2008.12.026", "10.1037/0022-3514.90.5.862", "10.1016/j.jrp.2007.02.003", "10.2307/2344614", "10.1007/978-3-319-09767-1_49", "10.1016/S0065-2601(08)60357-3", "10.1162/pres.16.4.337", "10.1007/978-1-4471-0277-9_9", "10.1016/j.chb.2013.01.008", "10.1037/0022-3514.54.6.1063", "10.1111/j.1468-5884.2012.00538.x", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1162/105474605774785235", "10.1111/j.1467-9280.2005.01619.x", "10.1007/978-1-4471-0277-9_8", "10.1093/scan/nsm017", "10.3791/4375", "10.1037/0022-3514.44.1.113", "10.1016/S0191-8869(99)00106-3", "10.1007/s11199-009-9599-3", "10.1037/0022-3514.54.5.733", "10.1007/s10803-005-0057-y", "10.1037//0022-3514.59.6.1216", "10.1080/15534510802643750", "10.1016/j.chb.2010.05.015", "10.3758/BF03212378", "10.1177/014616727800400131", "10.1016/j.chb.2008.12.026", "10.1037/0022-3514.90.5.862", "10.1016/j.jrp.2007.02.003", "10.2307/2344614", "10.1007/978-3-319-09767-1_49", "10.1016/S0065-2601(08)60357-3", "10.1162/pres.16.4.337", "10.1007/978-1-4471-0277-9_9", "10.1016/j.chb.2013.01.008", "10.1037/0022-3514.54.6.1063", "10.1111/j.1468-5884.2012.00538.x", "10.1162/105474601753272844", "10.1177/0146167203029007002", "10.1162/105474605774785235", "10.1111/j.1467-9280.2005.01619.x", "10.1007/978-1-4471-0277-9_8", "10.1093/scan/nsm017", "10.3791/4375", "10.1037/0022-3514.44.1.113", "10.1016/S0191-8869(99)00106-3", "10.1007/s11199-009-9599-3", "10.1037/0022-3514.54.5.733", "10.1007/s10803-005-0057-y", "10.1037//0022-3514.59.6.1216", "10.1080/15534510802643750", "10.1016/j.chb.2010.05.015", "10.3758/BF03212378", "10.1177/014616727800400131", "10.1016/j.chb.2008.12.026", "10.1037/0022-3514.90.5.862", "10.1016/j.jrp.2007.02.003", "10.2307/2344614", "10.1007/978-3-319-09767-1_49", "10.1016/S0065-2601(08)60357-3", "10.1162/pres.16.4.337", "10.1007/978-1-4471-0277-9_9", "10.1016/j.chb.2013.01.008", "10.1037/0022-3514.54.6.1063", "10.1111/j.1468-5884.2012.00538.x"]}}