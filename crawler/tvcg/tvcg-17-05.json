{"10.1109/TVCG.2017.2674498": {"doi": "10.1109/TVCG.2017.2674498", "author": ["K. Subr", "L. Wei"], "title": "Guest Editor's Introduction to the Special Section on the ACM Symposium on Interactive 3D Graphics and Games (I3D)", "year": "2017", "abstract": "The papers in this special section were presented at the 2016 ACM Symposium on Interactive 3D Graphics and Games that was held in Redmond, WA, 27-28 February 2016.", "keywords": ["Special issues and sections", "Meetings", "Real-time systems", "Computer graphics", "Games", "Three dimensional displays"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2656889": {"doi": "10.1109/TVCG.2017.2656889", "author": ["P. Lecocq", "A. Dufay", "G. Sourimant", "J. Marvie"], "title": "Analytic Approximations for Real-Time Area Light Shading", "year": "2017", "abstract": "We introduce analytic approximations for accurate real-time rendering of surfaces lit by non-occluded area light sources. Our solution leverages the Irradiance Tensors developed by Arvo for the shading of Phong surfaces lit by a polygonal light source. Using a reformulation of the 1D boundary edge integral, we develop a general framework for approximating and evaluating the integral in constant time using simple peak shape functions. To overcome the Phong restriction, we propose a low cost edge splitting strategy that accounts for the spherical warp introduced by the half vector parametrization. Thanks to this novel extension, we accurately approximate common microfacet BRDFs, providing a practical method producing specular stretches that closely match the ground truth in real-time. Finally, using the same approximation framework, we introduce support for spherical and disc area light sources, based on an original polygon spinning method supporting non-uniform scaling operations and horizon clipping. Implemented on a GPU, our method achieves real-time performances without any assumption on area light shape nor surface roughness.", "keywords": ["approximation theory", "computational geometry", "analytic approximations", "real-time area light shading", "polygonal light source", "1D boundary edge integral", "Phong restriction", "half vector parametrization", "polygon spinning method", "GPU", "Real-time systems", "Lighting", "Light sources", "Rendering (computer graphics)", "Tensile stress", "Shape", "Integral equations", "Area light", "shading", "analytic", "microfacet", "axial moment", "real-time"], "referenced_by": ["10.1111/cgf.13546", "10.1111/cgf.13851", "10.1007/s00371-020-01954-x"], "referencing": ["10.1109/38.124286", "10.1109/TVCG.2012.152", "10.1109/38.124286", "10.1109/TVCG.2012.152", "10.1109/38.124286", "10.1109/TVCG.2012.152", "10.1145/218380.218467", "10.1145/2897824.2925895", "10.1145/218380.218500", "10.1145/258734.258769", "10.1145/1186822.1073318", "10.1145/1599301.1599383", "10.1145/1661412.1618479", "10.1145/2508363.2508386", "10.1145/2669024.2669025", "10.1145/2533687", "10.1145/258734.258801", "10.1145/218380.218467", "10.1145/2897824.2925895", "10.1145/218380.218500", "10.1145/258734.258769", "10.1145/1186822.1073318", "10.1145/1599301.1599383", "10.1145/1661412.1618479", "10.1145/2508363.2508386", "10.1145/2669024.2669025", "10.1145/2533687", "10.1145/258734.258801", "10.1145/218380.218467", "10.1145/2897824.2925895", "10.1145/218380.218500", "10.1145/258734.258769", "10.1145/1186822.1073318", "10.1145/1599301.1599383", "10.1145/1661412.1618479", "10.1145/2508363.2508386", "10.1145/2669024.2669025", "10.1145/2533687", "10.1145/258734.258801", "10.1201/b16721-8", "10.1111/cgf.12151", "10.1016/0097-8493(93)90097-S", "10.1111/1467-8659.00160", "10.1016/0097-8493(91)90079-W", "10.1016/j.cag.2010.08.002", "10.1080/10867651.2000.10487522", "10.1007/978-3-7091-6303-0_13", "10.1201/b16721-8", "10.1111/cgf.12151", "10.1016/0097-8493(93)90097-S", "10.1111/1467-8659.00160", "10.1016/0097-8493(91)90079-W", "10.1016/j.cag.2010.08.002", "10.1080/10867651.2000.10487522", "10.1007/978-3-7091-6303-0_13", "10.1201/b16721-8", "10.1111/cgf.12151", "10.1016/0097-8493(93)90097-S", "10.1111/1467-8659.00160", "10.1016/0097-8493(91)90079-W", "10.1016/j.cag.2010.08.002", "10.1080/10867651.2000.10487522", "10.1007/978-3-7091-6303-0_13"]}, "10.1109/TVCG.2017.2676089": {"doi": "10.1109/TVCG.2017.2676089", "author": ["P. Hedman", "T. Karras", "J. Lehtinen"], "title": "Sequential Monte Carlo Instant Radiosity", "year": "2017", "abstract": "Instant Radiosity and its derivatives are interactive methods for efficiently estimating global (indirect) illumination. They represent the last indirect bounce of illumination before the camera as the composite radiance field emitted by a set of virtual point light sources (VPLs). In complex scenes, current algorithms suffer from a difficult combination of two issues: it remains a challenge to distribute VPLs in a manner that simultaneously gives a high-quality indirect illumination solution for each frame, and to do so in a temporally coherent manner. We address both issues by building, and maintaining overtime, an adaptive and temporally coherent distribution of VPLs in locations where they bring indirect light to the image. We introduce a novel heuristic sampling method that strives to only move as few of the VPLs between frames as possible. The result is, to the best of our knowledge, the first interactive global illumination algorithm that works in complex, highly-occluded scenes, suffers little from temporal flickering, supports moving cameras and light sources, and is output-sensitive in the sense that it places VPLs in locations that matter most to the final result.", "keywords": ["brightness", "image sampling", "Monte Carlo methods", "rendering (computer graphics)", "sequential Monte Carlo instant radiosity", "interactive methods", "global-indirect illumination", "cameras", "composite radiance field", "virtual point light sources", "complex scenes", "high-quality indirect illumination solution", "adaptive temporally coherent VPL distribution", "heuristic sampling method", "interactive global illumination algorithm", "complex-highly-occluded scenes", "temporal flickering", "moving cameras", "light sources", "Lighting", "Cameras", "Light sources", "Coherence", "Rendering (computer graphics)", "Monte Carlo methods", "Brightness", "Instant radiosity", "global illumination"], "referenced_by": ["10.1109/ISMAR50242.2020.00017"], "referencing": ["10.1109/ISMAR.2010.5643556", "10.1109/JPROC.2007.893250", "10.1109/ISMAR.2010.5643556", "10.1109/JPROC.2007.893250", "10.1109/ISMAR.2010.5643556", "10.1109/JPROC.2007.893250", "10.1145/1053427.1053460", "10.1145/258734.258769", "10.1145/2856400.2856406", "10.1145/258734.258775", "10.1145/1073204.1073318", "10.1145/1409060.1409082", "10.1145/1283900.1283909", "10.1145/1661412.1618489", "10.1145/15922.15902", "10.1145/1015706.1015778", "10.1145/1015706.1015777", "10.1145/566654.566575", "10.1145/1053427.1053460", "10.1145/258734.258769", "10.1145/2856400.2856406", "10.1145/258734.258775", "10.1145/1073204.1073318", "10.1145/1409060.1409082", "10.1145/1283900.1283909", "10.1145/1661412.1618489", "10.1145/15922.15902", "10.1145/1015706.1015778", "10.1145/1015706.1015777", "10.1145/566654.566575", "10.1145/1053427.1053460", "10.1145/258734.258769", "10.1145/2856400.2856406", "10.1145/258734.258775", "10.1145/1073204.1073318", "10.1145/1409060.1409082", "10.1145/1283900.1283909", "10.1145/1661412.1618489", "10.1145/15922.15902", "10.1145/1015706.1015778", "10.1145/1015706.1015777", "10.1145/566654.566575", "10.1111/j.1467-8659.2007.01065.x", "10.1007/978-3-7091-6242-2_25", "10.1111/cgf.12256", "10.1111/j.1467-8659.2012.02093.x", "10.1111/cgf.12585", "10.1201/b10685", "10.1111/j.1467-8659.2011.01998.x", "10.1111/j.1467-8659.2012.03075.x", "10.1111/j.1467-8659.2008.01248.x", "10.1111/cgf.12154", "10.1214/aoms/1177700079", "10.1007/978-1-4471-0103-1_25", "10.1111/cgf.12680", "10.1111/j.1467-8659.2007.01065.x", "10.1007/978-3-7091-6242-2_25", "10.1111/cgf.12256", "10.1111/j.1467-8659.2012.02093.x", "10.1111/cgf.12585", "10.1201/b10685", "10.1111/j.1467-8659.2011.01998.x", "10.1111/j.1467-8659.2012.03075.x", "10.1111/j.1467-8659.2008.01248.x", "10.1111/cgf.12154", "10.1214/aoms/1177700079", "10.1007/978-1-4471-0103-1_25", "10.1111/cgf.12680", "10.1111/j.1467-8659.2007.01065.x", "10.1007/978-3-7091-6242-2_25", "10.1111/cgf.12256", "10.1111/j.1467-8659.2012.02093.x", "10.1111/cgf.12585", "10.1201/b10685", "10.1111/j.1467-8659.2011.01998.x", "10.1111/j.1467-8659.2012.03075.x", "10.1111/j.1467-8659.2008.01248.x", "10.1111/cgf.12154", "10.1214/aoms/1177700079", "10.1007/978-1-4471-0103-1_25", "10.1111/cgf.12680"]}, "10.1109/TVCG.2016.2642963": {"doi": "10.1109/TVCG.2016.2642963", "author": ["H. Wang", "J. Ond\u0159ej", "C. O\u2019Sullivan"], "title": "Trending Paths: A New Semantic-Level Metric for Comparing Simulated and Real Crowd Data", "year": "2017", "abstract": "We propose a new semantic-level crowd evaluation metric in this paper. Crowd simulation has been an active and important area for several decades. However, only recently has there been an increased focus on evaluating the fidelity of the results with respect to real-world situations. The focus to date has been on analyzing the properties of low-level features such as pedestrian trajectories, or global features such as crowd densities. We propose the first approach based on finding semantic information represented by latent Path Patterns in both real and simulated data in order to analyze and compare them. Unsupervised clustering by non-parametric Bayesian inference is used to learn the patterns, which themselves provide a rich visualization of the crowd behavior. To this end, we present a new Stochastic Variational Dual Hierarchical Dirichlet Process (SV-DHDP) model. The fidelity of the patterns is computed with respect to a reference, thus allowing the outputs of different algorithms to be compared with each other and/or with real data accordingly. Detailed evaluations and comparisons with existing metrics show that our method is a good alternative for comparing crowd data at a different level and also works with more types of data, holds fewer assumptions and is more robust to noise.", "keywords": ["belief networks", "digital simulation", "feature extraction", "inference mechanisms", "pattern clustering", "stochastic processes", "unsupervised learning", "variational techniques", "semantic-level crowd evaluation metric", "crowd simulation", "real-world situations", "low-level features", "pedestrian trajectories", "semantic information representation", "latent path patterns", "unsupervised clustering", "nonparametric Bayesian inference", "crowd behavior visualization", "stochastic variational dual hierarchical Dirichlet process model", "SV-DHDP model", "Trajectory", "Measurement", "Computational modeling", "Data models", "Bayes methods", "Stochastic processes", "Analytical models", "Crowd simulation", "crowd comparison", "data-driven", "clustering", "hierarchical Dirichlet process", "stochastic optimization"], "referenced_by": ["10.1109/SKIMA.2017.8294092", "10.1109/TVCG.2018.2873695", "10.1145/3272127.3275079", "10.1145/3386569.3392407", "10.1111/cgf.13303", "10.1002/cav.1866", "10.1111/cgf.13591", "10.1016/j.jocs.2019.05.003", "10.1016/j.cag.2020.03.005"], "referencing": ["10.1109/ROBOT.2008.4543489", "10.1109/CVPR.2011.5995459", "10.1109/CVPR.2007.383072", "10.1109/34.868688", "10.1109/ROBOT.1985.1087247", "10.1109/ROBOT.2008.4543489", "10.1109/CVPR.2011.5995459", "10.1109/CVPR.2007.383072", "10.1109/34.868688", "10.1109/ROBOT.1985.1087247", "10.1109/ROBOT.2008.4543489", "10.1109/CVPR.2011.5995459", "10.1109/CVPR.2007.383072", "10.1109/34.868688", "10.1109/ROBOT.1985.1087247", "10.1145/2856400.2856410", "10.1145/311535.311538", "10.1145/1141911.1142008", "10.1145/1599470.1599495", "10.1145/1599470.1599496", "10.1145/2485895.2485910", "10.1145/1360612.1360625", "10.1145/2019406.2019413", "10.1145/1870076.1870078", "10.1145/2159616.2159626", "10.1145/2448196.2448200", "10.1145/1882261.1866162", "10.1145/2019406.2019414", "10.1145/2366145.2366209", "10.1145/2856400.2856410", "10.1145/311535.311538", "10.1145/1141911.1142008", "10.1145/1599470.1599495", "10.1145/1599470.1599496", "10.1145/2485895.2485910", "10.1145/1360612.1360625", "10.1145/2019406.2019413", "10.1145/1870076.1870078", "10.1145/2159616.2159626", "10.1145/2448196.2448200", "10.1145/1882261.1866162", "10.1145/2019406.2019414", "10.1145/2366145.2366209", "10.1145/2856400.2856410", "10.1145/311535.311538", "10.1145/1141911.1142008", "10.1145/1599470.1599495", "10.1145/1599470.1599496", "10.1145/2485895.2485910", "10.1145/1360612.1360625", "10.1145/2019406.2019413", "10.1145/1870076.1870078", "10.1145/2159616.2159626", "10.1145/2448196.2448200", "10.1145/1882261.1866162", "10.1145/2019406.2019414", "10.1145/2366145.2366209", "10.1103/PhysRevE.51.4282", "10.1111/j.1467-8659.2012.03028.x", "10.1002/cav.277", "10.1002/cav.1423", "10.1111/cgf.12328", "10.1007/978-3-642-10347-6_7", "10.1111/cgf.12472", "10.1198/016214506000000302", "10.1007/978-3-319-46454-1_32", "10.1007/s11263-007-0122-4", "10.1098/rspb.2009.0405", "10.1111/j.1467-8659.2007.01090.x", "10.1007/978-1-4615-4022-9", "10.17815/CD.2016.1", "10.1111/j.1467-8659.2004.00782.x", "10.1007/978-3-642-25090-3_23", "10.1103/PhysRevE.51.4282", "10.1111/j.1467-8659.2012.03028.x", "10.1002/cav.277", "10.1002/cav.1423", "10.1111/cgf.12328", "10.1007/978-3-642-10347-6_7", "10.1111/cgf.12472", "10.1198/016214506000000302", "10.1007/978-3-319-46454-1_32", "10.1007/s11263-007-0122-4", "10.1098/rspb.2009.0405", "10.1111/j.1467-8659.2007.01090.x", "10.1007/978-1-4615-4022-9", "10.17815/CD.2016.1", "10.1111/j.1467-8659.2004.00782.x", "10.1007/978-3-642-25090-3_23", "10.1103/PhysRevE.51.4282", "10.1111/j.1467-8659.2012.03028.x", "10.1002/cav.277", "10.1002/cav.1423", "10.1111/cgf.12328", "10.1007/978-3-642-10347-6_7", "10.1111/cgf.12472", "10.1198/016214506000000302", "10.1007/978-3-319-46454-1_32", "10.1007/s11263-007-0122-4", "10.1098/rspb.2009.0405", "10.1111/j.1467-8659.2007.01090.x", "10.1007/978-1-4615-4022-9", "10.17815/CD.2016.1", "10.1111/j.1467-8659.2004.00782.x", "10.1007/978-3-642-25090-3_23"]}, "10.1109/TVCG.2017.2656082": {"doi": "10.1109/TVCG.2017.2656082", "author": ["M. McGuire", "M. Mara"], "title": "Phenomenological Transparency", "year": "2017", "abstract": "Translucent objects such as fog, clouds, smoke, glass, ice, and liquids are pervasive in cinematic environments because they frame scenes in depth and create visually-compelling shots. Unfortunately, they are hard to render in real-time and have thus previously been rendered poorly compared to opaque surfaces. This paper introduces the first model for a real-time rasterization algorithm that can simultaneously approximate the following transparency phenomena: wavelength-varying (\u201ccolored\u201d) transmission, translucent colored shadows, caustics, volumetric light and shadowing, partial coverage, diffusion, and refraction. All render efficiently with order-independent draw calls and low bandwidth. We include source code.", "keywords": ["natural scenes", "refraction", "rendering (computer graphics)", "phenomenological transparency", "cinematic environments", "translucent objects", "real-time rasterization algorithm", "wavelength-varying colored-transmission", "translucent colored shadows", "caustics", "volumetric light", "shadowing", "partial coverage", "diffusion", "refraction", "rendering", "Real-time systems", "Optical surface waves", "Glass", "Surface waves", "Rendering (computer graphics)", "Solid modeling", "Computational modeling", "Transparency", "order-independent", "refraction", "caustic", "shadow", "particle", "volumetric"], "referenced_by": ["10.1145/3203206"], "referencing": ["10.1109/TVCG.2011.282", "10.1109/TVCG.2007.32", "10.1109/MCG.2010.39", "10.1109/TVCG.2011.282", "10.1109/TVCG.2007.32", "10.1109/MCG.2010.39", "10.1109/TVCG.2011.282", "10.1109/TVCG.2007.32", "10.1109/MCG.2010.39", "10.1145/1944745.1944760", "10.1145/2556700.2556702", "10.1145/2556700.2556705", "10.1145/2856400.2856418", "10.1145/800031.808585", "10.1145/311534.311582", "10.1145/1230100.1230117", "10.1145/2448196.2448212", "10.1145/2018323.2018342", "10.1145/344779.344958", "10.1145/1507149.1507160", "10.1145/1730804.1730831", "10.1145/1730804.1730830", "10.1145/1111411.1111440", "10.1145/1073204.1073310", "10.1145/1837101.1837102", "10.1145/1944745.1944760", "10.1145/2556700.2556702", "10.1145/2556700.2556705", "10.1145/2856400.2856418", "10.1145/800031.808585", "10.1145/311534.311582", "10.1145/1230100.1230117", "10.1145/2448196.2448212", "10.1145/2018323.2018342", "10.1145/344779.344958", "10.1145/1507149.1507160", "10.1145/1730804.1730831", "10.1145/1730804.1730830", "10.1145/1111411.1111440", "10.1145/1073204.1073310", "10.1145/1837101.1837102", "10.1145/1944745.1944760", "10.1145/2556700.2556702", "10.1145/2556700.2556705", "10.1145/2856400.2856418", "10.1145/800031.808585", "10.1145/311534.311582", "10.1145/1230100.1230117", "10.1145/2448196.2448212", "10.1145/2018323.2018342", "10.1145/344779.344958", "10.1145/1507149.1507160", "10.1145/1730804.1730831", "10.1145/1730804.1730830", "10.1145/1111411.1111440", "10.1145/1073204.1073310", "10.1145/1837101.1837102", "10.1007/978-3-7091-6453-2_1", "10.1007/s00371-014-1021-7", "10.1007/s00371-009-0350-4", "10.1111/j.1467-8659.2009.01370.x", "10.1007/978-3-7091-6453-2_1", "10.1007/s00371-014-1021-7", "10.1007/s00371-009-0350-4", "10.1111/j.1467-8659.2009.01370.x", "10.1007/978-3-7091-6453-2_1", "10.1007/s00371-014-1021-7", "10.1007/s00371-009-0350-4", "10.1111/j.1467-8659.2009.01370.x"]}, "10.1109/TVCG.2016.2532335": {"doi": "10.1109/TVCG.2016.2532335", "author": ["Y. Guo", "X. Liu", "X. Xu"], "title": "A Unified Detail-Preserving Liquid Simulation by Two-Phase Lattice Boltzmann Modeling", "year": "2017", "abstract": "Traditional methods in graphics to simulate liquid-air dynamics under different scenarios usually employ separate approaches with sophisticated interface tracking/reconstruction techniques. In this paper, we propose a novel unified approach which is easy and effective to produce a variety of liquid-air interface phenomena. These phenomena, such as complex surface splashes, bubble interactions, as well as surface tension effects, can co-exist in one single simulation, and are created within the same computational framework. Such a framework is unique in that it is free from any complicated interface tracking/reconstruction procedures. Our approach is developed from the two-phase lattice Boltzmann method with the mean field model, which provides a unified framework for interface dynamics but is numerically unstable under turbulent conditions. Considering the drawbacks of the existing approaches, we propose techniques to suppress oscillations for significant stability enhancement, as well as derive a new subgrid-scale model to further improve stability, faithfully preserving liquid-air interface details without excessive diffusion by taking into account the density variation. The whole framework is highly parallel, enabling very efficient implementation. Comparisons with the related approaches show superiority on stable simulations with detail preservation and multiphase phenomena simultaneously involved. A set of animation results demonstrate the effectiveness of our method.", "keywords": ["bubbles", "computational fluid dynamics", "flow simulation", "fluid oscillations", "lattice Boltzmann methods", "surface tension", "turbulence", "two-phase flow", "unified detail-preserving liquid simulation", "two-phase lattice Boltzmann modeling", "liquid-air dynamics simulation", "liquid-air interface phenomena", "complex surface splashes", "bubble interactions", "surface tension effects", "computational framework", "interface tracking procedure", "mean field model", "unified framework", "interface dynamics", "numerical analysis", "turbulent conditions", "oscillation suppression", "stability enhancement", "subgrid-scale model", "stability improvement", "liquid-air interface details", "density variation", "multiphase phenomena", "interface reconstruction procedure", "Computational modeling", "Mathematical model", "Liquids", "Lattice Boltzmann methods", "Surface tension", "Indexes", "Flow simulation", "two-phase lattice Boltzmann method", "interface flow"], "referenced_by": ["10.1109/TVCG.2018.2859931", "10.1145/3386569.3392400", "10.1016/j.gmod.2017.09.001", "10.3390/w12010215", "10.1016/j.gmod.2020.101061", "10.1111/cgf.13955"], "referencing": ["10.1145/1141911.1141959", "10.1145/2185520.2185557", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1073204.1073298", "10.1145/2421636.2421641", "10.1145/1360612.1360647", "10.1145/2159516.2159522", "10.1145/1882261.1866198", "10.1145/2461912.2461991", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1015706.1015745", "10.1145/1073204.1073283", "10.1145/1882261.1866197", "10.1145/1315184.1315193", "10.1145/37401.37422", "10.1145/2766935", "10.1145/1141911.1141959", "10.1145/2185520.2185557", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1073204.1073298", "10.1145/2421636.2421641", "10.1145/1360612.1360647", "10.1145/2159516.2159522", "10.1145/1882261.1866198", "10.1145/2461912.2461991", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1015706.1015745", "10.1145/1073204.1073283", "10.1145/1882261.1866197", "10.1145/1315184.1315193", "10.1145/37401.37422", "10.1145/2766935", "10.1145/1141911.1141959", "10.1145/2185520.2185557", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1073204.1073298", "10.1145/2421636.2421641", "10.1145/1360612.1360647", "10.1145/2159516.2159522", "10.1145/1882261.1866198", "10.1145/2461912.2461991", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1015706.1015745", "10.1145/1073204.1073283", "10.1145/1882261.1866197", "10.1145/1315184.1315193", "10.1145/37401.37422", "10.1145/2766935", "10.1006/jcph.1999.6257", "10.1103/PhysRevE.71.036701", "10.1103/PhysRevE.81.036707", "10.1111/cgf.12058", "10.1146/annurev-fluid-121108-145519", "10.1063/1.2187070", "10.1103/PhysRevE.87.053301", "10.1016/j.compfluid.2011.04.001", "10.1016/j.jcp.2013.03.039", "10.1016/j.jcp.2004.12.001", "10.1103/PhysRevE.89.033309", "10.1002/cav.143", "10.1002/cav.256", "10.1103/PhysRevE.67.036302", "10.1098/rsta.2001.0955", "10.1103/PhysRevE.87.023304", "10.1063/1.857955", "10.1016/j.physa.2009.02.041", "10.1063/1.858280", "10.1103/PhysRevE.85.016701", "10.1006/jcph.1999.6257", "10.1103/PhysRevE.71.036701", "10.1103/PhysRevE.81.036707", "10.1111/cgf.12058", "10.1146/annurev-fluid-121108-145519", "10.1063/1.2187070", "10.1103/PhysRevE.87.053301", "10.1016/j.compfluid.2011.04.001", "10.1016/j.jcp.2013.03.039", "10.1016/j.jcp.2004.12.001", "10.1103/PhysRevE.89.033309", "10.1002/cav.143", "10.1002/cav.256", "10.1103/PhysRevE.67.036302", "10.1098/rsta.2001.0955", "10.1103/PhysRevE.87.023304", "10.1063/1.857955", "10.1016/j.physa.2009.02.041", "10.1063/1.858280", "10.1103/PhysRevE.85.016701", "10.1006/jcph.1999.6257", "10.1103/PhysRevE.71.036701", "10.1103/PhysRevE.81.036707", "10.1111/cgf.12058", "10.1146/annurev-fluid-121108-145519", "10.1063/1.2187070", "10.1103/PhysRevE.87.053301", "10.1016/j.compfluid.2011.04.001", "10.1016/j.jcp.2013.03.039", "10.1016/j.jcp.2004.12.001", "10.1103/PhysRevE.89.033309", "10.1002/cav.143", "10.1002/cav.256", "10.1103/PhysRevE.67.036302", "10.1098/rsta.2001.0955", "10.1103/PhysRevE.87.023304", "10.1063/1.857955", "10.1016/j.physa.2009.02.041", "10.1063/1.858280", "10.1103/PhysRevE.85.016701"]}, "10.1109/TVCG.2016.2535340": {"doi": "10.1109/TVCG.2016.2535340", "author": ["S. S. Alam", "R. Jianu"], "title": "Analyzing Eye-Tracking Information in Visualization and Data Space: From Where on the Screen to What on the Screen", "year": "2017", "abstract": "Eye-tracking data is currently analyzed in the image space that gaze-coordinates were recorded in, generally with the help of overlays such as heatmaps or scanpaths, or with the help of manually defined areas of interest (AOI). Such analyses, which focus predominantly on where on the screen users are looking, require significant manual input and are not feasible for studies involving many subjects, long sessions, and heavily interactive visual stimuli. Alternatively, we show that it is feasible to collect and analyze eye-tracking information in data space. Specifically, the visual layout of visualizations with open source code that can be instrumented is known at rendering time, and thus can be used to relate gaze-coordinates to visualization and data objects that users view, in real time. We demonstrate the effectiveness of this approach by showing that data collected using this methodology from nine users working with an interactive visualization, was well aligned with the tasks that those users were asked to solve, and similar to annotation data produced by five human coders. Moreover, we introduce an algorithm that, given our instrumented visualization, could translate gaze-coordinates into viewed objects with greater accuracy than simply binning gazes into dynamically defined AOIs. Finally, we discuss the challenges, opportunities, and benefits of analyzing eye-tracking in visualization and data space.", "keywords": ["data visualisation", "gaze tracking", "interactive systems", "eye-tracking information analysis", "data space visualization", "eye tracking data", "image space", "screen users", "interactive visual stimuli", "data space", "visual layout", "rendering time", "interactive visualization", "annotation data", "human coders", "instrumented visualization", "translate gaze-coordinates", "Data visualization", "Visualization", "Instruments", "Layout", "Rendering (computer graphics)", "Real-time systems", "Computers", "Eye-tracking", "area of interest analysis", "usability analysis", "evaluation"], "referenced_by": ["10.1109/TVCG.2017.2665498", "10.1109/ICALIP.2018.8455579", "10.1109/ACCESS.2018.2865754", "10.1109/ACCESS.2020.2980901", "10.1109/ESCI48226.2020.9167616", "10.1109/CogInfoCom50765.2020.9237910", "10.1109/ICCIS49240.2020.9257683", "10.1007/978-3-319-71084-6_26", "10.1007/s13369-019-04322-7"], "referencing": ["10.1109/TVCG.2011.193", "10.1109/PacificVis.2013.6596142", "10.1109/TVCG.2012.215", "10.1109/34.877520", "10.1109/TVCG.2012.276", "10.1109/TVCG.2012.252", "10.1109/TVCG.2011.185", "10.1109/TVCG.2011.193", "10.1109/PacificVis.2013.6596142", "10.1109/TVCG.2012.215", "10.1109/34.877520", "10.1109/TVCG.2012.276", "10.1109/TVCG.2012.252", "10.1109/TVCG.2011.185", "10.1109/TVCG.2011.193", "10.1109/PacificVis.2013.6596142", "10.1109/TVCG.2012.215", "10.1109/34.877520", "10.1109/TVCG.2012.276", "10.1109/TVCG.2012.252", "10.1109/TVCG.2011.185", "10.1145/1165387.275627", "10.1145/123078.128728", "10.1145/1377966.1377970", "10.1145/968363.968368", "10.1145/2449396.2449439", "10.1145/1842993.1843058", "10.1145/2644812", "10.1145/1344471.1344488", "10.1145/1165387.275627", "10.1145/123078.128728", "10.1145/1377966.1377970", "10.1145/968363.968368", "10.1145/2449396.2449439", "10.1145/1842993.1843058", "10.1145/2644812", "10.1145/1344471.1344488", "10.1145/1165387.275627", "10.1145/123078.128728", "10.1145/1377966.1377970", "10.1145/968363.968368", "10.1145/2449396.2449439", "10.1145/1842993.1843058", "10.1145/2644812", "10.1145/1344471.1344488", "10.1016/S0926-907X(05)80003-0", "10.1371/journal.pone.0093914", "10.1016/j.pain.2013.02.017", "10.1016/j.ridd.2014.03.043", "10.1002/rrq.91", "10.1016/j.learninstruc.2009.02.012", "10.1016/j.learninstruc.2009.02.009", "10.1111/cgf.12381", "10.1207/s15516709cog0000_29", "10.1111/cgf.12115", "10.1016/S0169-8141(98)00068-7", "10.1167/9.12.13", "10.1016/S0926-907X(05)80003-0", "10.1371/journal.pone.0093914", "10.1016/j.pain.2013.02.017", "10.1016/j.ridd.2014.03.043", "10.1002/rrq.91", "10.1016/j.learninstruc.2009.02.012", "10.1016/j.learninstruc.2009.02.009", "10.1111/cgf.12381", "10.1207/s15516709cog0000_29", "10.1111/cgf.12115", "10.1016/S0169-8141(98)00068-7", "10.1167/9.12.13", "10.1016/S0926-907X(05)80003-0", "10.1371/journal.pone.0093914", "10.1016/j.pain.2013.02.017", "10.1016/j.ridd.2014.03.043", "10.1002/rrq.91", "10.1016/j.learninstruc.2009.02.012", "10.1016/j.learninstruc.2009.02.009", "10.1111/cgf.12381", "10.1207/s15516709cog0000_29", "10.1111/cgf.12115", "10.1016/S0169-8141(98)00068-7", "10.1167/9.12.13"]}, "10.1109/TVCG.2016.2535234": {"doi": "10.1109/TVCG.2016.2535234", "author": ["G. Sun", "R. Liang", "H. Qu", "Y. Wu"], "title": "Embedding Spatio-Temporal Information into Maps by Route-Zooming", "year": "2017", "abstract": "Analysis and exploration of spatio-temporal data such as traffic flow and vehicle trajectories have become important in urban planning and management. In this paper, we present a novel visualization technique called route-zooming that can embed spatio-temporal information into a map seamlessly for occlusion-free visualization of both spatial and temporal data. The proposed technique can broaden a selected route in a map by deforming the overall road network. We formulate the problem of route-zooming as a nonlinear least squares optimization problem by defining an energy function that ensures the route is broadened successfully on demand while the distortion caused to the road network is minimized. The spatio-temporal information can then be embedded into the route to reveal both spatial and temporal patterns without occluding the spatial context information. The route-zooming technique is applied in two instantiations including an interactive metro map for city tourism and illustrative maps to highlight information on the broadened roads to prove its applicability. We demonstrate the usability of our spatio-temporal visualization approach with case studies on real traffic flow data. We also study various design choices in our method, including the encoding of the time direction and choices of temporal display, and conduct a comprehensive user study to validate our embedded visualization design.", "keywords": ["data analysis", "data visualisation", "geographic information systems", "interactive systems", "minimisation", "nonlinear programming", "road traffic", "spatiotemporal phenomena", "traffic engineering computing", "spatiotemporal information", "spatiotemporal data analysis", "spatiotemporal data exploration", "vehicle trajectories", "urban planning", "urban management", "occlusion-free visualization", "road network deformation", "nonlinear least squares optimization", "energy function", "distortion minimization", "temporal patterns", "spatial patterns", "route-zooming technique", "interactive metro map", "city tourism", "illustrative maps", "traffic flow data", "temporal display", "embedded visualization design", "Roads", "Data visualization", "Context", "Trajectory", "Optimization", "Spatial databases", "Nonlinear distortion", "Spatio-temporal visualization", "occlusion-free visualization", "least-square optimization"], "referenced_by": ["10.1109/EISIC.2017.22", "10.1109/TITS.2017.2683539", "10.1109/TVCG.2019.2934670", "10.1109/TVCG.2019.2934657", "10.1109/ACCESS.2020.2977673", "10.1109/TVCG.2019.2922597", "10.1002/1873-3468.12778", "10.1007/s00779-018-1120-y", "10.1007/s10618-018-0560-3", "10.1007/s12650-018-0481-7", "10.1016/j.visinf.2017.07.001", "10.1007/s12650-018-0517-z", "10.1007/s11227-019-02924-4", "10.1111/cgf.13685", "10.1007/s12650-019-00600-6", "10.1007/978-3-319-46922-5_34", "10.1111/cgf.13882", "10.1016/j.cola.2019.100936", "10.1088/1757-899X/688/2/022048", "10.3390/s20041084", "10.1007/978-3-030-65742-0_9"], "referencing": ["10.1109/TKDE.2011.200", "10.1109/TVCG.2014.2346449", "10.1109/TVCG.2014.2346746", "10.1109/TVCG.2010.193", "10.1109/TVCG.2011.195", "10.1109/TVCG.2009.98", "10.1109/TVCG.2011.191", "10.1109/TVCG.2015.2430290", "10.1109/TVCG.2007.70621", "10.1109/TVCG.2012.265", "10.1109/VAST.2011.6102455", "10.1109/TKDE.2011.200", "10.1109/TVCG.2014.2346449", "10.1109/TVCG.2014.2346746", "10.1109/TVCG.2010.193", "10.1109/TVCG.2011.195", "10.1109/TVCG.2009.98", "10.1109/TVCG.2011.191", "10.1109/TVCG.2015.2430290", "10.1109/TVCG.2007.70621", "10.1109/TVCG.2012.265", "10.1109/VAST.2011.6102455", "10.1109/TKDE.2011.200", "10.1109/TVCG.2014.2346449", "10.1109/TVCG.2014.2346746", "10.1109/TVCG.2010.193", "10.1109/TVCG.2011.195", "10.1109/TVCG.2009.98", "10.1109/TVCG.2011.191", "10.1109/TVCG.2015.2430290", "10.1109/TVCG.2007.70621", "10.1109/TVCG.2012.265", "10.1109/VAST.2011.6102455", "10.1145/345513.345271", "10.1145/22627.22342", "10.1145/882262.882291", "10.1145/1357054.1357286", "10.1145/1124772.1124775", "10.1145/345513.345271", "10.1145/22627.22342", "10.1145/882262.882291", "10.1145/1357054.1357286", "10.1145/1124772.1124775", "10.1145/345513.345271", "10.1145/22627.22342", "10.1145/882262.882291", "10.1145/1357054.1357286", "10.1145/1124772.1124775", "10.1559/152304009788988288", "10.1080/13658816.2014.887718", "10.1080/13658816.2010.508043", "10.1559/152304009788988288", "10.1080/13658816.2014.887718", "10.1080/13658816.2010.508043", "10.1559/152304009788988288", "10.1080/13658816.2014.887718", "10.1080/13658816.2010.508043"]}, "10.1109/TVCG.2016.2527649": {"doi": "10.1109/TVCG.2016.2527649", "author": ["L. Turban", "F. Urban", "P. Guillotel"], "title": "Extrafoveal Video Extension for an Immersive Viewing Experience", "year": "2017", "abstract": "Between the recent popularity of virtual reality (VR) and the development of 3D, immersion has become an integral part of entertainment concepts. Head-mounted Display (HMD) devices are often used to afford users a feeling of immersion in the environment. Another technique is to project additional material surrounding the viewer, as is achieved using cave systems. As a continuation of this technique, it could be interesting to extend surrounding projection to current television or cinema screens. The idea would be to entirely fill the viewer's field of vision, thus providing them with a more complete feeling of being in the scene and part of the story. The appropriate content can be captured using large field of view (FoV) technology, using a rig of cameras for 1100 to 3600 capture, or created using computer-generated images. The FoV is, however, rather limited in its use for existing (legacy) content, achieving between 36 to 90 degrees (0) field, depending on the distance from the screen. This paper seeks to improve this FoV limitation by proposing computer vision techniques to extend such legacy content to the peripheral (extrafoveal) vision without changing the original creative intent or damaging the viewer's experience. A new methodology is also proposed for performing user tests in order to evaluate the quality of the experience and confirm that the sense of immersion has been increased. This paper thus presents: i) an algorithm to spatially extend the video based on human vision characteristics, ii) its subjective results compared to state-of-the-art techniques, iii) the protocol required to evaluate the quality of the experience (QoE), and iv) the results of the user tests.", "keywords": ["computer vision", "helmet mounted displays", "video signal processing", "virtual reality", "extrafoveal video extension", "immersive viewing experience", "virtual reality", "VR", "entertainment concepts", "head mounted display", "HMD devices", "cave systems", "surrounding projection", "cinema screens", "television screens", "field of view", "FoV technology", "computer vision techniques", "human vision characteristics", "quality of the experience", "QoE", "Visualization", "Motion pictures", "TV", "Image color analysis", "Streaming media", "Cameras", "Light emitting diodes", "Augmented video", "large field of view", "human vision", "immersion"], "referenced_by": ["10.1109/ISMAR-Adjunct.2017.53", "10.1109/ICCE.2018.8326235", "10.1109/ICCE.2019.8662089", "10.1109/ICCE.2019.8661994", "10.1109/TIP.2020.2977171", "10.1145/3196492", "10.32362/2500-316X-2020-8-1-9-20"], "referencing": ["10.5594/J14028", "10.1109/JSTSP.2010.2065213", "10.1109/34.730558", "10.1109/TPAMI.2006.86", "10.1109/TCSVT.2012.2221191", "10.5594/J14028", "10.1109/JSTSP.2010.2065213", "10.1109/34.730558", "10.1109/TPAMI.2006.86", "10.1109/TCSVT.2012.2221191", "10.5594/J14028", "10.1109/JSTSP.2010.2065213", "10.1109/34.730558", "10.1109/TPAMI.2006.86", "10.1109/TCSVT.2012.2221191", "10.1145/2754391", "10.1145/882262.882264", "10.1145/2754391", "10.1145/882262.882264", "10.1145/2754391", "10.1145/882262.882264", "10.2174/2213275910902010075", "10.1016/0042-6989(84)90140-8", "10.1364/JOSAA.4.001594", "10.1016/0042-6989(95)00109-D", "10.1007/s12559-010-9086-8", "10.12988/ams.2013.311644", "10.1007/978-3-642-19328-6_41", "10.2174/2213275910902010075", "10.1016/0042-6989(84)90140-8", "10.1364/JOSAA.4.001594", "10.1016/0042-6989(95)00109-D", "10.1007/s12559-010-9086-8", "10.12988/ams.2013.311644", "10.1007/978-3-642-19328-6_41", "10.2174/2213275910902010075", "10.1016/0042-6989(84)90140-8", "10.1364/JOSAA.4.001594", "10.1016/0042-6989(95)00109-D", "10.1007/s12559-010-9086-8", "10.12988/ams.2013.311644", "10.1007/978-3-642-19328-6_41"]}, "10.1109/TVCG.2016.2535331": {"doi": "10.1109/TVCG.2016.2535331", "author": ["Y. Kuo", "H. Chu", "M. Chi", "R. Lee", "T. Lee"], "title": "Generating Ambiguous Figure-Ground Images", "year": "2017", "abstract": "Ambiguous figure-ground images, mostly represented as binary images, are fascinating as they present viewers a visual phenomena of perceiving multiple interpretations from a single image. In one possible interpretation, the white region is seen as a foreground figure while the black region is treated as shapeless background. Such perception can reverse instantly at any moment. In this paper, we investigate the theory behind this ambiguous perception and present an automatic algorithm to generate such images. We model the problem as a binary image composition using two object contours and approach it through a three-stage pipeline. The algorithm first performs a partial shape matching to find a good partial contour matching between objects. This matching is based on a content-aware shape matching metric, which captures features of ambiguous figure-ground images. Then we combine matched contours into a compound contour using an adaptive contour deformation, followed by computing an optimal cropping window and image binarization for the compound contour that maximize the completeness of object contours in the final composition. We have tested our system using a wide range of input objects and generated a large number of convincing examples with or without user guidance. The efficiency of our system and quality of results are verified through an extensive experimental study.", "keywords": ["image matching", "ambiguous figure-ground image generation", "visual phenomena", "white region", "foreground figure", "black region", "shapeless background", "binary image composition", "object contours", "three-stage pipeline", "partial shape matching", "partial contour matching", "content-aware shape matching metric", "compound contour", "matched contours", "adaptive contour deformation", "optimal cropping window", "image binarization", "Shape", "Art", "Compounds", "Computational modeling", "Shape measurement", "Visualization", "Figure-ground perception", "partial shape matching", "curve deformation", "image cropping", "image binarization"], "referenced_by": ["10.1109/ICAIT.2019.8935909"], "referencing": ["10.1109/34.993558", "10.1109/70.864240", "10.1109/34.1000236", "10.1109/34.993558", "10.1109/70.864240", "10.1109/34.1000236", "10.1109/34.993558", "10.1109/70.864240", "10.1109/34.1000236", "10.1145/1360612.1360661", "10.1145/1778765.1778788", "10.1145/2024676.2024681", "10.1145/344779.345022", "10.1145/1274871.1274873", "10.1145/2070781.2024189", "10.1145/1377980.1377990", "10.1145/1141911.1141920", "10.1145/1360612.1360661", "10.1145/1778765.1778788", "10.1145/2024676.2024681", "10.1145/344779.345022", "10.1145/1274871.1274873", "10.1145/2070781.2024189", "10.1145/1377980.1377990", "10.1145/1141911.1141920", "10.1145/1360612.1360661", "10.1145/1778765.1778788", "10.1145/2024676.2024681", "10.1145/344779.345022", "10.1145/1274871.1274873", "10.1145/2070781.2024189", "10.1145/1377980.1377990", "10.1145/1141911.1141920", "10.1068/p150197", "10.3758/BF03206951", "10.1037/0096-1523.34.2.251", "10.1167/8.16.4", "10.1037/0096-1523.17.4.1075", "10.1111/j.1467-8659.2008.01334.x", "10.1006/cviu.1995.1004", "10.1007/978-3-642-12307-8_26", "10.1007/978-3-642-15555-0_3", "10.3138/FM57-6770-U75U-7727", "10.1068/p150197", "10.3758/BF03206951", "10.1037/0096-1523.34.2.251", "10.1167/8.16.4", "10.1037/0096-1523.17.4.1075", "10.1111/j.1467-8659.2008.01334.x", "10.1006/cviu.1995.1004", "10.1007/978-3-642-12307-8_26", "10.1007/978-3-642-15555-0_3", "10.3138/FM57-6770-U75U-7727", "10.1068/p150197", "10.3758/BF03206951", "10.1037/0096-1523.34.2.251", "10.1167/8.16.4", "10.1037/0096-1523.17.4.1075", "10.1111/j.1467-8659.2008.01334.x", "10.1006/cviu.1995.1004", "10.1007/978-3-642-12307-8_26", "10.1007/978-3-642-15555-0_3", "10.3138/FM57-6770-U75U-7727"]}, "10.1109/TVCG.2016.2537341": {"doi": "10.1109/TVCG.2016.2537341", "author": ["S. Lan", "L. Wang", "Y. Song", "Y. Wang", "L. Yao", "K. Sun", "B. Xia", "Z. Xu"], "title": "Improving Separability of Structures with Similar Attributes in 2D Transfer Function Design", "year": "2017", "abstract": "The 2D transfer function based on scalar value and gradient magnitude (SG-TF) is popularly used in volume rendering. However, it is plagued by the boundary-overlapping problem: different structures with similar attributes have the same region in SG-TF space, and their boundaries are usually connected. The SG-TF thus often fails in separating these structures (or their boundaries) and has limited ability to classify different objects in real-world 3D images. To overcome such a difficulty, we propose a novel method for boundary separation by integrating spatial connectivity computation of the boundaries and set operations on boundary voxels into the SG-TF. Specifically, spatial positions of boundaries and their regions in the SG-TF space are computed, from which boundaries can be well separated and volume rendered in different colors. In the method, the boundaries are divided into three classes and different boundary-separation techniques are applied to them, respectively. The complex task of separating various boundaries in 3D images is then simplified by breaking it into several small separation problems. The method shows good object classification ability in real-world 3D images while avoiding the complexity of high-dimensional transfer functions. Its effectiveness and validation is demonstrated by many experimental results to visualize boundaries of different structures in complex real-world 3D images.", "keywords": ["gradient methods", "pattern classification", "rendering (computer graphics)", "transfer functions", "2D transfer function design", "structure separability", "scalar value", "gradient magnitude", "SG-TF", "volume rendering", "real-world 3D images", "boundary separation", "spatial connectivity computation", "boundary-separation techniques", "high-dimensional transfer functions", "object classification ability", "Three-dimensional displays", "Transfer functions", "Visualization", "Image color analysis", "Rendering (computer graphics)", "Computed tomography", "Complexity theory", "Transfer function", "volume rendering", "connectivity computation", "set operations", "boundaries", "classification ability"], "referenced_by": ["10.1109/JBHI.2016.2565502", "10.1109/ISBI.2019.8759432", "10.1109/SIBGRAPI.2019.00021", "10.1016/j.medengphy.2019.06.022", "10.12677/CSA.2019.911223", "10.1016/j.patcog.2020.107478"], "referencing": ["10.1109/38.920623", "10.1109/SVV.1998.729588", "10.1109/TVCG.2002.1021579", "10.1109/TVCG.2006.39", "10.1109/TVCG.2006.100", "10.1109/TVCG.2008.162", "10.1109/TVCG.2011.23", "10.1109/TVCG.2005.38", "10.1109/TVCG.2007.1051", "10.1109/TVCG.2007.47", "10.1109/2945.856997", "10.1109/38.511", "10.1109/TVCG.2011.74", "10.1109/TVCG.2008.169", "10.1109/TVCG.2010.35", "10.1109/TVCG.2012.105", "10.1109/TVCG.2010.195", "10.1109/VISUAL.1997.663875", "10.1109/VISUAL.2001.964515", "10.1109/PACIFICVIS.2010.5429615", "10.1109/TVCG.2009.189", "10.1109/42.668696", "10.1109/TITB.2006.889675", "10.1109/TVCG.2009.185", "10.1109/TVCG.2011.97", "10.1109/2945.942692", "10.1109/TVCG.2006.174", "10.1109/TVCG.2014.2312015", "10.1109/38.920623", "10.1109/SVV.1998.729588", "10.1109/TVCG.2002.1021579", "10.1109/TVCG.2006.39", "10.1109/TVCG.2006.100", "10.1109/TVCG.2008.162", "10.1109/TVCG.2011.23", "10.1109/TVCG.2005.38", "10.1109/TVCG.2007.1051", "10.1109/TVCG.2007.47", "10.1109/2945.856997", "10.1109/38.511", "10.1109/TVCG.2011.74", "10.1109/TVCG.2008.169", "10.1109/TVCG.2010.35", "10.1109/TVCG.2012.105", "10.1109/TVCG.2010.195", "10.1109/VISUAL.1997.663875", "10.1109/VISUAL.2001.964515", "10.1109/PACIFICVIS.2010.5429615", "10.1109/TVCG.2009.189", "10.1109/42.668696", "10.1109/TITB.2006.889675", "10.1109/TVCG.2009.185", "10.1109/TVCG.2011.97", "10.1109/2945.942692", "10.1109/TVCG.2006.174", "10.1109/TVCG.2014.2312015", "10.1109/38.920623", "10.1109/SVV.1998.729588", "10.1109/TVCG.2002.1021579", "10.1109/TVCG.2006.39", "10.1109/TVCG.2006.100", "10.1109/TVCG.2008.162", "10.1109/TVCG.2011.23", "10.1109/TVCG.2005.38", "10.1109/TVCG.2007.1051", "10.1109/TVCG.2007.47", "10.1109/2945.856997", "10.1109/38.511", "10.1109/TVCG.2011.74", "10.1109/TVCG.2008.169", "10.1109/TVCG.2010.35", "10.1109/TVCG.2012.105", "10.1109/TVCG.2010.195", "10.1109/VISUAL.1997.663875", "10.1109/PACIFICVIS.2010.5429615", "10.1109/TVCG.2009.189", "10.1109/42.668696", "10.1109/TITB.2006.889675", "10.1109/TVCG.2009.185", "10.1109/TVCG.2011.97", "10.1109/2945.942692", "10.1109/TVCG.2006.174", "10.1109/TVCG.2014.2312015", "10.1016/j.cag.2012.02.007", "10.1007/s11548-010-0480-1", "10.1016/j.acra.2004.01.013", "10.1007/s00371-011-0634-3", "10.1007/s11548-007-0079-3", "10.1016/j.cag.2009.06.006", "10.1016/j.patrec.2006.04.008", "10.1111/j.1467-8659.2012.03122.x", "10.1016/j.cag.2012.02.007", "10.1007/s11548-010-0480-1", "10.1016/j.acra.2004.01.013", "10.1007/s00371-011-0634-3", "10.1007/s11548-007-0079-3", "10.1016/j.cag.2009.06.006", "10.1016/j.patrec.2006.04.008", "10.1111/j.1467-8659.2012.03122.x", "10.1016/j.cag.2012.02.007", "10.1007/s11548-010-0480-1", "10.1016/j.acra.2004.01.013", "10.1007/s00371-011-0634-3", "10.1007/s11548-007-0079-3", "10.1016/j.cag.2009.06.006", "10.1016/j.patrec.2006.04.008", "10.1111/j.1467-8659.2012.03122.x"]}, "10.1109/TVCG.2016.2532329": {"doi": "10.1109/TVCG.2016.2532329", "author": ["F. Zhang", "J. Wang", "E. Shechtman", "Z. Zhou", "J. Shi", "S. Hu"], "title": "PlenoPatch: Patch-Based Plenoptic Image Manipulation", "year": "2017", "abstract": "Patch-based image synthesis methods have been successfully applied for various editing tasks on still images, videos and stereo pairs. In this work we extend patch-based synthesis to plenoptic images captured by consumer-level lenselet-based devices for interactive, efficient light field editing. In our method the light field is represented as a set of images captured from different viewpoints. We decompose the central view into different depth layers, and present it to the user for specifying the editing goals. Given an editing task, our method performs patch-based image synthesis on all affected layers of the central view, and then propagates the edits to all other views. Interaction is done through a conventional 2D image editing user interface that is familiar to novice users. Our method correctly handles object boundary occlusion with semi-transparency, thus can generate more realistic results than previous methods. We demonstrate compelling results on a wide range of applications such as hole-filling, object reshuffling and resizing, changing object depth, light field upscaling and parallax magnification.", "keywords": ["cameras", "image capture", "image representation", "PlenoPatch", "patch-based plenoptic image manipulation", "patch-based image synthesis methods", "consumer-level lenselet-based devices", "interactive light field editing", "plenoptic image capture", "2D image editing user interface", "object boundary occlusion", "semitransparency", "hole-filling", "object reshuffling", "object resizing", "object depth", "light field upscaling", "parallax magnification", "Cameras", "Image generation", "Image reconstruction", "Visualization", "Image color analysis", "Coherence", "Videos", "Plenoptic image editing", "light field", "patch-based synthesis"], "referenced_by": ["10.1109/CVPR.2017.178", "10.1109/JSTSP.2017.2746263", "10.1109/TIP.2017.2750419", "10.1109/TIP.2018.2791864", "10.1109/TVCG.2017.2702738", "10.1109/WACV.2018.00166", "10.1109/TCSVT.2018.2826052", "10.1109/TIP.2019.2895463", "10.1109/TPAMI.2018.2845393", "10.1109/TVCG.2018.2866090", "10.1109/TVCG.2018.2869326", "10.1109/TCI.2019.2897937", "10.1109/TIP.2020.2967600", "10.1109/ICCE46568.2020.9043168", "10.1109/TPAMI.2019.2893666", "10.1109/ACCESS.2020.2988094", "10.1109/VRW50115.2020.00153", "10.1109/TVCG.2018.2889297", "10.1109/TVCG.2019.2894627", "10.1109/TCI.2020.2986092", "10.1109/ICIP40778.2020.9190716", "10.1109/TCI.2020.3037413", "10.1109/TIP.2020.3042059", "10.1145/3130800.3130855", "10.1145/3130800.3130842", "10.1007/s41095-017-0102-8", "10.1007/978-3-030-01449-0_47", "10.1007/s41095-018-0126-8", "10.1364/AO.58.00A142", "10.1007/s41095-019-0130-7", "10.1007/s41095-019-0133-4", "10.1007/s11390-019-1925-9", "10.1016/j.cag.2019.05.007", "10.1007/s12530-019-09297-2", "10.1016/j.image.2019.115638", "10.1587/transfun.2018EAP1175", "10.1007/s41095-019-0158-8", "10.1049/ipr2.12086"], "referencing": ["10.1109/TVCG.2015.2440255", "10.1109/TPAMI.2012.213", "10.1109/TVCG.2015.2391859", "10.1109/TVCG.2014.2359466", "10.1109/CVPR.2011.5995372", "10.1109/TVCG.2012.319", "10.1109/TVCG.2005.11", "10.1109/TPAMI.2007.60", "10.1109/TVCG.2015.2440255", "10.1109/TPAMI.2012.213", "10.1109/TVCG.2015.2391859", "10.1109/TIP.2013.2273668", "10.1109/TVCG.2014.2359466", "10.1109/CVPR.2011.5995372", "10.1109/TVCG.2012.319", "10.1109/TVCG.2005.11", "10.1109/TPAMI.2007.60", "10.1109/TVCG.2015.2440255", "10.1109/TPAMI.2012.213", "10.1109/TVCG.2015.2391859", "10.1109/TIP.2013.2273668", "10.1109/TVCG.2014.2359466", "10.1109/CVPR.2011.5995372", "10.1109/TVCG.2012.319", "10.1109/TVCG.2005.11", "10.1109/TPAMI.2007.60", "10.1145/1531326.1531330", "10.1145/1053427.1053450", "10.1145/2185520.2185578", "10.1145/383259.383296", "10.1145/1944846.1944849", "10.1145/2070781.2024221", "10.1145/1230100.1230121", "10.1145/2601097.2601125", "10.1145/2010324.1964950", "10.1145/237170.237199", "10.1145/1882261.1866173", "10.1145/2366145.2366201", "10.1145/1618452.1618453", "10.1145/2185520.2185576", "10.1145/1073204.1073259", "10.1145/566654.566602", "10.1145/1015706.1015766", "10.1145/1531326.1531330", "10.1145/1053427.1053450", "10.1145/2185520.2185578", "10.1145/383259.383296", "10.1145/1944846.1944849", "10.1145/2070781.2024221", "10.1145/1230100.1230121", "10.1145/2601097.2601125", "10.1145/2010324.1964950", "10.1145/237170.237199", "10.1145/1882261.1866173", "10.1145/2366145.2366201", "10.1145/1618452.1618453", "10.1145/2185520.2185576", "10.1145/1073204.1073259", "10.1145/566654.566602", "10.1145/1015706.1015766", "10.1145/1531326.1531330", "10.1145/1053427.1053450", "10.1145/2185520.2185578", "10.1145/383259.383296", "10.1145/1944846.1944849", "10.1145/2070781.2024221", "10.1145/1230100.1230121", "10.1145/2601097.2601125", "10.1145/2010324.1964950", "10.1145/237170.237199", "10.1145/1882261.1866173", "10.1145/2366145.2366201", "10.1145/1618452.1618453", "10.1145/2185520.2185576", "10.1145/1073204.1073259", "10.1145/566654.566602", "10.1145/1015706.1015766", "10.1111/j.1467-8659.2012.03008.x", "10.1111/cgf.12201", "10.1111/j.1365-2818.2009.03195.x", "10.1007/s41095-015-0016-2", "10.1023/A:1016046923611", "10.1561/0600000019", "10.1111/j.1467-8659.2012.03008.x", "10.1111/cgf.12201", "10.1111/j.1365-2818.2009.03195.x", "10.1007/s41095-015-0016-2", "10.1023/A:1016046923611", "10.1561/0600000019", "10.1111/j.1467-8659.2012.03008.x", "10.1111/cgf.12201", "10.1111/j.1365-2818.2009.03195.x", "10.1007/s41095-015-0016-2", "10.1023/A:1016046923611", "10.1561/0600000019"]}}