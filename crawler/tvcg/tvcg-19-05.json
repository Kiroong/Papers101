{"10.1109/TVCG.2019.2902968": {"doi": "10.1109/TVCG.2019.2902968", "author": [""], "title": "IEEE Transactions on Visualization and Computer Graphics", "year": "2019", "abstract": "Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902969": {"doi": "10.1109/TVCG.2019.2902969", "author": [""], "title": "Table of Contents", "year": "2019", "abstract": "Presents the table of contents for this issue of the publication.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902964": {"doi": "10.1109/TVCG.2019.2902964", "author": ["K. Mueller", "D. Schmalstieg"], "title": "Introducing the IEEE Virtual Reality 2019 Special Issue", "year": "2019", "abstract": "The thirty-three papers included in this special issue were presented at the 2019 8th Virtual Reality Conference that was held in Osaka, Japan, March 23-27, 2019.", "keywords": ["Special issues and sections", "Meetings", "Virtual reality"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902965": {"doi": "10.1109/TVCG.2019.2902965", "author": ["B. Thomas", "G. Welch", "T. Kuhlen", "K. Johnsen"], "title": "Preface", "year": "2019", "abstract": "Presents the preface to the 2019 Virtual Reality Conference.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2906438": {"doi": "10.1109/TVCG.2019.2906438", "author": [""], "title": "IEEE Visualization and Graphics Technical Committee (VGTC)", "year": "2019", "abstract": "Presents the mission and members of the IEEE Visualization and Graphics Technical Committee (VGTC).", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902970": {"doi": "10.1109/TVCG.2019.2902970", "author": [""], "title": "Conference Committee", "year": "2019", "abstract": "Presents a listing of the 2019 Virtual Reality Conference Committee.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902971": {"doi": "10.1109/TVCG.2019.2902971", "author": [""], "title": "Committee", "year": "2019", "abstract": "Presents a listing of the 2019 Virtual Reality Conference Committee members.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2902967": {"doi": "10.1109/TVCG.2019.2902967", "author": [""], "title": "Papers Reviewers for Journal Papers", "year": "2019", "abstract": "Presents a listing of reviewers who contributed to the journal papers for the 2019 Virtual Reality Conference.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2898757": {"doi": "10.1109/TVCG.2019.2898757", "author": ["A. Serrano", "I. Kim", "Z. Chen", "S. DiVerdi", "D. Gutierrez", "A. Hertzmann", "B. Masia"], "title": "Motion parallax for 360\u00b0 RGBD video", "year": "2019", "abstract": "We present a method for adding parallax and real-time playback of 360\u00b0 videos in Virtual Reality headsets. In current video players, the playback does not respond to translational head movement, which reduces the feeling of immersion, and causes motion sickness for some viewers. Given a 360\u00b0 video and its corresponding depth (provided by current stereo 360\u00b0 stitching algorithms), a naive image-based rendering approach would use the depth to generate a 3D mesh around the viewer, then translate it appropriately as the viewer moves their head. However, this approach breaks at depth discontinuities, showing visible distortions, whereas cutting the mesh at such discontinuities leads to ragged silhouettes and holes at disocclusions. We address these issues by improving the given initial depth map to yield cleaner, more natural silhouettes. We rely on a three-layer scene representation, made up of a foreground layer and two static background layers, to handle disocclusions by propagating information from multiple frames for the first background layer, and then inpainting for the second one. Our system works with input from many of today's most popular 360\u00b0 stereo capture devices (e.g., Yi Halo or GoPro Odyssey), and works well even if the original video does not provide depth information. Our user studies confirm that our method provides a more compelling viewing experience than without parallax, increasing immersion while reducing discomfort and nausea.", "keywords": ["image colour analysis", "image motion analysis", "image representation", "image texture", "rendering (computer graphics)", "stereo image processing", "video signal processing", "virtual reality", "motion parallax", "360\u00b0 RGBD video", "translational head movement", "three-layer scene representation", "static background layers", "background layer", "360\u00b0 stereo capture devices", "virtual reality headsets", "stereo 360\u00b0 stitching algorithms", "naive image-based rendering", "Three-dimensional displays", "Cameras", "Head", "Rendering (computer graphics)", "Streaming media", "Virtual reality", "Visualization", "Immersive environments", "Virtual Reality video"], "referenced_by": ["IKEY:8942315", "IKEY:8744321", "IKEY:9090570", "IKEY:9090528", "IKEY:9090437", "IKEY:9105244", "IKEY:9191168"], "referencing": ["IKEY:7926682", "IKEY:7299076", "IKEY:661192", "IKEY:7600448", "IKEY:1323101", "IKEY:7892248", "IKEY:6751339", "IKEY:1541285", "IKEY:8100182", "IKEY:6319316", "IKEY:8019543", "IKEY:5459462", "IKEY:7780667", "IKEY:4359322", "IKEY:8260916", "IKEY:6619010", "IKEY:8019417", "IKEY:6909906", "IKEY:6519291", "IKEY:7552858", "IKEY:7926682", "IKEY:7299076", "IKEY:661192", "IKEY:7600448", "IKEY:1323101", "IKEY:7892248", "IKEY:6751339", "IKEY:1541285", "IKEY:8100182", "IKEY:6319316", "IKEY:8019543", "IKEY:5459462", "IKEY:7780667", "IKEY:4359322", "IKEY:8260916", "IKEY:6619010", "IKEY:8019417", "IKEY:6909906", "IKEY:6519291", "IKEY:7552858", "IKEY:7926682", "IKEY:7299076", "IKEY:661192", "IKEY:7600448", "IKEY:1323101", "IKEY:7892248", "IKEY:6751339", "IKEY:1541285", "IKEY:8100182", "IKEY:6319316", "IKEY:8019543", "IKEY:5459462", "IKEY:7780667", "IKEY:4359322", "IKEY:8260916", "IKEY:6619010", "IKEY:8019417", "IKEY:6909906", "IKEY:6519291", "IKEY:7552858", "10.1145/344779.344972", "10.1145/383259.383309", "10.1145/2820903.2820920", "10.1145/1276377.1276506", "10.1145/237170.237191", "10.1145/1877791.1877798", "10.1145/2508363.2508376", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3119881.3119890", "10.1145/3197517.3201384", "10.1145/2980179.2980251", "10.1145/3072959.3073617", "10.1145/1276377.1276497", "10.1145/1015706.1015780", "10.1145/1531326.1531348", "10.1145/218380.218398", "10.1145/3190834.3190846", "10.1145/3139131.3139158", "10.1145/237170.237196", "10.1145/3072959.3073668", "10.1145/3072959.2999533", "10.1145/1015706.1015766", "10.1145/344779.344972", "10.1145/383259.383309", "10.1145/2820903.2820920", "10.1145/1276377.1276506", "10.1145/237170.237191", "10.1145/1877791.1877798", "10.1145/2508363.2508376", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3119881.3119890", "10.1145/3197517.3201384", "10.1145/2980179.2980251", "10.1145/3072959.3073617", "10.1145/1276377.1276497", "10.1145/1015706.1015780", "10.1145/1531326.1531348", "10.1145/218380.218398", "10.1145/3190834.3190846", "10.1145/3139131.3139158", "10.1145/237170.237196", "10.1145/3072959.3073668", "10.1145/3072959.2999533", "10.1145/1015706.1015766", "10.1145/344779.344972", "10.1145/383259.383309", "10.1145/2820903.2820920", "10.1145/1276377.1276506", "10.1145/237170.237191", "10.1145/1877791.1877798", "10.1145/2508363.2508376", "10.1145/1778765.1778832", "10.1145/237170.237200", "10.1145/3119881.3119890", "10.1145/3197517.3201384", "10.1145/2980179.2980251", "10.1145/3072959.3073617", "10.1145/1276377.1276497", "10.1145/1015706.1015780", "10.1145/1531326.1531348", "10.1145/218380.218398", "10.1145/3190834.3190846", "10.1145/3139131.3139158", "10.1145/237170.237196", "10.1145/3072959.3073668", "10.1145/3072959.2999533", "10.1145/1015706.1015766", "10.1016/j.cag.2018.02.001", "10.1023/A:1008160311296", "10.1016/j.visres.2015.03.005", "10.1186/s13634-017-0487-7", "10.1111/j.1467-8659.2011.01981.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1016/j.apergo.2017.12.016", "10.1111/j.1467-8659.2010.01824.x", "10.2352/ISSN.2470-1173.2018.05.PMII-352", "10.1016/j.cag.2018.02.001", "10.1023/A:1008160311296", "10.1016/j.visres.2015.03.005", "10.1186/s13634-017-0487-7", "10.1111/j.1467-8659.2011.01981.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1016/j.apergo.2017.12.016", "10.1111/j.1467-8659.2010.01824.x", "10.2352/ISSN.2470-1173.2018.05.PMII-352", "10.1016/j.cag.2018.02.001", "10.1023/A:1008160311296", "10.1016/j.visres.2015.03.005", "10.1186/s13634-017-0487-7", "10.1111/j.1467-8659.2011.01981.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1016/j.apergo.2017.12.016", "10.1111/j.1467-8659.2010.01824.x", "10.2352/ISSN.2470-1173.2018.05.PMII-352"]}, "10.1109/TVCG.2019.2898799": {"doi": "10.1109/TVCG.2019.2898799", "author": ["T. Bertel", "N. D. F. Campbell", "C. Richardt"], "title": "MegaParallax: Casual 360\u00b0 Panoramas with Motion Parallax", "year": "2019", "abstract": "The ubiquity of smart mobile devices, such as phones and tablets, enables users to casually capture 360\u00b0 panoramas with a single camera sweep to share and relive experiences. However, panoramas lack motion parallax as they do not provide different views for different viewpoints. The motion parallax induced by translational head motion is a crucial depth cue in daily life. Alternatives, such as omnidirectional stereo panoramas, provide different views for each eye (binocular disparity), but they also lack motion parallax as the left and right eye panoramas are stitched statically. Methods based on explicit scene geometry reconstruct textured 3D geometry, which provides motion parallax, but suffers from visible reconstruction artefacts. The core of our method is a novel multi-perspective panorama representation, which can be casually captured and rendered with motion parallax for each eye on the fly. This provides a more realistic perception of panoramic environments which is particularly useful for virtual reality applications. Our approach uses a single consumer video camera to acquire 200-400 views of a real 360\u00b0 environment with a single sweep. By using novel-view synthesis with flow-based blending, we show how to turn these input views into an enriched 360\u00b0 panoramic experience that can be explored in real time, without relying on potentially unreliable reconstruction of scene geometry. We compare our results with existing omnidirectional stereo and image-based rendering methods to demonstrate the benefit of our approach, which is the first to enable casual consumers to capture and view high-quality 360\u00b0 panoramas with motion parallax.", "keywords": ["computational geometry", "image capture", "image motion analysis", "image reconstruction", "image representation", "image texture", "mobile computing", "rendering (computer graphics)", "stereo image processing", "video cameras", "video signal processing", "virtual reality", "motion parallax", "translational head motion", "360\u00b0 panoramas", "smart mobile device ubiquity", "textured 3D geometry reconstruction", "multiperspective panorama representation", "virtual reality applications", "video camera", "flow-based blending", "image-based rendering methods", "MegaParallax", "Cameras", "Rendering (computer graphics)", "Geometry", "Image reconstruction", "Three-dimensional displays", "Real-time systems", "Streaming media", "Casual 360\u00b0 scene capture", "plenoptic modeling", "image-based rendering", "novel-view synthesis", "virtual reality"], "referenced_by": ["IKEY:9090528", "IKEY:9090561", "IKEY:9225004"], "referencing": ["IKEY:7892229", "IKEY:139591", "IKEY:8260916", "IKEY:7219438", "IKEY:910880", "IKEY:6619010", "IKEY:7780814", "IKEY:7552858", "IKEY:4270320", "IKEY:7892229", "IKEY:139591", "IKEY:8260916", "IKEY:7219438", "IKEY:910880", "IKEY:6619010", "IKEY:7780814", "IKEY:7552858", "IKEY:4270320", "IKEY:7892229", "IKEY:139591", "IKEY:8260916", "IKEY:7219438", "IKEY:910880", "IKEY:6619010", "IKEY:7780814", "IKEY:7552858", "IKEY:4270320", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2787626.2787648", "10.1145/237170.237200", "10.1145/2980179.2982420", "10.1145/3130800.3130828", "10.1145/237170.237199", "10.1145/3072959.3073645", "10.1145/218380.218398", "10.1145/3272127.3275031", "10.1145/3130800.3130855", "10.1145/3225150", "10.1145/237170.237196", "10.1145/311535.311573", "10.1145/258734.258859", "10.1145/3197517.3201323", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2787626.2787648", "10.1145/237170.237200", "10.1145/2980179.2982420", "10.1145/3130800.3130828", "10.1145/237170.237199", "10.1145/3072959.3073645", "10.1145/218380.218398", "10.1145/3272127.3275031", "10.1145/3130800.3130855", "10.1145/3225150", "10.1145/237170.237196", "10.1145/311535.311573", "10.1145/258734.258859", "10.1145/3197517.3201323", "10.1145/2980179.2980257", "10.1145/383259.383309", "10.1145/2487228.2487238", "10.1145/166117.166153", "10.1145/2787626.2787648", "10.1145/237170.237200", "10.1145/2980179.2982420", "10.1145/3130800.3130828", "10.1145/237170.237199", "10.1145/3072959.3073645", "10.1145/218380.218398", "10.1145/3272127.3275031", "10.1145/3130800.3130855", "10.1145/3225150", "10.1145/237170.237196", "10.1145/311535.311573", "10.1145/258734.258859", "10.1145/3197517.3201323", "10.1111/j.1467-8659.2012.03009.x", "10.1111/j.1467-8659.2008.01138.x", "10.1561/0600000052", "10.1093/acprof:oso/9780195367607.001.0001", "10.1111/cgf.12541", "10.1007/978-3-319-46487-9_31", "10.1162/pres.1994.3.2.130", "10.1561/0600000009", "10.1111/j.1467-8659.2012.03009.x", "10.1111/j.1467-8659.2008.01138.x", "10.1561/0600000052", "10.1093/acprof:oso/9780195367607.001.0001", "10.1111/cgf.12541", "10.1007/978-3-319-46487-9_31", "10.1162/pres.1994.3.2.130", "10.1561/0600000009", "10.1111/j.1467-8659.2012.03009.x", "10.1111/j.1467-8659.2008.01138.x", "10.1561/0600000052", "10.1093/acprof:oso/9780195367607.001.0001", "10.1111/cgf.12541", "10.1007/978-3-319-46487-9_31", "10.1162/pres.1994.3.2.130", "10.1561/0600000009"]}, "10.1109/TVCG.2019.2898721": {"doi": "10.1109/TVCG.2019.2898721", "author": ["W. Liang", "J. Liu", "Y. Lang", "B. Ning", "L. -F. Yu"], "title": "Functional Workspace Optimization via Learning Personal Preferences from Virtual Experiences", "year": "2019", "abstract": "The functionality of a workspace is one of the most important considerations in both virtual world design and interior design. To offer appropriate functionality to the user, designers usually take some general rules into account, e.g., general workflow and average stature of users, which are summarized from the population statistics. Yet, such general rules cannot reflect the personal preferences of a single individual, which vary from person to person. In this paper, we intend to optimize a functional workspace according to the personal preferences of the specific individual who will use it. We come up with an approach to learn the individual's personal preferences from his activities while using a virtual version of the workspace via virtual reality devices. Then, we construct a cost function, which incorporates personal preferences, spatial constraints, pose assessments, and visual field. At last, the cost function is optimized to achieve an optimal layout. To evaluate the approach, we experimented with different settings. The results of the user study show that the workspaces updated in this way better fit the users.", "keywords": ["ergonomics", "learning (artificial intelligence)", "virtual reality", "functional workspace optimization", "virtual experiences", "virtual world design", "interior design", "cost function", "personal preference learning", "Layout", "Task analysis", "Cost function", "Visualization", "Software", "Three-dimensional displays", "Affordance", "Human-centered Design", "Virtual Environments", "Workspace Design", "Remodeling", "Algorithms", "Computer Graphics", "Computer Simulation", "Ergonomics", "Humans", "Interior Design and Furnishings", "Posture", "Task Performance and Analysis", "User-Computer Interface", "Virtual Reality", "Workplace"], "referenced_by": ["IKEY:9180166"], "referencing": ["IKEY:8099744", "IKEY:7785081", "IKEY:8448290", "IKEY:7829397", "IKEY:5206744", "IKEY:8374559", "IKEY:8578716", "IKEY:8099744", "IKEY:7785081", "IKEY:8448290", "IKEY:7829397", "IKEY:5206744", "IKEY:8374559", "IKEY:8578716", "IKEY:8099744", "IKEY:7785081", "IKEY:8448290", "IKEY:7829397", "IKEY:5206744", "IKEY:8374559", "IKEY:8578716", "10.1145/2366145.2366154", "10.1145/2816795.2818057", "10.1145/3130800.3130805", "10.1145/345124.345139", "10.1145/2010324.1964982", "10.1145/3197517.3201362", "10.1145/2185520.2185552", "10.1145/2010324.1964981", "10.1145/2366145.2366154", "10.1145/2816795.2818057", "10.1145/3130800.3130805", "10.1145/345124.345139", "10.1145/2010324.1964982", "10.1145/3197517.3201362", "10.1145/2185520.2185552", "10.1145/2010324.1964981", "10.1145/2366145.2366154", "10.1145/2816795.2818057", "10.1145/3130800.3130805", "10.1145/345124.345139", "10.1145/2010324.1964982", "10.1145/3197517.3201362", "10.1145/2185520.2185552", "10.1145/2010324.1964981", "10.1097/00043764-199509000-00016", "10.1007/s11263-018-1103-5", "10.1016/0003-6870(77)90164-8", "10.1016/0003-6870(95)00022-5", "10.1177/154193129804201905", "10.1016/0003-6870(93)90080-S", "10.1111/cgf.12048", "10.1177/2041669517708205", "10.1177/001872087401600602", "10.1371/journal.pone.0204358", "10.1080/00140139308967940", "10.1016/j.jbiomech.2008.06.024", "10.1097/00043764-199509000-00016", "10.1007/s11263-018-1103-5", "10.1016/0003-6870(77)90164-8", "10.1016/0003-6870(95)00022-5", "10.1177/154193129804201905", "10.1016/0003-6870(93)90080-S", "10.1111/cgf.12048", "10.1177/2041669517708205", "10.1177/001872087401600602", "10.1371/journal.pone.0204358", "10.1080/00140139308967940", "10.1016/j.jbiomech.2008.06.024", "10.1097/00043764-199509000-00016", "10.1007/s11263-018-1103-5", "10.1016/0003-6870(77)90164-8", "10.1016/0003-6870(95)00022-5", "10.1177/154193129804201905", "10.1016/0003-6870(93)90080-S", "10.1111/cgf.12048", "10.1177/2041669517708205", "10.1177/001872087401600602", "10.1371/journal.pone.0204358", "10.1080/00140139308967940", "10.1016/j.jbiomech.2008.06.024"]}, "10.1109/TVCG.2019.2898765": {"doi": "10.1109/TVCG.2019.2898765", "author": ["C. R. A. Chaitanya", "J. M. Snyder", "K. Godin", "D. Nowrouzezahrai", "N. Raghuvanshi"], "title": "Adaptive Sampling for Sound Propagation", "year": "2019", "abstract": "Precomputed sound propagation samples acoustics at discrete scene probe positions to support dynamic listener locations. An offline 3D numerical simulation is performed at each probe and the resulting field is encoded for runtime rendering with dynamic sources. Prior work place probes on a uniform grid, requiring high density to resolve narrow spaces. Our adaptive sampling approach varies probe density based on a novel \u201clocal diameter\u201d measure of the space surrounding a given point, evaluated by stochastically tracing paths in the scene. We apply this measure to layout probes so as to smoothly adapt resolution and eliminate undersampling in corners, narrow corridors and stairways, while coarsening appropriately in more open areas. Coupled with a new runtime interpolator based on radial weights over geodesic paths, we achieve smooth acoustic effects that respect scene boundaries as both the source or listener move, unlike existing visibility-based solutions. We consistently demonstrate quality improvement over prior work at fixed cost.", "keywords": ["acoustic signal processing", "acoustic wave propagation", "interpolation", "rendering (computer graphics)", "signal resolution", "signal sampling", "smooth acoustic effects", "discrete scene probe positions", "dynamic listener locations", "offline 3D numerical simulation", "runtime rendering", "adaptive sampling approach", "probe density", "runtime interpolator", "geodesic paths", "local diameter measure", "visibility-based solutions", "sound propagation samples acoustics", "Probes", "Runtime", "Interpolation", "Acoustics", "Games", "Geometry", "Three-dimensional displays", "Diffraction", "interpolation", "mean free path", "radial basis function", "ray tracing", "reciprocity", "room acoustics", "wave simulation"], "referenced_by": [], "referencing": ["IKEY:8016655", "IKEY:4082128", "IKEY:7014276", "IKEY:5165582", "IKEY:1284395", "IKEY:8016655", "IKEY:4082128", "IKEY:7014276", "IKEY:5165582", "IKEY:1284395", "IKEY:8016655", "IKEY:4082128", "IKEY:7014276", "IKEY:5165582", "IKEY:1284395", "10.1145/2980179.2982431", "10.1145/1198555.1198709", "10.1145/1141911.1141983", "10.1145/3023368.3023378", "10.1145/2601097.2601184", "10.1145/3197517.3201339", "10.1145/1778765.1778805", "10.1145/383259.383317", "10.1145/2601097.2601216", "10.1145/3130800.3130852", "10.1145/2980179.2982431", "10.1145/1198555.1198709", "10.1145/1141911.1141983", "10.1145/3023368.3023378", "10.1145/2601097.2601184", "10.1145/3197517.3201339", "10.1145/1778765.1778805", "10.1145/383259.383317", "10.1145/2601097.2601216", "10.1145/3130800.3130852", "10.1145/2980179.2982431", "10.1145/1198555.1198709", "10.1145/1141911.1141983", "10.1145/3023368.3023378", "10.1145/2601097.2601184", "10.1145/3197517.3201339", "10.1145/1778765.1778805", "10.1145/383259.383317", "10.1145/2601097.2601216", "10.1145/3130800.3130852", "10.1121/1.4926438", "10.1121/1.4926438", "10.1121/1.4926438"]}, "10.1109/TVCG.2019.2898822": {"doi": "10.1109/TVCG.2019.2898822", "author": ["A. Sterling", "N. Rewkowski", "R. L. Klatzky", "M. C. Lin"], "title": "Audio-Material Reconstruction for Virtualized Reality Using a Probabilistic Damping Model", "year": "2019", "abstract": "Modal sound synthesis has been used to create realistic sounds from rigid-body objects, but requires accurate real-world material parameters. These material parameters can be estimated from recorded sounds of an impacted object, but external factors can interfere with accurate parameter estimation. We present a novel technique for estimating the damping parameters of materials from recorded impact sounds that probabilistically models these external factors. We represent the combined effects of material damping, support damping, and sampling inaccuracies with a probabilistic generative model, then use maximum likelihood estimation to fit a damping model to recorded data. This technique greatly reduces the human effort needed and does not require the precise object geometry or the exact hit location. We validate the effectiveness of this technique with a comprehensive analysis of a synthetic dataset and a perceptual study on object identification. We also present a study establishing human performance on the same parameter estimation task for comparison.", "keywords": ["audio signal processing", "damping", "image reconstruction", "maximum likelihood estimation", "probability", "virtual reality", "object identification", "parameter estimation task", "audio-material reconstruction", "virtualized reality", "probabilistic damping model", "modal sound synthesis", "rigid-body objects", "real-world material parameters", "impacted object", "material damping", "support damping", "probabilistic generative model", "maximum likelihood estimation", "Damping", "Vibrations", "Analytical models", "Geometry", "Probabilistic logic", "Real-time systems", "Parameter estimation", "Damping modeling", "sound synthesis", "modal analysis", "statistical modeling"], "referenced_by": ["IKEY:8998301"], "referencing": ["IKEY:6788060", "IKEY:6479182", "IKEY:6787986", "IKEY:6788060", "IKEY:6479182", "IKEY:6787986", "IKEY:6788060", "IKEY:6479182", "IKEY:6787986", "10.1145/2816795.2818108", "10.1145/1661412.1618465", "10.1145/1179352.1141983", "10.1145/2601097.2601177", "10.1145/2820612", "10.1145/1944745.1944755", "10.1145/545261.545290", "10.1145/383259.383268", "10.1145/1111411.1111429", "10.1145/2421636.2421637", "10.1145/2856400.2856419", "10.1145/3197517.3201318", "10.1145/1964921.1964933", "10.1145/2816795.2818108", "10.1145/1661412.1618465", "10.1145/1179352.1141983", "10.1145/2601097.2601177", "10.1145/2820612", "10.1145/1944745.1944755", "10.1145/545261.545290", "10.1145/383259.383268", "10.1145/1111411.1111429", "10.1145/2421636.2421637", "10.1145/2856400.2856419", "10.1145/3197517.3201318", "10.1145/1964921.1964933", "10.1145/2816795.2818108", "10.1145/1661412.1618465", "10.1145/1179352.1141983", "10.1145/2601097.2601177", "10.1145/2820612", "10.1145/1944745.1944755", "10.1145/545261.545290", "10.1145/383259.383268", "10.1145/1111411.1111429", "10.1145/2421636.2421637", "10.1145/2856400.2856419", "10.1145/3197517.3201318", "10.1145/1964921.1964933", "10.1016/j.jsv.2005.09.034", "10.1115/1.3643949", "10.1115/1.3627262", "10.1167/14.4.12", "10.1121/1.2149839", "10.1037/a0018388", "10.3758/BF03206491", "10.1021/ac60319a011", "10.2307/2313748", "10.2307/3680062", "10.1121/1.3466867", "10.1038/nn.3347", "10.2307/3680788", "10.2307/3680470", "10.1073/pnas.1612524113", "10.1006/jsvi.1998.1709", "10.1016/j.jsv.2005.09.034", "10.1115/1.3643949", "10.1115/1.3627262", "10.1167/14.4.12", "10.1121/1.2149839", "10.1037/a0018388", "10.3758/BF03206491", "10.1021/ac60319a011", "10.2307/2313748", "10.2307/3680062", "10.1121/1.3466867", "10.1038/nn.3347", "10.2307/3680788", "10.2307/3680470", "10.1073/pnas.1612524113", "10.1006/jsvi.1998.1709", "10.1016/j.jsv.2005.09.034", "10.1115/1.3643949", "10.1115/1.3627262", "10.1167/14.4.12", "10.1121/1.2149839", "10.1037/a0018388", "10.3758/BF03206491", "10.1021/ac60319a011", "10.2307/2313748", "10.2307/3680062", "10.1121/1.3466867", "10.1038/nn.3347", "10.2307/3680788", "10.2307/3680470", "10.1073/pnas.1612524113", "10.1006/jsvi.1998.1709"]}, "10.1109/TVCG.2019.2898823": {"doi": "10.1109/TVCG.2019.2898823", "author": ["E. Doukakis", "K. Debattista", "T. Bashford-Rogers", "A. Dhokia", "A. Asadipour", "A. Chalmers", "C. Harvey"], "title": "Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual Environments", "year": "2019", "abstract": "Virtual Environments (VEs) provide the opportunity to simulate a wide range of applications, from training to entertainment, in a safe and controlled manner. For applications which require realistic representations of real world environments, the VEs need to provide multiple, physically accurate sensory stimuli. However, simulating all the senses that comprise the human sensory system (HSS) is a task that requires significant computational resources. Since it is intractable to deliver all senses at the highest quality, we propose a resource distribution scheme in order to achieve an optimal perceptual experience within the given computational budgets. This paper investigates resource balancing for multi-modal scenarios composed of aural, visual and olfactory stimuli. Three experimental studies were conducted. The first experiment identified perceptual boundaries for olfactory computation. In the second experiment, participants ($N=25$) were asked, across a fixed number of budgets ($M=5$), to identify what they perceived to be the best visual, acoustic and olfactory stimulus quality for a given computational budget. Results demonstrate that participants tend to prioritize visual quality compared to other sensory stimuli. However, as the budget size is increased, users prefer a balanced distribution of resources with an increased preference for having smell impulses in the VE. Based on the collected data, a quality prediction model is proposed and its accuracy is validated against previously unused budgets and an untested scenario in a third and final experiment.", "keywords": ["Olfactory", "Visualization", "Computational modeling", "Resource management", "Mathematical model", "Auditory system", "Virtual environments", "Multi-Modal", "Cross-Modal", "Tri-Modal", "Sound", "Graphics", "Olfactory", "Acoustic Stimulation", "Adult", "Computer Graphics", "Computer Simulation", "Female", "Hearing", "Humans", "Male", "Middle Aged", "Photic Stimulation", "Psychophysics", "Resource Allocation", "Sensation", "Smell", "User-Computer Interface", "Virtual Reality", "Vision, Ocular", "Young Adult"], "referenced_by": [], "referencing": ["IKEY:7060810", "IKEY:7006282", "IKEY:7835714", "IKEY:7833192", "IKEY:4089317", "IKEY:756955", "IKEY:1667665", "IKEY:7060810", "IKEY:7006282", "IKEY:7835714", "IKEY:7833192", "IKEY:4089317", "IKEY:756955", "IKEY:1667665", "IKEY:7060810", "IKEY:7006282", "IKEY:7835714", "IKEY:7833192", "IKEY:4089317", "IKEY:756955", "IKEY:1667665", "10.1145/2957753", "10.1145/15886.15902", "10.1145/1778765.1778829", "10.1145/2617994", "10.1145/1230100.1230133", "10.1145/2957753", "10.1145/15886.15902", "10.1145/1778765.1778829", "10.1145/2617994", "10.1145/1230100.1230133", "10.1145/2957753", "10.1145/15886.15902", "10.1145/1778765.1778829", "10.1145/2617994", "10.1145/1230100.1230133", "10.1093/acprof:oso/9780198524861.003.0007", "10.1016/S0079-6123(06)55014-9", "10.1038/415429a", "10.1098/rspb.2013.1729", "10.1111/cgf.13258", "10.1016/S0959-4388(98)80147-5", "10.1016/S1476-9271(02)00092-0", "10.1111/cgf.13295", "10.1111/cgf.12793", "10.1162/pres.1996.5.3.290", "10.3758/BF03210754", "10.1111/j.1467-8659.2011.02086.x", "10.1007/978-3-540-49127-9_4", "10.1063/1.3253094", "10.1371/journal.pone.0157568", "10.1117/1.JEI.24.1.010501", "10.1016/j.atmosenv.2008.09.033", "10.1016/j.powtec.2014.05.019", "10.1068/p6740", "10.1016/j.appet.2010.02.011", "10.1121/1.2766781", "10.1093/acprof:oso/9780198524861.003.0007", "10.1016/S0079-6123(06)55014-9", "10.1038/415429a", "10.1098/rspb.2013.1729", "10.1111/cgf.13258", "10.1016/S0959-4388(98)80147-5", "10.1016/S1476-9271(02)00092-0", "10.1111/cgf.13295", "10.1111/cgf.12793", "10.1162/pres.1996.5.3.290", "10.3758/BF03210754", "10.1111/j.1467-8659.2011.02086.x", "10.1007/978-3-540-49127-9_4", "10.1063/1.3253094", "10.1371/journal.pone.0157568", "10.1117/1.JEI.24.1.010501", "10.1016/j.atmosenv.2008.09.033", "10.1016/j.powtec.2014.05.019", "10.1068/p6740", "10.1016/j.appet.2010.02.011", "10.1121/1.2766781", "10.1093/acprof:oso/9780198524861.003.0007", "10.1016/S0079-6123(06)55014-9", "10.1038/415429a", "10.1098/rspb.2013.1729", "10.1111/cgf.13258", "10.1016/S0959-4388(98)80147-5", "10.1016/S1476-9271(02)00092-0", "10.1111/cgf.13295", "10.1111/cgf.12793", "10.1162/pres.1996.5.3.290", "10.3758/BF03210754", "10.1111/j.1467-8659.2011.02086.x", "10.1007/978-3-540-49127-9_4", "10.1063/1.3253094", "10.1371/journal.pone.0157568", "10.1117/1.JEI.24.1.010501", "10.1016/j.atmosenv.2008.09.033", "10.1016/j.powtec.2014.05.019", "10.1068/p6740", "10.1016/j.appet.2010.02.011", "10.1121/1.2766781"]}, "10.1109/TVCG.2019.2898787": {"doi": "10.1109/TVCG.2019.2898787", "author": ["A. Andreasen", "M. Geronazzo", "N. C. Nilsson", "J. Zovnercuka", "K. Konovalov", "S. Serafin"], "title": "Auditory Feedback for Navigation with Echoes in Virtual Environments: Training Procedure and Orientation Strategies", "year": "2019", "abstract": "Being able to hear objects in an environment, for example using echolocation, is a challenging task. The main goal of the current work is to use virtual environments (VEs) to train novice users to navigate using echolocation. Previous studies have shown that musicians are able to differentiate sound pulses from reflections. This paper presents design patterns for VE simulators for both training and testing procedures, while classifying users' navigation strategies in the VE. Moreover, the paper presents features that increase users' performance in VEs. We report the findings of two user studies: a pilot test that helped improve the sonic interaction design, and a primary study exposing participants to a spatial orientation task during four conditions which were early reflections (RF), late reverberation (RV), early reflections-reverberation (RR) and visual stimuli (V). The latter study allowed us to identify navigation strategies among the users. Some users (10/26) reported an ability to create spatial cognitive maps during the test with auditory echoes, which may explain why this group performed better than the remaining participants in the RR condition.", "keywords": ["acoustic signal processing", "cognitive systems", "computer based training", "hearing", "human computer interaction", "navigation", "reverberation", "sonar signal processing", "user interfaces", "virtual reality", "reflections-reverberation", "echolocation", "orientation strategies", "training procedure", "virtual environments", "auditory feedback", "auditory echoes", "visual stimuli", "sonic interaction design", "VE simulators", "sound pulses", "Navigation", "Visualization", "Training", "Task analysis", "Reverberation", "Auditory system", "Human echolocation", "navigation", "spatial cognition", "virtual reality", "sonic interactions", "spatial audio", "binaural synthesis", "Acoustic Stimulation", "Animals", "Auditory Perception", "Chiroptera", "Computer Graphics", "Echolocation", "Feedback, Sensory", "Female", "Humans", "Male", "Orientation, Spatial", "Space Perception", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8998401"], "referencing": ["IKEY:8446448", "IKEY:4543709", "IKEY:6791208", "IKEY:8613754", "IKEY:8329208", "IKEY:7014276", "IKEY:7383327", "IKEY:8331264", "IKEY:8446448", "IKEY:4543709", "IKEY:6791208", "IKEY:8613754", "IKEY:8329208", "IKEY:7014276", "IKEY:7383327", "IKEY:8331264", "IKEY:8446448", "IKEY:4543709", "IKEY:6791208", "IKEY:8613754", "IKEY:8329208", "IKEY:7014276", "IKEY:7383327", "IKEY:8331264", "10.1016/j.heares.2013.06.001", "10.1177/0013916596284003", "10.3389/fnins.2018.00021", "10.1016/j.tics.2006.10.005", "10.1016/j.neuron.2015.09.021", "10.1016/S0960-9822(00)00740-5", "10.1371/journal.pone.0082491", "10.1007/s00221-016-4833-z", "10.1068/p250967", "10.1126/science.100.2609.589", "10.1016/j.cub.2009.02.033", "10.1242/jeb.01528", "10.1007/s10055-012-0213-6", "10.1037/1076-898X.12.4.223", "10.1016/j.heares.2014.01.010", "10.1016/j.neuropsychologia.2015.11.013", "10.1016/j.neuropsychologia.2012.11.017", "10.1177/0145482X0710100403", "10.2307/2183914", "10.1007/s10055-012-0211-8", "10.1068/p6473", "10.1002/wcs.1375", "10.1121/1.2935199", "10.1111/j.1468-2885.2009.01340.x", "10.3758/BF03196157", "10.1002/wcs.1408", "10.1371/journal.pcbi.1005670", "10.1371/journal.pone.0156654", "10.3389/fnins.2014.00283", "10.1002/9781118392683", "10.1518/001872006777724507", "10.1121/1.1917119", "10.1098/rspb.2013.1428", "10.1080/13875860902906496", "10.1080/13875860902906496", "10.1523/JNEUROSCI.2146-07.2007", "10.1016/j.cub.2011.11.018", "10.1016/j.heares.2013.06.001", "10.1177/0013916596284003", "10.3389/fnins.2018.00021", "10.1016/j.tics.2006.10.005", "10.1016/j.neuron.2015.09.021", "10.1016/S0960-9822(00)00740-5", "10.1371/journal.pone.0082491", "10.1007/s00221-016-4833-z", "10.1068/p250967", "10.1126/science.100.2609.589", "10.1016/j.cub.2009.02.033", "10.1242/jeb.01528", "10.1007/s10055-012-0213-6", "10.1037/1076-898X.12.4.223", "10.1016/j.heares.2014.01.010", "10.1016/j.neuropsychologia.2015.11.013", "10.1016/j.neuropsychologia.2012.11.017", "10.1177/0145482X0710100403", "10.2307/2183914", "10.1007/s10055-012-0211-8", "10.1068/p6473", "10.1002/wcs.1375", "10.1121/1.2935199", "10.1111/j.1468-2885.2009.01340.x", "10.3758/BF03196157", "10.1002/wcs.1408", "10.1371/journal.pcbi.1005670", "10.1371/journal.pone.0156654", "10.3389/fnins.2014.00283", "10.1002/9781118392683", "10.1518/001872006777724507", "10.1121/1.1917119", "10.1098/rspb.2013.1428", "10.1080/13875860902906496", "10.1080/13875860902906496", "10.1523/JNEUROSCI.2146-07.2007", "10.1016/j.cub.2011.11.018", "10.1016/j.heares.2013.06.001", "10.1177/0013916596284003", "10.3389/fnins.2018.00021", "10.1016/j.tics.2006.10.005", "10.1016/j.neuron.2015.09.021", "10.1016/S0960-9822(00)00740-5", "10.1371/journal.pone.0082491", "10.1007/s00221-016-4833-z", "10.1068/p250967", "10.1126/science.100.2609.589", "10.1016/j.cub.2009.02.033", "10.1242/jeb.01528", "10.1007/s10055-012-0213-6", "10.1037/1076-898X.12.4.223", "10.1016/j.heares.2014.01.010", "10.1016/j.neuropsychologia.2015.11.013", "10.1016/j.neuropsychologia.2012.11.017", "10.1177/0145482X0710100403", "10.2307/2183914", "10.1007/s10055-012-0211-8", "10.1068/p6473", "10.1002/wcs.1375", "10.1121/1.2935199", "10.1111/j.1468-2885.2009.01340.x", "10.3758/BF03196157", "10.1002/wcs.1408", "10.1371/journal.pcbi.1005670", "10.1371/journal.pone.0156654", "10.3389/fnins.2014.00283", "10.1002/9781118392683", "10.1518/001872006777724507", "10.1121/1.1917119", "10.1098/rspb.2013.1428", "10.1080/13875860902906496", "10.1080/13875860902906496", "10.1523/JNEUROSCI.2146-07.2007", "10.1016/j.cub.2011.11.018"]}, "10.1109/TVCG.2019.2898748": {"doi": "10.1109/TVCG.2019.2898748", "author": ["S. Pujades", "B. Mohler", "A. Thaler", "J. Tesch", "N. Mahmood", "N. Hesse", "H. H. B\u00fclthoff", "M. J. Black"], "title": "The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements", "year": "2019", "abstract": "Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned.", "keywords": ["avatars", "clothing", "computer games", "ergonomics", "solid modelling", "Virtual Caliper", "VR game controllers", "body measurements users", "distance measurements", "SMPL body model", "3D measurements", "3D body shape", "avatars creation", "Avatars", "Shape", "Three-dimensional displays", "Shape measurement", "Solid modeling", "Tools", "Distance measurement", "Full body avatars", "metric accuracy", "rapid creation", "Anthropometry", "Body Image", "Body Size", "Computer Graphics", "Computer Systems", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Self Concept", "Software", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8998305", "IKEY:9251122", "IKEY:9284694"], "referencing": ["IKEY:609848", "IKEY:4270338", "IKEY:6931117", "IKEY:7410622", "IKEY:1159612", "IKEY:6861875", "IKEY:5539853", "IKEY:781210", "IKEY:6365454", "IKEY:8099983", "IKEY:7892240", "IKEY:6213238", "IKEY:6599094", "IKEY:7035823", "IKEY:6599084", "IKEY:6165146", "IKEY:8263407", "IKEY:6126465", "IKEY:6909698", "IKEY:8100065", "IKEY:6909487", "IKEY:609848", "IKEY:4270338", "IKEY:6931117", "IKEY:7410622", "IKEY:1159612", "IKEY:6861875", "IKEY:5539853", "IKEY:781210", "IKEY:6365454", "IKEY:8099983", "IKEY:7892240", "IKEY:6213238", "IKEY:6599094", "IKEY:7035823", "IKEY:6599084", "IKEY:6165146", "IKEY:8263407", "IKEY:6126465", "IKEY:6909698", "IKEY:8100065", "IKEY:6909487", "IKEY:609848", "IKEY:4270338", "IKEY:6931117", "IKEY:7410622", "IKEY:1159612", "IKEY:6861875", "IKEY:5539853", "IKEY:781210", "IKEY:6365454", "IKEY:8099983", "IKEY:7892240", "IKEY:6213238", "IKEY:6599094", "IKEY:7035823", "IKEY:6599084", "IKEY:6165146", "IKEY:8263407", "IKEY:6126465", "IKEY:6909698", "IKEY:8100065", "IKEY:6909487", "10.1145/3139131.3139154", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2994258.2994259", "10.1145/311535.311556", "10.1145/280814.280823", "10.1145/1882262.1866174", "10.1145/2508363.2508407", "10.1145/3130800.3130813", "10.1145/2661229.2661273", "10.1145/2816795.2818013", "10.1145/2766957", "10.1145/1882261.1866161", "10.1145/2897824.2925981", "10.1145/1833349.1778863", "10.1145/2601097.2601165", "10.1145/3139131.3139154", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2994258.2994259", "10.1145/311535.311556", "10.1145/280814.280823", "10.1145/1882262.1866174", "10.1145/2508363.2508407", "10.1145/3130800.3130813", "10.1145/2661229.2661273", "10.1145/2816795.2818013", "10.1145/2766957", "10.1145/1882261.1866161", "10.1145/2897824.2925981", "10.1145/1833349.1778863", "10.1145/2601097.2601165", "10.1145/3139131.3139154", "10.1145/1201775.882311", "10.1145/1186822.1073207", "10.1145/2994258.2994259", "10.1145/311535.311556", "10.1145/280814.280823", "10.1145/1882262.1866174", "10.1145/2508363.2508407", "10.1145/3130800.3130813", "10.1145/2661229.2661273", "10.1145/2816795.2818013", "10.1145/2766957", "10.1145/1882261.1866161", "10.1145/2897824.2925981", "10.1145/1833349.1778863", "10.1145/2601097.2601165", "10.1007/978-3-319-46454-1_34", "10.1007/s00138-011-0353-9", "10.1016/j.compind.2010.03.004", "10.1007/978-3-319-46493-0_6", "10.1007/978-3-642-33718-5_1", "10.1016/j.actpsy.2013.06.012", "10.1016/j.ergon.2010.06.002", "10.1016/j.cag.2009.03.026", "10.1111/j.1467-8659.2009.01373.x", "10.1016/j.cie.2007.06.027", "10.1371/journal.pone.0163921", "10.1093/ajcn/20.4.305", "10.3758/s13428-011-0168-7", "10.1177/2041669517708205", "10.1007/978-3-319-46454-1_31", "10.1016/j.apergo.2005.07.009", "10.1016/j.gmod.2003.07.004", "10.1002/cav.1579", "10.1111/cgf.13131", "10.1016/j.cviu.2014.06.012", "10.1007/s00138-012-0472-y", "10.1007/978-3-319-46493-0_27", "10.1007/978-3-319-46454-1_34", "10.1007/s00138-011-0353-9", "10.1016/j.compind.2010.03.004", "10.1007/978-3-319-46493-0_6", "10.1007/978-3-642-33718-5_1", "10.1016/j.actpsy.2013.06.012", "10.1016/j.ergon.2010.06.002", "10.1016/j.cag.2009.03.026", "10.1111/j.1467-8659.2009.01373.x", "10.1016/j.cie.2007.06.027", "10.1371/journal.pone.0163921", "10.1093/ajcn/20.4.305", "10.3758/s13428-011-0168-7", "10.1177/2041669517708205", "10.1007/978-3-319-46454-1_31", "10.1016/j.apergo.2005.07.009", "10.1016/j.gmod.2003.07.004", "10.1002/cav.1579", "10.1111/cgf.13131", "10.1016/j.cviu.2014.06.012", "10.1007/s00138-012-0472-y", "10.1007/978-3-319-46493-0_27", "10.1007/978-3-319-46454-1_34", "10.1007/s00138-011-0353-9", "10.1016/j.compind.2010.03.004", "10.1007/978-3-319-46493-0_6", "10.1007/978-3-642-33718-5_1", "10.1016/j.actpsy.2013.06.012", "10.1016/j.ergon.2010.06.002", "10.1016/j.cag.2009.03.026", "10.1111/j.1467-8659.2009.01373.x", "10.1016/j.cie.2007.06.027", "10.1371/journal.pone.0163921", "10.1093/ajcn/20.4.305", "10.3758/s13428-011-0168-7", "10.1177/2041669517708205", "10.1007/978-3-319-46454-1_31", "10.1016/j.apergo.2005.07.009", "10.1016/j.gmod.2003.07.004", "10.1002/cav.1579", "10.1111/cgf.13131", "10.1016/j.cviu.2014.06.012", "10.1007/s00138-012-0472-y", "10.1007/978-3-319-46493-0_27"]}, "10.1109/TVCG.2019.2899227": {"doi": "10.1109/TVCG.2019.2899227", "author": ["J. Uijong", "J. Kang", "C. Wallraven"], "title": "You or Me? Personality Traits Predict Sacrificial Decisions in an Accident Situation", "year": "2019", "abstract": "Emergency situations during car driving sometimes force the driver to make a sudden decision. Predicting these decisions will have important applications in updating risk analyses in insurance applications, but also can give insights for drafting autonomous vehicle guidelines. Studying such behavior in experimental settings, however, is limited by ethical issues as it would endanger peoples' lives. Here, we employed the potential of virtual reality (VR) to investigate decision-making in an extreme situation in which participants would have to sacrifice others in order to save themselves. In a VR driving simulation, participants first trained to complete a difficult course with multiple crossroads in which the wrong turn would lead the car to fall down a cliff. In the testing phase, obstacles suddenly appeared on the \u201csafe\u201d turn of a crossroad: for the control group, obstacles consisted of trees, whereas for the experimental group, they were pedestrians. In both groups, drivers had to decide between falling down the cliff or colliding with the obstacles. Results showed that differences in personality traits were able to predict this decision: in the experimental group, drivers who collided with the pedestrians had significantly higher psychopathy and impulsivity traits, whereas impulsivity alone was to some degree predictive in the control group. Other factors like heart rate differences, gender, video game expertise, and driving experience were not predictive of the emergency decision in either group. Our results show that self-interest related personality traits affect decision-making when choosing between preservation of self or others in extreme situations and showcase the potential of virtual reality in studying and modeling human decision-making.", "keywords": ["computer simulation", "decision making", "human factors", "pedestrians", "risk analysis", "road accidents", "traffic engineering computing", "virtual reality", "virtual reality", "human decision-making", "sacrificial decisions", "accident situation", "emergency situations", "car driving", "risk analyses", "autonomous vehicle guidelines", "VR driving simulation", "pedestrians", "impulsivity traits", "personality traits", "Accidents", "Automobiles", "Decision making", "Training", "Autonomous vehicles", "Virtual reality", "Computing methodologies", "Virtual reality", "Applied computing", "Psychology", "Software and its engineering", "Interactive games", "Human-centered computing", "Laboratory experiments", "Accidents, Traffic", "Adult", "Arousal", "Artificial Intelligence", "Automobile Driving", "Automobiles", "Computer Graphics", "Decision Making", "Emergencies", "Female", "Heart Rate", "Humans", "Male", "Personality", "Surveys and Questionnaires", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": [], "referencing": ["IKEY:7504690", "IKEY:7892296", "IKEY:8446480", "IKEY:4359501", "IKEY:7504690", "IKEY:7892296", "IKEY:8446480", "IKEY:4359501", "IKEY:7504690", "IKEY:7892296", "IKEY:8446480", "IKEY:4359501", "10.1145/129888.129892", "10.1145/1502800.1502805", "10.1145/3236673", "10.1145/3116595.3116596", "10.1145/129888.129892", "10.1145/1502800.1502805", "10.1145/3236673", "10.1145/3116595.3116596", "10.1145/129888.129892", "10.1145/1502800.1502805", "10.1145/3236673", "10.1145/3116595.3116596", "10.1371/journal.pone.0184952", "10.1126/science.aaf2654", "10.1038/s41586-018-0637-6", "10.3389/fpsyg.2013.00250", "10.2307/796133", "10.3389/fnbeh.2017.00122", "10.1007/s11948-018-0020-x", "10.1016/S0001-4575(97)00017-1", "10.1016/j.ssci.2005.12.003", "10.1016/j.trf.2015.06.001", "10.1073/pnas.1009396107", "10.1016/j.cognition.2014.10.005", "10.1037/0022-3514.74.1.192", "10.1007/s10339-009-0335-2", "10.1016/0191-8869(95)00068-H", "10.1037/a0018251", "10.1371/journal.pone.0164374", "10.1080/17470919.2013.870091", "10.3758/BF03193146", "10.1037/0022-3514.68.1.151", "10.1037/0022-3514.44.1.113", "10.1002/1097-4679(199511)51:6&lt;768::AID-JCLP2270510607&gt;3.0.CO;2-1", "10.1177/0149206308318618", "10.1037/1040-3590.16.3.276", "10.1037/0021-843X.116.2.395", "10.1037/a0018135", "10.1177/1073191108319043", "10.1097/00043764-199208000-00014", "10.1016/j.biopsycho.2010.03.010", "10.1080/026999398379718", "10.1111/j.1469-8986.2006.00380.x", "10.1016/j.jeem.2008.08.002", "10.1207/S15327965PLI1302_01", "10.1371/journal.pone.0000039", "10.3389/neuro.09.029.2009", "10.1002/ejsp.297", "10.3389/neuro.08.059.2009", "10.1371/journal.pone.0052766", "10.1155/2015/151702", "10.1093/scan/nsr048", "10.1016/S0925-7535(01)00077-7", "10.1016/j.aap.2004.10.006", "10.1016/j.jsr.2007.04.005", "10.1080/00140138808966701", "10.1016/j.aap.2009.03.016", "10.1016/0001-4575(86)90044-8", "10.1016/j.aap.2010.11.015", "10.1016/j.copsyc.2015.09.007", "10.1207/s15327108ijap0303_3", "10.1371/journal.pone.0184952", "10.1126/science.aaf2654", "10.1038/s41586-018-0637-6", "10.3389/fpsyg.2013.00250", "10.2307/796133", "10.3389/fnbeh.2017.00122", "10.1007/s11948-018-0020-x", "10.1016/S0001-4575(97)00017-1", "10.1016/j.ssci.2005.12.003", "10.1016/j.trf.2015.06.001", "10.1073/pnas.1009396107", "10.1016/j.cognition.2014.10.005", "10.1037/0022-3514.74.1.192", "10.1007/s10339-009-0335-2", "10.1016/0191-8869(95)00068-H", "10.1037/a0018251", "10.1371/journal.pone.0164374", "10.1080/17470919.2013.870091", "10.3758/BF03193146", "10.1037/0022-3514.68.1.151", "10.1037/0022-3514.44.1.113", "10.1002/1097-4679(199511)51:6&lt;768::AID-JCLP2270510607&gt;3.0.CO;2-1", "10.1177/0149206308318618", "10.1037/1040-3590.16.3.276", "10.1037/0021-843X.116.2.395", "10.1037/a0018135", "10.1177/1073191108319043", "10.1097/00043764-199208000-00014", "10.1016/j.biopsycho.2010.03.010", "10.1080/026999398379718", "10.1111/j.1469-8986.2006.00380.x", "10.1016/j.jeem.2008.08.002", "10.1207/S15327965PLI1302_01", "10.1371/journal.pone.0000039", "10.3389/neuro.09.029.2009", "10.1002/ejsp.297", "10.3389/neuro.08.059.2009", "10.1371/journal.pone.0052766", "10.1155/2015/151702", "10.1093/scan/nsr048", "10.1016/S0925-7535(01)00077-7", "10.1016/j.aap.2004.10.006", "10.1016/j.jsr.2007.04.005", "10.1080/00140138808966701", "10.1016/j.aap.2009.03.016", "10.1016/0001-4575(86)90044-8", "10.1016/j.aap.2010.11.015", "10.1016/j.copsyc.2015.09.007", "10.1207/s15327108ijap0303_3", "10.1371/journal.pone.0184952", "10.1126/science.aaf2654", "10.1038/s41586-018-0637-6", "10.3389/fpsyg.2013.00250", "10.2307/796133", "10.3389/fnbeh.2017.00122", "10.1007/s11948-018-0020-x", "10.1016/S0001-4575(97)00017-1", "10.1016/j.ssci.2005.12.003", "10.1016/j.trf.2015.06.001", "10.1073/pnas.1009396107", "10.1016/j.cognition.2014.10.005", "10.1037/0022-3514.74.1.192", "10.1007/s10339-009-0335-2", "10.1016/0191-8869(95)00068-H", "10.1037/a0018251", "10.1371/journal.pone.0164374", "10.1080/17470919.2013.870091", "10.3758/BF03193146", "10.1037/0022-3514.68.1.151", "10.1037/0022-3514.44.1.113", "10.1002/1097-4679(199511)51:6&lt;768::AID-JCLP2270510607&gt;3.0.CO;2-1", "10.1177/0149206308318618", "10.1037/1040-3590.16.3.276", "10.1037/0021-843X.116.2.395", "10.1037/a0018135", "10.1177/1073191108319043", "10.1097/00043764-199208000-00014", "10.1016/j.biopsycho.2010.03.010", "10.1080/026999398379718", "10.1111/j.1469-8986.2006.00380.x", "10.1016/j.jeem.2008.08.002", "10.1207/S15327965PLI1302_01", "10.1371/journal.pone.0000039", "10.3389/neuro.09.029.2009", "10.1002/ejsp.297", "10.3389/neuro.08.059.2009", "10.1371/journal.pone.0052766", "10.1155/2015/151702", "10.1093/scan/nsr048", "10.1016/S0925-7535(01)00077-7", "10.1016/j.aap.2004.10.006", "10.1016/j.jsr.2007.04.005", "10.1080/00140138808966701", "10.1016/j.aap.2009.03.016", "10.1016/0001-4575(86)90044-8", "10.1016/j.aap.2010.11.015", "10.1016/j.copsyc.2015.09.007", "10.1207/s15327108ijap0303_3"]}, "10.1109/TVCG.2019.2898737": {"doi": "10.1109/TVCG.2019.2898737", "author": ["J. Young", "T. Langlotz", "M. Cook", "S. Mills", "H. Regenbrecht"], "title": "Immersive Telepresence and Remote Collaboration using Mobile and Wearable Devices", "year": "2019", "abstract": "The mobility and ubiquity of mobile head-mounted displays make them a promising platform for telepresence research as they allow for spontaneous and remote use cases not possible with stationary hardware. In this work we present a system that provides immersive telepresence and remote collaboration on mobile and wearable devices by building a live spherical panoramic representation of a user's environment that can be viewed in real time by a remote user who can independently choose the viewing direction. The remote user can then interact with this environment as if they were actually there through intuitive gesture-based interaction. Each user can obtain independent views within this environment by rotating their device, and their current field of view is shared to allow for simple coordination of viewpoints. We present several different approaches to create this shared live environment and discuss their implementation details, individual challenges, and performance on modern mobile hardware; by doing so we provide key insights into the design and implementation of next generation mobile telepresence systems, guiding future research in this domain. The results of a preliminary user study confirm the ability of our system to induce the desired sense of presence in its users.", "keywords": ["gesture recognition", "groupware", "helmet mounted displays", "human computer interaction", "mobile computing", "telecontrol", "virtual reality", "mobile hardware", "next generation mobile telepresence systems", "shared live environment", "intuitive gesture-based interaction", "live spherical panoramic representation", "wearable devices", "mobile devices", "remote collaboration", "immersive telepresence", "stationary hardware", "remote use cases", "telepresence research", "mobile head-mounted displays", "Telepresence", "Collaboration", "Task analysis", "Cameras", "Hardware", "Mobile handsets", "Resists", "Telepresence", "Remote Collaboration", "CSCW", "Computer Graphics", "Computer Systems", "Gestures", "Humans", "Orientation, Spatial", "Social Behavior", "User-Computer Interface", "Videoconferencing", "Virtual Reality", "Wearable Electronic Devices"], "referenced_by": ["IKEY:9089490", "IKEY:9148582", "IKEY:9284659"], "referencing": ["IKEY:806696", "IKEY:4480745", "IKEY:6788202", "IKEY:6948412", "IKEY:7745185", "IKEY:7456508", "IKEY:6126544", "IKEY:6790846", "IKEY:572043", "IKEY:806696", "IKEY:4480745", "IKEY:6788202", "IKEY:6948412", "IKEY:7745185", "IKEY:7456508", "IKEY:6126544", "IKEY:6790846", "IKEY:572043", "IKEY:806696", "IKEY:4480745", "IKEY:6788202", "IKEY:6948412", "IKEY:7745185", "IKEY:7456508", "IKEY:6126544", "IKEY:6790846", "IKEY:572043", "10.1145/1186822.1073268", "10.1145/358916.358947", "10.1145/2371574.2371610", "10.1145/2642918.2647372", "10.1145/2441955.2441994", "10.1145/2468356.2468610", "10.1145/2582051.2582097", "10.1145/1124772.1124951", "10.1145/2786567.2787134", "10.1145/2628363.2628430", "10.1145/142750.142980", "10.1145/2470654.2466173", "10.1145/1015706.1015720", "10.1145/2470654.2470679", "10.1145/3064663.3064707", "10.1145/1186822.1073268", "10.1145/358916.358947", "10.1145/2371574.2371610", "10.1145/2642918.2647372", "10.1145/2441955.2441994", "10.1145/2468356.2468610", "10.1145/2582051.2582097", "10.1145/1124772.1124951", "10.1145/2786567.2787134", "10.1145/2628363.2628430", "10.1145/142750.142980", "10.1145/2470654.2466173", "10.1145/1015706.1015720", "10.1145/2470654.2470679", "10.1145/3064663.3064707", "10.1145/1186822.1073268", "10.1145/358916.358947", "10.1145/2371574.2371610", "10.1145/2642918.2647372", "10.1145/2441955.2441994", "10.1145/2468356.2468610", "10.1145/2582051.2582097", "10.1145/1124772.1124951", "10.1145/2786567.2787134", "10.1145/2628363.2628430", "10.1145/142750.142980", "10.1145/2470654.2466173", "10.1145/1015706.1015720", "10.1145/2470654.2470679", "10.1145/3064663.3064707", "10.3745/JIPS.02.0002", "10.1016/j.pragma.2008.09.016", "10.1177/0735275113489811", "10.1207/s15327051hci1903_3", "10.1207/s15327108ijap0303_3", "10.1207/S15327051HCI1812_3", "10.3745/JIPS.02.0002", "10.1016/j.pragma.2008.09.016", "10.1177/0735275113489811", "10.1207/s15327051hci1903_3", "10.1207/s15327108ijap0303_3", "10.1207/S15327051HCI1812_3", "10.3745/JIPS.02.0002", "10.1016/j.pragma.2008.09.016", "10.1177/0735275113489811", "10.1207/s15327051hci1903_3", "10.1207/s15327108ijap0303_3", "10.1207/S15327051HCI1812_3"]}, "10.1109/TVCG.2019.2899186": {"doi": "10.1109/TVCG.2019.2899186", "author": ["T. M. Lee", "J. Yoon", "I. Lee"], "title": "Motion Sickness Prediction in Stereoscopic Videos using 3D Convolutional Neural Networks", "year": "2019", "abstract": "In this paper, we propose a three-dimensional (3D) convolutional neural network (CNN)-based method for predicting the degree of motion sickness induced by a 360\u00b0 stereoscopic video. We consider the user's eye movement as a new feature, in addition to the motion velocity and depth features of a video used in previous work. For this purpose, we use saliency, optical flow, and disparity maps of an input video, which represent eye movement, velocity, and depth, respectively, as the input of the 3D CNN. To train our machine-learning model, we extend the dataset established in the previous work using two data augmentation techniques: frame shifting and pixel shifting. Consequently, our model can predict the degree of motion sickness more precisely than the previous method, and the results have a more similar correlation to the distribution of ground-truth sickness.", "keywords": ["convolutional neural nets", "image motion analysis", "image sequences", "learning (artificial intelligence)", "stereo image processing", "video signal processing", "motion sickness prediction", "stereoscopic videos", "3D convolutional neural networks", "three-dimensional convolutional neural network-based method", "eye movement", "motion velocity", "depth features", "3D CNN", "machine-learning model", "optical flow", "disparity maps", "Videos", "Stereo image processing", "Three-dimensional displays", "Optical imaging", "Machine learning", "Optical saturation", "Optical computing", "360\u00b0 Stereoscopic video", "motion sickness", "virtual reality", "saliency", "machine learning", "3D CNN"], "referenced_by": ["IKEY:9089551", "IKEY:9090494", "IKEY:9089513", "IKEY:9201649", "IKEY:9284761"], "referencing": ["IKEY:7274431", "IKEY:7900174", "IKEY:7460053", "IKEY:7727386", "IKEY:8237584", "IKEY:7780459", "IKEY:4359315", "IKEY:6795963", "IKEY:8099726", "IKEY:7900038", "IKEY:730558", "IKEY:5539950", "IKEY:6165309", "IKEY:310690", "IKEY:8463413", "IKEY:726791", "IKEY:996519", "IKEY:8267239", "IKEY:8100121", "IKEY:7485869", "IKEY:8269807", "IKEY:7298594", "IKEY:7410867", "IKEY:7940083", "IKEY:8237506", "IKEY:7274431", "IKEY:7900174", "IKEY:7460053", "IKEY:7727386", "IKEY:8237584", "IKEY:7780459", "IKEY:4359315", "IKEY:6795963", "IKEY:8099726", "IKEY:7900038", "IKEY:730558", "IKEY:5539950", "IKEY:6165309", "IKEY:310690", "IKEY:8463413", "IKEY:726791", "IKEY:996519", "IKEY:8267239", "IKEY:8100121", "IKEY:7485869", "IKEY:8269807", "IKEY:7298594", "IKEY:7410867", "IKEY:7940083", "IKEY:8237506", "IKEY:7274431", "IKEY:7900174", "IKEY:7460053", "IKEY:7727386", "IKEY:8237584", "IKEY:7780459", "IKEY:4359315", "IKEY:6795963", "IKEY:8099726", "IKEY:7900038", "IKEY:730558", "IKEY:5539950", "IKEY:6165309", "IKEY:310690", "IKEY:8463413", "IKEY:726791", "IKEY:996519", "IKEY:8267239", "IKEY:8100121", "IKEY:7485869", "IKEY:8269807", "IKEY:7298594", "IKEY:7410867", "IKEY:7940083", "IKEY:8237506", "10.1145/3139131.3139137", "10.1145/1180495.1180567", "10.1145/566654.566630", "10.1145/2993369.2993402", "10.1145/2993369.2996349", "10.1145/3139131.3139137", "10.1145/1180495.1180567", "10.1145/566654.566630", "10.1145/2993369.2993402", "10.1145/2993369.2996349", "10.1145/3139131.3139137", "10.1145/1180495.1180567", "10.1145/566654.566630", "10.1145/2993369.2993402", "10.1145/2993369.2996349", "10.1068/p2891", "10.7205/MILMED-D-14-00424", "10.3357/ASEM.2394.2009", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1117/1.JEI.21.1.011008", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/PL00005624", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008", "10.1177/1071181311551254", "10.1068/p2891", "10.7205/MILMED-D-14-00424", "10.3357/ASEM.2394.2009", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1117/1.JEI.21.1.011008", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/PL00005624", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008", "10.1177/1071181311551254", "10.1068/p2891", "10.7205/MILMED-D-14-00424", "10.3357/ASEM.2394.2009", "10.1016/j.paid.2006.01.012", "10.1162/pres.1992.1.3.306", "10.1117/1.JEI.21.1.011008", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/PL00005624", "10.1518/001872001775898223", "10.1371/journal.pone.0056160", "10.1016/j.cag.2008.11.008", "10.1177/1071181311551254"]}, "10.1109/TVCG.2019.2898781": {"doi": "10.1109/TVCG.2019.2898781", "author": ["K. Ak\u015fit", "P. Chakravarthula", "K. Rathinavel", "Y. Jeong", "R. Albert", "H. Fuchs", "D. Luebke"], "title": "Manufacturing Application-Driven Foveated Near-Eye Displays", "year": "2019", "abstract": "Traditional optical manufacturing poses a great challenge to near-eye display designers due to large lead times in the order of multiple weeks, limiting the abilities of optical designers to iterate fast and explore beyond conventional designs. We present a complete near-eye display manufacturing pipeline with a day lead time using commodity hardware. Our novel manufacturing pipeline consists of several innovations including a rapid production technique to improve surface of a 3D printed component to optical quality suitable for near-eye display application, a computational design methodology using machine learning and ray tracing to create freeform static projection screen surfaces for near-eye displays that can represent arbitrary focal surfaces, and a custom projection lens design that distributes pixels non-uniformly for a foveated near-eye display hardware design candidate. We have demonstrated untethered augmented reality near-eye display prototypes to assess success of our technique, and show that a ski-goggles form factor, a large monocular field of view $(30^{o}\\times 55^{o})$, and a resolution of 12 cycles per degree can be achieved.", "keywords": ["Three-dimensional displays", "Optical refraction", "Optical waveguides", "Optical design", "Adaptive optics", "Optical surface waves", "Near-eye displays", "See-through Displays", "Application Adaptive Displays", "Computational Displays", "Augmented Reality Displays", "3D printed optical components", "Waveguides", "projection displays"], "referenced_by": ["IKEY:8999630", "IKEY:8999805"], "referencing": ["10.1145/1186562.1015804", "10.1145/1936652.1936657", "10.1145/1449715.1449729", "10.1145/1979742.1979719", "10.1145/2501988.2502027", "10.1145/1278240.1278243", "10.1145/2782782.2792493", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/1661412.1618474", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2642918.2647413", "10.1145/2366145.2366205", "10.1145/2980179.2980246", "10.1145/2602140", "10.1145/2858036.2858140", "10.1145/2207676.2207781", "10.1145/2601097.2601200", "10.1145/3130800.3130832", "10.1145/3130800.3130807", "10.1145/3025453.3025608", "10.1145/2461912.2462011", "10.1145/1936652.1936654", "10.1145/1531326.1531338", "10.1145/2380116.2380190", "10.1145/2580946", "10.1145/1186562.1015804", "10.1145/1936652.1936657", "10.1145/1449715.1449729", "10.1145/1979742.1979719", "10.1145/2501988.2502027", "10.1145/1278240.1278243", "10.1145/2782782.2792493", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/1661412.1618474", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2642918.2647413", "10.1145/2366145.2366205", "10.1145/2980179.2980246", "10.1145/2602140", "10.1145/2858036.2858140", "10.1145/2207676.2207781", "10.1145/2601097.2601200", "10.1145/3130800.3130832", "10.1145/3130800.3130807", "10.1145/3025453.3025608", "10.1145/2461912.2462011", "10.1145/1936652.1936654", "10.1145/1531326.1531338", "10.1145/2380116.2380190", "10.1145/2580946", "10.1145/1186562.1015804", "10.1145/1936652.1936657", "10.1145/1449715.1449729", "10.1145/1979742.1979719", "10.1145/2501988.2502027", "10.1145/1278240.1278243", "10.1145/2782782.2792493", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/3072959.3073590", "10.1145/1661412.1618474", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2642918.2647413", "10.1145/2366145.2366205", "10.1145/2980179.2980246", "10.1145/2602140", "10.1145/2858036.2858140", "10.1145/2207676.2207781", "10.1145/2601097.2601200", "10.1145/3130800.3130832", "10.1145/3130800.3130807", "10.1145/3025453.3025608", "10.1145/2461912.2462011", "10.1145/1936652.1936654", "10.1145/1531326.1531338", "10.1145/2380116.2380190", "10.1145/2580946", "10.1038/s41467-018-04186-9", "10.1111/cgf.13327", "10.1364/OE.16.001583", "10.1117/12.923660", "10.1080/713826091", "10.1016/j.visres.2006.12.001", "10.1364/OE.23.010224", "10.1364/JOSA.42.000492", "10.1117/12.529999", "10.1889/1.2176112", "10.1167/8.3.33", "10.1117/12.60427", "10.1007/s11042-016-3502-3", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1117/12.2015654", "10.1007/978-3-642-33709-3_8", "10.1002/jsid.435", "10.1111/j.1467-8659.2011.01876.x", "10.1518/0018720024497015", "10.3758/BF03200168", "10.1364/AO.37.004183", "10.1117/12.940468", "10.1364/JOSAA.4.001524", "10.1016/0042-6989(95)00109-D", "10.1007/s00170-012-4605-2", "10.1016/0042-6989(85)90113-0", "10.1364/JOSAA.2.001087", "10.1364/OE.26.004863", "10.1038/s41467-018-04186-9", "10.1111/cgf.13327", "10.1364/OE.16.001583", "10.1117/12.923660", "10.1080/713826091", "10.1016/j.visres.2006.12.001", "10.1364/OE.23.010224", "10.1364/JOSA.42.000492", "10.1117/12.529999", "10.1889/1.2176112", "10.1167/8.3.33", "10.1117/12.60427", "10.1007/s11042-016-3502-3", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1117/12.2015654", "10.1007/978-3-642-33709-3_8", "10.1002/jsid.435", "10.1111/j.1467-8659.2011.01876.x", "10.1518/0018720024497015", "10.3758/BF03200168", "10.1364/AO.37.004183", "10.1117/12.940468", "10.1364/JOSAA.4.001524", "10.1016/0042-6989(95)00109-D", "10.1007/s00170-012-4605-2", "10.1016/0042-6989(85)90113-0", "10.1364/JOSAA.2.001087", "10.1364/OE.26.004863", "10.1038/s41467-018-04186-9", "10.1111/cgf.13327", "10.1364/OE.16.001583", "10.1117/12.923660", "10.1080/713826091", "10.1016/j.visres.2006.12.001", "10.1364/OE.23.010224", "10.1364/JOSA.42.000492", "10.1117/12.529999", "10.1889/1.2176112", "10.1167/8.3.33", "10.1117/12.60427", "10.1007/s11042-016-3502-3", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1117/12.2015654", "10.1007/978-3-642-33709-3_8", "10.1002/jsid.435", "10.1111/j.1467-8659.2011.01876.x", "10.1518/0018720024497015", "10.3758/BF03200168", "10.1364/AO.37.004183", "10.1117/12.940468", "10.1364/JOSAA.4.001524", "10.1016/0042-6989(95)00109-D", "10.1007/s00170-012-4605-2", "10.1016/0042-6989(85)90113-0", "10.1364/JOSAA.2.001087", "10.1364/OE.26.004863"]}, "10.1109/TVCG.2019.2898821": {"doi": "10.1109/TVCG.2019.2898821", "author": ["H. Yu", "M. Bemana", "M. Wernikowski", "M. Chwesiuk", "O. T. Tursun", "G. Singh", "K. Myszkowski", "R. Mantiuk", "H. Seidel", "P. Didyk"], "title": "A Perception-driven Hybrid Decomposition for Multi-layer Accommodative Displays", "year": "2019", "abstract": "Multi-focal plane and multi-layered light-field displays are promising solutions for addressing all visual cues observed in the real world. Unfortunately, these devices usually require expensive optimizations to compute a suitable decomposition of the input light field or focal stack to drive individual display layers. Although these methods provide near-correct image reconstruction, a significant computational cost prevents real-time applications. A simple alternative is a linear blending strategy which decomposes a single 2D image using depth information. This method provides real-time performance, but it generates inaccurate results at occlusion boundaries and on glossy surfaces. This paper proposes a perception-based hybrid decomposition technique which combines the advantages of the above strategies and achieves both real-time performance and high-fidelity results. The fundamental idea is to apply expensive optimizations only in regions where it is perceptually superior, e.g., depth discontinuities at the fovea, and fall back to less costly linear blending otherwise. We present a complete, perception-informed analysis and model that locally determine which of the two strategies should be applied. The prediction is later utilized by our new synthesis method which performs the image decomposition. The results are analyzed and validated in user experiments on a custom multi-plane display.", "keywords": ["computer vision", "image reconstruction", "visual perception", "multilayer accommodative displays", "visual cues", "linear blending strategy", "perception-informed analysis", "image decomposition", "image reconstruction", "depth information", "multifocal plane display", "multilayered light-field display", "perception-driven hybrid decomposition", "Rendering (computer graphics)", "Three-dimensional displays", "Optimization", "Real-time systems", "Computational efficiency", "Computer architecture", "Visualization", "3D displays", "Rendering", "Accommodation", "Perception"], "referenced_by": [], "referencing": ["IKEY:8099879", "IKEY:7829412", "IKEY:504", "IKEY:7831415", "IKEY:7226865", "IKEY:7904663", "IKEY:5539854", "IKEY:6671761", "IKEY:8007187", "IKEY:1284395", "IKEY:8099879", "IKEY:7829412", "IKEY:504", "IKEY:7831415", "IKEY:7226865", "IKEY:7904663", "IKEY:5539854", "IKEY:6671761", "IKEY:8007187", "IKEY:1284395", "IKEY:8099879", "IKEY:7829412", "IKEY:504", "IKEY:7831415", "IKEY:7226865", "IKEY:7904663", "IKEY:5539854", "IKEY:6671761", "IKEY:8007187", "IKEY:1284395", "10.1145/1015706.1015804", "10.1145/3130800.3130892", "10.1145/3130800.3130815", "10.1145/2366145.2366183", "10.1145/2461912.2461925", "10.1145/2766922", "10.1145/2782782.2792493", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/1344471.1344488", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/3072959.3073624", "10.1145/2010324.1964935", "10.1145/3072959.3073590", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2980179.2980246", "10.1145/3130800.3130807", "10.1145/2931002.2931011", "10.1145/2010324.1964990", "10.1145/3196493", "10.1145/1015706.1015804", "10.1145/3130800.3130892", "10.1145/3130800.3130815", "10.1145/2366145.2366183", "10.1145/2461912.2461925", "10.1145/2766922", "10.1145/2782782.2792493", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/1344471.1344488", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/3072959.3073624", "10.1145/2010324.1964935", "10.1145/3072959.3073590", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2980179.2980246", "10.1145/3130800.3130807", "10.1145/2931002.2931011", "10.1145/2010324.1964990", "10.1145/3196493", "10.1145/1015706.1015804", "10.1145/3130800.3130892", "10.1145/3130800.3130815", "10.1145/2366145.2366183", "10.1145/2461912.2461925", "10.1145/2766922", "10.1145/2782782.2792493", "10.1145/3072959.3073594", "10.1145/3072959.3073622", "10.1145/1344471.1344488", "10.1145/2508363.2508366", "10.1145/2070781.2024220", "10.1145/2897824.2925971", "10.1145/3072959.3073624", "10.1145/2010324.1964935", "10.1145/3072959.3073590", "10.1145/3130800.3130846", "10.1145/2766909", "10.1145/2980179.2980246", "10.1145/3130800.3130807", "10.1145/2931002.2931011", "10.1145/2010324.1964990", "10.1145/3196493", "10.1177/016173468400600107", "10.1146/annurev-vision-082114-035800", "10.1137/S1052623494240456", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/JOSA.70.001458", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1016/0042-6989(94)90026-4", "10.1364/OE.19.020940", "10.1167/11.5.10", "10.3758/BF03202828", "10.1364/OE.23.032025", "10.1167/16.6.17", "10.1177/016173468400600107", "10.1146/annurev-vision-082114-035800", "10.1137/S1052623494240456", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/JOSA.70.001458", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1016/0042-6989(94)90026-4", "10.1364/OE.19.020940", "10.1167/11.5.10", "10.3758/BF03202828", "10.1364/OE.23.032025", "10.1167/16.6.17", "10.1177/016173468400600107", "10.1146/annurev-vision-082114-035800", "10.1137/S1052623494240456", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1364/JOSA.70.001458", "10.1364/OE.17.015716", "10.1167/10.8.22", "10.1016/0042-6989(94)90026-4", "10.1364/OE.19.020940", "10.1167/11.5.10", "10.3758/BF03202828", "10.1364/OE.23.032025", "10.1167/16.6.17"]}, "10.1109/TVCG.2019.2899229": {"doi": "10.1109/TVCG.2019.2899229", "author": ["Y. Itoh", "T. Langlotz", "D. Iwai", "K. Kiyokawa", "T. Amano"], "title": "Light Attenuation Display: Subtractive See-Through Near-Eye Display via Spatial Color Filtering", "year": "2019", "abstract": "We present a display for optical see-through near-eye displays based on light attenuation, a new paradigm that forms images by spatially subtracting colors of light. Existing optical see-through head-mounted displays (OST-HMDs) form virtual images in an additive manner-they optically combine the light from an embedded light source such as a microdisplay into the users' field of view (FoV). Instead, our light attenuation display filters the color of the real background light pixel-wise in the users' see-through view, resulting in an image as a spatial color filter. Our image formation is complementary to existing light-additive OST-HMDs. The core optical component in our system is a phase-only spatial light modulator (PSLM), a liquid crystal module that can control the phase of the light in each pixel. By combining PSLMs with polarization optics, our system realizes a spatially programmable color filter. In this paper, we introduce our optics design, evaluate the spatial color filter, consider applications including image rendering and FoV color control, and discuss the limitations of the current prototype.", "keywords": ["colour displays", "helmet mounted displays", "light attenuation", "light polarisation", "light sources", "liquid crystal displays", "microdisplays", "optical filters", "spatial filters", "spatial light modulators", "spatially subtracting colors", "embedded light source", "background light pixel-wise", "core optical component", "phase-only spatial light modulator", "polarization optics", "spatially programmable color filter", "image rendering", "FoV color control", "optical see-through head-mounted displays", "virtual imaging", "subtractive see-through near-eye display", "light-additive OST-HMD", "optical see-through near-eye displays", "microdisplay", "light attenuation display filters", "PSLM", "liquid crystal module", "Image color analysis", "Lighting", "Attenuation", "Liquid crystal displays", "Optical imaging", "Optical attenuators", "Optical polarization", "Light attenuation display", "phase modulation", "see-through display", "vision augmentation", "augmented reality", "Birefringence", "Color", "Computer Graphics", "Equipment Design", "Eye Movements", "Humans", "Optical Devices", "Optical Phenomena", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8827571", "IKEY:8998139", "IKEY:9199567"], "referencing": ["IKEY:1383039", "IKEY:4405577", "IKEY:6402574", "IKEY:8007218", "IKEY:1240696", "IKEY:7523376", "IKEY:6671761", "IKEY:1383039", "IKEY:4405577", "IKEY:6402574", "IKEY:8007218", "IKEY:1240696", "IKEY:7523376", "IKEY:6671761", "IKEY:1383039", "IKEY:4405577", "IKEY:6402574", "IKEY:8007218", "IKEY:1240696", "IKEY:7523376", "IKEY:6671761", "10.1145/2857051", "10.1145/2601097.2601140", "10.1145/3173574.3173964", "10.1145/3072959.3073624", "10.1145/2461912.2461937", "10.1145/3072959.3073590", "10.1145/1476589.1476686", "10.1145/3197517.3201299", "10.1145/2857051", "10.1145/2601097.2601140", "10.1145/3173574.3173964", "10.1145/3072959.3073624", "10.1145/2461912.2461937", "10.1145/3072959.3073590", "10.1145/1476589.1476686", "10.1145/3197517.3201299", "10.1145/2857051", "10.1145/2601097.2601140", "10.1145/3173574.3173964", "10.1145/3072959.3073624", "10.1145/2461912.2461937", "10.1145/3072959.3073590", "10.1145/1476589.1476686", "10.1145/3197517.3201299", "10.1364/OE.17.011426", "10.1016/j.optcom.2004.04.047", "10.1007/BFb0056282", "10.1111/cgf.13387", "10.1088/0022-3727/37/11/R01", "10.1364/OL.40.000581", "10.1085/jgp.7.2.235", "10.1117/12.722895", "10.1016/S0097-8493(01)00119-4", "10.1364/OE.24.002189", "10.1002/9783527648443.ch1", "10.1889/1.1985159", "10.1007/s007790200007", "10.1364/OE.23.011912", "10.1116/1.590933", "10.1111/j.1467-8659.2010.01660.x", "10.1364/AO.55.00A144", "10.1038/lsa.2014.94", "10.1364/OE.17.011426", "10.1016/j.optcom.2004.04.047", "10.1007/BFb0056282", "10.1111/cgf.13387", "10.1088/0022-3727/37/11/R01", "10.1364/OL.40.000581", "10.1085/jgp.7.2.235", "10.1117/12.722895", "10.1016/S0097-8493(01)00119-4", "10.1364/OE.24.002189", "10.1002/9783527648443.ch1", "10.1889/1.1985159", "10.1007/s007790200007", "10.1364/OE.23.011912", "10.1116/1.590933", "10.1111/j.1467-8659.2010.01660.x", "10.1364/AO.55.00A144", "10.1038/lsa.2014.94", "10.1364/OE.17.011426", "10.1016/j.optcom.2004.04.047", "10.1007/BFb0056282", "10.1111/cgf.13387", "10.1088/0022-3727/37/11/R01", "10.1364/OL.40.000581", "10.1085/jgp.7.2.235", "10.1117/12.722895", "10.1016/S0097-8493(01)00119-4", "10.1364/OE.24.002189", "10.1002/9783527648443.ch1", "10.1889/1.1985159", "10.1007/s007790200007", "10.1364/OE.23.011912", "10.1116/1.590933", "10.1111/j.1467-8659.2010.01660.x", "10.1364/AO.55.00A144", "10.1038/lsa.2014.94"]}, "10.1109/TVCG.2019.2899249": {"doi": "10.1109/TVCG.2019.2899249", "author": ["T. Hamasaki", "Y. Itoh"], "title": "Varifocal Occlusion for Optical See-Through Head-Mounted Displays using a Slide Occlusion Mask", "year": "2019", "abstract": "We propose a varifocal occlusion technique for optical see-through head-mounted displays (OST-HMDs). Occlusion in OST-HMDs is a powerful visual cue that enables depth perception in augmented reality (AR). Without occlusion, virtual objects rendered by an OST-HMD appear semi-transparent and less realistic. A common occlusion technique is to use spatial light modulators (SLMs) to block incoming light rays at each pixel on the SLM selectively. However, most of the existing methods create an occlusion mask only at a single, fixed depth-typically at infinity. With recent advances in varifocal OST-HMDs, such traditional fixed-focus occlusion causes a mismatch in depth between the occlusion mask plane and the virtual object to be occluded, leading to an uncomfortable user experience with blurred occlusion masks. In this paper, we thus propose an OST-HMD system with varifocal occlusion capability: we physically slide a transmissive liquid crystal display (LCD) to optically shift the occlusion plane along the optical path so that the mask appears sharp and aligns to a virtual image at a given depth. Our solution has several benefits over existing varifocal occlusion methods: it is computationally less demanding and, more importantly, it is optically consistent, i.e., when a user loses focus on the corresponding virtual image, the mask again gets blurred consistently as the virtual image does. In the experiment, we build a proof-of-concept varifocal occlusion system implemented with a custom retinal projection display and demonstrate that the system can shift the occlusion plane to depths ranging from 25 cm to infinity.", "keywords": ["augmented reality", "eye", "helmet mounted displays", "hidden feature removal", "liquid crystal displays", "rendering (computer graphics)", "spatial light modulators", "augmented reality", "LCD", "fixed depth", "single depth", "optical see-through head-mounted displays", "custom retinal projection display", "proof-of-concept varifocal occlusion system", "corresponding virtual image", "varifocal occlusion methods", "given depth", "optical path", "transmissive liquid crystal display", "varifocal occlusion capability", "OST-HMD system", "blurred occlusion masks", "occlusion mask plane", "traditional fixed-focus occlusion", "varifocal OST-HMDs", "incoming light rays", "spatial light modulators", "common occlusion technique", "virtual object", "depth perception", "powerful visual cue", "varifocal occlusion technique", "slide occlusion mask", "size 25.0 cm", "Optical imaging", "Liquid crystal displays", "Cameras", "Adaptive optics", "Lenses", "Optical distortion", "Glass", "Occlusion", "varifocal", "see-through display", "augmented reality"], "referenced_by": ["IKEY:8827571", "IKEY:8998139", "IKEY:9199567", "IKEY:9284767", "IKEY:9284699", "10.1364/OL.393194", "10.1002/9781119282211.ch8", "10.1364/OE.401778", "10.1002/sdtp.13932"], "referencing": ["IKEY:1240696", "IKEY:1383039", "IKEY:6402574", "IKEY:6671761", "IKEY:8007218", "IKEY:1240688", "IKEY:5643530", "IKEY:7226865", "IKEY:8052554", "IKEY:6798846", "IKEY:7836529", "IKEY:1240696", "IKEY:1383039", "IKEY:6402574", "IKEY:6671761", "IKEY:8007218", "IKEY:1240688", "IKEY:5643530", "IKEY:7226865", "IKEY:8052554", "IKEY:6798846", "IKEY:7836529", "IKEY:1240696", "IKEY:1383039", "IKEY:6402574", "IKEY:6671761", "IKEY:8007218", "IKEY:1240688", "IKEY:5643530", "IKEY:7226865", "IKEY:8052554", "IKEY:6798846", "IKEY:7836529", "10.1145/2614066.2614080", "10.1145/315762.315813", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2766922", "10.1145/3174910.3174911", "10.1145/3130800.3130889", "10.1145/2614066.2614080", "10.1145/315762.315813", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2766922", "10.1145/3174910.3174911", "10.1145/3130800.3130889", "10.1145/2614066.2614080", "10.1145/315762.315813", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2766922", "10.1145/3174910.3174911", "10.1145/3130800.3130889", "10.1016/S0097-8493(01)00119-4", "10.1117/12.2015937", "10.1364/AO.55.00A144", "10.1007/BFb0056282", "10.1364/OE.25.030539", "10.1117/12.2314664", "10.1889/1.1833916", "10.1073/pnas.1617251114", "10.1117/12.237010", "10.1364/OE.22.006526", "10.1117/12.209726", "10.1364/OE.18.015223", "10.1007/978-3-540-79567-4_140", "10.1016/S0097-8493(01)00119-4", "10.1117/12.2015937", "10.1364/AO.55.00A144", "10.1007/BFb0056282", "10.1364/OE.25.030539", "10.1117/12.2314664", "10.1889/1.1833916", "10.1073/pnas.1617251114", "10.1117/12.237010", "10.1364/OE.22.006526", "10.1117/12.209726", "10.1364/OE.18.015223", "10.1007/978-3-540-79567-4_140", "10.1016/S0097-8493(01)00119-4", "10.1117/12.2015937", "10.1364/AO.55.00A144", "10.1007/BFb0056282", "10.1364/OE.25.030539", "10.1117/12.2314664", "10.1889/1.1833916", "10.1073/pnas.1617251114", "10.1117/12.237010", "10.1364/OE.22.006526", "10.1117/12.209726", "10.1364/OE.18.015223", "10.1007/978-3-540-79567-4_140"]}, "10.1109/TVCG.2019.2899233": {"doi": "10.1109/TVCG.2019.2899233", "author": ["A. Blate", "M. Whitton", "M. Singh", "G. Welch", "A. State", "T. Whitted", "H. Fuchs"], "title": "Implementation and Evaluation of a 50 kHz, $28\\mu\\mathrm{s}$ Motion-to-Pose Latency Head Tracking Instrument", "year": "2019", "abstract": "This paper presents the implementation and evaluation of a 50,000-pose-sample-per-second, 6-degree-of-freedom optical head tracking instrument with motion-to-pose latency of $28\\mu\\mathrm{s}$ and dynamic precision of 1\u20132 arcminutes. The instrument uses high-intensity infrared emitters and two duo-lateral photodiode-based optical sensors to triangulate pose. This instrument serves two purposes: it is the first step towards the requisite head tracking component in sub-$100\\mu\\mathrm{s}$ motion-to-photon latency optical see-through augmented reality (OST AR) head-mounted display (HMD) systems; and it enables new avenues of research into human visual perception \u2013 including measuring the thresholds for perceptible real-virtual displacement during head rotation and other human research requiring high-sample-rate motion tracking. The instrument's tracking volume is limited to about $120\\times 120\\times 250$ but allows for the full range of natural head rotation and is sufficient for research involving seated users. We discuss how the instrument's tracking volume is scalable in multiple ways and some of the trade-offs involved therein. Finally, we introduce a novel laser-pointer-based measurement technique for assessing the instrument's tracking latency and repeatability. We show that the instrument's motion-to-pose latency is $28\\mu\\mathrm{s}$ and that it is repeatable within 1\u20132 arcminutes at mean rotational velocities (yaw) in excess of 500\u00b0/sec.", "keywords": ["Instruments", "Target tracking", "Optical sensors", "Photodiodes", "Adaptive optics", "Tracking", "head tracker", "lateral-effect photodiodes", "augmented reality", "low-latency augmented reality", "dynamic tracking error", "perception", "motion tracking", "Computer Graphics", "Equipment Design", "Head Movements", "Humans", "Imaging, Three-Dimensional", "Smart Glasses", "Time Factors", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:9089584"], "referencing": ["IKEY:7383304", "IKEY:7829409", "IKEY:6797199", "IKEY:1191132", "IKEY:4657377", "IKEY:7523411", "IKEY:1240685", "IKEY:7836493", "IKEY:4538850", "IKEY:6777458", "IKEY:5643548", "IKEY:341085", "IKEY:601246", "IKEY:1451379", "IKEY:1697831", "IKEY:5384743", "IKEY:7383304", "IKEY:7829409", "IKEY:6797199", "IKEY:1191132", "IKEY:4657377", "IKEY:7523411", "IKEY:1240685", "IKEY:7836493", "IKEY:4538850", "IKEY:6777458", "IKEY:5643548", "IKEY:341085", "IKEY:601246", "IKEY:1451379", "IKEY:1697831", "IKEY:5384743", "IKEY:7383304", "IKEY:7829409", "IKEY:6797199", "IKEY:1191132", "IKEY:4657377", "IKEY:7523411", "IKEY:1240685", "IKEY:7836493", "IKEY:4538850", "IKEY:6777458", "IKEY:5643548", "IKEY:341085", "IKEY:601246", "IKEY:1451379", "IKEY:1697831", "IKEY:5384743", "10.1145/3023368.3023379", "10.1145/323663.323664", "10.1145/91394.91447", "10.1145/3145690.3145727", "10.1145/3023368.3023379", "10.1145/323663.323664", "10.1145/91394.91447", "10.1145/3145690.3145727", "10.1145/3023368.3023379", "10.1145/323663.323664", "10.1145/91394.91447", "10.1145/3145690.3145727", "10.1118/1.4823757", "10.1117/12.653448", "10.3389/fict.2016.00034", "10.1016/0004-3702(95)00022-4", "10.1146/annurev.ps.38.020187.000245", "10.1118/1.4823757", "10.1117/12.653448", "10.3389/fict.2016.00034", "10.1016/0004-3702(95)00022-4", "10.1146/annurev.ps.38.020187.000245", "10.1118/1.4823757", "10.1117/12.653448", "10.3389/fict.2016.00034", "10.1016/0004-3702(95)00022-4", "10.1146/annurev.ps.38.020187.000245"]}, "10.1109/TVCG.2019.2898820": {"doi": "10.1109/TVCG.2019.2898820", "author": ["Y. Ujitoko", "Y. Ban", "K. Hirota"], "title": "Modulating Fine Roughness Perception of Vibrotactile Textured Surface using Pseudo-haptic Effect", "year": "2019", "abstract": "Playing back vibrotactile signals through actuators is commonly used to simulate tactile feelings of virtual textured surfaces. However, there is often a small mismatch between the simulated tactile feelings and intended tactile feelings by tactile designers. Thus, a method of modulating the vibrotactile perception is required. We focus on fine roughness perception and we propose a method using a pseudo-haptic effect to modulate fine roughness perception of vibrotactile texture. Specifically, we visually modify the pointer's position on the screen slightly, which indicates the touch position on textured surfaces. We hypothesized that if users receive vibrational feedback watching the pointer visually oscillating back/forth and left/right, users would believe the vibrotactile surfaces more uneven. We also hypothesized that as the size of visual oscillation is getting larger, the amount of modification of roughness perception of vibrotactile surfaces would be larger. We conducted user studies to test the hypotheses. Results of first user study suggested that users felt vibrotactile texture with our method rougher than they did without our method at a high probability. Results of second user study suggested that users felt different roughness for vibrational texture in response to the size of visual oscillation. These results confirmed our hypotheses and they suggested that our method was effective. Also, the same effect could potentially be applied to the visual movement of virtual hands or fingertips when users are interacting with virtual surfaces using their hands.", "keywords": ["haptic interfaces", "mobile computing", "surface roughness", "surface texture", "touch (physiological)", "vibrational signal processing", "fine roughness perception", "mobile devices", "simulated tactile feelings", "virtual textured surfaces", "vibrotactile signals", "vibrotactile textured surface", "virtual surfaces", "vibrational texture", "pseudohaptic effect", "vibrotactile perception", "Rough surfaces", "Surface roughness", "Visualization", "Surface texture", "Haptic interfaces", "Vibrations", "Modulation", "Haptic Technologies", "Cross-modal", "Pseudo-haptics", "Texture roughness", "Adult", "Female", "Fingers", "Humans", "Male", "Signal Processing, Computer-Assisted", "Surface Properties", "Touch", "User-Computer Interface", "Young Adult"], "referenced_by": ["IKEY:8816100", "IKEY:9034143"], "referencing": ["IKEY:6343880", "IKEY:6994832", "IKEY:6183793", "IKEY:6818579", "IKEY:7539397", "IKEY:6183819", "IKEY:6548380", "IKEY:5406524", "IKEY:7223322", "IKEY:6777424", "IKEY:6797406", "IKEY:4151885", "IKEY:5722959", "IKEY:7737070", "IKEY:4810877", "IKEY:6343880", "IKEY:6994832", "IKEY:6183793", "IKEY:6818579", "IKEY:7539397", "IKEY:6183819", "IKEY:6548380", "IKEY:5406524", "IKEY:7223322", "IKEY:6777424", "IKEY:6797406", "IKEY:4151885", "IKEY:5722959", "IKEY:7737070", "IKEY:4810877", "IKEY:6343880", "IKEY:6994832", "IKEY:6183793", "IKEY:6818579", "IKEY:7539397", "IKEY:6183819", "IKEY:6548380", "IKEY:5406524", "IKEY:7223322", "IKEY:6777424", "IKEY:6797406", "IKEY:4151885", "IKEY:5722959", "IKEY:7737070", "IKEY:4810877", "10.1145/2506206.2501599", "10.1145/2071423.2071448", "10.1145/985692.985723", "10.1145/3025453.3025812", "10.1145/1501750.1501856", "10.1145/2506206.2501599", "10.1145/2071423.2071448", "10.1145/985692.985723", "10.1145/3025453.3025812", "10.1145/1501750.1501856", "10.1145/2506206.2501599", "10.1145/2071423.2071448", "10.1145/985692.985723", "10.1145/3025453.3025812", "10.1145/1501750.1501856", "10.1007/978-3-642-31401-8_3", "10.1146/annurev-control-060117-105043", "10.3758/BF03202657", "10.1068/p3044", "10.1037/0096-1523.7.4.902", "10.1080/00140139408964917", "10.1121/1.426852", "10.1121/1.2715669", "10.1126/science.143.3606.594", "10.1007/978-3-642-31401-8_3", "10.1146/annurev-control-060117-105043", "10.3758/BF03202657", "10.1068/p3044", "10.1037/0096-1523.7.4.902", "10.1080/00140139408964917", "10.1121/1.426852", "10.1121/1.2715669", "10.1126/science.143.3606.594", "10.1007/978-3-642-31401-8_3", "10.1146/annurev-control-060117-105043", "10.3758/BF03202657", "10.1068/p3044", "10.1037/0096-1523.7.4.902", "10.1080/00140139408964917", "10.1121/1.426852", "10.1121/1.2715669", "10.1126/science.143.3606.594"]}, "10.1109/TVCG.2019.2898736": {"doi": "10.1109/TVCG.2019.2898736", "author": ["W. Xu", "H. -N. Liang", "Y. Zhao", "T. Zhang", "D. Yu", "D. Monteiro"], "title": "RingText: Dwell-free and hands-free Text Entry for Mobile Head-Mounted Displays using Head Motions", "year": "2019", "abstract": "In this paper, we present a case for text entry using a circular keyboard layout for mobile head-mounted displays (HMDs) that is dwell-free and does not require users to hold a dedicated input device for letter selection. To support the case, we have implemented RingText whose design is based on a circular layout with two concentric circles. The outer circle is subdivided into regions containing letters. Selection is made by using a virtual cursor controlled by the user's head movements-entering a letter region triggers a selection and moving back into the inner circle resets the selection. The design of RingText follows an iterative process, where we initially conduct one first study to investigate the optimal number of letters per region, inner circle size, and alphabet starting location. We then optimize its design by selecting the most suitable features from the first study: one letter per region, narrowing the trigger area to lower error rates, and creating candidate regions that incorporate two suggested words to appear next to the current letter region (close to the cursor) using a dynamic (rather than fixed) approach. Our second study compares text entry performance of RingText with four other hands-free techniques and the results show that RingText outperforms them. Finally, we run a third study lasting four consecutive days with 10 participants (5 novice users and 5 expert users) doing two daily sessions and the results show that RingText is quite efficient and yields a low error rate. At the end of the eighth session, the novice users can achieve a text entry speed of 11.30 WPM after 60 minutes of training while the expert (more experienced) users can reach an average text entry speed of 13.24 WPM after 90 minutes of training.", "keywords": ["feature selection", "helmet mounted displays", "keyboards", "mobile computing", "text analysis", "virtual reality", "RingText", "hands-free text entry", "mobile head-mounted displays", "head motions", "dwell-free text entry", "mobile HMD", "virtual cursor", "feature selection", "virtual reality", "Layout", "Keyboards", "Error analysis", "Training", "Head-mounted displays", "Resists", "Dynamics", "Virtual Reality", "text entry", "circular keyboard layout", "mobile head-worn/mounted displays", "dwell-free input", "Adolescent", "Adult", "Female", "Head Movements", "Humans", "Male", "Smart Glasses", "Text Messaging", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8723303", "IKEY:9089533", "IKEY:9284718"], "referencing": ["IKEY:8446059", "IKEY:8456570", "IKEY:7460039", "IKEY:7889285", "IKEY:8446059", "IKEY:8456570", "IKEY:7460039", "IKEY:7889285", "IKEY:8446059", "IKEY:8456570", "IKEY:7460039", "IKEY:7889285", "10.1145/2984751.2984783", "10.1145/3025453.3025964", "10.1145/1518701.1518758", "10.1145/2984511.2984576", "10.1145/3090083", "10.1145/2807442.2807504", "10.1145/507072.507076", "10.1145/2168556.2168605", "10.1145/1344471.1344475", "10.1145/1344471.1344477", "10.1145/2858036.2858335", "10.1145/2525194.2525288", "10.1145/2724728", "10.1145/968363.968389", "10.1145/288392.288611", "10.1145/1124772.1124842", "10.1145/1344471.1344483", "10.1145/3173574.3173755", "10.1145/3025453.3025454", "10.1145/169059.169426", "10.1145/3173574.3173655", "10.1145/2642918.2647354", "10.1145/2674396.2674443", "10.1145/3173574.3174221", "10.1145/765968.765971", "10.1145/642611.642632", "10.1145/2254556.2254585", "10.1145/502716.502753", "10.1145/1240624.1240727", "10.1145/2984751.2984783", "10.1145/3025453.3025964", "10.1145/1518701.1518758", "10.1145/2984511.2984576", "10.1145/3090083", "10.1145/2807442.2807504", "10.1145/507072.507076", "10.1145/2168556.2168605", "10.1145/1344471.1344475", "10.1145/1344471.1344477", "10.1145/2858036.2858335", "10.1145/2525194.2525288", "10.1145/2724728", "10.1145/968363.968389", "10.1145/288392.288611", "10.1145/1124772.1124842", "10.1145/1344471.1344483", "10.1145/3173574.3173755", "10.1145/3025453.3025454", "10.1145/169059.169426", "10.1145/3173574.3173655", "10.1145/2642918.2647354", "10.1145/2674396.2674443", "10.1145/3173574.3174221", "10.1145/765968.765971", "10.1145/642611.642632", "10.1145/2254556.2254585", "10.1145/502716.502753", "10.1145/1240624.1240727", "10.1145/2984751.2984783", "10.1145/3025453.3025964", "10.1145/1518701.1518758", "10.1145/2984511.2984576", "10.1145/3090083", "10.1145/2807442.2807504", "10.1145/507072.507076", "10.1145/2168556.2168605", "10.1145/1344471.1344475", "10.1145/1344471.1344477", "10.1145/2858036.2858335", "10.1145/2525194.2525288", "10.1145/2724728", "10.1145/968363.968389", "10.1145/288392.288611", "10.1145/1124772.1124842", "10.1145/1344471.1344483", "10.1145/3173574.3173755", "10.1145/3025453.3025454", "10.1145/169059.169426", "10.1145/3173574.3173655", "10.1145/2642918.2647354", "10.1145/2674396.2674443", "10.1145/3173574.3174221", "10.1145/765968.765971", "10.1145/642611.642632", "10.1145/2254556.2254585", "10.1145/502716.502753", "10.1145/1240624.1240727", "10.1007/978-3-319-58750-9_26", "10.1037/0096-3445.121.3.262", "10.1177/154193120204602611", "10.1108/JAT-12-2013-0037", "10.1207/s15327108ijap0303_3", "10.1207/s15327051hci0701_3", "10.1007/978-1-84882-352-5_11", "10.1007/978-3-319-58750-9_26", "10.1037/0096-3445.121.3.262", "10.1177/154193120204602611", "10.1108/JAT-12-2013-0037", "10.1207/s15327108ijap0303_3", "10.1207/s15327051hci0701_3", "10.1007/978-1-84882-352-5_11", "10.1007/978-3-319-58750-9_26", "10.1037/0096-3445.121.3.262", "10.1177/154193120204602611", "10.1108/JAT-12-2013-0037", "10.1207/s15327108ijap0303_3", "10.1207/s15327051hci0701_3", "10.1007/978-1-84882-352-5_11"]}, "10.1109/TVCG.2019.2899187": {"doi": "10.1109/TVCG.2019.2899187", "author": ["Z. Hu", "C. Zhang", "S. Li", "G. Wang", "D. Manocha"], "title": "SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction", "year": "2019", "abstract": "We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.", "keywords": ["eye", "gaze tracking", "helmet mounted displays", "human computer interaction", "rendering (computer graphics)", "statistical analysis", "virtual reality", "SGaze", "data-driven eye-head coordination model", "head rotation angular velocities", "software-based realtime gaze predictor", "head movement", "eye movement", "realtime gaze position prediction", "immersive HMD", "statistical analysis", "gaze-contingent rendering", "Head", "Solid modeling", "Predictive models", "Visualization", "Computational modeling", "Rendering (computer graphics)", "Navigation", "Eye-head coordination", "gaze prediction", "Pearsons correlation coefficient", "eye tracking", "saliency", "Adolescent", "Adult", "Computer Graphics", "Databases, Factual", "Eye Movements", "Female", "Head Movements", "Humans", "Imaging, Three-Dimensional", "Male", "User-Computer Interface", "Video Recording", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8998133", "IKEY:8998375", "IKEY:9090417", "IKEY:9089439", "IKEY:9150219"], "referencing": ["IKEY:7293665", "IKEY:6180177", "IKEY:6247710", "IKEY:6871397", "IKEY:7457641", "IKEY:730558", "IKEY:8237775", "IKEY:1246946", "IKEY:8269807", "IKEY:7164324", "IKEY:8578657", "IKEY:7293665", "IKEY:6180177", "IKEY:6247710", "IKEY:6871397", "IKEY:7457641", "IKEY:730558", "IKEY:8237775", "IKEY:1246946", "IKEY:8269807", "IKEY:7164324", "IKEY:8578657", "IKEY:7293665", "IKEY:6180177", "IKEY:6247710", "IKEY:6871397", "IKEY:7457641", "IKEY:730558", "IKEY:8237775", "IKEY:1246946", "IKEY:8269807", "IKEY:7164324", "IKEY:8578657", "10.1145/2366145.2366183", "10.1145/2980179.2980246", "10.1145/3083187.3083218", "10.1145/2824840.2824863", "10.1145/2931002.2931011", "10.1145/332040.332443", "10.1145/2366145.2366183", "10.1145/2980179.2980246", "10.1145/3083187.3083218", "10.1145/2824840.2824863", "10.1145/2931002.2931011", "10.1145/332040.332443", "10.1145/2366145.2366183", "10.1145/2980179.2980246", "10.1145/3083187.3083218", "10.1145/2824840.2824863", "10.1145/2931002.2931011", "10.1145/332040.332443", "10.1007/BF00237188", "10.1016/j.physa.2003.09.011", "10.1016/j.visres.2014.06.016", "10.1016/j.cag.2018.04.002", "10.1080/09548980701671094", "10.1371/journal.pone.0121035", "10.1007/s00221-008-1504-8", "10.1007/BF00247595", "10.1113/jphysiol.1986.sp016043", "10.1016/j.visres.2015.10.001", "10.1371/journal.pone.0092284", "10.1007/BF00230842", "10.1007/s002210050715", "10.1007/BF00238893", "10.1007/978-1-4899-5379-7", "10.1111/j.1749-6632.1981.tb30899.x", "10.1007/BF00237188", "10.1016/j.physa.2003.09.011", "10.1016/j.visres.2014.06.016", "10.1016/j.cag.2018.04.002", "10.1080/09548980701671094", "10.1371/journal.pone.0121035", "10.1007/s00221-008-1504-8", "10.1007/BF00247595", "10.1113/jphysiol.1986.sp016043", "10.1016/j.visres.2015.10.001", "10.1371/journal.pone.0092284", "10.1007/BF00230842", "10.1007/s002210050715", "10.1007/BF00238893", "10.1007/978-1-4899-5379-7", "10.1111/j.1749-6632.1981.tb30899.x", "10.1007/BF00237188", "10.1016/j.physa.2003.09.011", "10.1016/j.visres.2014.06.016", "10.1016/j.cag.2018.04.002", "10.1080/09548980701671094", "10.1371/journal.pone.0121035", "10.1007/s00221-008-1504-8", "10.1007/BF00247595", "10.1113/jphysiol.1986.sp016043", "10.1016/j.visres.2015.10.001", "10.1371/journal.pone.0092284", "10.1007/BF00230842", "10.1007/s002210050715", "10.1007/BF00238893", "10.1007/978-1-4899-5379-7", "10.1111/j.1749-6632.1981.tb30899.x"]}, "10.1109/TVCG.2019.2898763": {"doi": "10.1109/TVCG.2019.2898763", "author": ["S. Mirhosseini", "I. Gutenko", "S. Ojal", "J. Marino", "A. Kaufman"], "title": "Immersive Virtual Colonoscopy", "year": "2019", "abstract": "Virtual colonoscopy (VC) is a non-invasive screening tool for colorectal polyps which employs volume visualization of a colon model reconstructed from a CT scan of the patient's abdomen. We present an immersive analytics system for VC which enhances and improves the traditional desktop VC through the use of VR technologies. Our system, using a head-mounted display (HMD), includes all of the standard VC features, such as the volume rendered endoluminal fly-through, measurement tool, bookmark modes, electronic biopsy, and slice views. The use of VR immersion, stereo, and wider field of view and field of regard has a positive effect on polyp search and analysis tasks in our immersive VC system, a volumetric-based immersive analytics application. Navigation includes enhanced automatic speed and direction controls, based on the user's head orientation, in conjunction with physical navigation for exploration of local proximity. In order to accommodate the resolution and frame rate requirements for HMDs, new rendering techniques have been developed, including mesh-assisted volume raycasting and a novel lighting paradigm. Feedback and further suggestions from expert radiologists show the promise of our system for immersive analysis for VC and encourage new avenues for exploring the use of VR in visualization systems for medical diagnosis.", "keywords": ["biological organs", "cancer", "computerised tomography", "data visualisation", "helmet mounted displays", "image reconstruction", "medical image processing", "physiological models", "rendering (computer graphics)", "virtual reality", "head-mounted display", "electronic biopsy", "volumetric-based immersive analytics application", "mesh-assisted volume raycasting", "noninvasive screening tool", "colorectal polyps", "volume visualization", "colon model", "CT scan", "immersive virtual colonoscopy system", "volume rendered endoluminal fly-through", "abdomen", "Colon", "Rendering (computer graphics)", "Tools", "Computed tomography", "Biopsy", "Task analysis", "Haptic interfaces", "Immersive Environments", "Immersive Analytics", "Interaction Design", "Volume Rendering", "Biomedical Visualization", "Colon Cancer Screening", "Medical Diagnosis", "Abdomen", "Colonography, Computed Tomographic", "Computer Graphics", "Humans", "Imaging, Three-Dimensional", "Smart Glasses", "Virtual Reality"], "referenced_by": [], "referencing": ["10.1145/1166253.1166257", "10.1145/1357054.1357300", "10.1145/258734.258750", "10.1145/1643928.1643955", "10.1145/1643928.1643947", "10.1145/300523.300542", "10.1145/566654.566612", "10.1145/1978942.1979140", "10.1145/1166253.1166257", "10.1145/1357054.1357300", "10.1145/258734.258750", "10.1145/1643928.1643955", "10.1145/1643928.1643947", "10.1145/300523.300542", "10.1145/566654.566612", "10.1145/1978942.1979140", "10.1145/1166253.1166257", "10.1145/1357054.1357300", "10.1145/258734.258750", "10.1145/1643928.1643955", "10.1145/1643928.1643947", "10.1145/300523.300542", "10.1145/566654.566612", "10.1145/1978942.1979140", "10.1016/j.cag.2012.12.003", "10.1016/j.cgh.2018.08.021", "10.1016/j.jacr.2008.01.014", "10.1007/978-3-642-40483-2_19", "10.1148/rg.e42", "10.1007/s00330-008-0969-y", "10.1111/j.1572-0241.2007.01429.x", "10.1016/j.jacr.2008.05.003", "10.1007/BFb0056277", "10.1080/00222895.1987.10735426", "10.6028/jres.112.019", "10.1136/gutjnl-2018-317058", "10.1117/12.709665", "10.1111/j.1467-8659.2012.03115.x", "10.1016/j.ijhcs.2008.05.001", "10.1117/12.878295", "10.7763/IJCTE.2016.V8.1090", "10.1056/NEJMoa031618", "10.1016/j.rcl.2012.09.005", "10.1007/s10278-012-9459-5", "10.1007/s11548-010-0519-3", "10.1016/j.ejmp.2014.06.026", "10.2214/ajr.181.1.1810037", "10.1007/3DRes.01(2012)3", "10.1186/s12938-018-0433-4", "10.1097/RCT.0b013e31817710d5", "10.1016/j.cag.2012.12.003", "10.1016/j.cgh.2018.08.021", "10.1016/j.jacr.2008.01.014", "10.1007/978-3-642-40483-2_19", "10.1148/rg.e42", "10.1007/s00330-008-0969-y", "10.1111/j.1572-0241.2007.01429.x", "10.1016/j.jacr.2008.05.003", "10.1007/BFb0056277", "10.1080/00222895.1987.10735426", "10.6028/jres.112.019", "10.1136/gutjnl-2018-317058", "10.1117/12.709665", "10.1111/j.1467-8659.2012.03115.x", "10.1016/j.ijhcs.2008.05.001", "10.1117/12.878295", "10.7763/IJCTE.2016.V8.1090", "10.1056/NEJMoa031618", "10.1016/j.rcl.2012.09.005", "10.1007/s10278-012-9459-5", "10.1007/s11548-010-0519-3", "10.1016/j.ejmp.2014.06.026", "10.2214/ajr.181.1.1810037", "10.1007/3DRes.01(2012)3", "10.1186/s12938-018-0433-4", "10.1097/RCT.0b013e31817710d5", "10.1016/j.cag.2012.12.003", "10.1016/j.cgh.2018.08.021", "10.1016/j.jacr.2008.01.014", "10.1007/978-3-642-40483-2_19", "10.1148/rg.e42", "10.1007/s00330-008-0969-y", "10.1111/j.1572-0241.2007.01429.x", "10.1016/j.jacr.2008.05.003", "10.1007/BFb0056277", "10.1080/00222895.1987.10735426", "10.6028/jres.112.019", "10.1136/gutjnl-2018-317058", "10.1117/12.709665", "10.1111/j.1467-8659.2012.03115.x", "10.1016/j.ijhcs.2008.05.001", "10.1117/12.878295", "10.7763/IJCTE.2016.V8.1090", "10.1056/NEJMoa031618", "10.1016/j.rcl.2012.09.005", "10.1007/s10278-012-9459-5", "10.1007/s11548-010-0519-3", "10.1016/j.ejmp.2014.06.026", "10.2214/ajr.181.1.1810037", "10.1007/3DRes.01(2012)3", "10.1186/s12938-018-0433-4", "10.1097/RCT.0b013e31817710d5"]}, "10.1109/TVCG.2019.2898764": {"doi": "10.1109/TVCG.2019.2898764", "author": ["E. R. Bachmann", "E. Hodgson", "C. Hoffbauer", "J. Messinger"], "title": "Multi-User Redirected Walking and Resetting Using Artificial Potential Fields", "year": "2019", "abstract": "Head-mounted displays (HMDs) and large area position tracking systems can enable users to navigate virtual worlds through natural walking. Redirected walking (RDW) imperceptibly steers immersed users away from physical world obstacles allowing them to explore unbounded virtual worlds while walking in limited physical space. In cases of imminent collisions, resetting techniques can reorient them into open space. This work introduces categorically new RDW and resetting algorithms based on the use of artificial potential fields that \u201cpush\u201d users away from obstacles and other users. Data from human subject experiments indicate that these methods reduce potential single-user resets by 66% and increase the average distance between resets by 86% compared to previous techniques. A live multi-user study demonstrates the viability of the algorithm with up to 3 concurrent users, and simulation results indicate that the algorithm scales efficiently up to at least 8 users and is effective with larger groups.", "keywords": ["collision avoidance", "gait analysis", "helmet mounted displays", "virtual reality", "multiuser redirected walking", "artificial potential fields", "large area position tracking systems", "natural walking", "RDW", "resetting algorithms", "head-mounted display", "HMDs", "Legged locomotion", "Navigation", "Space vehicles", "Tracking", "Force", "Orbits", "Virtual environment", "redirected walking", "resetting", "artificial potential field", "collision avoidance", "Algorithms", "Computer Graphics", "Humans", "Orientation", "Smart Glasses", "Virtual Reality", "Walking"], "referenced_by": ["IKEY:9019652", "IKEY:8998141", "IKEY:8998297", "IKEY:9089554", "IKEY:9090595", "IKEY:9089501", "IKEY:9089561", "IKEY:9089532", "IKEY:9089569"], "referencing": ["IKEY:7892235", "IKEY:6549377", "IKEY:6798852", "IKEY:6479192", "IKEY:6777456", "IKEY:1087247", "IKEY:1087068", "IKEY:6200791", "IKEY:8255772", "IKEY:5444816", "IKEY:4480761", "IKEY:4663065", "IKEY:5072212", "IKEY:5759455", "IKEY:6180877", "IKEY:6520845", "IKEY:7892235", "IKEY:6549377", "IKEY:6798852", "IKEY:6479192", "IKEY:6777456", "IKEY:1087247", "IKEY:1087068", "IKEY:6200791", "IKEY:8255772", "IKEY:5444816", "IKEY:4480761", "IKEY:4663065", "IKEY:5072212", "IKEY:5759455", "IKEY:6180877", "IKEY:6520845", "IKEY:7892235", "IKEY:6549377", "IKEY:6798852", "IKEY:6479192", "IKEY:6777456", "IKEY:1087247", "IKEY:1087068", "IKEY:6200791", "IKEY:8255772", "IKEY:5444816", "IKEY:4480761", "IKEY:4663065", "IKEY:5072212", "IKEY:5759455", "IKEY:6180877", "IKEY:6520845", "10.1145/1450579.1450612", "10.1145/2043603.2043604", "10.1145/3173574.3173815", "10.1145/3027063.3053180", "10.1145/1450579.1450611", "10.1145/2782782.2792496", "10.1145/1272582.1272590", "10.1145/1450579.1450612", "10.1145/2043603.2043604", "10.1145/3173574.3173815", "10.1145/3027063.3053180", "10.1145/1450579.1450611", "10.1145/2782782.2792496", "10.1145/1272582.1272590", "10.1145/1450579.1450612", "10.1145/2043603.2043604", "10.1145/3173574.3173815", "10.1145/3027063.3053180", "10.1145/1450579.1450611", "10.1145/2782782.2792496", "10.1145/1272582.1272590", "10.3758/BF03192976", "10.3758/BF03192976", "10.3758/BF03192976"]}, "10.1109/TVCG.2019.2899228": {"doi": "10.1109/TVCG.2019.2899228", "author": ["L. B\u00f6lling", "N. Stein", "F. Steinicke", "M. Lappe"], "title": "Shrinking Circles: Adaptation to Increased Curvature Gain in Redirected Walking", "year": "2019", "abstract": "Real walking is the most natural way to locomote in virtual reality (VR), but a confined physical walking space limits its applicability. Redirected walking (RDW) is a collection of techniques to solve this problem. One of these techniques aims to imperceptibly rotate the user's view of the virtual scene in order to steer her along a confined path whilst giving the impression of walking in a straight line in a large virtual space. Measurement of perceptual thresholds for the detection of such a modified curvature gain have indicated a radius that is still larger than most room sizes. Since the brain is an adaptive system and thresholds usually depend on previous stimulations, we tested if prolonged exposure to an immersive virtual environment (IVE) with increased curvature gain produces adaptation to that gain and modifies thresholds such that, over time, larger curvature gains can be applied for RDW. Therefore, participants first completed a measurement of their perceptual threshold for curvature gain. In a second session, the same participants were exposed to an IVE with a constant curvature gain in which they walked between two targets for about 20 minutes. Afterwards, their perceptual thresholds were measured again. The results show that the psychometric curves shifted after the exposure session and perceptual thresholds for increased curvature gain further increased. The increase of the detection threshold suggests that participants adapt to the manipulation and stronger curvature gains can be applied in RDW, and therefore improves its applicability in such situations.", "keywords": ["gait analysis", "psychometric testing", "virtual reality", "redirected walking", "virtual reality", "RDW", "virtual scene", "virtual space", "immersive virtual environment", "curvature gains", "shrinking circles", "physical walking space", "IVE", "psychometric curves", "Legged locomotion", "Resists", "Tracking", "Gain measurement", "Visualization", "Glass", "Atmospheric measurements", "Virtual reality", "locomotion", "redirected walking", "psychophysical experiments", "Adaptation, Psychological", "Adult", "Algorithms", "Computer Graphics", "Female", "Humans", "Male", "Psychophysics", "Surveys and Questionnaires", "Virtual Reality", "Walking", "Young Adult"], "referenced_by": ["IKEY:9019652", "IKEY:8998133", "IKEY:8998141", "IKEY:9089480", "IKEY:9090671", "IKEY:9089561"], "referencing": ["IKEY:7460032", "IKEY:7010955", "IKEY:6165134", "IKEY:6479192", "IKEY:6777456", "IKEY:7833190", "IKEY:6200791", "IKEY:8255772", "IKEY:5072212", "IKEY:5759455", "IKEY:6165136", "IKEY:6787863", "IKEY:7460032", "IKEY:7010955", "IKEY:6165134", "IKEY:6479192", "IKEY:6777456", "IKEY:7833190", "IKEY:6200791", "IKEY:8255772", "IKEY:5072212", "IKEY:5759455", "IKEY:6165136", "IKEY:6787863", "IKEY:7460032", "IKEY:7010955", "IKEY:6165134", "IKEY:6479192", "IKEY:6777456", "IKEY:7833190", "IKEY:6200791", "IKEY:8255772", "IKEY:5072212", "IKEY:5759455", "IKEY:6165136", "IKEY:6787863", "10.1145/2043603.2043604", "10.1145/3234253.3234291", "10.1145/1643928.1643940", "10.1145/311535.311589", "10.1145/2043603.2043604", "10.1145/3234253.3234291", "10.1145/1643928.1643940", "10.1145/311535.311589", "10.1145/2043603.2043604", "10.1145/3234253.3234291", "10.1145/1643928.1643940", "10.1145/311535.311589", "10.1016/j.displa.2012.10.007", "10.1177/0956797610372635", "10.1016/j.cub.2007.10.059", "10.1167/12.3.4", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-08234-9_253-1", "10.1152/jn.00129.2004", "10.1017/S0140525X01000115", "10.1016/j.neubiorev.2004.12.004", "10.1016/S0960-9822(07)00492-7", "10.1167/11.3.15", "10.1016/j.visres.2016.02.002", "10.1007/978-1-4419-8432-6", "10.3758/BF03211756", "10.1016/j.displa.2012.10.007", "10.1177/0956797610372635", "10.1016/j.cub.2007.10.059", "10.1167/12.3.4", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-08234-9_253-1", "10.1152/jn.00129.2004", "10.1017/S0140525X01000115", "10.1016/j.neubiorev.2004.12.004", "10.1016/S0960-9822(07)00492-7", "10.1167/11.3.15", "10.1016/j.visres.2016.02.002", "10.1007/978-1-4419-8432-6", "10.3758/BF03211756", "10.1016/j.displa.2012.10.007", "10.1177/0956797610372635", "10.1016/j.cub.2007.10.059", "10.1167/12.3.4", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-08234-9_253-1", "10.1152/jn.00129.2004", "10.1017/S0140525X01000115", "10.1016/j.neubiorev.2004.12.004", "10.1016/S0960-9822(07)00492-7", "10.1167/11.3.15", "10.1016/j.visres.2016.02.002", "10.1007/978-1-4419-8432-6", "10.3758/BF03211756"]}, "10.1109/TVCG.2019.2898742": {"doi": "10.1109/TVCG.2019.2898742", "author": ["Q. Zhou", "G. Hagemann", "D. Fafard", "I. Stavness", "S. Fels"], "title": "An Evaluation of Depth and Size Perception on a Spherical Fish Tank Virtual Reality Display", "year": "2019", "abstract": "Fish Tank Virtual Reality (FTVR) displays create a compelling 3D spatial effect by rendering to the perspective of the viewer with head-tracking. Combining FTVR with a spherical display enhances the 3D experience with unique properties of the spherical screen such as the enclosing shape, consistent curved surface, and borderless views from all angles around the display. The ability to generate a strong 3D effect on a spherical display with head-tracked rendering is promising for increasing user's performance in 3D tasks. An unanswered question is whether these natural affordances of spherical FTVR displays can improve spatial perception in comparison to traditional flat FTVR displays. To investigate this question, we conducted an experiment to see whether users can perceive the depth and size of virtual objects better on a spherical FTVR display compared to a flat FTVR display on two tasks. Using the spherical display, we found significantly that users had 1cm depth accuracy compared to 6.5cm accuracy using the flat display on a depth-ranking task. Likewise, their performance on a size-matching task was also significantly better with the size error of 2.3mm on the spherical display compared to 3.1mm on the flat display. Furthermore, the perception of size-constancy is stronger on the spherical display than the flat display. This study indicates that the natural affordances provided by the spherical form factor improve depth and size perception in 3D compared to a flat display. We believe that spherical FTVR displays have potential as a 3D virtual environment to provide better task performance for various 3D applications such as 3D designs, scientific visualizations, and virtual surgery.", "keywords": ["rendering (computer graphics)", "tanks (containers)", "three-dimensional displays", "virtual reality", "visual perception", "depth perception", "spherical fish tank virtual reality display", "3D spatial effect", "head-tracked rendering", "size perception", "flat FTVR display", "spherical FTVR display", "Three-dimensional displays", "Task analysis", "Shape", "Virtual reality", "Fish", "Rendering (computer graphics)", "Visualization", "Fish tank virtual reality", "spherical display", "depth cues", "3D perception", "size constancy", "3D visualization", "Adolescent", "Adult", "Animals", "Computer Graphics", "Depth Perception", "Female", "Fishes", "Humans", "Imaging, Three-Dimensional", "Male", "Size Perception", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8936339"], "referencing": ["IKEY:7517464", "IKEY:490521", "IKEY:1492264", "IKEY:6797703", "IKEY:5759242", "IKEY:4161006", "IKEY:6365433", "IKEY:6479210", "IKEY:809883", "IKEY:6874768", "IKEY:7926707", "IKEY:7892376", "IKEY:7517464", "IKEY:490521", "IKEY:1492264", "IKEY:6797703", "IKEY:5759242", "IKEY:4161006", "IKEY:6365433", "IKEY:6479210", "IKEY:809883", "IKEY:6874768", "IKEY:7926707", "IKEY:7892376", "IKEY:7517464", "IKEY:490521", "IKEY:1492264", "IKEY:6797703", "IKEY:5759242", "IKEY:4161006", "IKEY:6365433", "IKEY:6479210", "IKEY:809883", "IKEY:6874768", "IKEY:7926707", "IKEY:7892376", "10.1145/159161.155359", "10.1145/2207676.2207704", "10.1145/1449715.1449729", "10.1145/2642918.2647402", "10.1145/3025453.3025806", "10.1145/1979742.1979719", "10.1145/2145204.2145286", "10.1145/3027063.3052963", "10.1145/1133265.1133305", "10.1145/1029632.1029644", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/2207676.2208640", "10.1145/1140491.1140493", "10.1145/1140491.1140502", "10.1145/2543581.2543590", "10.1145/1753326.1753535", "10.1145/1179849.1180055", "10.1145/3025453.3025685", "10.1145/3025453.3025685", "10.1145/234972.234975", "10.1145/3214907.3214914", "10.1145/159161.155359", "10.1145/2207676.2207704", "10.1145/1449715.1449729", "10.1145/2642918.2647402", "10.1145/3025453.3025806", "10.1145/1979742.1979719", "10.1145/2145204.2145286", "10.1145/3027063.3052963", "10.1145/1133265.1133305", "10.1145/1029632.1029644", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/2207676.2208640", "10.1145/1140491.1140493", "10.1145/1140491.1140502", "10.1145/2543581.2543590", "10.1145/1753326.1753535", "10.1145/1179849.1180055", "10.1145/3025453.3025685", "10.1145/3025453.3025685", "10.1145/234972.234975", "10.1145/3214907.3214914", "10.1145/159161.155359", "10.1145/2207676.2207704", "10.1145/1449715.1449729", "10.1145/2642918.2647402", "10.1145/3025453.3025806", "10.1145/1979742.1979719", "10.1145/2145204.2145286", "10.1145/3027063.3052963", "10.1145/1133265.1133305", "10.1145/1029632.1029644", "10.1145/1394281.1394283", "10.1145/3106155", "10.1145/2207676.2208640", "10.1145/1140491.1140493", "10.1145/1140491.1140502", "10.1145/2543581.2543590", "10.1145/1753326.1753535", "10.1145/1179849.1180055", "10.1145/3025453.3025685", "10.1145/3025453.3025685", "10.1145/234972.234975", "10.1145/3214907.3214914", "10.1016/j.aca.2014.04.057", "10.1089/cpb.2007.9935", "10.1007/s10055-014-0257-x", "10.1037/a0027524", "10.3758/s13414-013-0503-4", "10.1068/p5218", "10.1037/xhp0000401", "10.1037/xap0000051", "10.1093/acprof:oso/9780199597277.003.0004", "10.1016/j.aca.2014.04.057", "10.1089/cpb.2007.9935", "10.1007/s10055-014-0257-x", "10.1037/a0027524", "10.3758/s13414-013-0503-4", "10.1068/p5218", "10.1037/xhp0000401", "10.1037/xap0000051", "10.1093/acprof:oso/9780199597277.003.0004", "10.1016/j.aca.2014.04.057", "10.1089/cpb.2007.9935", "10.1007/s10055-014-0257-x", "10.1037/a0027524", "10.3758/s13414-013-0503-4", "10.1068/p5218", "10.1037/xhp0000401", "10.1037/xap0000051", "10.1093/acprof:oso/9780199597277.003.0004"]}, "10.1109/TVCG.2019.2898798": {"doi": "10.1109/TVCG.2019.2898798", "author": ["J. A. Jones", "J. E. Hopper", "M. T. Bolas", "D. M. Krum"], "title": "Orientation Perception in Real and Virtual Environments", "year": "2019", "abstract": "Spatial perception in virtual environments has been a topic of intense research. Arguably, the majority of this work has focused on distance perception. However, orientation perception is also an important factor. In this paper, we systematically investigate allocentric orientation judgments in both real and virtual contexts over the course of four experiments. A pattern of sinusoidal judgment errors known to exist in 2D perspective displays is found to persist in immersive virtual environments. This pattern also manifests itself in a real world setting using two differing judgment methods. The findings suggest the presence of a radial anisotropy that persists across viewing contexts. Additionally, there is some evidence to suggest that observers have multiple strategies for processing orientations but further investigation is needed to fully describe this phenomenon. We also offer design suggestions for 3D user interfaces where users may perform orientation judgments.", "keywords": ["user interfaces", "virtual reality", "visual perception", "distance perception", "orientation perception", "allocentric orientation judgments", "sinusoidal judgment errors", "2D perspective displays", "immersive virtual environments", "spatial perception", "3D user interfaces", "Virtual environments", "Task analysis", "Observers", "Anisotropic magnetoresistance", "Visualization", "Gravity", "Legged locomotion", "Virtual Environments", "Perception", "Spatial Orientation", "Visual Orientation", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Orientation", "Space Perception", "Virtual Reality", "Visual Perception"], "referenced_by": ["IKEY:8797756"], "referencing": ["IKEY:5759434", "IKEY:1667620", "IKEY:7210403", "IKEY:6479211", "IKEY:6777445", "IKEY:6777444", "IKEY:5759485", "IKEY:6797268", "IKEY:7547900", "IKEY:6790867", "IKEY:4135650", "IKEY:5759434", "IKEY:1667620", "IKEY:7210403", "IKEY:6479211", "IKEY:6777445", "IKEY:6777444", "IKEY:5759485", "IKEY:6797268", "IKEY:7547900", "IKEY:6790867", "IKEY:4135650", "IKEY:5759434", "IKEY:1667620", "IKEY:7210403", "IKEY:6479211", "IKEY:6777445", "IKEY:6777444", "IKEY:5759485", "IKEY:6797268", "IKEY:7547900", "IKEY:6790867", "IKEY:4135650", "10.1145/2983631", "10.1145/2077451.2077457", "10.1145/2077451.2077464", "10.1145/3165286", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1077399.1077401", "10.1145/1836248.1836277", "10.1145/2983631", "10.1145/2077451.2077457", "10.1145/2077451.2077464", "10.1145/3165286", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1077399.1077401", "10.1145/1836248.1836277", "10.1145/2983631", "10.1145/2077451.2077457", "10.1145/2077451.2077464", "10.1145/3165286", "10.1145/1140491.1140493", "10.1145/1227134.1227138", "10.1145/1077399.1077401", "10.1145/1836248.1836277", "10.1037/h0033117", "10.1371/journal.pone.0019289", "10.1016/j.brainres.2008.07.075", "10.3758/BF03208083", "10.1007/s00221-006-0405-y", "10.1068/p240981", "10.1177/154193129003401910", "10.1177/001872088702900401", "10.1518/001872098779591278", "10.1177/154193129103502005", "10.1016/0001-6918(93)E0057-9", "10.1167/14.12.17", "10.1006/jmla.1996.0007", "10.1037/0278-7393.15.2.179", "10.1021/bk-2013-1142.ch008", "10.1177/154193129604002201", "10.1080/00223980.1958.9916283", "10.1080/17470215208416615", "10.1518/001872007X200139", "10.1007/s00426-006-0085-z", "10.1037/h0093106", "10.1016/S0306-4522(97)00393-X", "10.1037/h0060386", "10.1163/156856894X00017", "10.1037/h0033117", "10.1371/journal.pone.0019289", "10.1016/j.brainres.2008.07.075", "10.3758/BF03208083", "10.1007/s00221-006-0405-y", "10.1068/p240981", "10.1177/154193129003401910", "10.1177/001872088702900401", "10.1518/001872098779591278", "10.1177/154193129103502005", "10.1016/0001-6918(93)E0057-9", "10.1167/14.12.17", "10.1006/jmla.1996.0007", "10.1037/0278-7393.15.2.179", "10.1021/bk-2013-1142.ch008", "10.1177/154193129604002201", "10.1080/00223980.1958.9916283", "10.1080/17470215208416615", "10.1518/001872007X200139", "10.1007/s00426-006-0085-z", "10.1037/h0093106", "10.1016/S0306-4522(97)00393-X", "10.1037/h0060386", "10.1163/156856894X00017", "10.1037/h0033117", "10.1371/journal.pone.0019289", "10.1016/j.brainres.2008.07.075", "10.3758/BF03208083", "10.1007/s00221-006-0405-y", "10.1068/p240981", "10.1177/154193129003401910", "10.1177/001872088702900401", "10.1518/001872098779591278", "10.1177/154193129103502005", "10.1016/0001-6918(93)E0057-9", "10.1167/14.12.17", "10.1006/jmla.1996.0007", "10.1037/0278-7393.15.2.179", "10.1021/bk-2013-1142.ch008", "10.1177/154193129604002201", "10.1080/00223980.1958.9916283", "10.1080/17470215208416615", "10.1518/001872007X200139", "10.1007/s00426-006-0085-z", "10.1037/h0093106", "10.1016/S0306-4522(97)00393-X", "10.1037/h0060386", "10.1163/156856894X00017"]}, "10.1109/TVCG.2019.2898738": {"doi": "10.1109/TVCG.2019.2898738", "author": ["T. Fukiage", "T. Kawabe", "S. Nishida"], "title": "Perceptually Based Adaptive Motion Retargeting to Animate Real Objects by Light Projection", "year": "2019", "abstract": "A recently developed light projection technique can add dynamic impressions to static real objects without changing their original visual attributes such as surface colors and textures. It produces illusory motion impressions in the projection target by projecting gray-scale motion-inducer patterns that selectively drive the motion detectors in the human visual system. Since a compelling illusory motion can be produced by an inducer pattern weaker than necessary to perfectly reproduce the shift of the original pattern on an object's surface, the technique works well under bright environmental light conditions. However, determining the best deformation sizes is often difficult: When users try to add a large deformation, the deviation in the projected patterns from the original surface pattern on the target object becomes apparent. Therefore, to obtain satisfactory results, they have to spend much time and effort to manually adjust the shift sizes. Here, to overcome this limitation, we propose an optimization framework that adaptively retargets the displacement vectors based on a perceptual model. The perceptual model predicts the subjective inconsistency between a projected pattern and an original one by simulating responses in the human visual system. The displacement vectors are adaptively optimized so that the projection effect is maximized within the tolerable range predicted by the model. We extensively evaluated the perceptual model and optimization method through a psychophysical experiment as well as user studies.", "keywords": ["brightness", "computer animation", "data visualisation", "image colour analysis", "image motion analysis", "image texture", "optimisation", "visual perception", "surface colors", "illusory motion impressions", "projection target", "gray-scale motion-inducer patterns", "motion detectors", "human visual system", "bright environmental light conditions", "projected pattern", "target object", "displacement vectors", "perceptual model", "projection effect", "perceptually based adaptive motion", "dynamic impressions", "visual attributes", "surface pattern", "light projection technique", "Computational modeling", "Adaptation models", "Visualization", "Predictive models", "Surface texture", "Image quality", "Optimization", "Spatial augmented reality", "human visual system", "perceptual model", "Computer Graphics", "Humans", "Image Processing, Computer-Assisted", "Light", "Psychophysics", "Virtual Reality", "Visual Perception"], "referenced_by": ["10.1016/j.cag.2020.07.008"], "referencing": ["IKEY:8446481", "IKEY:6595980", "IKEY:1640444", "IKEY:1095851", "IKEY:4359477", "IKEY:8417321", "IKEY:6909637", "IKEY:8260962", "IKEY:7165660", "IKEY:6162895", "IKEY:413502", "IKEY:1565410", "IKEY:1292216", "IKEY:8578166", "IKEY:8446481", "IKEY:6595980", "IKEY:1640444", "IKEY:1095851", "IKEY:4359477", "IKEY:8417321", "IKEY:6909637", "IKEY:8260962", "IKEY:7165660", "IKEY:6162895", "IKEY:413502", "IKEY:1565410", "IKEY:1292216", "IKEY:8578166", "IKEY:8446481", "IKEY:6595980", "IKEY:1640444", "IKEY:1095851", "IKEY:4359477", "IKEY:8417321", "IKEY:6909637", "IKEY:8260962", "IKEY:7165660", "IKEY:6162895", "IKEY:413502", "IKEY:1565410", "IKEY:1292216", "IKEY:8578166", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/1401132.1401239", "10.1145/1073204.1073273", "10.1145/258734.258884", "10.1145/2874358", "10.1145/1037957.1037964", "10.1145/1964921.1964935", "10.1145/2766891", "10.1145/311535.311543", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/1401132.1401239", "10.1145/1073204.1073273", "10.1145/258734.258884", "10.1145/2874358", "10.1145/1037957.1037964", "10.1145/1964921.1964935", "10.1145/2766891", "10.1145/311535.311543", "10.1145/2508363.2508416", "10.1145/1409060.1409103", "10.1145/1401132.1401239", "10.1145/1073204.1073273", "10.1145/258734.258884", "10.1145/2874358", "10.1145/1037957.1037964", "10.1145/1964921.1964935", "10.1145/2766891", "10.1145/311535.311543", "10.1117/12.135952", "10.1117/12.907612", "10.1007/3-540-45103-X_50", "10.1117/12.171150", "10.1002/jsid.572", "10.1017/S0952523800009640", "10.2352/ISSN.2470-1173.2016.16.HVEI-103", "10.1364/JOSAA.27.000852", "10.1364/JOSA.70.001458", "10.1142/9789812831200_0010", "10.1117/12.586757", "10.1007/978-1-4612-2544-7_17", "10.1068/p5321", "10.1201/b10705", "10.1016/j.patrec.2010.05.009", "10.1111/cgf.12940", "10.1038/328645a0", "10.1016/0042-6989(87)90146-5", "10.1038/275055a0", "10.1364/JOSAA.14.002379", "10.1117/12.135952", "10.1117/12.907612", "10.1007/3-540-45103-X_50", "10.1117/12.171150", "10.1002/jsid.572", "10.1017/S0952523800009640", "10.2352/ISSN.2470-1173.2016.16.HVEI-103", "10.1364/JOSAA.27.000852", "10.1364/JOSA.70.001458", "10.1142/9789812831200_0010", "10.1117/12.586757", "10.1007/978-1-4612-2544-7_17", "10.1068/p5321", "10.1201/b10705", "10.1016/j.patrec.2010.05.009", "10.1111/cgf.12940", "10.1038/328645a0", "10.1016/0042-6989(87)90146-5", "10.1038/275055a0", "10.1364/JOSAA.14.002379", "10.1117/12.135952", "10.1117/12.907612", "10.1007/3-540-45103-X_50", "10.1117/12.171150", "10.1002/jsid.572", "10.1017/S0952523800009640", "10.2352/ISSN.2470-1173.2016.16.HVEI-103", "10.1364/JOSAA.27.000852", "10.1364/JOSA.70.001458", "10.1142/9789812831200_0010", "10.1117/12.586757", "10.1007/978-1-4612-2544-7_17", "10.1068/p5321", "10.1201/b10705", "10.1016/j.patrec.2010.05.009", "10.1111/cgf.12940", "10.1038/328645a0", "10.1016/0042-6989(87)90146-5", "10.1038/275055a0", "10.1364/JOSAA.14.002379"]}, "10.1109/TVCG.2019.2898741": {"doi": "10.1109/TVCG.2019.2898741", "author": ["G. Denes", "K. Maruszczyk", "G. Ash", "R. K. Mantiuk"], "title": "Temporal Resolution Multiplexing: Exploiting the limitations of spatio-temporal vision for more efficient VR rendering", "year": "2019", "abstract": "Rendering in virtual reality (VR) requires substantial computational power to generate 90 frames per second at high resolution with good-quality antialiasing. The video data sent to a VR headset requires high bandwidth, achievable only on dedicated links. In this paper we explain how rendering requirements and transmission bandwidth can be reduced using a conceptually simple technique that integrates well with existing rendering pipelines. Every even-numbered frame is rendered at a lower resolution, and every odd-numbered frame is kept at high resolution but is modified in order to compensate for the previous loss of high spatial frequencies. When the frames are seen at a high frame rate, they are fused and perceived as high-resolution and high-frame-rate animation. The technique relies on the limited ability of the visual system to perceive high spatio-temporal frequencies. Despite its conceptual simplicity, correct execution of the technique requires a number of non-trivial steps: display photometric temporal response must be modeled, flicker and motion artifacts must be avoided, and the generated signal must not exceed the dynamic range of the display. Our experiments, performed on a high-frame-rate LCD monitor and OLED-based VR headsets, explore the parameter space of the proposed technique and demonstrate that its perceived quality is indistinguishable from full-resolution rendering. The technique is an attractive alternative to reprojection and resolution reduction of all frames.", "keywords": ["antialiasing", "computer animation", "image resolution", "liquid crystal displays", "photometry", "rendering (computer graphics)", "video signal processing", "virtual reality", "spatio-temporal vision", "good-quality antialiasing", "rendering requirements", "rendering pipelines", "high spatial frequencies", "high-frame-rate animation", "high spatio-temporal frequencies", "high-frame-rate LCD monitor", "full-resolution rendering", "reprojection", "resolution reduction", "VR rendering", "temporal resolution multiplexing", "virtual reality", "video data", "transmission bandwidth", "OLED-based VR headsets", "photometric temporal response", "Rendering (computer graphics)", "Bandwidth", "Liquid crystal displays", "Switches", "Multiplexing", "Visualization", "Encoding", "Temporal multiplexing", "rendering", "graphics", "perception", "virtual reality"], "referenced_by": ["IKEY:9005240"], "referencing": ["IKEY:4014067", "IKEY:7306410", "IKEY:4712388", "IKEY:7383304", "IKEY:4014067", "IKEY:7306410", "IKEY:4712388", "IKEY:7383304", "IKEY:4014067", "IKEY:7306410", "IKEY:4712388", "IKEY:7383304", "10.1145/2159516.2159521", "10.1145/1778765.1778850", "10.1145/2601097.2601144", "10.1145/2816795.2818070", "10.1145/1198555.1198798", "10.1145/2185520.2185575", "10.1145/1185657.1185830", "10.1145/2159516.2159521", "10.1145/1778765.1778850", "10.1145/2601097.2601144", "10.1145/2816795.2818070", "10.1145/1198555.1198798", "10.1145/2185520.2185575", "10.1145/1185657.1185830", "10.1145/2159516.2159521", "10.1145/1778765.1778850", "10.1145/2601097.2601144", "10.1145/2816795.2818070", "10.1145/1198555.1198798", "10.1145/2185520.2185575", "10.1145/1185657.1185830", "10.1016/0141-9382(96)01011-6", "10.1163/156856897X00357", "10.1117/12.2273625", "10.1111/j.1467-8659.2009.01641.x", "10.1016/0042-6989(79)90227-X", "10.1889/JSID19.3.271", "10.1364/JOSA.69.001340", "10.1364/JOSAA.1.000107", "10.1117/12.647870", "10.1364/JOSA.44.000060", "10.1007/BF00363977", "10.1111/j.1467-8659.2012.03075.x", "10.1117/12.586425", "10.2352/ISSN.2470-1173.2016.16.HVEI-102", "10.1364/JOSAA.3.000300", "10.1016/0141-9382(96)01011-6", "10.1163/156856897X00357", "10.1117/12.2273625", "10.1111/j.1467-8659.2009.01641.x", "10.1016/0042-6989(79)90227-X", "10.1889/JSID19.3.271", "10.1364/JOSA.69.001340", "10.1364/JOSAA.1.000107", "10.1117/12.647870", "10.1364/JOSA.44.000060", "10.1007/BF00363977", "10.1111/j.1467-8659.2012.03075.x", "10.1117/12.586425", "10.2352/ISSN.2470-1173.2016.16.HVEI-102", "10.1364/JOSAA.3.000300", "10.1016/0141-9382(96)01011-6", "10.1163/156856897X00357", "10.1117/12.2273625", "10.1111/j.1467-8659.2009.01641.x", "10.1016/0042-6989(79)90227-X", "10.1889/JSID19.3.271", "10.1364/JOSA.69.001340", "10.1364/JOSAA.1.000107", "10.1117/12.647870", "10.1364/JOSA.44.000060", "10.1007/BF00363977", "10.1111/j.1467-8659.2012.03075.x", "10.1117/12.586425", "10.2352/ISSN.2470-1173.2016.16.HVEI-102", "10.1364/JOSAA.3.000300"]}, "10.1109/TVCG.2019.2898782": {"doi": "10.1109/TVCG.2019.2898782", "author": ["L. Wang", "J. Wu", "X. Yang", "V. Popescu"], "title": "VR Exploration Assistance through Automatic Occlusion Removal", "year": "2019", "abstract": "Virtual Reality (VR) applications allow a user to explore a scene intuitively through a tracked head-mounted display (HMD). However, in complex scenes, occlusions make scene exploration inefficient, as the user has to navigate around occluders to gain line of sight to potential regions of interest. When a scene region proves to be of no interest, the user has to retrace their path, and such a sequential scene exploration implies significant amounts of wasted navigation. Furthermore, as the virtual world is typically much larger than the tracked physical space hosting the VR application, the intuitive one-to-one mapping between the virtual and real space has to be temporarily suspended for the user to teleport or redirect in order to conform to the physical space constraints. In this paper we introduce a method for improving VR exploration efficiency by automatically constructing a multiperspective visualization that removes occlusions. For each frame, the scene is first rendered conventionally, the z-buffer is analyzed to detect horizontal and vertical depth discontinuities, the discontinuities are used to define disocclusion portals which are 3D scene rectangles for routing rays around occluders, and the disocclusion portals are used to render a multiperpsective image that alleviates occlusions. The user controls the multiperspective disocclusion effect, deploying and retracting it with small head translations. We have quantified the VR exploration efficiency brought by our occlusion removal method in a study where participants searched for a stationary target, and chased a dynamic target. Our method showed an advantage over conventional VR exploration in terms of reducing the navigation distance, the view direction rotation, the number of redirections, and the task completion time. These advantages did not come at the cost of a reduction in depth perception or situational awareness, or of an increase in simulator sickness.", "keywords": ["data visualisation", "helmet mounted displays", "hidden feature removal", "rendering (computer graphics)", "user interfaces", "virtual reality", "disocclusion portals", "occlusion removal method", "VR exploration assistance", "automatic occlusion removal", "virtual world", "VR application", "depth discontinuities", "virtual reality applications", "head-mounted display", "disocclusion effect", "Visualization", "Resists", "Portals", "Navigation", "Task analysis", "Teleportation", "Virtual environments", "VR exploration", "occlusion removal", "disocclusion portal", "multiperspective visualization"], "referenced_by": ["IKEY:8998141", "IKEY:9284661"], "referencing": ["IKEY:4811002", "IKEY:6788011", "IKEY:5613463", "IKEY:8260942", "IKEY:7384536", "IKEY:8447553", "IKEY:615446", "IKEY:7131725", "IKEY:8446130", "IKEY:4811001", "IKEY:8260946", "IKEY:7547900", "IKEY:5336461", "IKEY:6165137", "IKEY:4811002", "IKEY:6788011", "IKEY:5613463", "IKEY:8260942", "IKEY:7384536", "IKEY:8447553", "IKEY:615446", "IKEY:7131725", "IKEY:8446130", "IKEY:4811001", "IKEY:8260946", "IKEY:7547900", "IKEY:5336461", "IKEY:6165137", "IKEY:4811002", "IKEY:6788011", "IKEY:5613463", "IKEY:8260942", "IKEY:7384536", "IKEY:8447553", "IKEY:615446", "IKEY:7131725", "IKEY:8446130", "IKEY:4811001", "IKEY:8260946", "IKEY:7547900", "IKEY:5336461", "IKEY:6165137", "10.1145/2967934.2968105", "10.1145/3130800.3130893", "10.1145/3197517.3201335", "10.1145/1661412.1618504", "10.1145/3197517.3201294", "10.1145/2897824.2925883", "10.1145/3025453.3025521", "10.1145/1508044.1508094", "10.1145/2967934.2968105", "10.1145/3130800.3130893", "10.1145/3197517.3201335", "10.1145/1661412.1618504", "10.1145/3197517.3201294", "10.1145/2897824.2925883", "10.1145/3025453.3025521", "10.1145/1508044.1508094", "10.1145/2967934.2968105", "10.1145/3130800.3130893", "10.1145/3197517.3201335", "10.1145/1661412.1618504", "10.1145/3197517.3201294", "10.1145/2897824.2925883", "10.1145/3025453.3025521", "10.1145/1508044.1508094", "10.1177/154193129804202101", "10.1089/cpb.2006.9.157", "10.1207/s15327108ijap0303_3", "10.1111/j.1467-8659.2005.00858.x", "10.1177/0013916586186004", "10.22237/jmasm/1257035100", "10.1016/0010-0285(82)90019-6", "10.1007/978-3-030-01790-3_15", "10.3102/10769986022003349", "10.1177/154193129804202101", "10.1089/cpb.2006.9.157", "10.1207/s15327108ijap0303_3", "10.1111/j.1467-8659.2005.00858.x", "10.1177/0013916586186004", "10.22237/jmasm/1257035100", "10.1016/0010-0285(82)90019-6", "10.1007/978-3-030-01790-3_15", "10.3102/10769986022003349", "10.1177/154193129804202101", "10.1089/cpb.2006.9.157", "10.1207/s15327108ijap0303_3", "10.1111/j.1467-8659.2005.00858.x", "10.1177/0013916586186004", "10.22237/jmasm/1257035100", "10.1016/0010-0285(82)90019-6", "10.1007/978-3-030-01790-3_15", "10.3102/10769986022003349"]}, "10.1109/TVCG.2019.2898650": {"doi": "10.1109/TVCG.2019.2898650", "author": ["W. Xu", "A. Chatterjee", "M. Zollh\u00f6fer", "H. Rhodin", "P. Fua", "H. -P. Seidel", "C. Theobalt"], "title": "Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera", "year": "2019", "abstract": "We propose the first real-time system for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60 Hz on a consumer-level GPU. In addition to the lightweight hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.", "keywords": ["cameras", "convolutional neural nets", "graphics processing units", "image motion analysis", "image sensors", "pose estimation", "stereo image processing", "egocentric viewpoint", "real-time mobile 3D motion capture", "real-time system", "egocentric estimation", "3D human body", "unconstrained everyday activities", "standard baseball cap", "high-quality pose estimation", "single cap-mounted fisheye camera", "3D pose estimation approach", "lightweight hardware setup", "fisheye images", "lightweight setup", "Mo2Cap2", "3D joint error", "CNN based 3D pose estimation", "consumer-level GPU", "egocentric live stream", "frequency 60.0 Hz", "Three-dimensional displays", "Cameras", "Pose estimation", "Real-time systems", "Two dimensional displays", "Hardware", "Distortion", "Egocentric", "Monocular", "Mobile motion capture", "Databases, Factual", "Deep Learning", "Human Activities", "Humans", "Imaging, Three-Dimensional", "Posture", "Smart Glasses", "Software", "Video Recording"], "referenced_by": ["IKEY:9008555", "IKEY:9144978"], "referencing": ["IKEY:6909866", "IKEY:6126356", "IKEY:698581", "IKEY:6619308", "IKEY:8237668", "IKEY:7785123", "IKEY:7299005", "IKEY:6126269", "IKEY:5206859", "IKEY:8237584", "IKEY:6193117", "IKEY:6682899", "IKEY:8265309", "IKEY:5995318", "IKEY:6103287", "IKEY:7410738", "IKEY:5995406", "IKEY:7780578", "IKEY:8374605", "IKEY:7780707", "IKEY:8099622", "IKEY:8099621", "IKEY:8099984", "IKEY:7410451", "IKEY:7785089", "IKEY:4059340", "IKEY:5995316", "IKEY:8099977", "IKEY:7298941", "IKEY:6126338", "IKEY:7780482", "IKEY:8100086", "IKEY:8099975", "IKEY:7780621", "IKEY:8265289", "IKEY:7780904", "IKEY:7153142", "IKEY:7780906", "IKEY:6909866", "IKEY:6126356", "IKEY:698581", "IKEY:6619308", "IKEY:8237668", "IKEY:7785123", "IKEY:7299005", "IKEY:6126269", "IKEY:5206859", "IKEY:8237584", "IKEY:6193117", "IKEY:6682899", "IKEY:8265309", "IKEY:5995318", "IKEY:6103287", "IKEY:7410738", "IKEY:5995406", "IKEY:7780578", "IKEY:8374605", "IKEY:7780707", "IKEY:8099622", "IKEY:8099621", "IKEY:8099984", "IKEY:7410451", "IKEY:7785089", "IKEY:4059340", "IKEY:5995316", "IKEY:8099977", "IKEY:7298941", "IKEY:6126338", "IKEY:7780482", "IKEY:8100086", "IKEY:8099975", "IKEY:7780621", "IKEY:8265289", "IKEY:7780904", "IKEY:7153142", "IKEY:7780906", "IKEY:6909866", "IKEY:6126356", "IKEY:698581", "IKEY:6619308", "IKEY:8237668", "IKEY:7785123", "IKEY:7299005", "IKEY:6126269", "IKEY:5206859", "IKEY:8237584", "IKEY:6193117", "IKEY:6682899", "IKEY:8265309", "IKEY:5995318", "IKEY:6103287", "IKEY:7410738", "IKEY:5995406", "IKEY:7780578", "IKEY:8374605", "IKEY:7780707", "IKEY:8099622", "IKEY:8099621", "IKEY:8099984", "IKEY:7410451", "IKEY:7785089", "IKEY:4059340", "IKEY:5995316", "IKEY:8099977", "IKEY:7298941", "IKEY:6126338", "IKEY:7780482", "IKEY:8100086", "IKEY:8099975", "IKEY:7780621", "IKEY:8265289", "IKEY:7780904", "IKEY:7153142", "IKEY:7780906", "10.1145/2380116.2380139", "10.1145/3072959.3073596", "10.1145/2980179.2980235", "10.1145/2010324.1964926", "10.1145/2807442.2807445", "10.1145/1966394.1966397", "10.1145/2366145.2366207", "10.1145/2380116.2380139", "10.1145/3072959.3073596", "10.1145/2980179.2980235", "10.1145/2010324.1964926", "10.1145/2807442.2807445", "10.1145/1966394.1966397", "10.1145/2366145.2366207", "10.1145/2380116.2380139", "10.1145/3072959.3073596", "10.1145/2980179.2980235", "10.1145/2010324.1964926", "10.1145/2807442.2807445", "10.1145/1966394.1966397", "10.1145/2366145.2366207", "10.1007/978-3-319-66709-6_28", "10.1007/s11263-008-0173-1", "10.1007/978-3-319-10602-1_48", "10.1007/978-0-85729-997-0", "10.1007/s11263-009-0273-6", "10.1007/s11263-011-0493-4", "10.1016/j.patcog.2016.07.031", "10.5244/C.30.130", "10.1007/978-3-642-12392-4_6", "10.1016/j.cviu.2006.08.006", "10.1111/cgf.13131", "10.1007/978-3-642-33709-3_59", "10.1007/978-3-319-66709-6_28", "10.1007/s11263-008-0173-1", "10.1007/978-3-319-10602-1_48", "10.1007/978-0-85729-997-0", "10.1007/s11263-009-0273-6", "10.1007/s11263-011-0493-4", "10.1016/j.patcog.2016.07.031", "10.5244/C.30.130", "10.1007/978-3-642-12392-4_6", "10.1016/j.cviu.2006.08.006", "10.1111/cgf.13131", "10.1007/978-3-642-33709-3_59", "10.1007/978-3-319-66709-6_28", "10.1007/s11263-008-0173-1", "10.1007/978-3-319-10602-1_48", "10.1007/978-0-85729-997-0", "10.1007/s11263-009-0273-6", "10.1007/s11263-011-0493-4", "10.1016/j.patcog.2016.07.031", "10.5244/C.30.130", "10.1007/978-3-642-12392-4_6", "10.1016/j.cviu.2006.08.006", "10.1111/cgf.13131", "10.1007/978-3-642-33709-3_59"]}, "10.1109/TVCG.2019.2899231": {"doi": "10.1109/TVCG.2019.2899231", "author": ["P. Stotko", "S. Krumpen", "M. B. Hullin", "M. Weinmann", "R. Klein"], "title": "SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for Immersive Multi-Client Live Telepresence", "year": "2019", "abstract": "Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VR/AR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.", "keywords": ["augmented reality", "client-server systems", "data structures", "graphics processing units", "image reconstruction", "multi-threading", "rendering (computer graphics)", "solid modelling", "telecontrol", "network bandwidth", "client-side computation", "immersive lag-free viewing experience", "fluid lag-free viewing experience", "real-time 3D reconstruction", "immersive multiclient live telepresence", "real-time 3D scene reconstruction", "RGB-D sensor data", "VR/AR settings", "telepresence systems", "transmission bandwidth", "remote collaboration", "real-time capture", "many-user exploration", "static 3D scenes", "interactive frame rates", "lag-free view updates", "thread level", "transmission scheme", "client-server system", "marching cubes geometry reconstruction", "thread-safe GPU hash map data structure", "SLAMCast", "rendering", "3D model", "voxel block hashing framework", "Three-dimensional displays", "Real-time systems", "Telepresence", "Collaboration", "Bandwidth", "Servers", "Hardware", "Remote collaboration", "live telepresence", "real-time reconstruction", "voxel hashing", "RGB-D", "real-time streaming", "Computer Communication Networks", "Humans", "Imaging, Three-Dimensional", "Videoconferencing"], "referenced_by": ["IKEY:8769053", "IKEY:8943776", "IKEY:8968598", "IKEY:9089500", "IKEY:9240504"], "referencing": ["IKEY:8425196", "IKEY:7996611", "IKEY:7490357", "IKEY:6861875", "IKEY:8492363", "IKEY:6599102", "IKEY:7823595", "IKEY:7165673", "IKEY:7368096", "IKEY:580394", "IKEY:7429295", "IKEY:4480795", "IKEY:6365430", "IKEY:7836456", "IKEY:880933", "IKEY:6162880", "IKEY:7298631", "IKEY:6948420", "IKEY:6385773", "IKEY:7284329", "IKEY:5727958", "IKEY:6788002", "IKEY:8425196", "IKEY:7996611", "IKEY:7490357", "IKEY:6861875", "IKEY:8492363", "IKEY:6599102", "IKEY:7823595", "IKEY:7165673", "IKEY:7368096", "IKEY:580394", "IKEY:7429295", "IKEY:4480795", "IKEY:6365430", "IKEY:7836456", "IKEY:880933", "IKEY:6162880", "IKEY:7298631", "IKEY:6948420", "IKEY:6385773", "IKEY:7284329", "IKEY:5727958", "IKEY:6788002", "IKEY:8425196", "IKEY:7996611", "IKEY:7490357", "IKEY:6861875", "IKEY:8492363", "IKEY:6599102", "IKEY:7823595", "IKEY:7165673", "IKEY:7368096", "IKEY:580394", "IKEY:7429295", "IKEY:4480795", "IKEY:6365430", "IKEY:7836456", "IKEY:880933", "IKEY:6162880", "IKEY:7298631", "IKEY:6948420", "IKEY:6385773", "IKEY:7284329", "IKEY:5727958", "IKEY:6788002", "10.1145/2461912.2461940", "10.1145/3054739", "10.1145/2897824.2925969", "10.1145/3083165.3083180", "10.1145/2024156.2024195", "10.1145/2047196.2047270", "10.1145/2642918.2647383", "10.1145/2531602.2531727", "10.1145/1141911.1141926", "10.1145/2492045.2492053", "10.1145/37401.37422", "10.1145/3097895.3097901", "10.1145/2984511.2984517", "10.1145/1152399.1152421", "10.1145/2461912.2461940", "10.1145/3054739", "10.1145/2897824.2925969", "10.1145/3083165.3083180", "10.1145/2024156.2024195", "10.1145/2047196.2047270", "10.1145/2642918.2647383", "10.1145/2531602.2531727", "10.1145/1141911.1141926", "10.1145/2492045.2492053", "10.1145/37401.37422", "10.1145/3097895.3097901", "10.1145/2984511.2984517", "10.1145/1152399.1152421", "10.1145/2461912.2461940", "10.1145/3054739", "10.1145/2897824.2925969", "10.1145/3083165.3083180", "10.1145/2024156.2024195", "10.1145/2047196.2047270", "10.1145/2642918.2647383", "10.1145/2531602.2531727", "10.1145/1141911.1141926", "10.1145/2492045.2492053", "10.1145/37401.37422", "10.1145/3097895.3097901", "10.1145/2984511.2984517", "10.1145/1152399.1152421", "10.1016/j.is.2012.06.002", "10.1162/pres.1992.1.4.482", "10.1162/pres.1992.1.1.109", "10.1007/978-3-319-46484-8_30", "10.1016/j.cag.2012.04.011", "10.1007/978-3-642-31205-2_13", "10.1155/2010/247108", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1142/S0219024901000900", "10.1016/j.is.2012.06.002", "10.1162/pres.1992.1.4.482", "10.1162/pres.1992.1.1.109", "10.1007/978-3-319-46484-8_30", "10.1016/j.cag.2012.04.011", "10.1007/978-3-642-31205-2_13", "10.1155/2010/247108", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1142/S0219024901000900", "10.1016/j.is.2012.06.002", "10.1162/pres.1992.1.4.482", "10.1162/pres.1992.1.1.109", "10.1007/978-3-319-46484-8_30", "10.1016/j.cag.2012.04.011", "10.1007/978-3-642-31205-2_13", "10.1155/2010/247108", "10.5244/C.26.112", "10.1177/0278364914551008", "10.1142/S0219024901000900"]}, "10.1109/TVCG.2019.2898800": {"doi": "10.1109/TVCG.2019.2898800", "author": ["S. Narang", "A. Best", "D. Manocha"], "title": "Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments", "year": "2019", "abstract": "We present a real-time algorithm to infer the intention of a user's avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar's hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual's intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user's avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.", "keywords": ["avatars", "Bayes methods", "human computer interaction", "inference mechanisms", "multi-agent systems", "shared avatar-agent virtual environments", "real-time algorithm", "observed proxemics", "human behavior", "virtual agent", "gaze-based behaviors", "inference algorithm", "human-agent interactions", "controlled multiagent simulation", "Bayesian Theory of Mind", "user intent", "Avatars", "Virtual environments", "Computational modeling", "Inference algorithms", "Two dimensional displays", "Bayes methods", "Heuristic algorithms", "multi-agent simulation", "virtual reality", "avatars", "human agents", "interactive navigation", "Adult", "Algorithms", "Bayes Theorem", "Computer Graphics", "Cues", "Female", "Humans", "Image Processing, Computer-Assisted", "Intention", "Male", "Virtual Reality"], "referenced_by": ["IKEY:9090666"], "referencing": ["IKEY:6789045", "IKEY:7014249", "IKEY:8446180", "IKEY:8446152", "IKEY:7367470", "IKEY:7223327", "IKEY:7935624", "IKEY:6789045", "IKEY:7014249", "IKEY:8446180", "IKEY:8446152", "IKEY:7367470", "IKEY:7223327", "IKEY:7935624", "IKEY:6789045", "IKEY:7014249", "IKEY:8446180", "IKEY:8446152", "IKEY:7367470", "IKEY:7223327", "IKEY:7935624", "10.1145/2407516.2407573", "10.1145/2159616.2159632", "10.1145/2931002.2931010", "10.1145/1477926.1477936", "10.1145/3206505.3206522", "10.1145/566654.566605", "10.1145/1857893.1857896", "10.1145/2993369.2993378", "10.1145/3126686.3126766", "10.1145/2821592.2821604", "10.1145/2534329.2534336", "10.1145/1514095.1514117", "10.1145/1073368.1073371", "10.1145/1179352.1142008", "10.1145/2407516.2407573", "10.1145/2159616.2159632", "10.1145/2931002.2931010", "10.1145/1477926.1477936", "10.1145/3206505.3206522", "10.1145/566654.566605", "10.1145/1857893.1857896", "10.1145/2993369.2993378", "10.1145/3126686.3126766", "10.1145/2821592.2821604", "10.1145/2534329.2534336", "10.1145/1514095.1514117", "10.1145/1073368.1073371", "10.1145/1179352.1142008", "10.1145/2407516.2407573", "10.1145/2159616.2159632", "10.1145/2931002.2931010", "10.1145/1477926.1477936", "10.1145/3206505.3206522", "10.1145/566654.566605", "10.1145/1857893.1857896", "10.1145/2993369.2993378", "10.1145/3126686.3126766", "10.1145/2821592.2821604", "10.1145/2534329.2534336", "10.1145/1514095.1514117", "10.1145/1073368.1073371", "10.1145/1179352.1142008", "10.5898/JHRI.6.1.Admoni", "10.1207/s15327051hci1204_5", "10.1016/j.cognition.2009.07.005", "10.3389/fpsyg.2015.00869", "10.2307/3033565", "10.1016/S0149-7634(00)00025-7", "10.3389/fpsyg.2013.00859", "10.1002/cav.293", "10.1007/978-3-642-03320-9_20", "10.1038/35035023", "10.1007/s00221-001-0983-7", "10.1103/PhysRevLett.113.238701", "10.1098/rsif.2016.0414", "10.3389/frobt.2018.00082", "10.1016/j.artint.2005.04.006", "10.1111/j.1467-9280.2009.02464.x", "10.1017/S0140525X00076512", "10.1162/PRES_a_00294", "10.1007/978-3-642-25090-3_9", "10.1002/cav.403", "10.3758/s13414-017-1303-z", "10.1177/0956797614544510", "10.1007/978-3-642-19457-3_1", "10.3791/58155", "10.5898/JHRI.6.1.Admoni", "10.1207/s15327051hci1204_5", "10.1016/j.cognition.2009.07.005", "10.3389/fpsyg.2015.00869", "10.2307/3033565", "10.1016/S0149-7634(00)00025-7", "10.3389/fpsyg.2013.00859", "10.1002/cav.293", "10.1007/978-3-642-03320-9_20", "10.1038/35035023", "10.1007/s00221-001-0983-7", "10.1103/PhysRevLett.113.238701", "10.1098/rsif.2016.0414", "10.3389/frobt.2018.00082", "10.1016/j.artint.2005.04.006", "10.1111/j.1467-9280.2009.02464.x", "10.1017/S0140525X00076512", "10.1162/PRES_a_00294", "10.1007/978-3-642-25090-3_9", "10.1002/cav.403", "10.3758/s13414-017-1303-z", "10.1177/0956797614544510", "10.1007/978-3-642-19457-3_1", "10.3791/58155", "10.5898/JHRI.6.1.Admoni", "10.1207/s15327051hci1204_5", "10.1016/j.cognition.2009.07.005", "10.3389/fpsyg.2015.00869", "10.2307/3033565", "10.1016/S0149-7634(00)00025-7", "10.3389/fpsyg.2013.00859", "10.1002/cav.293", "10.1007/978-3-642-03320-9_20", "10.1038/35035023", "10.1007/s00221-001-0983-7", "10.1103/PhysRevLett.113.238701", "10.1098/rsif.2016.0414", "10.3389/frobt.2018.00082", "10.1016/j.artint.2005.04.006", "10.1111/j.1467-9280.2009.02464.x", "10.1017/S0140525X00076512", "10.1162/PRES_a_00294", "10.1007/978-3-642-25090-3_9", "10.1002/cav.403", "10.3758/s13414-017-1303-z", "10.1177/0956797614544510", "10.1007/978-3-642-19457-3_1", "10.3791/58155"]}, "10.1109/TVCG.2019.2899232": {"doi": "10.1109/TVCG.2019.2899232", "author": ["L. E. Buck", "J. J. Rieser", "G. Narasimham", "B. Bodenheimer"], "title": "Interpersonal Affordances and Social Dynamics in Collaborative Immersive Virtual Environments: Passing Together Through Apertures", "year": "2019", "abstract": "An essential question in understanding how to develop and build collaborative immersive virtual environments (IVEs) is recognizing how people perform actions together. Many actions in the real world require that people act without prior planning, and these actions are executed quite successfully. In this paper, we study the common action of two people passing through an aperture together in both the real world (Experiment 1) and in a distributed, collaborative IVE (Experiment 2). The aperture's width is varied from too narrow to be passable to so wide as to be easily passable by both participants together simultaneously. We do this in the real world for all possible gender-based pairings. In virtual reality, however, there is potential for the gender of the participant and the gender of the self-avatar to be different. We also investigate the joint action for all possible gender-based pairings in the distributed IVE. Results indicated that, in the real world, social dynamics between gendered pairings emerged; male-male pairings refused to concede to one another until absolutely necessary while other pairings did not. Male-female pairings were most likely to provide ample space to one another during passage. These behaviors seemed not to appear in the IVE, and avatar gender across all pairings generated no significant behavioral differences. In addition, participants tended to require wider gaps to allow for passage in the IVE. These findings establish base knowledge of social dynamics and affordance behaviors within multi-user IVEs.", "keywords": ["avatars", "gender issues", "collaborative immersive virtual environments", "virtual reality", "social dynamics", "male-male pairings", "male-female pairings", "avatar gender", "multiuser IVEs", "interpersonal affordances", "apertures", "IVE", "gender-based pairings", "Task analysis", "Avatars", "Apertures", "Collaboration", "Virtual environments", "Legged locomotion", "Virtual reality", "perception", "collaborative virtual reality", "proxemics", "affordances", "avatars", "Adult", "Computer Graphics", "Cooperative Behavior", "Female", "Humans", "Image Processing, Computer-Assisted", "Interpersonal Relations", "Male", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:9089645", "IKEY:9089660", "IKEY:9090672", "IKEY:9089501", "IKEY:9089552", "IKEY:9284694"], "referencing": ["10.1145/2804408.2804426", "10.1145/3196885", "10.1145/2407336.2407361", "10.1145/3084363.3085083", "10.1145/1477862.1477904", "10.1145/1836248.1836259", "10.1145/3106155", "10.1145/2338676.2338678", "10.1145/2720020", "10.1145/2492494.2492511", "10.1145/2077451.2077465", "10.1145/1857893.1857896", "10.1145/1140491.1140493", "10.1145/1836248.1836271", "10.1145/585740.585769", "10.1145/3190834.3190845", "10.1145/2543581.2543590", "10.1145/1643928.1643943", "10.1145/2628257.2628272", "10.1145/1460563.1460593", "10.1145/1190036.1190041", "10.1145/2804408.2804410", "10.1145/3119881.3119887", "10.1145/2804408.2804426", "10.1145/3196885", "10.1145/2407336.2407361", "10.1145/3084363.3085083", "10.1145/1477862.1477904", "10.1145/1836248.1836259", "10.1145/3106155", "10.1145/2338676.2338678", "10.1145/2720020", "10.1145/2492494.2492511", "10.1145/2077451.2077465", "10.1145/1857893.1857896", "10.1145/1140491.1140493", "10.1145/1836248.1836271", "10.1145/585740.585769", "10.1145/3190834.3190845", "10.1145/2543581.2543590", "10.1145/1643928.1643943", "10.1145/2628257.2628272", "10.1145/1460563.1460593", "10.1145/1190036.1190041", "10.1145/2804408.2804410", "10.1145/3119881.3119887", "10.1145/2804408.2804426", "10.1145/3196885", "10.1145/2407336.2407361", "10.1145/3084363.3085083", "10.1145/1477862.1477904", "10.1145/1836248.1836259", "10.1145/3106155", "10.1145/2338676.2338678", "10.1145/2720020", "10.1145/2492494.2492511", "10.1145/2077451.2077465", "10.1145/1857893.1857896", "10.1145/1140491.1140493", "10.1145/1836248.1836271", "10.1145/585740.585769", "10.1145/3190834.3190845", "10.1145/2543581.2543590", "10.1145/1643928.1643943", "10.1145/2628257.2628272", "10.1145/1460563.1460593", "10.1145/1190036.1190041", "10.1145/2804408.2804410", "10.1145/3119881.3119887", "10.1177/0146167203029007002", "10.1073/pnas.1306779110", "10.1068/p7618", "10.1038/35784", "10.1111/j.2044-8260.1978.tb00254.x", "10.3200/35-08-095", "10.1016/bs.plm.2014.09.006", "10.1068/p6712", "10.1038/srep05794", "10.1016/j.gaitpost.2007.03.015", "10.1123/mcj.9.3.242", "10.1007/BF01434994", "10.1525/aa.1963.65.5.02a00020", "10.1037/0033-2909.94.2.293", "10.1007/1-4020-3898-4_5", "10.1007/s00221-006-0525-4", "10.1007/978-3-319-91581-4_24", "10.1371/journal.pone.0111511", "10.1016/j.jenvp.2016.01.004", "10.3758/PBR.17.3.342", "10.1207/s15326969eco0404_2", "10.1068/p7545", "10.1007/s10055-009-0118-1", "10.1037/0096-1523.13.3.361", "10.3389/fnhum.2014.00693", "10.1080/00223980.1965.10544805", "10.1007/978-3-319-39907-2_6", "10.1037/0096-1523.24.4.1037", "10.1371/journal.pone.0189078", "10.1007/BF02173410", "10.1016/j.concog.2013.04.016", "10.1016/j.cortex.2017.08.033", "10.1123/mcj.9.2.129", "10.1007/s10055-012-0216-3", "10.1080/00224545.1995.9713958", "10.1037/0096-1523.33.4.845", "10.1007/s00426-016-0806-x", "10.1002/cav.1579", "10.1371/journal.pone.0010564", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3758/APP.72.5.1338", "10.1348/014466605X58384", "10.1037/0096-1523.10.5.683", "10.1037/0096-1523.13.3.371", "10.1089/cpb.2006.9984", "10.1037/0003-066X.35.2.151", "10.1177/0146167203029007002", "10.1073/pnas.1306779110", "10.1068/p7618", "10.1038/35784", "10.1111/j.2044-8260.1978.tb00254.x", "10.3200/35-08-095", "10.1016/bs.plm.2014.09.006", "10.1068/p6712", "10.1038/srep05794", "10.1016/j.gaitpost.2007.03.015", "10.1123/mcj.9.3.242", "10.1007/BF01434994", "10.1525/aa.1963.65.5.02a00020", "10.1037/0033-2909.94.2.293", "10.1007/1-4020-3898-4_5", "10.1007/s00221-006-0525-4", "10.1007/978-3-319-91581-4_24", "10.1371/journal.pone.0111511", "10.1016/j.jenvp.2016.01.004", "10.3758/PBR.17.3.342", "10.1207/s15326969eco0404_2", "10.1068/p7545", "10.1007/s10055-009-0118-1", "10.1037/0096-1523.13.3.361", "10.3389/fnhum.2014.00693", "10.1080/00223980.1965.10544805", "10.1007/978-3-319-39907-2_6", "10.1037/0096-1523.24.4.1037", "10.1371/journal.pone.0189078", "10.1007/BF02173410", "10.1016/j.concog.2013.04.016", "10.1016/j.cortex.2017.08.033", "10.1123/mcj.9.2.129", "10.1007/s10055-012-0216-3", "10.1080/00224545.1995.9713958", "10.1037/0096-1523.33.4.845", "10.1007/s00426-016-0806-x", "10.1002/cav.1579", "10.1371/journal.pone.0010564", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3758/APP.72.5.1338", "10.1348/014466605X58384", "10.1037/0096-1523.10.5.683", "10.1037/0096-1523.13.3.371", "10.1089/cpb.2006.9984", "10.1037/0003-066X.35.2.151", "10.1177/0146167203029007002", "10.1073/pnas.1306779110", "10.1068/p7618", "10.1038/35784", "10.1111/j.2044-8260.1978.tb00254.x", "10.3200/35-08-095", "10.1016/bs.plm.2014.09.006", "10.1068/p6712", "10.1038/srep05794", "10.1016/j.gaitpost.2007.03.015", "10.1123/mcj.9.3.242", "10.1007/BF01434994", "10.1525/aa.1963.65.5.02a00020", "10.1037/0033-2909.94.2.293", "10.1007/1-4020-3898-4_5", "10.1007/s00221-006-0525-4", "10.1007/978-3-319-91581-4_24", "10.1371/journal.pone.0111511", "10.1016/j.jenvp.2016.01.004", "10.3758/PBR.17.3.342", "10.1207/s15326969eco0404_2", "10.1068/p7545", "10.1007/s10055-009-0118-1", "10.1037/0096-1523.13.3.361", "10.3389/fnhum.2014.00693", "10.1080/00223980.1965.10544805", "10.1007/978-3-319-39907-2_6", "10.1037/0096-1523.24.4.1037", "10.1371/journal.pone.0189078", "10.1007/BF02173410", "10.1016/j.concog.2013.04.016", "10.1016/j.cortex.2017.08.033", "10.1123/mcj.9.2.129", "10.1007/s10055-012-0216-3", "10.1080/00224545.1995.9713958", "10.1037/0096-1523.33.4.845", "10.1007/s00426-016-0806-x", "10.1002/cav.1579", "10.1371/journal.pone.0010564", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.3758/APP.72.5.1338", "10.1348/014466605X58384", "10.1037/0096-1523.10.5.683", "10.1037/0096-1523.13.3.371", "10.1089/cpb.2006.9984", "10.1037/0003-066X.35.2.151"]}, "10.1109/TVCG.2019.2899250": {"doi": "10.1109/TVCG.2019.2899250", "author": ["M. E. Latoschik", "F. Kern", "J. -P. Stauffert", "A. Bartl", "M. Botsch", "J. -L. Lugrin"], "title": "Not Alone Here?! Scalability and User Experience of Embodied Ambient Crowds in Distributed Social Virtual Reality", "year": "2019", "abstract": "This article investigates performance and user experience in Social Virtual Reality (SVR) targeting distributed, embodied, and immersive, face-to-face encounters. We demonstrate the close relationship between scalability, reproduction accuracy, and the resulting performance characteristics, as well as the impact of these characteristics on users co-located with larger groups of embodied virtual others. System scalability provides a variable number of co-located avatars and Al-controlled agents with a variety of different appearances, including realistic-looking virtual humans generated from photogrammetry scans. The article reports on how to meet the requirements of embodied SVR with today's technical off-the-shelf solutions and what to expect regarding features, performance, and potential limitations. Special care has been taken to achieve low latencies and sufficient frame rates necessary for reliable communication of embodied social signals. We propose a hybrid evaluation approach which coherently relates results from technical benchmarks to subjective ratings and which confirms required performance characteristics for the target scenario of larger distributed groups. A user-study reveals positive effects of an increasing number of co-located social companions on the quality of experience of virtual worlds, i.e., on presence, possibility of interaction, and co-presence. It also shows that variety in avatar/agent appearance might increase eeriness but might also stimulate an increased interest of participants about the environment.", "keywords": ["avatars", "distributed processing", "user experience", "user experience", "embodied ambient crowds", "co-located avatars", "Al-controlled agents", "photogrammetry scans", "co-located social companions", "quality of experience", "virtual worlds", "distributed social virtual reality", "avatar appearance", "agent appearance", "realistic-looking virtual humans", "Avatars", "Open area test sites", "Scalability", "Animation", "Bandwidth", "Tracking", "Social Virtual Reality", "quality of experience", "performance characteristics", "co-location", "co-presence", "possibility of interaction", "ambient crowds", "avatars and agents", "computer-mediated communication", "multi-user virtual environment", "Adult", "Communication", "Computer Graphics", "Female", "Humans", "Male", "Photogrammetry", "Social Behavior", "User-Computer Interface", "Video Recording", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:8998353", "IKEY:8998141", "IKEY:9284661", "IKEY:9284667"], "referencing": ["IKEY:6797446", "IKEY:6479190", "IKEY:8446480", "IKEY:6797513", "IKEY:8013501", "IKEY:6777458", "IKEY:7893357", "IKEY:6797281", "IKEY:8446229", "IKEY:6790778", "IKEY:7504688", "IKEY:4811013", "IKEY:8447550", "IKEY:749116", "IKEY:8263407", "IKEY:6797446", "IKEY:6479190", "IKEY:8446480", "IKEY:6797513", "IKEY:8013501", "IKEY:6777458", "IKEY:7893357", "IKEY:6797281", "IKEY:8446229", "IKEY:6790778", "IKEY:7504688", "IKEY:4811013", "IKEY:8447550", "IKEY:749116", "IKEY:8263407", "IKEY:6797446", "IKEY:6479190", "IKEY:8446480", "IKEY:6797513", "IKEY:8013501", "IKEY:6777458", "IKEY:7893357", "IKEY:6797281", "IKEY:8446229", "IKEY:6790778", "IKEY:7504688", "IKEY:4811013", "IKEY:8447550", "IKEY:749116", "IKEY:8263407", "10.1145/3139131.3139154", "10.1145/91385.91409", "10.1145/2964284.2967303", "10.1145/882262.882350", "10.1145/1349822.1349845", "10.1145/1449956.1450012", "10.1145/2993369.2996308", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/3197391.3205451", "10.1145/280814.280861", "10.1145/2993369.2996302", "10.1145/3173574.3173863", "10.1145/1450579.1450606", "10.1145/2821592.2821607", "10.1145/3139131.3139154", "10.1145/91385.91409", "10.1145/2964284.2967303", "10.1145/882262.882350", "10.1145/1349822.1349845", "10.1145/1449956.1450012", "10.1145/2993369.2996308", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/3197391.3205451", "10.1145/280814.280861", "10.1145/2993369.2996302", "10.1145/3173574.3173863", "10.1145/1450579.1450606", "10.1145/2821592.2821607", "10.1145/3139131.3139154", "10.1145/91385.91409", "10.1145/2964284.2967303", "10.1145/882262.882350", "10.1145/1349822.1349845", "10.1145/1449956.1450012", "10.1145/2993369.2996308", "10.1145/2993369.2993399", "10.1145/3139131.3139156", "10.1145/3197391.3205451", "10.1145/280814.280861", "10.1145/2993369.2996302", "10.1145/3173574.3173863", "10.1145/1450579.1450606", "10.1145/2821592.2821607", "10.1111/j.1468-2958.2008.00322.x", "10.1007/s12369-016-0380-9", "10.1007/978-3-319-95282-6_47", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-24075-6_43", "10.3389/fict.2016.00026", "10.3389/fnhum.2013.00083", "10.1093/acprof:oso/9780195371284.001.0001", "10.3389/frobt.2014.00003", "10.3389/neuro.09.006.2008", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.1111/j.1468-2958.2007.00299.x", "10.1111/j.1468-2958.2008.00322.x", "10.1007/s12369-016-0380-9", "10.1007/978-3-319-95282-6_47", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-24075-6_43", "10.3389/fict.2016.00026", "10.3389/fnhum.2013.00083", "10.1093/acprof:oso/9780195371284.001.0001", "10.3389/frobt.2014.00003", "10.3389/neuro.09.006.2008", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.1111/j.1468-2958.2007.00299.x", "10.1111/j.1468-2958.2008.00322.x", "10.1007/s12369-016-0380-9", "10.1007/978-3-319-95282-6_47", "10.1207/s15327108ijap0303_3", "10.1007/978-3-319-24075-6_43", "10.3389/fict.2016.00026", "10.3389/fnhum.2013.00083", "10.1093/acprof:oso/9780195371284.001.0001", "10.3389/frobt.2014.00003", "10.3389/neuro.09.006.2008", "10.3389/frobt.2014.00009", "10.1007/978-3-319-10190-3_11", "10.1111/j.1468-2958.2007.00299.x"]}, "10.1109/TVCG.2019.2898796": {"doi": "10.1109/TVCG.2019.2898796", "author": ["J. Novotny", "J. Tveite", "M. L. Turner", "S. Gatesy", "F. Drury", "P. Falkingham", "D. H. Laidlaw"], "title": "Developing Virtual Reality Visualizations for Unsteady Flow Analysis of Dinosaur Track Formation using Scientific Sketching", "year": "2019", "abstract": "We present the results of a two-year design study to developing virtual reality (VR) flow visualization tools for the analysis of dinosaur track creation in a malleable substrate. Using Scientific Sketching methodology, we combined input from illustration artists, visualization experts, and domain scientists to create novel visualization methods. By iteratively improving visualization concepts at multiple levels of abstraction we helped domain scientists to gain insights into the relationship between dinosaur foot movements and substrate deformations. We involved over 20 art and computer science students from a VR design course in a rapid visualization sketching cycle, guided by our paleontologist collaborators through multiple critique sessions. This allowed us to explore a wide range of potential visualization methods and select the most promising methods for actual implementation. Our resulting visualization methods provide paleontologists with effective tools to analyze their data through particle, pathline and time surface visualizations. We also introduce a set of visual metaphors to compare foot motion in relation to substrate deformation by using pathsurfaces. This is one of the first large-scale projects using Scientific Sketching as a development methodology. We discuss how the research questions of our collaborators have evolved during the sketching and prototyping phases. Finally, we provide lessons learned and usage considerations for Scientific Sketching based on the experiences gathered during this project.", "keywords": ["data visualisation", "palaeontology", "virtual reality", "scientific sketching methodology", "dinosaur track creation", "pathsurfaces", "malleable substrate", "track creation", "virtual reality flow visualization tools", "dinosaur track formation", "unsteady flow analysis", "virtual reality visualizations", "visual metaphors", "time surface visualizations", "paleontologist collaborators", "rapid visualization sketching cycle", "VR design course", "substrate deformations", "dinosaur foot movements", "illustration artists", "Data visualization", "Visualization", "Dinosaurs", "Three-dimensional displays", "Tracking", "Foot", "Substrates", "Virtual Reality", "Scientific Visualization", "Flow Visualization", "Design Study", "Animals", "Art", "Computer Graphics", "Dinosaurs", "Humans", "Imaging, Three-Dimensional", "Locomotion", "Smart Glasses", "Virtual Reality"], "referenced_by": ["IKEY:8805462", "IKEY:8805418"], "referencing": ["IKEY:1608019", "IKEY:7192675", "IKEY:1703373", "IKEY:4658170", "IKEY:4447665", "IKEY:6802065", "IKEY:5290738", "IKEY:6777465", "IKEY:6231627", "IKEY:6261311", "IKEY:1260740", "IKEY:567777", "IKEY:1608019", "IKEY:7192675", "IKEY:1703373", "IKEY:4658170", "IKEY:4447665", "IKEY:6802065", "IKEY:5290738", "IKEY:6777465", "IKEY:6231627", "IKEY:6261311", "IKEY:1260740", "IKEY:567777", "IKEY:1608019", "IKEY:7192675", "IKEY:1703373", "IKEY:4658170", "IKEY:4447665", "IKEY:6802065", "IKEY:5290738", "IKEY:6777465", "IKEY:6231627", "IKEY:6261311", "IKEY:1260740", "IKEY:567777", "10.1145/1268517.1268564", "10.1145/1315184.1315205", "10.1145/1268517.1268564", "10.1145/1315184.1315205", "10.1145/1268517.1268564", "10.1145/1315184.1315205", "10.1144/gsjgs.146.4.0600", "10.1016/j.compbiomed.2013.10.004", "10.1073/pnas.1416252111", "10.1002/jez.588", "10.1038/20167", "10.1002/9781118062142.ch94", "10.1016/j.apergo.2015.06.024", "10.1111/j.1467-8659.2010.01650.x", "10.1016/j.palaeo.2004.12.022", "10.1088/0965-0393/18/1/015012", "10.1177/107769589705200202", "10.1016/S0097-8493(02)00113-9", "10.1144/gsjgs.146.4.0600", "10.1016/j.compbiomed.2013.10.004", "10.1073/pnas.1416252111", "10.1002/jez.588", "10.1038/20167", "10.1002/9781118062142.ch94", "10.1016/j.apergo.2015.06.024", "10.1111/j.1467-8659.2010.01650.x", "10.1016/j.palaeo.2004.12.022", "10.1088/0965-0393/18/1/015012", "10.1177/107769589705200202", "10.1016/S0097-8493(02)00113-9", "10.1144/gsjgs.146.4.0600", "10.1016/j.compbiomed.2013.10.004", "10.1073/pnas.1416252111", "10.1002/jez.588", "10.1038/20167", "10.1002/9781118062142.ch94", "10.1016/j.apergo.2015.06.024", "10.1111/j.1467-8659.2010.01650.x", "10.1016/j.palaeo.2004.12.022", "10.1088/0965-0393/18/1/015012", "10.1177/107769589705200202", "10.1016/S0097-8493(02)00113-9"]}}