{"10.1109/TVCG.2017.2676777": {"doi": "10.1109/TVCG.2017.2676777", "author": ["J. Barreira", "M. Bessa", "L. Barbosa", "L. Magalh\u00e3es"], "title": "A Context-Aware Method for Authentically Simulating Outdoors Shadows for Mobile Augmented Reality", "year": "2018", "abstract": "Visual coherence between virtual and real objects is a major issue in creating convincing augmented reality (AR) applications. To achieve this seamless integration, actual light conditions must be determined in real time to ensure that virtual objects are correctly illuminated and cast consistent shadows. In this paper, we propose a novel method to estimate daylight illumination and use this information in outdoor AR applications to render virtual objects with coherent shadows. The illumination parameters are acquired in real time from context-aware live sensor data. The method works under unprepared natural conditions. We also present a novel and rapid implementation of a state-of-the-art skylight model, from which the illumination parameters are derived. The Sun's position is calculated based on the user location and time of day, with the relative rotational differences estimated from a gyroscope, compass and accelerometer. The results illustrated that our method can generate visually credible AR scenes with consistent shadows rendered from recovered illumination.", "keywords": ["augmented reality", "mobile computing", "rendering (computer graphics)", "context-aware live sensor data", "natural conditions", "virtual object rendering", "augmented reality applications", "skylight model", "gyroscope", "compass", "accelerometer", "coherent shadows", "daylight illumination", "actual light conditions", "seamless integration", "visual coherence", "mobile augmented reality", "outdoors shadows", "context-aware method", "recovered illumination", "visually credible AR scenes", "illumination parameters", "Lighting", "Sun", "Augmented reality", "Rendering (computer graphics)", "Cameras", "Context awareness", "Augmented reality", "context-awareness", "shadows coherence", "photometric registration"], "referenced_by": ["IKEY:8442093", "IKEY:8697999"], "referencing": []}, "10.1109/TVCG.2017.2665498": {"doi": "10.1109/TVCG.2017.2665498", "author": ["R. Jianu", "S. S. Alam"], "title": "A Data Model and Task Space for Data of Interest (DOI) Eye-Tracking Analyses", "year": "2018", "abstract": "Eye-tracking data is traditionally analyzed by looking at where on a visual stimulus subjects fixate, or, to facilitate more advanced analyses, by using area-of-interests (AOI) defined onto visual stimuli. Recently, there is increasing interest in methods that capture what users are looking at rather than where they are looking. By instrumenting visualization code that transforms a data model into visual content, gaze coordinates reported by an eye-tracker can be mapped directly to granular data shown on the screen, producing temporal sequences of data objects that subjects viewed in an experiment. Such data collection, which is called gaze to object mapping (GTOM) or data-of-interest analysis (DOI), can be done reliably with limited overhead and can facilitate research workflows not previously possible. Our paper contributes to establishing a foundation of DOI analyses by defining a DOI data model and highlighting its differences to AOI data in structure and scale; by defining and exemplifying a space of DOI enabled tasks; by describing three concrete examples of DOI experimentation in three different domains; and by discussing immediate research challenges in creating a framework of visual support for DOI experimentation and analysis.", "keywords": ["data models", "data visualisation", "eye", "gaze tracking", "area-of-interests", "visual stimuli", "visualization code", "visual content", "gaze coordinates", "granular data", "temporal sequences", "data objects", "data collection", "data-of-interest analysis", "DOI data model", "AOI data", "DOI experimentation", "task space", "visual stimulus subjects", "eye-tracking analysis", "data model", "Data visualization", "Visualization", "Three-dimensional displays", "Semantics", "Data models", "Taxonomy", "Context", "Eye-tracking", "taxonomies", "visual analysis models", "Computer-Assisted Instruction", "Data Analysis", "Eye Movements", "Fixation, Ocular", "Humans", "Image Processing, Computer-Assisted", "Models, Theoretical", "Visual Perception"], "referenced_by": ["IKEY:8438455", "IKEY:9036879", "IKEY:9215978"], "referencing": []}, "10.1109/TVCG.2017.2666150": {"doi": "10.1109/TVCG.2017.2666150", "author": ["C. Schissler", "C. Loftin", "D. Manocha"], "title": "Acoustic Classification and Optimization for Multi-Modal Rendering of Real-World Scenes", "year": "2018", "abstract": "We present a novel algorithm to generate virtual acoustic effects in captured 3D models of real-world scenes for multimodal augmented reality. We leverage recent advances in 3D scene reconstruction in order to automatically compute acoustic material properties. Our technique consists of a two-step procedure that first applies a convolutional neural network (CNN) to estimate the acoustic material properties, including frequency-dependent absorption coefficients, that are used for interactive sound propagation. In the second step, an iterative optimization algorithm is used to adjust the materials determined by the CNN until a virtual acoustic simulation converges to measured acoustic impulse responses. We have applied our algorithm to many reconstructed real-world indoor scenes and evaluated its fidelity for augmented reality applications.", "keywords": ["acoustic imaging", "acoustic wave propagation", "augmented reality", "convolution", "feedforward neural nets", "image classification", "image reconstruction", "iterative methods", "optimisation", "rendering (computer graphics)", "solid modelling", "measured acoustic impulse responses", "virtual acoustic simulation converges", "iterative optimization algorithm", "frequency-dependent absorption coefficients", "CNN", "convolutional neural network", "acoustic material properties", "3D scene reconstruction", "multimodal augmented reality", "captured 3D models", "virtual acoustic effects", "multimodal rendering", "augmented reality applications", "real-world indoor scenes", "Three-dimensional displays", "Acoustics", "Computational modeling", "Acoustic materials", "Acoustic measurements", "Solid modeling", "Image reconstruction", "Sound propagation", "material optimization", "recognition"], "referenced_by": ["IKEY:8521256", "IKEY:8798247", "IKEY:9025860", "IKEY:8998301", "IKEY:9053873", "IKEY:9090553", "IKEY:9196743"], "referencing": ["IKEY:5165582", "IKEY:844375", "IKEY:6162880", "IKEY:6205760", "IKEY:1240986", "IKEY:4155795", "IKEY:1168980", "IKEY:5165582", "IKEY:844375", "IKEY:6162880", "IKEY:6205760", "IKEY:1240986", "IKEY:4155795", "IKEY:1168980", "IKEY:5165582", "IKEY:844375", "IKEY:6162880", "IKEY:6205760", "IKEY:1240986", "IKEY:4155795", "IKEY:1168980", "10.1145/2451236.2451245", "10.1145/1778765.1778805", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2647868.2654889", "10.1145/2451236.2451245", "10.1145/1778765.1778805", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2647868.2654889", "10.1145/2451236.2451245", "10.1145/1778765.1778805", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/2601097.2601216", "10.1145/383259.383323", "10.1145/2647868.2654889", "10.1007/978-1-84882-733-2_8", "10.1121/1.2164987", "10.1121/1.390983", "10.1121/1.398336", "10.1121/1.428489", "10.1121/1.428071", "10.1121/1.4788035", "10.1121/1.4915063", "10.1007/s41095-015-0029-x", "10.1016/S0031-3203(97)00074-5", "10.1007/978-3-642-37484-5_9", "10.1007/978-3-540-73040-8_40", "10.3813/AAA.918049", "10.5244/C.25.48", "10.1007/978-1-84882-733-2_8", "10.1121/1.2164987", "10.1121/1.390983", "10.1121/1.398336", "10.1121/1.428489", "10.1121/1.428071", "10.1121/1.4788035", "10.1121/1.4915063", "10.1007/s41095-015-0029-x", "10.1016/S0031-3203(97)00074-5", "10.1007/978-3-642-37484-5_9", "10.1007/978-3-540-73040-8_40", "10.3813/AAA.918049", "10.5244/C.25.48", "10.1007/978-1-84882-733-2_8", "10.1121/1.2164987", "10.1121/1.390983", "10.1121/1.398336", "10.1121/1.428489", "10.1121/1.428071", "10.1121/1.4788035", "10.1121/1.4915063", "10.1007/s41095-015-0029-x", "10.1016/S0031-3203(97)00074-5", "10.1007/978-3-642-37484-5_9", "10.1007/978-3-540-73040-8_40", "10.3813/AAA.918049", "10.5244/C.25.48"]}, "10.1109/TVCG.2017.2665551": {"doi": "10.1109/TVCG.2017.2665551", "author": ["X. Liao", "W. Si", "Z. Yuan", "H. Sun", "J. Qin", "Q. Wang", "P. Heng"], "title": "Animating Wall-Bounded Turbulent Smoke via Filament-Mesh Particle-Particle Method", "year": "2018", "abstract": "Turbulent vortices in smoke flows are crucial for a visually interesting appearance. Unfortunately, it is challenging to efficiently simulate these appealing effects in the framework of vortex filament methods. The vortex filaments in grids scheme allows to efficiently generate turbulent smoke with macroscopic vortical structures, but suffers from the projection-related dissipation, and thus the small-scale vortical structures under grid resolution are hard to capture. In addition, this scheme cannot be applied in wall-bounded turbulent smoke simulation, which requires efficiently handling smoke-obstacle interaction and creating vorticity at the obstacle boundary. To tackle above issues, we propose an effective filament-mesh particle-particle (FMPP) method for fast wall-bounded turbulent smoke simulation with ample details. The Filament-Mesh component approximates the smooth long-range interactions by splatting vortex filaments on grid, solving the Poisson problem with a fast solver, and then interpolating back to smoke particles. The Particle-Particle component introduces smoothed particle hydrodynamics (SPH) turbulence model for particles in the same grid, where interactions between particles cannot be properly captured under grid resolution. Then, we sample the surface of obstacles with boundary particles, allowing the interaction between smoke and obstacle being treated as pressure forces in SPH. Besides, the vortex formation region is defined at the back of obstacles, providing smoke particles flowing by the separation particles with a vorticity force to simulate the subsequent vortex shedding phenomenon. The proposed approach can synthesize the lost small-scale vortical structures and also achieve the smoke-obstacle interaction with vortex shedding at obstacle boundaries in a lightweight manner. The experimental results demonstrate that our FMPP method can achieve more appealing visual effects than vortex filaments in grids scheme by efficiently simulating more vivid thin turbulent features.", "keywords": ["boundary layer turbulence", "computational fluid dynamics", "flow simulation", "mesh generation", "smoke", "smoothed particle hydrodynamics", "stochastic processes", "two-phase flow", "vortices", "Poisson problem", "smoothed particle hydrodynamics turbulence model", "pressure forces", "vortex shedding", "smooth long-range interactions", "wall-bounded turbulent smoke simulation", "projection-related dissipation", "filament-mesh particle-particle method", "vorticity", "FMPP method", "subsequent vortex shedding phenomenon", "vorticity force", "separation particles", "vortex formation region", "boundary particles", "Particle-Particle component", "smoke particles", "long-range interactions", "Filament-Mesh component", "obstacle boundary", "smoke-obstacle interaction", "grid resolution", "small-scale vortical structures", "macroscopic vortical structures", "vortex filament methods", "smoke flows", "turbulent vortices", "grids scheme", "Computational modeling", "Solids", "Mathematical model", "Solid modeling", "Numerical models", "Boundary conditions", "Adaptation models", "Wall-bounded turbulent smoke", "fluid-solid Interaction", "vortex shedding"], "referenced_by": [], "referencing": ["IKEY:6171149", "IKEY:6171149", "IKEY:6171149", "10.1145/2661229.2661261", "10.1145/258734.258838", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1073204.1073282", "10.1145/1618452.1618467", "10.1145/1618452.1618466", "10.1145/2185520.2335463", "10.1145/1073368.1073380", "10.1145/1882261.1866196", "10.1145/1073368.1073406", "10.1145/2366145.2366167", "10.1145/1276377.1276502", "10.1145/1360612.1360645", "10.1145/1015706.1015733", "10.1145/1073204.1073299", "10.1145/1073368.1073400", "10.1145/2185520.2335413", "10.1145/2461912.2461984", "10.1145/1731047.1731054", "10.1145/2661229.2661261", "10.1145/258734.258838", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1073204.1073282", "10.1145/1618452.1618467", "10.1145/1618452.1618466", "10.1145/2185520.2335463", "10.1145/1073368.1073380", "10.1145/1882261.1866196", "10.1145/1073368.1073406", "10.1145/2366145.2366167", "10.1145/1276377.1276502", "10.1145/1360612.1360645", "10.1145/1015706.1015733", "10.1145/1073204.1073299", "10.1145/1073368.1073400", "10.1145/2185520.2335413", "10.1145/2461912.2461984", "10.1145/1731047.1731054", "10.1145/2661229.2661261", "10.1145/258734.258838", "10.1145/311535.311548", "10.1145/383259.383260", "10.1145/1073204.1073282", "10.1145/1618452.1618467", "10.1145/1618452.1618466", "10.1145/2185520.2335463", "10.1145/1073368.1073380", "10.1145/1882261.1866196", "10.1145/1073368.1073406", "10.1145/2366145.2366167", "10.1145/1276377.1276502", "10.1145/1360612.1360645", "10.1145/1015706.1015733", "10.1145/1073204.1073299", "10.1145/1073368.1073400", "10.1145/2185520.2335413", "10.1145/2461912.2461984", "10.1145/1731047.1731054", "10.1016/j.euromechflu.2011.04.002", "10.1016/0021-9991(90)90001-H", "10.1073/pnas.0604159103", "10.1063/1.3081559", "10.1111/j.1467-8659.2012.03195.x", "10.1006/gmip.1996.0039", "10.1002/cav.18", "10.1002/cav.1499", "10.1017/CBO9780511526442", "10.1146/annurev.aa.30.090192.002551", "10.1063/1.1706053", "10.1016/0021-9991(87)90140-9", "10.1016/j.compfluid.2012.08.002", "10.1016/j.cpc.2012.09.011", "10.1017/S0022112073002016", "10.1088/1468-5248/1/1/011", "10.1002/nme.3036", "10.1002/jcc.540100313", "10.1006/jcph.1994.1006", "10.1006/jcph.1998.5982", "10.1016/j.euromechflu.2011.04.002", "10.1016/0021-9991(90)90001-H", "10.1073/pnas.0604159103", "10.1063/1.3081559", "10.1111/j.1467-8659.2012.03195.x", "10.1006/gmip.1996.0039", "10.1002/cav.18", "10.1002/cav.1499", "10.1017/CBO9780511526442", "10.1146/annurev.aa.30.090192.002551", "10.1063/1.1706053", "10.1016/0021-9991(87)90140-9", "10.1016/j.compfluid.2012.08.002", "10.1016/j.cpc.2012.09.011", "10.1017/S0022112073002016", "10.1088/1468-5248/1/1/011", "10.1002/nme.3036", "10.1002/jcc.540100313", "10.1006/jcph.1994.1006", "10.1006/jcph.1998.5982", "10.1016/j.euromechflu.2011.04.002", "10.1016/0021-9991(90)90001-H", "10.1073/pnas.0604159103", "10.1063/1.3081559", "10.1111/j.1467-8659.2012.03195.x", "10.1006/gmip.1996.0039", "10.1002/cav.18", "10.1002/cav.1499", "10.1017/CBO9780511526442", "10.1146/annurev.aa.30.090192.002551", "10.1063/1.1706053", "10.1016/0021-9991(87)90140-9", "10.1016/j.compfluid.2012.08.002", "10.1016/j.cpc.2012.09.011", "10.1017/S0022112073002016", "10.1088/1468-5248/1/1/011", "10.1002/nme.3036", "10.1002/jcc.540100313", "10.1006/jcph.1994.1006", "10.1006/jcph.1998.5982"]}, "10.1109/TVCG.2017.2659744": {"doi": "10.1109/TVCG.2017.2659744", "author": ["J. Harper", "M. Agrawala"], "title": "Converting Basic D3 Charts into Reusable Style Templates", "year": "2018", "abstract": "We present a technique for converting a basic D3 chart into a reusable style template. Then, given a new data source we can apply the style template to generate a chart that depicts the new data, but in the style of the template. To construct the style template we first deconstruct the input D3 chart to recover its underlying structure: the data, the marks and the mappings that describe how the marks encode the data. We then rank the perceptual effectiveness of the deconstructed mappings. To apply the resulting style template to a new data source we first obtain importance ranks for each new data field. We then adjust the template mappings to depict the source data by matching the most important data fields to the most perceptually effective mappings. We show how the style templates can be applied to source data in the form of either a data table or another D3 chart. While our implementation focuses on generating templates for basic chart types (e.g., variants of bar charts, line charts, dot plots, scatterplots, etc.), these are the most commonly used chart types today. Users can easily find such basic D3 charts on the Web, turn them into templates, and immediately see how their own data would look in the visual style (e.g., colors, shapes, fonts, etc.) of the templates. We demonstrate the effectiveness of our approach by applying a diverse set of style templates to a variety of source datasets.", "keywords": ["data analysis", "data visualisation", "encoding", "reusable style template", "data source", "data field", "template mappings", "data table", "basic chart types", "bar charts", "line charts", "visual style", "Basic D3 Charts conversion", "data encoding", "visualization tools", "Visualization", "Shape", "Bars", "Image color analysis", "Encoding", "Data visualization", "Electronic mail", "Chart restyling", "reusable style templates", "declarative representation", "D3 deconstruction", "vega-lite"], "referenced_by": ["IKEY:8809832", "10.1016/j.jvlc.2017.10.001"], "referencing": ["IKEY:4376133", "IKEY:6064996", "IKEY:7192728", "IKEY:1382905", "IKEY:981851", "IKEY:6327269", "IKEY:946629", "IKEY:7332976", "IKEY:7539624", "IKEY:7536218", "IKEY:7539580", "IKEY:4376133", "IKEY:6064996", "IKEY:7192728", "IKEY:1382905", "IKEY:981851", "IKEY:6327269", "IKEY:946629", "IKEY:7332976", "IKEY:7539624", "IKEY:7536218", "IKEY:7539580", "IKEY:4376133", "IKEY:6064996", "IKEY:7192728", "IKEY:1382905", "IKEY:981851", "IKEY:6327269", "IKEY:946629", "IKEY:7332976", "IKEY:7539624", "IKEY:7536218", "IKEY:7539580", "10.1145/22949.22950", "10.1145/1753326.1753667", "10.1145/2642918.2647411", "10.1145/2047196.2047247", "10.1145/2556288.2557241", "10.1145/383259.383296", "10.1145/383259.383295", "10.1145/1618452.1618492", "10.1145/1778765.1778862", "10.1145/1978942.1979262", "10.1145/2858036.2858435", "10.1145/2501988.2502046", "10.1145/22949.22950", "10.1145/1753326.1753667", "10.1145/2642918.2647411", "10.1145/2047196.2047247", "10.1145/2556288.2557241", "10.1145/383259.383296", "10.1145/383259.383295", "10.1145/1618452.1618492", "10.1145/1778765.1778862", "10.1145/1978942.1979262", "10.1145/2858036.2858435", "10.1145/2501988.2502046", "10.1145/22949.22950", "10.1145/1753326.1753667", "10.1145/2642918.2647411", "10.1145/2047196.2047247", "10.1145/2556288.2557241", "10.1145/383259.383296", "10.1145/383259.383295", "10.1145/1618452.1618492", "10.1145/1778765.1778862", "10.1145/1978942.1979262", "10.1145/2858036.2858435", "10.1145/2501988.2502046", "10.1111/cgf.12391", "10.1007/11669487_29", "10.1111/cgf.12391", "10.1007/11669487_29", "10.1111/cgf.12391", "10.1007/11669487_29"]}, "10.1109/TVCG.2017.2666146": {"doi": "10.1109/TVCG.2017.2666146", "author": ["S. Kim", "S. Jeong", "I. Woo", "Y. Jang", "R. Maciejewski", "D. S. Ebert"], "title": "Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information", "year": "2018", "abstract": "Geographic visualization research has focused on a variety of techniques to represent and explore spatiotemporal data. The goal of those techniques is to enable users to explore events and interactions over space and time in order to facilitate the discovery of patterns, anomalies and relationships within the data. However, it is difficult to extract and visualize data flow patterns over time for non-directional statistical data without trajectory information. In this work, we develop a novel flow analysis technique to extract, represent, and analyze flow maps of non-directional spatiotemporal data unaccompanied by trajectory information. We estimate a continuous distribution of these events over space and time, and extract flow fields for spatial and temporal changes utilizing a gravity model. Then, we visualize the spatiotemporal patterns in the data by employing flow visualization techniques. The user is presented with temporal trends of geo-referenced discrete events on a map. As such, overall spatiotemporal data flow patterns help users analyze geo-referenced temporal events, such as disease outbreaks, crime patterns, etc. To validate our model, we discard the trajectory information in an origin-destination dataset and apply our technique to the data and compare the derived trajectories and the original. Finally, we present spatiotemporal trend analysis for statistical datasets including twitter data, maritime search and rescue events, and syndromic surveillance.", "keywords": ["cartography", "data flow analysis", "data visualisation", "diseases", "pattern clustering", "social networking (online)", "spatiotemporal phenomena", "statistical analysis", "spatiotemporal trend analysis", "statistical datasets", "twitter data", "maritime search", "rescue events", "data flow analysis", "spatiotemporal statistical data", "trajectory information", "geographic visualization research", "nondirectional statistical data", "novel flow analysis technique", "nondirectional spatiotemporal data", "flow fields", "spatial changes", "temporal changes", "flow visualization techniques", "discrete events", "spatiotemporal data flow patterns", "data visualization", "flow map analysis", "data spatiotemporal patterns", "geo-referenced temporal events", "syndromic surveillance", "geographic flow maps", "Data visualization", "Spatiotemporal phenomena", "Gravity", "Trajectory", "Data mining", "Market research", "Heating", "Spatiotemporal data visualization", "kernel density estimation", "flow map", "gravity model"], "referenced_by": ["IKEY:8320847", "IKEY:8752446", "IKEY:8846208", "IKEY:8854790", "IKEY:8809223", "IKEY:8807296"], "referencing": ["IKEY:5432167", "IKEY:6065021", "IKEY:4475461", "IKEY:5290732", "IKEY:1320137", "IKEY:6875983", "IKEY:4577974", "IKEY:1512024", "IKEY:1359732", "IKEY:4909115", "IKEY:5226628", "IKEY:6102460", "IKEY:1509075", "IKEY:601035", "IKEY:5432167", "IKEY:6065021", "IKEY:4475461", "IKEY:5290732", "IKEY:1320137", "IKEY:6875983", "IKEY:4577974", "IKEY:1512024", "IKEY:1359732", "IKEY:4909115", "IKEY:5226628", "IKEY:6102460", "IKEY:1509075", "IKEY:601035", "IKEY:5432167", "IKEY:6065021", "IKEY:4475461", "IKEY:5290732", "IKEY:1320137", "IKEY:6875983", "IKEY:4577974", "IKEY:1512024", "IKEY:1359732", "IKEY:4909115", "IKEY:5226628", "IKEY:6102460", "IKEY:1509075", "IKEY:601035", "10.1145/166117.166151", "10.1145/585147.585174", "10.1145/2254556.2254597", "10.1145/2063212.2063216", "10.1145/984952.984987", "10.1145/237170.237285", "10.1145/1526709.1526816", "10.1145/166117.166151", "10.1145/585147.585174", "10.1145/2254556.2254597", "10.1145/2063212.2063216", "10.1145/984952.984987", "10.1145/237170.237285", "10.1145/1526709.1526816", "10.1145/166117.166151", "10.1145/585147.585174", "10.1145/2254556.2254597", "10.1145/2063212.2063216", "10.1145/984952.984987", "10.1145/237170.237285", "10.1145/1526709.1526816", "10.3386/w16576", "10.3390/ijerph9124346", "10.2307/1925976", "10.1007/BF01936872", "10.1080/000368400421093", "10.1080/19942060.2008.11015227", "10.1111/j.1467-8659.2004.00753.x", "10.1179/000870487787859048", "10.1016/j.econlet.2007.06.019", "10.1007/978-3-642-15300-6_21", "10.3390/ijerph8083134", "10.1111/j.1467-9671.2010.01194.x", "10.1057/palgrave.ivs.9500007", "10.1111/j.1540-5982.2009.01564.x", "10.1007/978-1-4899-3324-9", "10.2307/143141", "10.1559/152304087783875273", "10.1371/journal.pcbi.1002699", "10.3386/w16576", "10.3390/ijerph9124346", "10.2307/1925976", "10.1007/BF01936872", "10.1080/000368400421093", "10.1080/19942060.2008.11015227", "10.1111/j.1467-8659.2004.00753.x", "10.1179/000870487787859048", "10.1016/j.econlet.2007.06.019", "10.1007/978-3-642-15300-6_21", "10.3390/ijerph8083134", "10.1111/j.1467-9671.2010.01194.x", "10.1057/palgrave.ivs.9500007", "10.1111/j.1540-5982.2009.01564.x", "10.1007/978-1-4899-3324-9", "10.2307/143141", "10.1559/152304087783875273", "10.1371/journal.pcbi.1002699", "10.3386/w16576", "10.3390/ijerph9124346", "10.2307/1925976", "10.1007/BF01936872", "10.1080/000368400421093", "10.1080/19942060.2008.11015227", "10.1111/j.1467-8659.2004.00753.x", "10.1179/000870487787859048", "10.1016/j.econlet.2007.06.019", "10.1007/978-3-642-15300-6_21", "10.3390/ijerph8083134", "10.1111/j.1467-9671.2010.01194.x", "10.1057/palgrave.ivs.9500007", "10.1111/j.1540-5982.2009.01564.x", "10.1007/978-1-4899-3324-9", "10.2307/143141", "10.1559/152304087783875273", "10.1371/journal.pcbi.1002699"]}, "10.1109/TVCG.2017.2661309": {"doi": "10.1109/TVCG.2017.2661309", "author": ["H. Nguyen", "P. Rosen"], "title": "DSPCP: A Data Scalable Approach for Identifying Relationships in Parallel Coordinates", "year": "2018", "abstract": "Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point/line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.", "keywords": ["computational geometry", "data analysis", "data structures", "data visualisation", "data scalable approach", "identifying relationships", "multiattribute datasets", "data items", "data scalable design", "representing exploring data relationships", "relationship summarizations", "geometry-based approaches", "nonlinear pattern identification", "mixed linear pattern identification", "visualization", "parallel coordinates plots", "Market research", "Data visualization", "Correlation", "Visualization", "Shape", "Histograms", "Encoding", "Correlation", "parallel coordinates plot", "large data visualization"], "referenced_by": ["IKEY:8019881"], "referencing": ["IKEY:6064998", "IKEY:146402", "IKEY:4376168", "IKEY:5613448", "IKEY:4015444", "IKEY:5290705", "IKEY:4658160", "IKEY:5613439", "IKEY:6065025", "IKEY:4015422", "IKEY:5290770", "IKEY:5613469", "IKEY:4658159", "IKEY:6064954", "IKEY:346302", "IKEY:4459449", "IKEY:663916", "IKEY:5190784", "IKEY:1017616", "IKEY:4015444", "IKEY:6064954", "IKEY:6875978", "IKEY:6064998", "IKEY:146402", "IKEY:4376168", "IKEY:5613448", "IKEY:4015444", "IKEY:5290705", "IKEY:4658160", "IKEY:5613439", "IKEY:6065025", "IKEY:4015422", "IKEY:5290770", "IKEY:5613469", "IKEY:4658159", "IKEY:6064954", "IKEY:346302", "IKEY:4459449", "IKEY:663916", "IKEY:5190784", "IKEY:1017616", "IKEY:4015444", "IKEY:6064954", "IKEY:6875978", "IKEY:6064998", "IKEY:146402", "IKEY:4376168", "IKEY:5613448", "IKEY:4015444", "IKEY:5290705", "IKEY:4658160", "IKEY:5613439", "IKEY:6065025", "IKEY:4015422", "IKEY:5290770", "IKEY:5613469", "IKEY:4658159", "IKEY:6064954", "IKEY:346302", "IKEY:4459449", "IKEY:663916", "IKEY:5190784", "IKEY:1017616", "IKEY:4015444", "IKEY:6064954", "IKEY:6875978", "10.1145/502512.502530", "10.1145/502512.502530", "10.1145/502512.502530", "10.1111/j.1467-8659.2012.03094.x", "10.1111/cgf.12641", "10.1007/BF01898350", "10.1007/978-0-387-68628-8", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2008.01239.x", "10.1057/palgrave.ivs.9500117", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-1-4613-9154-8_2", "10.1080/01621459.1990.10474926", "10.1198/jcgs.2009.0003", "10.1111/j.1467-8659.2011.01914.x", "10.1111/j.1467-8659.2011.01913.x", "10.1098/rspl.1895.0041", "10.1057/ivs.2008.13", "10.1111/cgf.12845", "10.1007/978-1-4757-1904-8", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1111/cgf.12901", "10.1111/j.1467-8659.2012.03094.x", "10.1111/cgf.12641", "10.1007/BF01898350", "10.1007/978-0-387-68628-8", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2008.01239.x", "10.1057/palgrave.ivs.9500117", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-1-4613-9154-8_2", "10.1080/01621459.1990.10474926", "10.1198/jcgs.2009.0003", "10.1111/j.1467-8659.2011.01914.x", "10.1111/j.1467-8659.2011.01913.x", "10.1098/rspl.1895.0041", "10.1057/ivs.2008.13", "10.1111/cgf.12845", "10.1007/978-1-4757-1904-8", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1111/cgf.12901", "10.1111/j.1467-8659.2012.03094.x", "10.1111/cgf.12641", "10.1007/BF01898350", "10.1007/978-0-387-68628-8", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2008.01239.x", "10.1057/palgrave.ivs.9500117", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-1-4613-9154-8_2", "10.1080/01621459.1990.10474926", "10.1198/jcgs.2009.0003", "10.1111/j.1467-8659.2011.01914.x", "10.1111/j.1467-8659.2011.01913.x", "10.1098/rspl.1895.0041", "10.1057/ivs.2008.13", "10.1111/cgf.12845", "10.1007/978-1-4757-1904-8", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1111/cgf.12901"]}, "10.1109/TVCG.2017.2680452": {"doi": "10.1109/TVCG.2017.2680452", "author": ["B. Saket", "A. Srinivasan", "E. D. Ragan", "A. Endert"], "title": "Evaluating Interactive Graphical Encodings for Data Visualization", "year": "2018", "abstract": "User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.", "keywords": ["data visualisation", "interactive systems", "user interfaces", "data visualization", "user interfaces", "user interaction", "visual representation", "visualization parameters", "embedded interactions", "interactive graphical encodings", "Encoding", "Data visualization", "Bars", "Visualization", "Computational modeling", "Estimation", "Information visualization", "user interaction", "graphical encodings", "graphical perception"], "referenced_by": ["IKEY:8440855", "IKEY:8354901", "IKEY:8805424", "IKEY:8794768", "IKEY:8809678", "10.1142/S0218001418500167"], "referencing": ["IKEY:329404", "IKEY:7539327", "IKEY:6102449", "IKEY:7534876", "IKEY:6875985", "IKEY:6327275", "IKEY:6327257", "IKEY:4376144", "IKEY:4376132", "IKEY:6562729", "IKEY:8160423", "IKEY:6064996", "IKEY:7536218", "IKEY:6327267", "IKEY:329404", "IKEY:7539327", "IKEY:6102449", "IKEY:7534876", "IKEY:6875985", "IKEY:6327275", "IKEY:6327257", "IKEY:4376144", "IKEY:4376132", "IKEY:6562729", "IKEY:8160423", "IKEY:6064996", "IKEY:7536218", "IKEY:6327267", "IKEY:329404", "IKEY:7539327", "IKEY:6102449", "IKEY:7534876", "IKEY:6875985", "IKEY:6327275", "IKEY:6327257", "IKEY:4376144", "IKEY:4376132", "IKEY:6562729", "IKEY:8160423", "IKEY:6064996", "IKEY:7536218", "IKEY:6327267", "10.1145/2207676.2207741", "10.1145/956863.956874", "10.1145/1753326.1753357", "10.1145/365024.365028", "10.1145/142750.142794", "10.1145/1357054.1357203", "10.1145/133160.133216", "10.1145/245882.245893", "10.1145/253671.253708", "10.1145/332040.332473", "10.1145/2556288.2557231", "10.1145/22949.22950", "10.1145/1978942.1979033", "10.1145/2858036.2858063", "10.1145/2207676.2207741", "10.1145/956863.956874", "10.1145/1753326.1753357", "10.1145/365024.365028", "10.1145/142750.142794", "10.1145/1357054.1357203", "10.1145/133160.133216", "10.1145/245882.245893", "10.1145/253671.253708", "10.1145/332040.332473", "10.1145/2556288.2557231", "10.1145/22949.22950", "10.1145/1978942.1979033", "10.1145/2858036.2858063", "10.1145/2207676.2207741", "10.1145/956863.956874", "10.1145/1753326.1753357", "10.1145/365024.365028", "10.1145/142750.142794", "10.1145/1357054.1357203", "10.1145/133160.133216", "10.1145/245882.245893", "10.1145/253671.253708", "10.1145/332040.332473", "10.1145/2556288.2557231", "10.1145/22949.22950", "10.1145/1978942.1979033", "10.1145/2858036.2858063", "10.2307/2288400", "10.1080/01621459.1989.10478821", "10.1080/01621459.1987.10478448", "10.1002/acp.2350050106", "10.1111/cgf.12888", "10.1126/science.229.4716.828", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1214/ss/1177013104", "10.1177/1473871611413180", "10.1111/j.1467-8659.2009.01678.x", "10.1007/978-3-540-70956-5", "10.1026//1618-3169.49.4.243", "10.1111/cgf.12391", "10.2307/2288400", "10.1080/01621459.1989.10478821", "10.1080/01621459.1987.10478448", "10.1002/acp.2350050106", "10.1111/cgf.12888", "10.1126/science.229.4716.828", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1214/ss/1177013104", "10.1177/1473871611413180", "10.1111/j.1467-8659.2009.01678.x", "10.1007/978-3-540-70956-5", "10.1026//1618-3169.49.4.243", "10.1111/cgf.12391", "10.2307/2288400", "10.1080/01621459.1989.10478821", "10.1080/01621459.1987.10478448", "10.1002/acp.2350050106", "10.1111/cgf.12888", "10.1126/science.229.4716.828", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1214/ss/1177013104", "10.1177/1473871611413180", "10.1111/j.1467-8659.2009.01678.x", "10.1007/978-3-540-70956-5", "10.1026//1618-3169.49.4.243", "10.1111/cgf.12391"]}, "10.1109/TVCG.2017.2668405": {"doi": "10.1109/TVCG.2017.2668405", "author": ["I. Cho", "J. Li", "Z. Wartell"], "title": "Multi-Scale 7DOF View Adjustment", "year": "2018", "abstract": "Multi-scale virtual environments contain geometric details ranging over several orders of magnitude and typically employ out-of-core rendering techniques. When displayed in virtual reality systems this entails using a 7 degree-of-freedom (DOF) view model where view scale is a separate 7th DOF in addition to 6DOF view pose. Dynamic adjustment of this and other view parameters become very important to usability. In this paper, we evaluate how two adjustment techniques interact with uni- and bi-manual 7DOF navigation in DesktopVR and a CAVE. The travel task has two stages, an initial targeted zoom and a detailed geometric inspection. The results show benefits of the auto-adjustments on completion time and stereo fusion issues, but only in certain circumstances. Peculiar view configuration examples show the difficulty of creating robust adjustment rules.", "keywords": ["pose estimation", "rendering (computer graphics)", "virtual reality", "stereo fusion issues", "6DOF view pose", "CAVE", "DesktopVR", "adjustment techniques interact", "dynamic adjustment", "separate 7th DOF", "virtual reality systems", "out-of-core rendering techniques", "multiscale virtual environments", "multiscale 7DOF view adjustment", "robust adjustment rules", "detailed geometric inspection", "bi-manual 7DOF navigation", "Three-dimensional displays", "Navigation", "Solid modeling", "Stereo image processing", "Rendering (computer graphics)", "Optical distortion", "Virtual environments", "Multi-scale virtual environment", "3D user interface", "7DOF user interaction", "virtual reality", "view adjustment"], "referenced_by": ["IKEY:8580399"], "referencing": []}, "10.1109/TVCG.2017.2669983": {"doi": "10.1109/TVCG.2017.2669983", "author": ["D. Meister", "J. Bittner"], "title": "Parallel Locally-Ordered Clustering for Bounding Volume Hierarchy Construction", "year": "2018", "abstract": "We propose a novel massively parallel construction algorithm for Bounding Volume Hierarchies (BVHs) based on locally-ordered agglomerative clustering. Our method builds the BVH iteratively from bottom to top by merging a batch of cluster pairs in each iteration. To efficiently find the neighboring clusters, we keep the clusters ordered along the Morton curve. This ordering allows us to identify approximate nearest neighbors very efficiently and in parallel. We implemented our algorithm in CUDA and evaluated it in the context of GPU ray tracing. For complex scenes, our method achieves up to a twofold reduction of build times while providing up to 17 percent faster trace times compared with the state-of-the-art methods.", "keywords": ["computational geometry", "parallel algorithms", "pattern clustering", "ray tracing", "Morton curve", "GPU ray tracing", "massively parallel construction algorithm", "bounding volume hierarchy construction", "approximate nearest neighbors", "cluster pairs", "locally-ordered agglomerative clustering", "parallel locally-ordered clustering", "Clustering algorithms", "Graphics processing units", "Ray tracing", "Approximation algorithms", "Nearest neighbor searches", "Sorting", "Buffer storage", "Ray tracing", "object hierarchies", "three-dimensional graphics and realism"], "referenced_by": ["IKEY:8782481", "IKEY:9174969"], "referencing": ["IKEY:4634626", "IKEY:4057175", "IKEY:4061548", "IKEY:4342588", "IKEY:4342590", "IKEY:4634624", "IKEY:4634620", "IKEY:4634618", "IKEY:4342593", "IKEY:5669303", "IKEY:4634626", "IKEY:4057175", "IKEY:4061548", "IKEY:4342588", "IKEY:4342590", "IKEY:4634624", "IKEY:4634620", "IKEY:4634618", "IKEY:4342593", "IKEY:5669303", "IKEY:4634626", "IKEY:4057175", "IKEY:4061548", "IKEY:4342588", "IKEY:4342590", "IKEY:4634624", "IKEY:4634620", "IKEY:4634618", "IKEY:4342593", "IKEY:5669303", "10.1145/2492045.2492054", "10.1145/2790060.2790065", "10.1145/965105.807479", "10.1145/357332.357335", "10.1145/15886.15916", "10.1145/2461912.2462025", "10.1145/2492045.2492055", "10.1145/2492045.2492056", "10.1145/1572769.1572793", "10.1145/1572769.1572772", "10.1145/1572769.1572771", "10.1145/2018323.2018333", "10.1145/1365490.1365500", "10.1145/1572769.1572792", "10.1145/2492045.2492054", "10.1145/2790060.2790065", "10.1145/965105.807479", "10.1145/357332.357335", "10.1145/15886.15916", "10.1145/2461912.2462025", "10.1145/2492045.2492055", "10.1145/2492045.2492056", "10.1145/1572769.1572793", "10.1145/1572769.1572772", "10.1145/1572769.1572771", "10.1145/2018323.2018333", "10.1145/1365490.1365500", "10.1145/1572769.1572792", "10.1145/2492045.2492054", "10.1145/2790060.2790065", "10.1145/965105.807479", "10.1145/357332.357335", "10.1145/15886.15916", "10.1145/2461912.2462025", "10.1145/2492045.2492055", "10.1145/2492045.2492056", "10.1145/1572769.1572793", "10.1145/1572769.1572772", "10.1145/1572769.1572771", "10.1145/2018323.2018333", "10.1145/1365490.1365500", "10.1145/1572769.1572792", "10.1007/BF01911006", "10.1111/cgf.12000", "10.1111/j.1467-8659.2008.01261.x", "10.1111/cgf.12831", "10.1111/cgf.12769", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12140", "10.1007/s00371-016-1241-0", "10.1142/S0129626411000187", "10.1007/BF01911006", "10.1111/cgf.12000", "10.1111/j.1467-8659.2008.01261.x", "10.1111/cgf.12831", "10.1111/cgf.12769", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12140", "10.1007/s00371-016-1241-0", "10.1142/S0129626411000187", "10.1007/BF01911006", "10.1111/cgf.12000", "10.1111/j.1467-8659.2008.01261.x", "10.1111/cgf.12831", "10.1111/cgf.12769", "10.1111/j.1467-8659.2009.01377.x", "10.1111/cgf.12140", "10.1007/s00371-016-1241-0", "10.1142/S0129626411000187"]}, "10.1109/TVCG.2017.2662238": {"doi": "10.1109/TVCG.2017.2662238", "author": ["M. Lin", "T. Shao", "Y. Zheng", "N. J. Mitra", "K. Zhou"], "title": "Recovering Functional Mechanical Assemblies from Raw Scans", "year": "2018", "abstract": "This paper presents a method to reconstruct a functional mechanical assembly from raw scans. Given multiple input scans of a mechanical assembly, our method first extracts the functional mechanical parts using a motion-guided, patch-based hierarchical registration and labeling algorithm. The extracted functional parts are then parameterized from the segments and their internal mechanical relations are encoded by a graph. We use a joint optimization to solve for the best geometry, placement, and orientation of each part, to obtain a final workable mechanical assembly. We demonstrated our algorithm on various types of mechanical assemblies with diverse settings and validated our output using physical fabrication.", "keywords": ["assembling", "CAD", "feature extraction", "geometry", "image registration", "image segmentation", "mechanical engineering computing", "optimisation", "raw scans", "functional mechanical parts", "hierarchical registration", "labeling algorithm", "internal mechanical relations", "functional mechanical assembly recovery", "multiple input scans", "workable mechanical assembly", "functional part extraction", "graph encoding", "joint optimization", "geometry", "physical fabrication", "Geometry", "Shape", "Gears", "Motion segmentation", "Parametric statistics", "Fabrication", "Three-dimensional displays", "3D scanning", "mechanical assembly", "functionality", "mechanical constraints", "motion"], "referenced_by": ["IKEY:9089042"], "referencing": ["IKEY:121791", "IKEY:121791", "IKEY:121791", "10.1145/2010324.1964947", "10.1145/2451236.2451246", "10.1145/1882261.1866176", "10.1145/2766995", "10.1145/1531326.1531341", "10.1145/2766914", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/1531326.1531339", "10.1145/2366145.2366199", "10.1145/2516971.2516975", "10.1145/2601097.2601102", "10.1145/2487228.2487237", "10.1145/1778765.1778839", "10.1145/2070781.2024160", "10.1145/2010324.1964947", "10.1145/2451236.2451246", "10.1145/1882261.1866176", "10.1145/2766995", "10.1145/1531326.1531341", "10.1145/2766914", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/1531326.1531339", "10.1145/2366145.2366199", "10.1145/2516971.2516975", "10.1145/2601097.2601102", "10.1145/2487228.2487237", "10.1145/1778765.1778839", "10.1145/2070781.2024160", "10.1145/2010324.1964947", "10.1145/2451236.2451246", "10.1145/1882261.1866176", "10.1145/2766995", "10.1145/1531326.1531341", "10.1145/2766914", "10.1145/1778765.1778795", "10.1145/2366145.2366146", "10.1145/1531326.1531339", "10.1145/2366145.2366199", "10.1145/2516971.2516975", "10.1145/2601097.2601102", "10.1145/2487228.2487237", "10.1145/1778765.1778839", "10.1145/2070781.2024160", "10.1017/CBO9780511546860", "10.1007/s00371-010-0416-3", "10.1016/S0010-4485(01)00100-2", "10.1111/j.1467-8659.2007.01016.x", "10.1016/j.cad.2013.06.004", "10.1016/j.cag.2013.05.020", "10.1007/s00170-004-2391-1", "10.1111/j.1467-8659.2011.01880.x", "10.1111/cgf.12039", "10.1111/cgf.12309", "10.1007/s00371-006-0059-6", "10.1111/cgf.12446", "10.1017/CBO9780511547126", "10.1007/BF01179082", "10.1007/BFb0014497", "10.1017/CBO9780511546860", "10.1007/s00371-010-0416-3", "10.1016/S0010-4485(01)00100-2", "10.1111/j.1467-8659.2007.01016.x", "10.1016/j.cad.2013.06.004", "10.1016/j.cag.2013.05.020", "10.1007/s00170-004-2391-1", "10.1111/j.1467-8659.2011.01880.x", "10.1111/cgf.12039", "10.1111/cgf.12309", "10.1007/s00371-006-0059-6", "10.1111/cgf.12446", "10.1017/CBO9780511547126", "10.1007/BF01179082", "10.1007/BFb0014497", "10.1017/CBO9780511546860", "10.1007/s00371-010-0416-3", "10.1016/S0010-4485(01)00100-2", "10.1111/j.1467-8659.2007.01016.x", "10.1016/j.cad.2013.06.004", "10.1016/j.cag.2013.05.020", "10.1007/s00170-004-2391-1", "10.1111/j.1467-8659.2011.01880.x", "10.1111/cgf.12039", "10.1111/cgf.12309", "10.1007/s00371-006-0059-6", "10.1111/cgf.12446", "10.1017/CBO9780511547126", "10.1007/BF01179082", "10.1007/BFb0014497"]}, "10.1109/TVCG.2017.2660490": {"doi": "10.1109/TVCG.2017.2660490", "author": ["D. Meneveaux", "B. Bringier", "E. Tauzia", "M. Ribardi\u00e8re", "L. Simonot"], "title": "Rendering Rough Opaque Materials with Interfaced Lambertian Microfacets", "year": "2018", "abstract": "Specular microfacet distributions have been successfully employed by many authors for representing glossiness of materials. They are generally combined with a Lambertian term to account for the colored aspect. These representations make use of the Fresnel reflectance factor at the interface, but the transmission factor at the interface should also be managed. One solution is to employ a multi-layered model with a single layer for the rough interface, which requires a numerical simulation for handling the multiple reflections of light between the substrate and the interface. In this paper, we propose rather to use a representation corresponding to a Fresnel interface lying on a Lambertian substrate, for which the multiple reflections of light between the interface and the substrate can be expressed analytically. With this interfaced Lambertian model, we show how Fresnel transmission affects the material appearance for flat and rough surfaces with isotropic and anisotropic distributions, that produce light backscattering effects. We also propose a methodology for using such materials in any physically based Monte Carlo rendering system, as well as an approximate representation, suitable for GPU applications or measured data fitting. Our approach generalizes several previous models, including flat Lambertian materials as well as specular and Lambertian microfacets. Our results illustrate the wide range of materials that can be rendered with this representation.", "keywords": ["Fresnel diffraction", "light scattering", "Monte Carlo methods", "rendering (computer graphics)", "rough surfaces", "surface roughness", "rough opaque materials", "interfaced lambertian microfacets", "specular microfacet distributions", "colored aspect", "Fresnel reflectance factor", "transmission factor", "rough interface", "numerical simulation", "Fresnel interface", "Lambertian substrate", "interfaced Lambertian model", "Fresnel transmission", "material appearance", "isotropic distributions", "anisotropic distributions", "light backscattering effects", "physically based Monte Carlo rendering system", "approximate representation", "flat Lambertian materials", "specular microfacets", "Rough surfaces", "Surface roughness", "Substrates", "Refractive index", "Backscatter", "Rendering (computer graphics)", "Surface appearance", "BRDF", "microfacets", "importance sampling"], "referenced_by": [], "referencing": ["IKEY:626170", "IKEY:999622", "IKEY:1138991", "IKEY:626170", "IKEY:999622", "IKEY:1138991", "IKEY:626170", "IKEY:999622", "IKEY:1138991", "10.1145/357290.357293", "10.1145/192161.192213", "10.1145/360825.360839", "10.1145/563858.563893", "10.1145/133994.134075", "10.1145/258734.258801", "10.1145/133994.134078", "10.1145/344779.344814", "10.1145/2601097.2601139", "10.1145/1722991.1722996", "10.1145/1321261.1321292", "10.1145/2508363.2508422", "10.1145/2897824.2925943", "10.1145/1201775.882343", "10.1145/357290.357293", "10.1145/192161.192213", "10.1145/360825.360839", "10.1145/563858.563893", "10.1145/133994.134075", "10.1145/258734.258801", "10.1145/133994.134078", "10.1145/344779.344814", "10.1145/2601097.2601139", "10.1145/1722991.1722996", "10.1145/1321261.1321292", "10.1145/2508363.2508422", "10.1145/2897824.2925943", "10.1145/1201775.882343", "10.1145/357290.357293", "10.1145/192161.192213", "10.1145/360825.360839", "10.1145/563858.563893", "10.1145/133994.134075", "10.1145/258734.258801", "10.1145/133994.134078", "10.1145/344779.344814", "10.1145/2601097.2601139", "10.1145/1722991.1722996", "10.1145/1321261.1321292", "10.1145/2508363.2508422", "10.1145/2897824.2925943", "10.1145/1201775.882343", "10.1111/1467-8659.1320109", "10.1111/j.1467-8659..00718.x", "10.1016/j.gmod.2014.12.003", "10.1364/JOSA.57.001105", "10.1364/AO.37.000130", "10.1364/OE.19.003881", "10.1364/AO.55.000027", "10.1364/AO.48.005793", "10.1111/j.1467-8659.2012.03147.x", "10.6028/NBS.MONO.160", "10.1016/S0030-4018(01)01087-2", "10.1364/AO.38.002068", "10.1364/JOSA.65.000531", "10.1111/1467-8659.1320109", "10.1111/j.1467-8659..00718.x", "10.1016/j.gmod.2014.12.003", "10.1364/JOSA.57.001105", "10.1364/AO.37.000130", "10.1364/OE.19.003881", "10.1364/AO.55.000027", "10.1364/AO.48.005793", "10.1111/j.1467-8659.2012.03147.x", "10.6028/NBS.MONO.160", "10.1016/S0030-4018(01)01087-2", "10.1364/AO.38.002068", "10.1364/JOSA.65.000531", "10.1111/1467-8659.1320109", "10.1111/j.1467-8659..00718.x", "10.1016/j.gmod.2014.12.003", "10.1364/JOSA.57.001105", "10.1364/AO.37.000130", "10.1364/OE.19.003881", "10.1364/AO.55.000027", "10.1364/AO.48.005793", "10.1111/j.1467-8659.2012.03147.x", "10.6028/NBS.MONO.160", "10.1016/S0030-4018(01)01087-2", "10.1364/AO.38.002068", "10.1364/JOSA.65.000531"]}, "10.1109/TVCG.2017.2668409": {"doi": "10.1109/TVCG.2017.2668409", "author": ["C. Li", "G. Baciu", "Y. Han"], "title": "StreamMap: Smooth Dynamic Visualization of High-Density Streaming Points", "year": "2018", "abstract": "Interactive visualization of streaming points for real-time scatterplots and linear blending of correlation patterns is increasingly becoming the dominant mode of visual analytics for both big data and streaming data from active sensors and broadcasting media. To better visualize and interact with inter-stream patterns, it is generally necessary to smooth out gaps or distortions in the streaming data. Previous approaches either animate the points directly or present a sampled static heat-map. We propose a new approach, called StreamMap, to smoothly blend high-density streaming points and create a visual flow that emphasizes the density pattern distributions. In essence, we present three new contributions for the visualization of high-density streaming points. The first contribution is a density-based method called super kernel density estimation that aggregates streaming points using an adaptive kernel to solve the overlapping problem. The second contribution is a robust density morphing algorithm that generates several smooth intermediate frames for a given pair of frames. The third contribution is a trend representation design that can help convey the flow directions of the streaming points. The experimental results on three datasets demonstrate the effectiveness of StreamMap when dynamic visualization and visual analysis of trend patterns on streaming points are required.", "keywords": ["Big Data", "data analysis", "data visualisation", "high-density streaming points", "interactive visualization", "visual analytics", "visual flow", "density pattern distributions", "super kernel density estimation", "visual analysis", "smooth dynamic visualization", "interstream patterns", "real-time scatterplots", "big data", "active sensors", "broadcasting media", "streaming data distortion", "StreamMap", "adaptive kernel", "trend representation design", "dynamic visualization", "robust density morphing algorithm", "streaming points aggregation", "Data visualization", "Visualization", "Market research", "Interpolation", "Estimation", "Heuristic algorithms", "Kernel", "Information visualization", "trend visualization", "streaming data", "density map", "time-varying", "scatterplots"], "referenced_by": ["10.3390/app8020199"], "referencing": ["IKEY:6875982", "IKEY:6691712", "IKEY:6361385", "IKEY:1333626", "IKEY:6634178", "IKEY:6634155", "IKEY:6484064", "IKEY:6691713", "IKEY:4658146", "IKEY:4515862", "IKEY:299407", "IKEY:4376195", "IKEY:6205760", "IKEY:6064996", "IKEY:1284395", "IKEY:601035", "IKEY:6875982", "IKEY:6691712", "IKEY:6361385", "IKEY:1333626", "IKEY:6634178", "IKEY:6634155", "IKEY:6484064", "IKEY:6691713", "IKEY:4658146", "IKEY:4515862", "IKEY:299407", "IKEY:4376195", "IKEY:6205760", "IKEY:6064996", "IKEY:1284395", "IKEY:601035", "IKEY:6875982", "IKEY:6691712", "IKEY:6361385", "IKEY:1333626", "IKEY:6634178", "IKEY:6634155", "IKEY:6484064", "IKEY:6691713", "IKEY:4658146", "IKEY:4515862", "IKEY:299407", "IKEY:4376195", "IKEY:6205760", "IKEY:6064996", "IKEY:1284395", "IKEY:601035", "10.1145/2702123.2702476", "10.1145/1531326.1531348", "10.1145/325334.325247", "10.1145/566654.566646", "10.1145/2702123.2702476", "10.1145/1531326.1531348", "10.1145/325334.325247", "10.1145/566654.566646", "10.1145/2702123.2702476", "10.1145/1531326.1531348", "10.1145/325334.325247", "10.1145/566654.566646", "10.1006/ijhc.2002.1017", "10.1016/S1361-8415(98)80022-4", "10.1111/j.1467-8659.2009.01440.x", "10.1177/1473871612457601", "10.1111/cgf.12117", "10.1007/978-1-4899-3324-9", "10.1016/0004-3702(81)90024-2", "10.1111/j.1467-8659.2011.01920.x", "10.1117/3.353798", "10.1006/ijhc.2002.1017", "10.1016/S1361-8415(98)80022-4", "10.1111/j.1467-8659.2009.01440.x", "10.1177/1473871612457601", "10.1111/cgf.12117", "10.1007/978-1-4899-3324-9", "10.1016/0004-3702(81)90024-2", "10.1111/j.1467-8659.2011.01920.x", "10.1117/3.353798", "10.1006/ijhc.2002.1017", "10.1016/S1361-8415(98)80022-4", "10.1111/j.1467-8659.2009.01440.x", "10.1177/1473871612457601", "10.1111/cgf.12117", "10.1007/978-1-4899-3324-9", "10.1016/0004-3702(81)90024-2", "10.1111/j.1467-8659.2011.01920.x", "10.1117/3.353798"]}, "10.1109/TVCG.2017.2671341": {"doi": "10.1109/TVCG.2017.2671341", "author": ["F. Miranda", "L. Lins", "J. T. Klosowski", "C. T. Silva"], "title": "TopKube: A Rank-Aware Data Cube for Real-Time Exploration of Spatiotemporal Data", "year": "2018", "abstract": "From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, \u201cwhat's trending\u201d is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TopKube: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.", "keywords": ["data structures", "Internet", "query processing", "top-k queries", "data structure", "real-time visual exploratory tool", "spatiotemporal data", "real-time exploration", "rank-aware data cube", "TopKube", "Data visualization", "Visualization", "Data structures", "Proposals", "Real-time systems", "Benchmark testing", "Urban areas", "Interactive visualization", "data cube", "top-K queries", "rank merging"], "referenced_by": ["IKEY:8809847", "IKEY:8706590", "IKEY:9308629"], "referencing": ["IKEY:6634137", "IKEY:6691710", "IKEY:329404", "IKEY:6876022", "IKEY:6816674", "IKEY:981851", "IKEY:1196005", "IKEY:4812466", "IKEY:6065004", "IKEY:4376138", "IKEY:6634146", "IKEY:7536648", "IKEY:6634137", "IKEY:6691710", "IKEY:329404", "IKEY:6876022", "IKEY:6816674", "IKEY:981851", "IKEY:1196005", "IKEY:4812466", "IKEY:6065004", "IKEY:4376138", "IKEY:6634146", "IKEY:7536648", "IKEY:6634137", "IKEY:6691710", "IKEY:329404", "IKEY:6876022", "IKEY:6816674", "IKEY:981851", "IKEY:1196005", "IKEY:4812466", "IKEY:6065004", "IKEY:4376138", "IKEY:6634146", "IKEY:7536648", "10.1145/2452376.2452419", "10.1145/1376616.1376627", "10.1145/1391729.1391730", "10.1145/2556288.2557379", "10.1145/2812802", "10.1145/2452376.2452419", "10.1145/1376616.1376627", "10.1145/1391729.1391730", "10.1145/2556288.2557379", "10.1145/2812802", "10.1145/2452376.2452419", "10.1145/1376616.1376627", "10.1145/1391729.1391730", "10.1145/2556288.2557379", "10.1145/2812802", "10.1353/cul.2007.0016", "10.1111/cgf.12129", "10.2307/2289444", "10.1023/A:1009726021843", "10.14778/2535569.2448955", "10.1007/11535331_13", "10.1007/978-3-642-13818-8_8", "10.1007/978-3-642-22922-0_13", "10.1016/S0022-0000(03)00026-6", "10.1353/cul.2007.0016", "10.1111/cgf.12129", "10.2307/2289444", "10.1023/A:1009726021843", "10.14778/2535569.2448955", "10.1007/11535331_13", "10.1007/978-3-642-13818-8_8", "10.1007/978-3-642-22922-0_13", "10.1016/S0022-0000(03)00026-6", "10.1353/cul.2007.0016", "10.1111/cgf.12129", "10.2307/2289444", "10.1023/A:1009726021843", "10.14778/2535569.2448955", "10.1007/11535331_13", "10.1007/978-3-642-13818-8_8", "10.1007/978-3-642-22922-0_13", "10.1016/S0022-0000(03)00026-6"]}}