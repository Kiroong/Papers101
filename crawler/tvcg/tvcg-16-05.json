{"10.1109/TVCG.2015.2446474": {"doi": "10.1109/TVCG.2015.2446474", "author": ["Y. Yang", "X. Yang", "S. Yang"], "title": "A Fast Iterated Orthogonal Projection Framework for Smoke Simulation", "year": "2016", "abstract": "We present a fast iterated orthogonal projection (IOP) framework for smoke simulations. By modifying the IOP framework with a different means for convergence, our framework significantly reduces the number of iterations required to converge to the desired precision. Our new iteration framework adds a divergence redistributor component to IOP that can improve the impeded convergence logic of IOP. We tested Jacobi, GS and SOR as divergence redistributors and used the Multigrid scheme to generate a highly efficient Poisson solver. It provides a rapid convergence rate and requires less computation time. In all of our experiments, our method only requires 2-3 iterations to satisfy the convergence condition of 1e-5 and 5-7 iterations for 1e-10. Compared with the commonly used Incomplete Cholesky Preconditioned Conjugate Gradient(ICPCG) solver, our Poisson solver accelerates the overall speed to approximately 7- to 30-fold faster for grids ranging from $128^3$ to $256^3$ . Our solver can accelerate more on larger grids because of the property that the iteration count required to satisfy the convergence condition is independent of the problem size. We use various experimental scenes and settings to demonstrate the efficiency of our method. In addition, we present a feasible method for both IOP and our fast IOP to support free surfaces.", "keywords": ["Convergence", "Mathematical model", "Linear systems", "Jacobian matrices", "Acceleration", "Computational modeling", "Geometry", "Physically Based Animation", "Fluid Simulation", "Iterated Orthogonal Projection", "Poisson Solver", "Physically based animation", "fluid simulation", "iterated orthogonal projection", "poisson solver"], "referenced_by": ["10.1109/TVCG.2018.2873375", "10.1145/3092818"], "referencing": ["10.1109/TVCG.2013.19", "10.1109/TVCG.2013.19", "10.1109/TVCG.2013.19", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/1186822.1073282", "10.1145/383259.383260", "10.1145/1186822.1073298", "10.1145/1964921.1964977", "10.1145/1275808.1276502", "10.1145/1399504.1360649", "10.1145/1576246.1531344", "10.1145/2019406.2019419", "10.1145/1186562.1015745", "10.1145/1833349.1778851", "10.1145/2370919.2370927", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/1186822.1073282", "10.1145/383259.383260", "10.1145/1186822.1073298", "10.1145/1964921.1964977", "10.1145/1275808.1276502", "10.1145/1399504.1360649", "10.1145/1576246.1531344", "10.1145/2019406.2019419", "10.1145/1186562.1015745", "10.1145/1833349.1778851", "10.1145/2370919.2370927", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/1186822.1073282", "10.1145/383259.383260", "10.1145/1186822.1073298", "10.1145/1964921.1964977", "10.1145/1275808.1276502", "10.1145/1399504.1360649", "10.1145/1576246.1531344", "10.1145/2019406.2019419", "10.1145/1186562.1015745", "10.1145/1833349.1778851", "10.1145/2370919.2370927", "10.1063/1.1761178", "10.1006/gmip.1996.0039", "10.1201/b10635", "10.1137/S1064827595288589", "10.1111/cgf.12059", "10.1007/BFb0069928", "10.1016/0021-9991(82)90057-2", "10.1063/1.1761178", "10.1006/gmip.1996.0039", "10.1201/b10635", "10.1137/S1064827595288589", "10.1111/cgf.12059", "10.1007/BFb0069928", "10.1016/0021-9991(82)90057-2", "10.1063/1.1761178", "10.1006/gmip.1996.0039", "10.1201/b10635", "10.1137/S1064827595288589", "10.1111/cgf.12059", "10.1007/BFb0069928", "10.1016/0021-9991(82)90057-2"]}, "10.1109/TVCG.2015.2440252": {"doi": "10.1109/TVCG.2015.2440252", "author": ["J. Tao", "C. Wang", "C. Shene", "R. A. Shaw"], "title": "A Vocabulary Approach to Partial Streamline Matching and Exploratory Flow Visualization", "year": "2016", "abstract": "Measuring the similarity of integral curves is fundamental to many important flow data analysis and visualization tasks such as feature detection, pattern querying, streamline clustering, and hierarchical exploration. In this paper, we introduce FlowString, a novel vocabulary approach that extracts shape invariant features from streamlines and utilizes a string-based method for exploratory streamline analysis and visualization. Our solution first resamples streamlines by considering their local feature scales. We then classify resampled points along streamlines based on the shape similarity around their local neighborhoods. We encode each streamline into a string of well-selected shape characters, from which we construct meaningful words for querying and retrieval. A unique feature of our approach is that it captures intrinsic streamline similarity that is invariant under translation, rotation and scaling. We design an intuitive interface and user interactions to support flexible querying, allowing exact and approximate searches for partial streamline matching. Users can perform queries at either the character level or the word level, and define their own characters or words conveniently for customized search. We demonstrate the effectiveness of FlowString with several flow field data sets of different sizes and characteristics. We also extend FlowString to handle multiple data sets and perform an empirical expert evaluation to confirm the usefulness of this approach.", "keywords": ["data analysis", "data visualisation", "feature extraction", "flow visualisation", "image classification", "image retrieval", "mechanical engineering computing", "pattern matching", "user interfaces", "empirical expert evaluation", "customized search", "approximate search", "exact search", "flexible querying", "user interaction", "intuitive interface", "intrinsic streamline similarity", "retrieval", "querying", "shape characters", "local neighborhoods", "shape similarity", "resampled point classification", "local feature scales", "exploratory streamline analysis", "string-based method", "shape invariant feature extraction", "FlowString", "visualization task", "flow data analysis", "integral curve similarity measurement", "exploratory flow visualization", "partial streamline matching", "vocabulary approach", "Shape", "Feature extraction", "Vocabulary", "Spirals", "Data visualization", "Windings", "Diffusion tensor imaging", "Streamline similarity", "shape invariant features", "partial matching", "exploratory flow visualization", "user interface", "Streamline similarity", "shape invariant features", "partial matching", "exploratory flow visualization", "user interface"], "referenced_by": ["10.1109/TVCG.2016.2598472", "10.1109/TVCG.2017.2773071", "10.1007/s12650-019-00552-x", "10.1016/j.visinf.2020.09.001"], "referencing": ["10.1109/TVCG.2007.70595", "10.1109/VISUAL.1999.809863", "10.1109/TVCG.2009.141", "10.1109/TVCG.2010.212", "10.1109/TVCG.2012.150", "10.1109/TVCG.2011.78", "10.1109/TVCG.2007.70579", "10.1109/TVCG.2012.143", "10.1109/VISUAL.1999.809865", "10.1109/TVCG.2003.1260740", "10.1109/TVCG.2007.70595", "10.1109/VISUAL.1999.809863", "10.1109/TVCG.2009.141", "10.1109/TVCG.2010.212", "10.1109/TVCG.2012.150", "10.1109/TVCG.2011.78", "10.1109/TVCG.2007.70579", "10.1109/TVCG.2012.143", "10.1109/VISUAL.1999.809865", "10.1109/TVCG.2003.1260740", "10.1109/TVCG.2007.70595", "10.1109/VISUAL.1999.809863", "10.1109/TVCG.2009.141", "10.1109/TVCG.2010.212", "10.1109/TVCG.2012.150", "10.1109/TVCG.2011.78", "10.1109/TVCG.2007.70579", "10.1109/TVCG.2012.143", "10.1109/VISUAL.1999.809865", "10.1109/TVCG.2003.1260740", "10.1007/978-3-540-30135-6_45", "10.1111/j.1467-8659.2008.01244.x", "10.1002/mrm.10415", "10.1364/JOSAA.4.000629", "10.1007/978-3-540-88606-8_6", "10.1007/BF01206331", "10.1007/978-3-540-30135-6_45", "10.1111/j.1467-8659.2008.01244.x", "10.1002/mrm.10415", "10.1364/JOSAA.4.000629", "10.1007/978-3-540-88606-8_6", "10.1007/BF01206331", "10.1007/978-3-540-30135-6_45", "10.1111/j.1467-8659.2008.01244.x", "10.1002/mrm.10415", "10.1364/JOSAA.4.000629", "10.1007/978-3-540-88606-8_6", "10.1007/BF01206331"]}, "10.1109/TVCG.2015.2440236": {"doi": "10.1109/TVCG.2015.2440236", "author": ["P. Wen", "W. Cheng", "Y. Wang", "H. Chu", "N. C. Tang", "H. M. Liao"], "title": "Court Reconstruction for Camera Calibration in Broadcast Basketball Videos", "year": "2016", "abstract": "We introduce a technique of calibrating camera motions in basketball videos. Our method particularly transforms player positions to standard basketball court coordinates and enables applications such as tactical analysis and semantic basketball video retrieval. To achieve a robust calibration, we reconstruct the panoramic basketball court from a video, followed by warping the panoramic court to a standard one. As opposed to previous approaches, which individually detect the court lines and corners of each video frame, our technique considers all video frames simultaneously to achieve calibration; hence, it is robust to illumination changes and player occlusions. To demonstrate the feasibility of our technique, we present a stroke-based system that allows users to retrieve basketball videos. Our system tracks player trajectories from broadcast basketball videos. It then rectifies the trajectories to a standard basketball court by using our camera calibration method. Consequently, users can apply stroke queries to indicate how the players move in gameplay during retrieval. The main advantage of this interface is an explicit query of basketball videos so that unwanted outcomes can be prevented. We show the results in Figs. 1, 7, 9, 10 and our accompanying video to exhibit the feasibility of our technique.", "keywords": ["calibration", "cameras", "edge detection", "image motion analysis", "image reconstruction", "video retrieval", "video signal processing", "broadcast basketball videos", "standard basketball court coordinates", "player positions", "robust camera calibration method", "panoramic basketball court reconstruction", "court line detection", "court corner detection", "video frame", "illumination changes", "player occlusions", "stroke-based system", "basketball video retrieval", "explicit query", "panoramic court warping", "Videos", "Calibration", "Cameras", "Trajectory", "Robustness", "Image color analysis", "Feature extraction", "Camera calibration", "basketball", "stroke", "player trajectory", "video retrieval", "Camera calibration", "basketball", "stroke", "player trajectory", "video retrieval", "Basketball", "Computer Graphics", "Humans", "Motion"], "referenced_by": ["10.1109/AIPR.2016.8010578", "10.1109/ICIEV.2017.8338571", "10.1109/WACV.2018.00038", "10.1109/ICASSP.2018.8461667", "10.1109/CVPRW.2019.00305", "10.1109/CVPR42600.2020.01364", "10.1007/s12283-018-0263-4", "10.1177/1729881418813778", "10.1016/j.jocs.2018.12.002", "10.1007/978-3-030-28374-2_55", "10.1089/big.2018.0124"], "referencing": ["10.1109/TMM.2010.2100373", "10.1109/WACV.2012.6163012", "10.1109/69.755615", "10.1109/TMM.2007.902875", "10.1109/76.988656", "10.1109/TMM.2010.2100373", "10.1109/WACV.2012.6163012", "10.1109/69.755615", "10.1109/TMM.2007.902875", "10.1109/76.988656", "10.1109/TMM.2010.2100373", "10.1109/WACV.2012.6163012", "10.1109/69.755615", "10.1109/TMM.2007.902875", "10.1109/76.988656", "10.1145/1291233.1291306", "10.1145/1282280.1282352", "10.1145/1646396.1646451", "10.1145/1345448.1345455", "10.1145/1291233.1291306", "10.1145/1282280.1282352", "10.1145/1646396.1646451", "10.1145/1345448.1345455", "10.1145/1291233.1291306", "10.1145/1282280.1282352", "10.1145/1646396.1646451", "10.1145/1345448.1345455", "10.1016/j.cviu.2008.01.006", "10.1016/j.jvcir.2008.11.008", "10.1117/12.908723", "10.4304/jsw.6.8.1468-1475", "10.1016/j.cviu.2010.01.005", "10.1016/j.cviu.2008.01.006", "10.1016/j.jvcir.2008.11.008", "10.1117/12.908723", "10.4304/jsw.6.8.1468-1475", "10.1016/j.cviu.2010.01.005", "10.1016/j.cviu.2008.01.006", "10.1016/j.jvcir.2008.11.008", "10.1117/12.908723", "10.4304/jsw.6.8.1468-1475", "10.1016/j.cviu.2010.01.005"]}, "10.1109/TVCG.2015.2443787": {"doi": "10.1109/TVCG.2015.2443787", "author": ["L. Chittaro"], "title": "Designing Serious Games for Safety Education: \u201cLearn to Brace\u201d versus Traditional Pictorials for Aircraft Passengers", "year": "2016", "abstract": "Serious games for safety education (SGSE) are a novel tool for preparing people to prevent and\\or handle risky situations. Although several SGSE have been developed, design and evaluation methods for SGSE need to be better grounded in and guided by safety-relevant psychological theories. In particular, this paper focuses on threat appeals and the assessment of variables, such as safety locus of control, that influence human behavior in real risky situations. It illustrates how we took into account such models in the design and evaluation of \u201cLearn to Brace\u201d, a first-of-its-kind serious game that deals with a major problem in aviation safety, i.e. the scarce effectiveness of the safety cards used by airlines. The study considered a sample of 48 users: half of them received instructions about the brace position through the serious game, the other half through a traditional safety card pictorial. Results showed that the serious game was much more effective than the traditional instructions both in terms of learning and of changing safety-relevant perceptions, especially safety locus of control and recommendation perception.", "keywords": ["computer aided instruction", "serious games (computing)", "aircraft passengers", "serious games for safety education", "SGSE", "design methods", "evaluation methods", "safety-relevant psychological theories", "safety locus", "human behavior", "learn to brace", "aviation safety", "recommendation perception", "Safety", "Games", "Education", "Aircraft", "Accidents", "Psychology", "Avatars", "serious games", "safety education", "aviation", "mobile devices", "threat appeals", "Serious games", "safety education", "aviation", "mobile devices", "threat appeals", "Accident Prevention", "Adult", "Aircraft", "Computer Graphics", "Computer-Assisted Instruction", "Female", "Humans", "Male", "Middle Aged", "Safety", "Video Games", "Young Adult"], "referenced_by": ["10.1109/ACCESS.2018.2890393", "10.1109/ACCESS.2019.2934990", "10.1109/MSMC.2019.2948654", "10.1109/TLT.2019.2902401", "10.1109/ICAICTA.2019.8904209", "10.1109/CogInfoCom47531.2019.9089895", "10.1109/ASET48392.2020.9118387", "10.1145/2786567.2786570", "10.1016/j.ssci.2017.10.012", "10.1016/j.trc.2017.10.007", "10.1080/10447318.2016.1142798", "10.1016/j.ijhcs.2018.07.006", "10.1016/j.eswa.2019.01.021", "10.1016/j.ssci.2019.02.005", "10.1027/2192-0923/a000153", "10.1016/j.jocs.2019.06.009", "10.1177/1046878119865913", "10.1016/j.compedu.2019.103698", "10.1007/978-3-030-11440-4_22", "10.1371/journal.pone.0229197", "10.1080/24721840.2020.1742123", "10.1007/s10055-020-00447-5", "10.1016/j.ijhcs.2020.102484", "10.1007/978-3-030-49663-0_23", "10.1016/B978-0-12-813844-1.00001-4"], "referencing": ["10.1109/TVCG.2015.2391853", "10.1109/VS-GAMES.2009.8", "10.1109/MC.2005.297", "10.1109/TVCG.2015.2391853", "10.1109/VS-GAMES.2009.8", "10.1109/MC.2005.297", "10.1109/TVCG.2015.2391853", "10.1109/VS-GAMES.2009.8", "10.1109/MC.2005.297", "10.1145/2671015.2671025", "10.1145/2671015.2671025", "10.1145/2671015.2671025", "10.1016/0749-5978(91)90020-T", "10.1155/2013/136864", "10.1016/j.aap.2010.12.009", "10.1016/j.ssci.2009.02.001", "10.1007/978-3-642-31037-9_19", "10.1007/978-3-642-13226-1_8", "10.1016/j.ridd.2006.07.001", "10.1016/j.compedu.2012.03.004", "10.1037/1089-2680.11.3.258", "10.1080/15534510802185836", "10.1111/j.1559-1816.2000.tb02323.x", "10.1111/j.1365-2729.2012.00489.x", "10.1016/j.aap.2011.06.002", "10.1016/0092-6566(73)90043-3", "10.1016/j.trf.2011.09.002", "10.1080/10508414.2012.663244", "10.1007/BF01013758", "10.1037/h0100799", "10.1016/j.aap.2013.12.003", "10.1007/978-3-642-40790-1_12", "10.1037/0021-9010.72.3.339", "10.1037/a0027779", "10.1080/08870440108405863", "10.1016/j.aei.2011.08.001", "10.1348/135910706X109684", "10.1016/j.firesaf.2008.11.004", "10.1007/s10055-008-0110-1", "10.1080/03637759209376276", "10.1177/109019810002700506", "10.1007/BF01014164", "10.1016/j.aap.2013.03.036", "10.1016/0749-5978(91)90020-T", "10.1155/2013/136864", "10.1016/j.aap.2010.12.009", "10.1016/j.ssci.2009.02.001", "10.1007/978-3-642-31037-9_19", "10.1007/978-3-642-13226-1_8", "10.1016/j.ridd.2006.07.001", "10.1016/j.compedu.2012.03.004", "10.1037/1089-2680.11.3.258", "10.1080/15534510802185836", "10.1111/j.1559-1816.2000.tb02323.x", "10.1111/j.1365-2729.2012.00489.x", "10.1016/j.aap.2011.06.002", "10.1016/0092-6566(73)90043-3", "10.1016/j.trf.2011.09.002", "10.1080/10508414.2012.663244", "10.1007/BF01013758", "10.1037/h0100799", "10.1016/j.aap.2013.12.003", "10.1007/978-3-642-40790-1_12", "10.1037/0021-9010.72.3.339", "10.1037/a0027779", "10.1080/08870440108405863", "10.1016/j.aei.2011.08.001", "10.1348/135910706X109684", "10.1016/j.firesaf.2008.11.004", "10.1007/s10055-008-0110-1", "10.1080/03637759209376276", "10.1177/109019810002700506", "10.1007/BF01014164", "10.1016/j.aap.2013.03.036", "10.1016/0749-5978(91)90020-T", "10.1155/2013/136864", "10.1016/j.aap.2010.12.009", "10.1016/j.ssci.2009.02.001", "10.1007/978-3-642-31037-9_19", "10.1007/978-3-642-13226-1_8", "10.1016/j.ridd.2006.07.001", "10.1016/j.compedu.2012.03.004", "10.1037/1089-2680.11.3.258", "10.1080/15534510802185836", "10.1111/j.1559-1816.2000.tb02323.x", "10.1111/j.1365-2729.2012.00489.x", "10.1016/j.aap.2011.06.002", "10.1016/0092-6566(73)90043-3", "10.1016/j.trf.2011.09.002", "10.1080/10508414.2012.663244", "10.1007/BF01013758", "10.1037/h0100799", "10.1016/j.aap.2013.12.003", "10.1007/978-3-642-40790-1_12", "10.1037/0021-9010.72.3.339", "10.1037/a0027779", "10.1080/08870440108405863", "10.1016/j.aei.2011.08.001", "10.1348/135910706X109684", "10.1016/j.firesaf.2008.11.004", "10.1007/s10055-008-0110-1", "10.1080/03637759209376276", "10.1177/109019810002700506", "10.1007/BF01014164", "10.1016/j.aap.2013.03.036"]}, "10.1109/TVCG.2015.2440272": {"doi": "10.1109/TVCG.2015.2440272", "author": ["S. Lee", "X. Hu", "H. Hua"], "title": "Effects of Optical Combiner and IPD Change for Convergence on Near-Field Depth Perception in an Optical See-Through HMD", "year": "2016", "abstract": "Many error sources have been explored in regards to the depth perception problem in augmented reality environments using optical see-through head-mounted displays (OST-HMDs). Nonetheless, two error sources are commonly neglected: the ray-shift phenomenon and the change in interpupillary distance (IPD). The first source of error arises from the difference in refraction for virtual and see-through optical paths caused by an optical combiner, which is required of OST-HMDs. The second occurs from the change in the viewer's IPD due to eye convergence. In this paper, we analyze the effects of these two error sources on near-field depth perception and propose methods to compensate for these two types of errors. Furthermore, we investigate their effectiveness through an experiment comparing the conditions with and without our error compensation methods applied. In our experiment, participants estimated the egocentric depth of a virtual and a physical object located at seven different near-field distances (40~200 cm) using a perceptual matching task. Although the experimental results showed different patterns depending on the target distance, the results demonstrated that the near-field depth perception error can be effectively reduced to a very small level (at most 1 percent error) by compensating for the two mentioned error sources.", "keywords": ["augmented reality", "error compensation", "helmet mounted displays", "optical combiner effect", "IPD change", "near-field depth perception", "optical see-through HMD", "depth perception problem", "augmented reality environments", "optical see-through head-mounted displays", "OST-HMD", "error sources", "ray-shift phenomenon", "interpupillary distance change", "virtual optical path", "see-through optical path", "eye convergence", "error compensation method", "egocentric depth", "virtual object", "physical object", "perceptual matching task", "Adaptive optics", "Optical imaging", "Optical refraction", "Calibration", "Optical distortion", "Accuracy", "Optical buffering", "Near-field depth perception", "mixed/augmented reality", "optical see-through head-mounted display", "Near-field depth perception", "mixed\\augmented reality", "optical see-through head-mounted display"], "referenced_by": ["10.1109/TVCG.2017.2657058", "10.1109/TVCG.2016.2518138", "10.1109/ISMAR-Adjunct.2018.00037", "10.1109/TVCG.2018.2869729", "10.1002/jsid.427", "10.1080/09286586.2019.1624782", "10.1364/OE.27.018169"], "referencing": ["10.1109/TVCG.2013.37", "10.1109/TVCG.2012.45", "10.1109/TVCG.2012.58", "10.1109/TVCG.2013.37", "10.1109/TVCG.2012.45", "10.1109/TVCG.2012.58", "10.1109/TVCG.2013.37", "10.1109/TVCG.2012.45", "10.1109/TVCG.2012.58", "10.1145/1823738.1823744", "10.1145/1394281.1394283", "10.1145/1577755.1577762", "10.1145/965105.807509", "10.1145/1836248.1836277", "10.1145/1823738.1823744", "10.1145/1394281.1394283", "10.1145/1577755.1577762", "10.1145/965105.807509", "10.1145/1836248.1836277", "10.1145/1823738.1823744", "10.1145/1394281.1394283", "10.1145/1577755.1577762", "10.1145/965105.807509", "10.1145/1836248.1836277", "10.1162/pres.1997.6.4.355", "10.1364/AO.48.002655", "10.1518/001872098779591278", "10.1016/j.displa.2008.03.001", "10.1016/j.jneumeth.2008.05.015", "10.1002/9783527699247", "10.1364/OE.21.030993", "10.1162/105474600566583", "10.1016/j.displa.2013.01.001", "10.1162/pres.1995.4.1.24", "10.1162/105474602321050730", "10.1177/154193129403800413", "10.1364/AO.47.002888", "10.1162/pres.1997.6.4.355", "10.1364/AO.48.002655", "10.1518/001872098779591278", "10.1016/j.displa.2008.03.001", "10.1016/j.jneumeth.2008.05.015", "10.1002/9783527699247", "10.1364/OE.21.030993", "10.1162/105474600566583", "10.1016/j.displa.2013.01.001", "10.1162/pres.1995.4.1.24", "10.1162/105474602321050730", "10.1177/154193129403800413", "10.1364/AO.47.002888", "10.1162/pres.1997.6.4.355", "10.1364/AO.48.002655", "10.1518/001872098779591278", "10.1016/j.displa.2008.03.001", "10.1016/j.jneumeth.2008.05.015", "10.1002/9783527699247", "10.1364/OE.21.030993", "10.1162/105474600566583", "10.1016/j.displa.2013.01.001", "10.1162/pres.1995.4.1.24", "10.1162/105474602321050730", "10.1177/154193129403800413", "10.1364/AO.47.002888"]}, "10.1109/TVCG.2015.2443804": {"doi": "10.1109/TVCG.2015.2443804", "author": ["M. Wu", "V. Popescu"], "title": "Multiperspective Focus+Context Visualization", "year": "2016", "abstract": "Occlusions are a severe bottleneck for the visualization of large and complex datasets. Conventional images only show dataset elements to which there is a direct line of sight, which significantly limits the information bandwidth of the visualization. Multiperspective visualization is a powerful approach for alleviating occlusions to show more than what is visible from a single viewpoint. However, constructing and rendering multiperspective visualizations is challenging. We present a framework for designing multiperspective focus+context visualizations with great flexibility by manipulating the underlying camera model. The focus region viewpoint is adapted to alleviate occlusions. The framework supports multiperspective visualization in three scenarios. In a first scenario, the viewpoint is altered independently for individual image regions to avoid occlusions. In a second scenario, conventional input images are connected into a multiperspective image. In a third scenario, one or several data subsets of interest (i.e., targets) are visualized where they would be seen in the absence of occluders, as the user navigates or the targets move. The multiperspective images are rendered at interactive rates, leveraging the camera model's fast projection operation. We demonstrate the framework on terrain, urban, and molecular biology geometric datasets, as well as on volume rendered density datasets.", "keywords": ["cameras", "data visualisation", "image processing", "rendering (computer graphics)", "multiperspective focus+context visualization", "complex dataset visualization", "information bandwidth", "occlusions", "multiperspective visualization rendering", "camera model", "image regions", "data subsets", "interactive rates", "camera model fast projection operation", "molecular biology geometric datasets", "volume rendered density datasets", "Cameras", "Data visualization", "Context", "Image segmentation", "Rendering (computer graphics)", "Solid modeling", "Three-dimensional displays", "Occlusion management", "camera models", "multiperspective visualization", "interactive visualization", "focus+context", "Occlusion management", "camera models", "multiperspective visualization", "interactive visualization", "focus+context", "Algorithms", "Computer Graphics", "Imaging, Three-Dimensional"], "referenced_by": ["10.1109/PACIFICVIS.2017.8031579", "10.1109/TVCG.2016.2599049", "10.1109/TVCG.2017.2778249", "10.1109/TVCG.2018.2864506", "10.1109/TVCG.2018.2864690", "10.1109/VR.2019.8798025", "10.1109/TVCG.2020.3023606", "10.1007/978-3-030-01790-3_15"], "referencing": ["10.1109/TVCG.2006.140", "10.1109/TVCG.2007.70565", "10.1109/TVCG.2008.124", "10.1109/TVCG.2008.59", "10.1109/TVCG.2006.124", "10.1109/38.610209", "10.1109/3DPVT.2006.26", "10.1109/VISUAL.2004.50", "10.1109/TVCG.2006.167", "10.1109/INFVIS.2003.1249008", "10.1109/TVCG.2006.140", "10.1109/TVCG.2007.70565", "10.1109/TVCG.2008.124", "10.1109/TVCG.2008.59", "10.1109/TVCG.2006.124", "10.1109/38.610209", "10.1109/3DPVT.2006.26", "10.1109/VISUAL.2004.50", "10.1109/TVCG.2006.167", "10.1109/INFVIS.2003.1249008", "10.1109/TVCG.2006.140", "10.1109/TVCG.2007.70565", "10.1109/TVCG.2008.124", "10.1109/TVCG.2008.59", "10.1109/TVCG.2006.124", "10.1109/38.610209", "10.1109/3DPVT.2006.26", "10.1109/VISUAL.2004.50", "10.1109/TVCG.2006.167", "10.1109/INFVIS.2003.1249008", "10.1145/1141911.1141966", "10.1145/1409060.1409107", "10.1145/1477926.1477928", "10.1145/1321261.1321263", "10.1145/1360612.1360700", "10.1145/882262.882362", "10.1145/1618452.1618504", "10.1145/280814.280871", "10.1145/258734.258859", "10.1145/1141911.1141966", "10.1145/1409060.1409107", "10.1145/1477926.1477928", "10.1145/1321261.1321263", "10.1145/1360612.1360700", "10.1145/882262.882362", "10.1145/1618452.1618504", "10.1145/280814.280871", "10.1145/258734.258859", "10.1145/1141911.1141966", "10.1145/1409060.1409107", "10.1145/1477926.1477928", "10.1145/1321261.1321263", "10.1145/1360612.1360700", "10.1145/882262.882362", "10.1145/1618452.1618504", "10.1145/280814.280871", "10.1145/258734.258859", "10.1007/978-3-7091-6303-0_12", "10.1093/nar/28.1.235", "10.1007/BF01901044", "10.1038/nsmb.1933", "10.1179/1743277411Y.0000000002", "10.1007/978-3-7091-9430-0_3", "10.1111/j.1467-8659.2005.00858.x", "10.1111/j.1467-8659.2008.01332.x", "10.1007/s00371-011-0599-2", "10.1111/j.1467-8659.2004.00794.x", "10.1007/978-3-540-24671-8_2", "10.1007/978-3-7091-6303-0_12", "10.1093/nar/28.1.235", "10.1007/BF01901044", "10.1038/nsmb.1933", "10.1179/1743277411Y.0000000002", "10.1007/978-3-7091-9430-0_3", "10.1111/j.1467-8659.2005.00858.x", "10.1111/j.1467-8659.2008.01332.x", "10.1007/s00371-011-0599-2", "10.1111/j.1467-8659.2004.00794.x", "10.1007/978-3-540-24671-8_2", "10.1007/978-3-7091-6303-0_12", "10.1093/nar/28.1.235", "10.1007/BF01901044", "10.1038/nsmb.1933", "10.1179/1743277411Y.0000000002", "10.1007/978-3-7091-9430-0_3", "10.1111/j.1467-8659.2005.00858.x", "10.1111/j.1467-8659.2008.01332.x", "10.1007/s00371-011-0599-2", "10.1111/j.1467-8659.2004.00794.x", "10.1007/978-3-540-24671-8_2"]}, "10.1109/TVCG.2015.2446494": {"doi": "10.1109/TVCG.2015.2446494", "author": ["Y. Huang", "M. Kallmann"], "title": "Planning Motions and Placements for Virtual Demonstrators", "year": "2016", "abstract": "In order to deliver information effectively, virtual human demonstrators must be able to address complex spatial constraints and at the same time replicate motion coordination patterns observed in human-human interactions. We introduce in this paper a whole-body motion planning and synthesis framework that coordinates locomotion, body positioning, action execution and gaze behavior for generic demonstration tasks among obstacles. Human-like solutions are achieved with a coordination model extracted from experiments with human subjects. Given an observer location and a target demonstration to be performed, the proposed planner automatically identifies body placements respecting visibility constraints, locomotion accessibility, and action feasibility among obstacles. Actions are modeled with clusters of example motions and a fast collision avoidance procedure in blending space is introduced to avoid nearby obstacles when needed. Locomotion towards new placements integrates planning among obstacles and is based on a motion capture database organized for efficient synthesis of motions with precise path following and arrival constraints. The proposed solution introduces effective approaches for modeling and solving complex demonstrative tasks for interactive applications.", "keywords": ["database management systems", "virtual reality", "arrival constraints", "path following", "motion capture database", "blending space", "collision avoidance procedure", "action feasibility", "locomotion accessibility", "visibility constraints", "target demonstration", "observer location", "coordination model", "human-like solutions", "generic demonstration task", "gaze behavior", "action execution", "body positioning", "locomotion", "synthesis framework", "whole-body motion planning", "human-human interactions", "motion coordination patterns", "complex spatial constraints", "virtual human demonstrator", "body placement", "Observers", "Planning", "Visualization", "Collision avoidance", "Data models", "Computational modeling", "Image color analysis", "Virtual Trainers", "Motion Planning", "Intelligent Virtual Humans", "Virtual trainers", "motion planning", "intelligent virtual humans", "Computer Graphics", "Computer Simulation", "Gestures", "Humans", "Interpersonal Relations", "Locomotion", "Models, Theoretical", "User-Computer Interface"], "referenced_by": ["10.1145/2897826.2927310", "10.1145/2980179.2982444", "10.1007/s11390-017-1742-y"], "referencing": ["10.1109/ICHR.2006.321322", "10.1109/38.708559", "10.1109/MCG.2011.109", "10.1109/ICHR.2006.321322", "10.1109/38.708559", "10.1109/MCG.2011.109", "10.1109/ICHR.2006.321322", "10.1109/38.708559", "10.1109/MCG.2011.109", "10.1145/280765.280842", "10.1145/1599470.1599495", "10.1145/2342896.2342938", "10.1145/1891903.1891915", "10.1145/1015706.1015756", "10.1145/1186822.1073313", "10.1145/1230100.1230123", "10.1145/1833349.1781157", "10.1145/1275808.1276386", "10.1145/1882261.1866160", "10.1145/2185520.2335379", "10.1145/1138450.1138457", "10.1145/2366145.2366175", "10.1145/2159616.2159632", "10.1145/2580947", "10.1145/1028523.1028568", "10.1145/280765.280842", "10.1145/1599470.1599495", "10.1145/2342896.2342938", "10.1145/1891903.1891915", "10.1145/1015706.1015756", "10.1145/1186822.1073313", "10.1145/1230100.1230123", "10.1145/1833349.1781157", "10.1145/1275808.1276386", "10.1145/1882261.1866160", "10.1145/2185520.2335379", "10.1145/1138450.1138457", "10.1145/2366145.2366175", "10.1145/2159616.2159632", "10.1145/2580947", "10.1145/1028523.1028568", "10.1145/280765.280842", "10.1145/1599470.1599495", "10.1145/2342896.2342938", "10.1145/1891903.1891915", "10.1145/1015706.1015756", "10.1145/1186822.1073313", "10.1145/1230100.1230123", "10.1145/1833349.1781157", "10.1145/1275808.1276386", "10.1145/1882261.1866160", "10.1145/2185520.2335379", "10.1145/1138450.1138457", "10.1145/2366145.2366175", "10.1145/2159616.2159632", "10.1145/2580947", "10.1145/1028523.1028568", "10.1007/978-3-319-09767-1_24", "10.1080/088395199117324", "10.1152/jn.01156.2003", "10.1152/jn.01379.2007", "10.1007/978-3-642-16958-8_23", "10.1002/cav.15", "10.1007/978-3-642-29050-3_5", "10.1111/j.1467-8659.2006.00965.x", "10.1111/j.1467-8659.2011.02051.x", "10.1177/0278364909352098", "10.1177/0278364910386985", "10.1002/cav.365", "10.1007/978-3-642-23974-8_17", "10.1016/j.ijhcs.2004.09.001", "10.1007/978-3-319-09767-1_24", "10.1080/088395199117324", "10.1152/jn.01156.2003", "10.1152/jn.01379.2007", "10.1007/978-3-642-16958-8_23", "10.1002/cav.15", "10.1007/978-3-642-29050-3_5", "10.1111/j.1467-8659.2006.00965.x", "10.1111/j.1467-8659.2011.02051.x", "10.1177/0278364909352098", "10.1177/0278364910386985", "10.1002/cav.365", "10.1007/978-3-642-23974-8_17", "10.1016/j.ijhcs.2004.09.001", "10.1007/978-3-319-09767-1_24", "10.1080/088395199117324", "10.1152/jn.01156.2003", "10.1152/jn.01379.2007", "10.1007/978-3-642-16958-8_23", "10.1002/cav.15", "10.1007/978-3-642-29050-3_5", "10.1111/j.1467-8659.2006.00965.x", "10.1111/j.1467-8659.2011.02051.x", "10.1177/0278364909352098", "10.1177/0278364910386985", "10.1002/cav.365", "10.1007/978-3-642-23974-8_17", "10.1016/j.ijhcs.2004.09.001"]}, "10.1109/TVCG.2015.2446472": {"doi": "10.1109/TVCG.2015.2446472", "author": ["T. Kwok", "Y. Zhang", "C. C. L. Wang", "Y. Liu", "K. Tang"], "title": "Styling Evolution for Tight-Fitting Garments", "year": "2016", "abstract": "We present an evolution method for designing the styling curves of garments. The procedure of evolution is driven by aesthetics-inspired scores to evaluate the quality of styling designs, where the aesthetic considerations are represented in the form of streamlines on human bodies. A dual representation is introduced in our platform to process the styling curves of designs, based on which robust methods for realizing the operations of evolution are developed. Starting from a given set of styling designs on human bodies, we demonstrate the effectiveness of set evolution inspired by aesthetic factors. The evolution is adaptive to the change of aesthetic inspirations. By this adaptation, our platform can automatically generate new designs fulfilling the demands of variations in different human bodies and poses.", "keywords": ["clothing", "design engineering", "production engineering computing", "quality control", "aesthetic inspirations", "aesthetic factors", "set evolution", "dual representation", "human bodies", "streamlines", "styling design quality", "aesthetics-inspired scores", "garment styling curve design", "tight-fitting garments", "styling evolution", "Clothing", "Three-dimensional displays", "Shape", "Solid modeling", "Histograms", "Algorithm design and analysis", "Robustness", "styling evolution", "aesthetics inspired", "design automation", "tight-fitting", "garment", "Styling evolution", "aesthetics inspired", "design automation", "tight-fitting", "garment", "Adult", "Clothing", "Computer Graphics", "Computer-Aided Design", "Esthetics", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Young Adult"], "referenced_by": ["10.1109/ICIP.2019.8803819", "10.1145/3197517.3201310", "10.1145/3272127.3275074", "10.1145/2897824.2925896", "10.1145/3306346.3322991", "10.1145/3386569.3392477", "10.1007/s11042-016-3688-4", "10.1016/j.cad.2017.03.002", "10.1016/j.cad.2018.10.004", "10.1155/2019/8069373", "10.1016/j.cad.2019.102789", "10.1016/j.cad.2020.102911", "10.1177/1558925020966664"], "referencing": ["10.1109/TVCG.2011.115", "10.1109/MCG.2007.1", "10.1109/MCG.2009.155", "10.1109/TVCG.2011.115", "10.1109/MCG.2007.1", "10.1109/MCG.2009.155", "10.1109/TVCG.2011.115", "10.1109/MCG.2007.1", "10.1109/MCG.2009.155", "10.1145/2010324.1964985", "10.1145/2185520.2185553", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964928", "10.1145/2185520.2185551", "10.1145/2461912.2461933", "10.1145/127719.122752", "10.1145/192161.192167", "10.1145/1015706.1015736", "10.1145/1073204.1073207", "10.1145/1015706.1015811", "10.1145/1015706.1015812", "10.1145/1015706.1015810", "10.1145/2070781.2024160", "10.1145/2366145.2366184", "10.1145/2366145.2366186", "10.1145/274363.274366", "10.1145/571647.571650", "10.1145/2461912.2461975", "10.1145/2185520.2335388", "10.1145/2601097.2601166", "10.1145/2185520.2185531", "10.1145/1276377.1276433", "10.1145/1276377.1276447", "10.1145/1073204.1073228", "10.1145/258734.258849", "10.1145/2010324.1964973", "10.1145/2010324.1964985", "10.1145/2185520.2185553", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964928", "10.1145/2185520.2185551", "10.1145/2461912.2461933", "10.1145/127719.122752", "10.1145/192161.192167", "10.1145/1015706.1015736", "10.1145/1073204.1073207", "10.1145/1015706.1015811", "10.1145/1015706.1015812", "10.1145/1015706.1015810", "10.1145/2070781.2024160", "10.1145/2366145.2366184", "10.1145/2366145.2366186", "10.1145/274363.274366", "10.1145/571647.571650", "10.1145/2461912.2461975", "10.1145/2185520.2335388", "10.1145/2601097.2601166", "10.1145/2185520.2185531", "10.1145/1276377.1276433", "10.1145/1276377.1276447", "10.1145/1073204.1073228", "10.1145/258734.258849", "10.1145/2010324.1964973", "10.1145/2010324.1964985", "10.1145/2185520.2185553", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964928", "10.1145/2185520.2185551", "10.1145/2461912.2461933", "10.1145/127719.122752", "10.1145/192161.192167", "10.1145/1015706.1015736", "10.1145/1073204.1073207", "10.1145/1015706.1015811", "10.1145/1015706.1015812", "10.1145/1015706.1015810", "10.1145/2070781.2024160", "10.1145/2366145.2366184", "10.1145/2366145.2366186", "10.1145/274363.274366", "10.1145/571647.571650", "10.1145/2461912.2461975", "10.1145/2185520.2335388", "10.1145/2601097.2601166", "10.1145/2185520.2185531", "10.1145/1276377.1276433", "10.1145/1276377.1276447", "10.1145/1073204.1073228", "10.1145/258734.258849", "10.1145/2010324.1964973", "10.1111/j.1467-8659.2006.00982.x", "10.1111/j.1467-8659.2005.00883.x", "10.1016/S0010-4485(03)00024-1", "10.1111/j.1467-8659.2012.03042.x", "10.1016/j.cag.2013.05.010", "10.1016/j.cad.2010.11.008", "10.1007/s00371-006-0067-6", "10.1111/j.1467-8659.2006.00982.x", "10.1111/j.1467-8659.2005.00883.x", "10.1016/S0010-4485(03)00024-1", "10.1111/j.1467-8659.2012.03042.x", "10.1016/j.cag.2013.05.010", "10.1016/j.cad.2010.11.008", "10.1007/s00371-006-0067-6", "10.1111/j.1467-8659.2006.00982.x", "10.1111/j.1467-8659.2005.00883.x", "10.1016/S0010-4485(03)00024-1", "10.1111/j.1467-8659.2012.03042.x", "10.1016/j.cag.2013.05.010", "10.1016/j.cad.2010.11.008", "10.1007/s00371-006-0067-6"]}, "10.1109/TVCG.2015.2440231": {"doi": "10.1109/TVCG.2015.2440231", "author": ["C. Camporesi", "M. Kallmann"], "title": "The Effects of Avatars, Stereo Vision and Display Size on Reaching and Motion Reproduction", "year": "2016", "abstract": "Thanks to recent advances on motion capture devices and stereoscopic consumer displays, animated virtual characters can now realistically interact with users in a variety of applications. We investigate in this paper the effect of avatars, stereo vision and display size on task execution in immersive virtual environments. We report results obtained with three experiments in varied configurations that are commonly used in rehabilitation applications. The first experiment analyzes the accuracy of reaching tasks under different system configurations: with and without an avatar, with and without stereo vision, and employing a 2D desktop monitor versus a large multi-tile visualization display. The second experiment analyzes the use of avatars and user-perspective stereo vision on the ability to perceive and subsequently reproduce motions demonstrated by an autonomous virtual character. The third experiment evaluates the overall user experience with a complete immersive user interface for motion modeling by direct demonstration. Our experiments expose and quantify the benefits of using stereo vision and avatars, and show that the use of avatars improve the quality of produced motions and the resemblance of replicated msotions; however, direct interaction in user-perspective leads to tasks executed in less time and to targets more accurately reached. These and additional tradeoffs are important for the effective design of avatar-based training systems.", "keywords": ["avatars", "data visualisation", "stereo image processing", "avatar-based training system", "replicated motion resemblance", "produced motion quality", "direct demonstration", "motion modeling", "immersive user interface", "overall user experience", "autonomous virtual character", "multitile visualization display", "2D desktop monitor", "immersive virtual environments", "task execution", "animated virtual characters", "stereoscopic consumer displays", "motion capture devices", "motion reproduction", "reaching", "display size", "stereo vision", "Avatars", "Stereo vision", "Training", "Trajectory", "Visualization", "Tracking", "Solid modeling", "Virtual Reality", "3D Interaction", "Avatars", "Motion Capture", "Perception", "Training Systems", "Virtual reality", "3D interaction", "avatars", "motion capture", "perception", "training systems"], "referenced_by": ["10.1002/jsid.618", "10.1016/j.apergo.2017.05.007", "10.1016/j.humov.2017.11.002", "10.3390/technologies5040077", "10.1080/10447318.2019.1699746", "10.1371/journal.pone.0239226", "10.1007/s10055-020-00481-3"], "referencing": ["10.1109/TVCG.2012.163", "10.1109/TVCG.2012.43", "10.1109/3DUI.2013.6550196", "10.1109/TVCG.2013.32", "10.1109/TVCG.2012.163", "10.1109/TVCG.2012.43", "10.1109/3DUI.2013.6550196", "10.1109/TVCG.2013.32", "10.1109/TVCG.2012.163", "10.1109/TVCG.2012.43", "10.1109/3DUI.2013.6550196", "10.1109/TVCG.2013.32", "10.1145/2470654.2466171", "10.1145/2501988.2502045", "10.1145/1080402.1080411", "10.1145/1498700.1498702", "10.1145/642611.642650", "10.1145/1056808.1056875", "10.1145/1180495.1180518", "10.1145/1101616.1101632", "10.1145/882262.882304", "10.1145/2185520.2185587", "10.1145/1577755.1577757", "10.1145/2470654.2466171", "10.1145/2501988.2502045", "10.1145/1080402.1080411", "10.1145/1498700.1498702", "10.1145/642611.642650", "10.1145/1056808.1056875", "10.1145/1180495.1180518", "10.1145/1101616.1101632", "10.1145/882262.882304", "10.1145/2185520.2185587", "10.1145/1577755.1577757", "10.1145/2470654.2466171", "10.1145/2501988.2502045", "10.1145/1080402.1080411", "10.1145/1498700.1498702", "10.1145/642611.642650", "10.1145/1056808.1056875", "10.1145/1180495.1180518", "10.1145/1101616.1101632", "10.1145/882262.882304", "10.1145/2185520.2185587", "10.1145/1577755.1577757", "10.1007/978-3-642-15892-6_9", "10.1016/j.apmr.2004.01.028", "10.1162/1054746042545292", "10.1162/105474698565640", "10.1162/1054746042545238", "10.2466/PMS.70.1.35-45", "10.1016/j.apmr.2003.09.020", "10.1146/annurev.psych.57.102904.190152", "10.3758/BF03204150", "10.1068/p5096", "10.3758/BF03337021", "10.3389/fneur.2012.00110", "10.1007/978-3-642-15892-6_9", "10.1016/j.apmr.2004.01.028", "10.1162/1054746042545292", "10.1162/105474698565640", "10.1162/1054746042545238", "10.2466/PMS.70.1.35-45", "10.1016/j.apmr.2003.09.020", "10.1146/annurev.psych.57.102904.190152", "10.3758/BF03204150", "10.1068/p5096", "10.3758/BF03337021", "10.3389/fneur.2012.00110", "10.1007/978-3-642-15892-6_9", "10.1016/j.apmr.2004.01.028", "10.1162/1054746042545292", "10.1162/105474698565640", "10.1162/1054746042545238", "10.2466/PMS.70.1.35-45", "10.1016/j.apmr.2003.09.020", "10.1146/annurev.psych.57.102904.190152", "10.3758/BF03204150", "10.1068/p5096", "10.3758/BF03337021", "10.3389/fneur.2012.00110"]}, "10.1109/TVCG.2015.2446467": {"doi": "10.1109/TVCG.2015.2446467", "author": ["S. Friston", "P. Karlstr\u00f6m", "A. Steed"], "title": "The Effects of Low Latency on Pointing and Steering Tasks", "year": "2016", "abstract": "Latency is detrimental to interactive systems, especially pseudo-physical systems that emulate real-world behaviour. It prevents users from making quick corrections to their movement, and causes their experience to deviate from their expectations. Latency is a result of the processing and transport delays inherent in current computer systems. As such, while a number of studies have hypothesized that any latency will have a degrading effect, few have been able to test this for latencies less than $\\scriptstyle \\sim$ 50 ms. In this study we investigate the effects of latency on pointing and steering tasks. We design an apparatus with a latency lower than typical interactive systems, using it to perform interaction tasks based on Fitts's law and the Steering law. We find evidence that latency begins to affect performance at $\\scriptstyle \\sim$ 16 ms, and that the effect is non-linear. Further, we find latency does not affect the various components of an aiming motion equally. We propose a three stage characterisation of pointing movements with each stage affected independently by latency. We suggest that understanding how users execute movement is essential for studying latency at low levels, as high level metrics such as total movement time may be misleading.", "keywords": ["Delays", "Mice", "Adaptation models", "Integrated circuit modeling", "Tracking", "Jitter", "Computers", "Latency", "Indirect Input", "HCI", "Fitts\u2019s law", "Human Factors", "Latency", "indirect input", "HCI", "Fitts's law", "human factors"], "referenced_by": ["10.1109/AIVR46125.2019.00066", "10.1109/VR46266.2020.00103", "10.1145/3390464", "10.1007/978-3-319-58475-1_1", "10.1007/s12209-017-0111-9", "10.1007/s41233-018-0023-z"], "referencing": ["10.1109/TVCG.2014.30", "10.1109/TVCG.2014.30", "10.1109/TVCG.2014.30", "10.1145/2087756.2087869", "10.1145/198425.198426", "10.1145/2087756.2087869", "10.1145/198425.198426", "10.1145/2087756.2087869", "10.1145/198425.198426", "10.1007/s00422-008-0235-z", "10.1111/ejn.12211", "10.1037/h0055392", "10.1007/s00422-009-0336-3", "10.1207/s15327051hci2003_3", "10.1016/j.ijhcs.2004.09.004", "10.1111/j.1467-9280.2006.01784.x", "10.1007/s00221-012-3277-3", "10.1007/s00221-007-0996-y", "10.1080/00222895.1989.10735486", "10.1037/0033-2909.127.3.342", "10.1037/0033-295X.95.3.340", "10.1007/s00422-008-0235-z", "10.1111/ejn.12211", "10.1037/h0055392", "10.1007/s00422-009-0336-3", "10.1207/s15327051hci2003_3", "10.1016/j.ijhcs.2004.09.004", "10.1111/j.1467-9280.2006.01784.x", "10.1007/s00221-012-3277-3", "10.1007/s00221-007-0996-y", "10.1080/00222895.1989.10735486", "10.1037/0033-2909.127.3.342", "10.1037/0033-295X.95.3.340", "10.1007/s00422-008-0235-z", "10.1111/ejn.12211", "10.1037/h0055392", "10.1007/s00422-009-0336-3", "10.1207/s15327051hci2003_3", "10.1016/j.ijhcs.2004.09.004", "10.1111/j.1467-9280.2006.01784.x", "10.1007/s00221-012-3277-3", "10.1007/s00221-007-0996-y", "10.1080/00222895.1989.10735486", "10.1037/0033-2909.127.3.342", "10.1037/0033-295X.95.3.340"]}, "10.1109/TVCG.2015.2440233": {"doi": "10.1109/TVCG.2015.2440233", "author": ["D. L\u00f3pez", "L. Oehlberg", "C. Doger", "T. Isenberg"], "title": "Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration", "year": "2016", "abstract": "We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.", "keywords": ["data visualisation", "stereo image processing", "mobile touch navigation", "stereoscopic viewing environment", "3D data exploration", "3D visualizations", "control-display space mapping", "monoscopic displays", "stereoscopic displays", "Three-dimensional displays", "Stereo image processing", "Cameras", "Data visualization", "Navigation", "Mobile handsets", "Visualization", "Visualization of 3D data", "human-computer interaction", "expert interaction", "direct-touch input", "mobile displays", "stereoscopic environments", "VR", "AR", "conceptual model of interaction", "interaction reference frame mapping", "observational study", "Visualization of 3D data", "human-computer interaction", "expert interaction", "direct-touch input", "mobile displays", "stereoscopic environments", "VR", "AR", "conceptual model of interaction", "interaction reference frame mapping", "observational study"], "referenced_by": ["10.1109/PACIFICVIS.2017.8031578", "10.1109/TVCG.2016.2599217", "10.1109/TVCG.2017.2745941", "10.1109/COGSIMA.2018.8423999", "10.1109/SIVE.2018.8577121", "10.1109/TVCG.2019.2898763", "10.1109/TVCG.2018.2848906", "10.1109/VR46266.2020.00057", "10.1109/TVCG.2020.3023567", "10.1145/3265748", "10.1007/978-3-319-45853-3_6", "10.1007/978-3-319-60922-5_28", "10.1162/PRES_a_00287", "10.1007/978-3-030-01388-2_4", "10.1111/cgf.13716", "10.1111/cgf.13710", "10.1007/978-3-030-29390-1_26"], "referencing": ["10.1109/TVCG.2011.283", "10.1109/MC.2013.178", "10.1109/38.279036", "10.1109/MMUL.2006.69", "10.1109/3DUI.2011.5759220", "10.1109/3DUI.2012.6184194", "10.1109/MCG.2010.30", "10.1109/TVCG.2012.292", "10.1109/TVCG.2011.224", "10.1109/38.963459", "10.1109/TVCG.2013.126", "10.1109/TVCG.2011.279", "10.1109/TVCG.2011.283", "10.1109/MC.2013.178", "10.1109/38.279036", "10.1109/MMUL.2006.69", "10.1109/3DUI.2011.5759220", "10.1109/3DUI.2012.6184194", "10.1109/MCG.2010.30", "10.1109/TVCG.2012.292", "10.1109/TVCG.2011.224", "10.1109/38.963459", "10.1109/TVCG.2013.126", "10.1109/TVCG.2011.279", "10.1109/TVCG.2011.283", "10.1109/MC.2013.178", "10.1109/38.279036", "10.1109/MMUL.2006.69", "10.1109/3DUI.2011.5759220", "10.1109/3DUI.2012.6184194", "10.1109/MCG.2010.30", "10.1109/TVCG.2012.292", "10.1109/TVCG.2011.224", "10.1109/38.963459", "10.1109/TVCG.2013.126", "10.1109/TVCG.2011.279", "10.1145/229459.229467", "10.1145/1731903.1731930", "10.1145/2512349.2512819", "10.1145/1978942.1979142", "10.1145/1753326.1753725", "10.1145/2556288.2557134", "10.1145/129888.129892", "10.1145/300523.300542", "10.1145/2396636.2396640", "10.1145/2406367.2406371", "10.1145/2350046.2350078", "10.1145/2030112.2030183", "10.1145/2350046.2350062", "10.1145/2037373.2037463", "10.1145/1409240.1409267", "10.1145/1517664.1517705", "10.1145/2512349.2512807", "10.1145/1978942.1979387", "10.1145/1124772.1124963", "10.1145/1753326.1753671", "10.1145/2399016.2399053", "10.1145/1643928.1643961", "10.1145/274644.274689", "10.1145/1012551.1012576", "10.1145/142920.134039", "10.1145/1188816.1188818", "10.1145/229459.229467", "10.1145/1731903.1731930", "10.1145/2512349.2512819", "10.1145/1978942.1979142", "10.1145/1753326.1753725", "10.1145/2556288.2557134", "10.1145/129888.129892", "10.1145/300523.300542", "10.1145/2396636.2396640", "10.1145/2406367.2406371", "10.1145/2350046.2350078", "10.1145/2030112.2030183", "10.1145/2350046.2350062", "10.1145/2037373.2037463", "10.1145/1409240.1409267", "10.1145/1517664.1517705", "10.1145/2512349.2512807", "10.1145/1978942.1979387", "10.1145/1124772.1124963", "10.1145/1753326.1753671", "10.1145/2399016.2399053", "10.1145/1643928.1643961", "10.1145/274644.274689", "10.1145/1012551.1012576", "10.1145/142920.134039", "10.1145/1188816.1188818", "10.1145/229459.229467", "10.1145/1731903.1731930", "10.1145/2512349.2512819", "10.1145/1978942.1979142", "10.1145/1753326.1753725", "10.1145/2556288.2557134", "10.1145/129888.129892", "10.1145/300523.300542", "10.1145/2396636.2396640", "10.1145/2406367.2406371", "10.1145/2350046.2350078", "10.1145/2030112.2030183", "10.1145/2350046.2350062", "10.1145/2037373.2037463", "10.1145/1409240.1409267", "10.1145/1517664.1517705", "10.1145/2512349.2512807", "10.1145/1978942.1979387", "10.1145/1124772.1124963", "10.1145/1753326.1753671", "10.1145/2399016.2399053", "10.1145/1643928.1643961", "10.1145/274644.274689", "10.1145/1012551.1012576", "10.1145/142920.134039", "10.1145/1188816.1188818", "10.1007/978-3-642-17274-8_35", "10.1016/j.procs.2013.11.028", "10.1111/1467-8659.00194", "10.1007/978-3-642-40483-2_19", "10.1515/icom.2013.0020", "10.1007/978-3-642-39330-3_76", "10.1007/978-3-642-23771-3_22", "10.1207/s15327051hci0702_1", "10.1007/978-3-540-73111-5_87", "10.1111/j.1467-8659.2012.03115.x", "10.1007/s11042-010-0660-6", "10.1162/pres.1997.6.4.399", "10.1016/S0079-7421(08)60293-5", "10.3389/fnhum.2013.00081", "10.1037/0096-1523.15.3.493", "10.1007/s002210100768", "10.1007/s002210100846", "10.1007/978-3-642-17274-8_35", "10.1016/j.procs.2013.11.028", "10.1111/1467-8659.00194", "10.1007/978-3-642-40483-2_19", "10.1515/icom.2013.0020", "10.1007/978-3-642-39330-3_76", "10.1007/978-3-642-23771-3_22", "10.1207/s15327051hci0702_1", "10.1007/978-3-540-73111-5_87", "10.1111/j.1467-8659.2012.03115.x", "10.1007/s11042-010-0660-6", "10.1162/pres.1997.6.4.399", "10.1016/S0079-7421(08)60293-5", "10.3389/fnhum.2013.00081", "10.1037/0096-1523.15.3.493", "10.1007/s002210100768", "10.1007/s002210100846", "10.1007/978-3-642-17274-8_35", "10.1016/j.procs.2013.11.028", "10.1111/1467-8659.00194", "10.1007/978-3-642-40483-2_19", "10.1515/icom.2013.0020", "10.1007/978-3-642-39330-3_76", "10.1007/978-3-642-23771-3_22", "10.1207/s15327051hci0702_1", "10.1007/978-3-540-73111-5_87", "10.1111/j.1467-8659.2012.03115.x", "10.1007/s11042-010-0660-6", "10.1162/pres.1997.6.4.399", "10.1016/S0079-7421(08)60293-5", "10.3389/fnhum.2013.00081", "10.1037/0096-1523.15.3.493", "10.1007/s002210100768", "10.1007/s002210100846"]}, "10.1109/TVCG.2015.2443783": {"doi": "10.1109/TVCG.2015.2443783", "author": ["G. Samaraweera", "R. Guo", "J. Quarles"], "title": "Head Tracking Latency in Virtual Environments Revisited: Do Users with Multiple Sclerosis Notice Latency Less?", "year": "2016", "abstract": "Latency (i.e., time delay) in a virtual environment is known to disrupt user performance, presence and induce simulator sickness. Thus, with emerging use of virtual rehabilitation, the target populations' latency perception thresholds need to be considered to fully understand and possibly control the implications of latency in a Virtual Rehabilitation environment. We present a study that quantifies the latency discrimination thresholds of a yet untested population - a specific subset of mobility impaired participants where participants suffer from Multiple Sclerosis - and compare the results to a control group of healthy participants. The study was modeled after previous latency discrimination research and shows significant differences in latency perception between the two populations with MS participants showing lower sensitivity to latency than healthy participants.", "keywords": ["handicapped aids", "patient rehabilitation", "virtual reality", "head tracking latency discrimination research", "virtual rehabilitation environment", "virtual reality", "Sociology", "Statistics", "Atmospheric measurements", "Particle measurements", "Training", "Visualization", "Three-dimensional displays", "Latency", "Multiple Sclerosis", "Rehabilitation", "User Studies", "Virtual Reality", "Latency", "multiple sclerosis", "rehabilitation", "user studies", "virtual reality", "Adult", "Computer Graphics", "Head Movements", "Humans", "Middle Aged", "Multiple Sclerosis", "User-Computer Interface"], "referenced_by": ["10.1109/VR.2018.8446432", "10.1109/PCS48520.2019.8954518", "10.3390/s17051037"], "referencing": ["10.1109/THFE.1963.231283", "10.1109/VR.2003.1191132", "10.1109/VR.2015.7223329", "10.1109/VR.1999.756954", "10.1109/VR.2001.913793", "10.1109/TVCG.2007.1029", "10.1109/THFE.1963.231283", "10.1109/VR.2003.1191132", "10.1109/VR.2015.7223329", "10.1109/VR.1999.756954", "10.1109/VR.2001.913793", "10.1109/TVCG.2007.1029", "10.1109/THFE.1963.231283", "10.1109/VR.2003.1191132", "10.1109/VR.2015.7223329", "10.1109/VR.1999.756954", "10.1109/VR.2001.913793", "10.1109/TVCG.2007.1029", "10.1145/311535.311569", "10.1145/1012551.1012559", "10.1145/1275511.1275514", "10.1145/505008.505026", "10.1145/1450579.1450606", "10.1145/311535.311569", "10.1145/1012551.1012559", "10.1145/1275511.1275514", "10.1145/505008.505026", "10.1145/1450579.1450606", "10.1145/311535.311569", "10.1145/1012551.1012559", "10.1145/1275511.1275514", "10.1145/505008.505026", "10.1145/1450579.1450606", "10.1089/109493103322011524", "10.1212/01.wnl.0000194255.82542.6b", "10.1177/154193120304702001", "10.1177/1352458509106712", "10.1177/154596830101500308", "10.1177/1545968308320641", "10.1177/154193120404802306", "10.1177/154193129904302203", "10.1207/s15327108ijap0303_3", "10.1310/tsr1705-345", "10.1089/109493103322011524", "10.1212/01.wnl.0000194255.82542.6b", "10.1177/154193120304702001", "10.1177/1352458509106712", "10.1177/154596830101500308", "10.1177/1545968308320641", "10.1177/154193120404802306", "10.1177/154193129904302203", "10.1207/s15327108ijap0303_3", "10.1310/tsr1705-345", "10.1089/109493103322011524", "10.1212/01.wnl.0000194255.82542.6b", "10.1177/154193120304702001", "10.1177/1352458509106712", "10.1177/154596830101500308", "10.1177/1545968308320641", "10.1177/154193120404802306", "10.1177/154193129904302203", "10.1207/s15327108ijap0303_3", "10.1310/tsr1705-345"]}, "10.1109/TVCG.2016.2532240": {"doi": "10.1109/TVCG.2016.2532240", "author": ["H. Strobelt", "D. Oelke", "B. C. Kwon", "T. Schreck", "H. Pfister"], "title": "Errata to \u201cGuidelines for Effective Usage of Text Highlighting Techniques\u201d [1]", "year": "2016", "abstract": "Presents corrections for the paper, \u201cGuidelines for effective usage of text highlighting techniques,\u201d (Strobelt, H., et al), IEEE Trans. Vis. Comput.Graph., vol. 22, no. 1, pp. 489\u2013498, Jan. 2016. ", "keywords": ["Text mining", "Statistical analysis"], "referenced_by": [], "referencing": []}}