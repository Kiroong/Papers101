{"10.1109/TVCG.2011.154": {"doi": "10.1109/TVCG.2011.154", "author": ["S. Hillaire", "A. Lecuyer", "T. Regia-Corte", "R. Cozot", "J. Royan", "G. Breton"], "title": "Design and Application of Real-Time Visual Attention Model for the Exploration of 3D Virtual Environments", "year": "2012", "abstract": "This paper studies the design and application of a novel visual attention model designed to compute user's gaze position automatically, i.e., without using a gaze-tracking system. The model we propose is specifically designed for real-time first-person exploration of 3D virtual environments. It is the first model adapted to this context which can compute in real time a continuous gaze point position instead of a set of 3D objects potentially observed by the user. To do so, contrary to previous models which use a mesh-based representation of visual objects, we introduce a representation based on surface-elements. Our model also simulates visual reflexes and the cognitive processes which take place in the brain such as the gaze behavior associated to first-person navigation in the virtual environment. Our visual attention model combines both bottom-up and top-down components to compute a continuous gaze point position on screen that hopefully matches the user's one. We conducted an experiment to study and compare the performance of our method with a state-of-the-art approach. Our results are found significantly better with sometimes more than 100 percent of accuracy gained. This suggests that computing a gaze point in a 3D virtual environment in real time is possible and is a valid approach, compared to object-based approaches. Finally, we expose different applications of our model when exploring virtual environments. We present different algorithms which can improve or adapt the visual feedback of virtual environments based on gaze information. We first propose a level-of-detail approach that heavily relies on multiple-texture sampling. We show that it is possible to use the gaze information of our visual attention model to increase visual quality where the user is looking, while maintaining a high-refresh rate. Second, we introduce the use of the visual attention model in three visual effects inspired by the human visual system namely: depth-of-field blur, camera motions, and dynamic luminance. All these effects are computed based on the simulated gaze of the user, and are meant to improve user's sensations in future virtual reality applications.", "keywords": ["mesh generation", "real-time systems", "solid modelling", "real-time visual attention model", "3D virtual environments", "3D objects", "mesh-based representation", "continuous gaze point position", "level-of-detail approach", "multiple-texture sampling", "Visualization", "Computational modeling", "Solid modeling", "Adaptation models", "Humans", "Three dimensional displays", "Real time systems", "Visual attention model", "first-person exploration", "gaze tracking", "visual effects.", "Adult", "Algorithms", "Computer Simulation", "Eye Movements", "Female", "Humans", "Image Processing, Computer-Assisted", "Male", "Models, Biological", "Software", "Stochastic Processes", "User-Computer Interface", "Visual Fields", "Visual Perception"], "referenced_by": ["10.1109/TASE.2012.2214772", "10.1109/TMM.2015.2484221", "10.1109/VCIP.2013.6706362", "10.1109/VR.2019.8797816", "10.1109/ICSGEA.2019.00054", "10.1109/TVCG.2019.2893668", "10.1145/2811258", "10.1145/3352763", "10.1016/j.jvcir.2013.11.009", "10.1111/cgf.12092", "10.1177/1473871615609787", "10.4018/978-1-4666-8723-3.ch002", "10.4018/978-1-5225-5204-8.ch069", "10.1155/2018/2601652", "10.1007/978-3-642-39405-8_12"], "referencing": ["10.1109/VR.2008.4480749", "10.1109/34.730558", "10.1109/TVCG.2008.82", "10.1109/MCG.2008.113", "10.1109/VR.2009.4811012", "10.1109/VR.2009.4811027", "10.1109/VR.2008.4480749", "10.1109/34.730558", "10.1109/TVCG.2008.82", "10.1109/MCG.2008.113", "10.1109/VR.2009.4811012", "10.1109/VR.2009.4811027", "10.1109/VR.2008.4480749", "10.1109/34.730558", "10.1109/TVCG.2008.82", "10.1109/MCG.2008.113", "10.1109/VR.2009.4811012", "10.1109/VR.2009.4811027", "10.1145/1643928.1643941", "10.1145/1394281.1394288", "10.1145/1404435.1404437", "10.1145/1944745.1944760", "10.1145/566570.566575", "10.1145/1108590.1108595", "10.1145/1643928.1643941", "10.1145/1394281.1394288", "10.1145/1404435.1404437", "10.1145/1944745.1944760", "10.1145/566570.566575", "10.1145/1108590.1108595", "10.1145/1643928.1643941", "10.1145/1394281.1394288", "10.1145/1404435.1404437", "10.1145/1944745.1944760", "10.1145/566570.566575", "10.1145/1108590.1108595", "10.1007/978-3-7091-6242-2_21", "10.1080/13506280444000661", "10.1016/j.visres.2004.07.042", "10.1037/h0087326", "10.1002/col.5080150308", "10.1113/jphysiol.1965.sp007718", "10.1111/j.1467-8659.2010.01651.x", "10.1016/0010-0285(80)90005-5", "10.1007/978-1-4899-5379-7", "10.1007/978-3-7091-6242-2_21", "10.1080/13506280444000661", "10.1016/j.visres.2004.07.042", "10.1037/h0087326", "10.1002/col.5080150308", "10.1113/jphysiol.1965.sp007718", "10.1111/j.1467-8659.2010.01651.x", "10.1016/0010-0285(80)90005-5", "10.1007/978-1-4899-5379-7", "10.1007/978-3-7091-6242-2_21", "10.1080/13506280444000661", "10.1016/j.visres.2004.07.042", "10.1037/h0087326", "10.1002/col.5080150308", "10.1113/jphysiol.1965.sp007718", "10.1111/j.1467-8659.2010.01651.x", "10.1016/0010-0285(80)90005-5", "10.1007/978-1-4899-5379-7"]}, "10.1109/TVCG.2011.129": {"doi": "10.1109/TVCG.2011.129", "author": ["A. Martinet", "G. Casiez", "L. Grisoni"], "title": "Integrality and Separability of Multitouch Interaction Techniques in 3D Manipulation Tasks", "year": "2012", "abstract": "Multitouch displays represent a promising technology for the display and manipulation of data. While the manipulation of 2D data has been widely explored, 3D manipulation with multitouch displays remains largely unexplored. Based on an analysis of the integration and separation of degrees of freedom, we propose a taxonomy for 3D manipulation techniques with multitouch displays. Using that taxonomy, we introduce Depth-Separated Screen-Space (DS3), a new 3D manipulation technique based on the separation of translation and rotation. In a controlled experiment, we compared DS3 with Sticky Tools and Screen-Space. Results show that separating the control of translation and rotation significantly affects performance for 3D manipulation, with DS3 performing faster than the two other techniques.", "keywords": ["human computer interaction", "three-dimensional displays", "touch sensitive screens", "integrality", "separability", "multitouch interaction techniques", "3D manipulation tasks", "multitouch displays", "2D data", "degrees of freedom", "taxonomy", "3D manipulation techniques", "depth-separated screen-space", "DS3", "controlled experiment", "sticky tools", "Three dimensional displays", "Measurement", "Taxonomy", "Visualization", "Jacobian matrices", "Humans", "Mice", "Multitouch displays", "3D manipulation task", "direct manipulation", "DOF separation.", "Adult", "Analysis of Variance", "Computer Graphics", "Depth Perception", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Rotation", "Task Performance and Analysis", "Touch"], "referenced_by": ["10.1109/3DUI.2014.6798879", "10.1109/3DUI.2015.7131733", "10.1109/3DUI.2016.7460025", "10.1109/ISMAR.2014.6948450", "10.1109/IV.2014.83", "10.1109/eCHALLENGES.2015.7441066", "10.1109/VR.2018.8447554", "10.1109/VR.2018.8446295", "10.1109/ACCESS.2019.2906394", "10.1109/TVCG.2020.2973034", "10.1145/2611519", "10.1145/2663204.2663234", "10.1145/2702123.2702244", "10.1145/2788940.2788941", "10.1145/2669485.2669510", "10.1145/3359207", "10.1007/978-3-319-58750-9_18", "10.1007/978-3-319-69487-0_12", "10.1007/s00779-012-0594-2", "10.1007/s41095-018-0105-0", "10.1016/j.cag.2015.10.013", "10.1016/j.entcom.2014.08.007", "10.1016/j.softx.2017.01.002", "10.1080/0144929X.2015.1122084", "10.1111/j.1467-8659.2012.03197.x", "10.2466/22.23.PMS.116.2.382-405", "10.2466/22.23.PMS.116.2a", "10.1007/978-3-319-92069-6_2", "10.3390/info10040152", "10.1007/978-3-642-40483-2_20", "10.1007/s12193-019-00314-x", "10.1016/j.ijhcs.2020.102433"], "referencing": ["10.1109/TABLETOP.2006.26", "10.1109/VR.2011.5759440", "10.1109/3DUI.2010.5444709", "10.1109/HAPTIC.2002.998967", "10.1109/TABLETOP.2006.26", "10.1109/VR.2011.5759440", "10.1109/3DUI.2010.5444709", "10.1109/HAPTIC.2002.998967", "10.1109/TABLETOP.2006.26", "10.1109/VR.2011.5759440", "10.1109/3DUI.2010.5444709", "10.1109/HAPTIC.2002.998967", "10.1145/263407.263408", "10.1145/274644.274688", "10.1145/503376.503397", "10.1145/174630.174631", "10.1145/1731903.1731930", "10.1145/1622176.1622190", "10.1145/1643928.1643942", "10.1145/274644.274689", "10.1145/332040.332403", "10.1145/1449715.1449728", "10.1145/1622176.1622203", "10.1145/1978942.1979387", "10.1145/123078.128726", "10.1145/1978942.1979173", "10.1145/263407.263408", "10.1145/274644.274688", "10.1145/503376.503397", "10.1145/174630.174631", "10.1145/1731903.1731930", "10.1145/1622176.1622190", "10.1145/1643928.1643942", "10.1145/274644.274689", "10.1145/332040.332403", "10.1145/1449715.1449728", "10.1145/1622176.1622203", "10.1145/1978942.1979387", "10.1145/123078.128726", "10.1145/1978942.1979173", "10.1145/263407.263408", "10.1145/274644.274688", "10.1145/503376.503397", "10.1145/174630.174631", "10.1145/1731903.1731930", "10.1145/1622176.1622190", "10.1145/1643928.1643942", "10.1145/274644.274689", "10.1145/332040.332403", "10.1145/1449715.1449728", "10.1145/1622176.1622203", "10.1145/1978942.1979387", "10.1145/123078.128726", "10.1145/1978942.1979173", "10.1007/s00221-008-1355-3", "10.1007/s00221-008-1355-3", "10.1007/s00221-008-1355-3"]}, "10.1109/TVCG.2011.271": {"doi": "10.1109/TVCG.2011.271", "author": ["B. Sajadi", "A. Majumder"], "title": "Autocalibration of Multiprojector CAVE-Like Immersive Environments", "year": "2012", "abstract": "In this paper, we present the first method for the geometric autocalibration of multiple projectors on a set of CAVE-like immersive display surfaces including truncated domes and 4 or 5-wall CAVEs (three side walls, floor, and/or ceiling). All such surfaces can be categorized as swept surfaces and multiple projectors can be registered on them using a single uncalibrated camera without using any physical markers on the surface. Our method can also handle nonlinear distortion in the projectors, common in compact setups where a short throw lens is mounted on each projector. Further, when the whole swept surface is not visible from a single camera view, we can register the projectors using multiple pan and tilted views of the same camera. Thus, our method scales well with different size and resolution of the display. Since we recover the 3D shape of the display, we can achieve registration that is correct from any arbitrary viewpoint appropriate for head-tracked single-user virtual reality systems. We can also achieve wallpapered registration, more appropriate for multiuser collaborative explorations. Though much more immersive than common surfaces like planes and cylinders, general swept surfaces are used today only for niche display environments. Even the more popular 4 or 5-wall CAVE is treated as a piecewise planar surface for calibration purposes and hence projectors are not allowed to be overlapped across the corners. Our method opens up the possibility of using such swept surfaces to create more immersive VR systems without compromising the simplicity of having a completely automatic calibration technique. Such calibration allows completely arbitrary positioning of the projectors in a 5-wall CAVE, without respecting the corners.", "keywords": ["calibration", "cameras", "computational geometry", "display devices", "image registration", "image resolution", "optical projectors", "virtual reality", "multiprojector CAVE-like immersive environments", "geometric autocalibration", "multiple projectors", "CAVE-like immersive display surfaces", "truncated domes", "uncalibrated camera", "physical markers", "nonlinear distortion", "compact setups", "short throw lens", "multiple pan", "tilted views", "display resolution", "3D shape", "arbitrary viewpoint", "head-tracked single-user virtual reality systems", "wallpapered registration", "multiuser collaborative explorations", "general swept surfaces", "niche display environments", "4-wall CAVE", "5-wall CAVE", "piecewise planar surface", "calibration purposes", "VR systems", "automatic calibration technique", "completely arbitrary positioning", "Cameras", "Three dimensional displays", "Shape", "Optimization", "Surface treatment", "Calibration", "Surface reconstruction", "Geometric registration", "calibration", "multiprojector displays", "tiled displays", "CAVEs", "immersive displays."], "referenced_by": ["10.1109/CVPR.2013.136", "10.1109/ISCID.2013.190", "10.1109/MC.2012.429", "10.1109/SciVis.2015.7429493", "10.1109/TVCG.2012.135", "10.1109/TVCG.2016.2532327", "10.1109/WACV.2017.124", "10.1007/978-3-319-48881-3_16", "10.1007/978-3-319-54190-7_32", "10.1007/s12555-012-0431-4", "10.1364/AO.54.000E15", "10.1364/AO.57.000283", "10.4218/etrij.13.0112.0597", "10.1117/12.2040402", "10.3390/s18093063", "10.1117/12.2284359", "10.1007/s11042-018-6930-4", "10.1007/978-981-13-9917-6_63", "10.1016/j.optcom.2019.124723", "10.1117/1.OE.59.1.015101"], "referencing": ["10.1109/TVCG.2009.166", "10.1109/VR.2010.5444797", "10.1109/VR.2000.840488", "10.1109/ISMAR.2004.30", "10.1109/VISUAL.1999.809883", "10.1109/CVPR.2008.4587709", "10.1109/VR.2007.352475", "10.1109/VISUAL.2002.1183793", "10.1109/ICPR.2004.1333994", "10.1109/VISUAL.2001.964508", "10.1109/TVCG.2005.27", "10.1109/CVPR.2004.1315159", "10.1109/TVCG.2007.70586", "10.1109/TVCG.2011.33", "10.1109/TVCG.2009.124", "10.1109/ICCV.1999.791289", "10.1109/TVCG.2009.166", "10.1109/VR.2010.5444797", "10.1109/VR.2000.840488", "10.1109/ISMAR.2004.30", "10.1109/VISUAL.1999.809883", "10.1109/CVPR.2008.4587709", "10.1109/VR.2007.352475", "10.1109/VISUAL.2002.1183793", "10.1109/ICPR.2004.1333994", "10.1109/VISUAL.2001.964508", "10.1109/TVCG.2005.27", "10.1109/CVPR.2004.1315159", "10.1109/TVCG.2007.70586", "10.1109/TVCG.2011.33", "10.1109/TVCG.2009.124", "10.1109/ICCV.1999.791289", "10.1109/TVCG.2009.166", "10.1109/VR.2010.5444797", "10.1109/VR.2000.840488", "10.1109/ISMAR.2004.30", "10.1109/VISUAL.1999.809883", "10.1109/CVPR.2008.4587709", "10.1109/VR.2007.352475", "10.1109/VISUAL.2002.1183793", "10.1109/ICPR.2004.1333994", "10.1109/VISUAL.2001.964508", "10.1109/TVCG.2005.27", "10.1109/CVPR.2004.1315159", "10.1109/TVCG.2007.70586", "10.1109/TVCG.2011.33", "10.1109/TVCG.2009.124", "10.1109/ICCV.1999.791289", "10.1145/1394622.1394624", "10.1145/1342250.1342258", "10.1145/1179352.1141964", "10.1145/280814.280861", "10.1145/1360612.1360676", "10.1145/566570.566639", "10.1145/1394622.1394624", "10.1145/1342250.1342258", "10.1145/1179352.1141964", "10.1145/280814.280861", "10.1145/1360612.1360676", "10.1145/566570.566639", "10.1145/1394622.1394624", "10.1145/1342250.1342258", "10.1145/1179352.1141964", "10.1145/280814.280861", "10.1145/1360612.1360676", "10.1145/566570.566639", "10.1111/j.1467-8659.2009.01676.x", "10.1111/j.1467-8659.2009.01676.x", "10.1111/j.1467-8659.2009.01676.x"]}, "10.1109/TVCG.2011.133": {"doi": "10.1109/TVCG.2011.133", "author": ["I. Karamouzas", "M. Overmars"], "title": "Simulating and Evaluating the Local Behavior of Small Pedestrian Groups", "year": "2012", "abstract": "Recent advancements in local methods have significantly improved the collision avoidance behavior of virtual characters. However, existing methods fail to take into account that in real life pedestrians tend to walk in small groups, consisting mainly of pairs or triples of individuals. We present a novel approach to simulate the walking behavior of such small groups. Our model describes how group members interact with each other, with other groups and individuals. We highlight the potential of our method through a wide range of test-case scenarios. We evaluate the results from our simulations using a number of quantitative quality metrics, and also provide visual and numerical comparisons with video footages of real crowds.", "keywords": ["behavioural sciences computing", "collision avoidance", "pedestrians", "virtual reality", "local behavior", "small pedestrian groups", "collision avoidance", "virtual characters", "quantitative quality metrics", "Legged locomotion", "Computational modeling", "Path planning", "Humans", "Collision avoidance", "Solid modeling", "Organizations", "Multiagent systems", "animation", "virtual reality", "kinematics and dynamics.", "Algorithms", "Computer Graphics", "Computer Simulation", "Humans", "Image Processing, Computer-Assisted", "Models, Theoretical", "Pattern Recognition, Automated", "Reproducibility of Results", "Walking"], "referenced_by": ["10.1109/CVPR.2014.285", "10.1109/CW.2014.13", "10.1109/CW.2014.19", "10.1109/ICRA.2015.7140038", "10.1109/ICRA.2016.7487147", "10.1109/TCSVT.2016.2539878", "10.1109/TVCG.2013.116", "10.1109/TVCG.2015.2391862", "10.1109/VR.2016.7504685", "10.1109/ACCESS.2018.2878733", "10.1109/IROS.2018.8594258", "10.1145/2668064.2668103", "10.1145/3117808", "10.1002/9781118796443.ch6", "10.1002/cav.1512", "10.1002/cav.1629", "10.1002/cav.1742", "10.1002/cplx.21633", "10.1007/978-3-319-70022-9_47", "10.1007/s00371-014-0946-1", "10.1007/s00371-015-1187-7", "10.1007/s10458-014-9252-6", "10.1007/s10462-017-9569-z", "10.1007/s10514-018-9719-4", "10.1007/s11390-014-1469-y", "10.1007/s11432-016-0074-9", "10.1007/s12046-017-0776-8", "10.1111/cgf.12472", "10.1111/cgf.12993", "10.1201/9781315370071-11", "10.1201/9781315370071-2", "10.3390/s18020423", "10.4018/978-1-4666-9629-7.ch025", "10.1134/S1054661818020074", "10.1088/1742-5468/aace27", "10.1016/j.physa.2018.08.046", "10.1007/s10015-018-0459-5", "10.1177/1687814019844060", "10.1016/j.ssci.2019.04.048", "10.1007/978-3-642-36279-8_14", "10.1007/978-3-662-45289-9_1"], "referencing": ["10.1109/ROBOT.2008.4543489", "10.1109/TPCG.2003.1206939", "10.1109/CASA.2003.1199317", "10.1109/MCG.2009.69", "10.1109/ROBOT.2003.1242251", "10.1109/ROBOT.2008.4543489", "10.1109/TPCG.2003.1206939", "10.1109/CASA.2003.1199317", "10.1109/MCG.2009.69", "10.1109/ROBOT.2003.1242251", "10.1109/ROBOT.2008.4543489", "10.1109/TPCG.2003.1206939", "10.1109/CASA.2003.1199317", "10.1109/MCG.2009.69", "10.1109/ROBOT.2003.1242251", "10.1145/1141911.1142008", "10.1145/1360612.1360653", "10.1145/1882261.1866162", "10.1145/311535.311538", "10.1145/1778765.1778860", "10.1145/1599470.1599494", "10.1145/37401.37406", "10.1145/1028523.1028526", "10.1145/1536513.1536540", "10.1145/1618452.1618468", "10.1145/1141911.1142008", "10.1145/1360612.1360653", "10.1145/1882261.1866162", "10.1145/311535.311538", "10.1145/1778765.1778860", "10.1145/1599470.1599494", "10.1145/37401.37406", "10.1145/1028523.1028526", "10.1145/1536513.1536540", "10.1145/1618452.1618468", "10.1145/1141911.1142008", "10.1145/1360612.1360653", "10.1145/1882261.1866162", "10.1145/311535.311538", "10.1145/1778765.1778860", "10.1145/1599470.1599494", "10.1145/37401.37406", "10.1145/1028523.1028526", "10.1145/1536513.1536540", "10.1145/1618452.1618468", "10.1111/j.1467-8659.2009.01404.x", "10.1111/j.1467-8659.2007.01089.x", "10.2307/2786507", "10.2307/2785927", "10.1371/journal.pone.0010047", "10.1017/CBO9780511546877", "10.1038/35035023", "10.1016/j.gmod.2007.09.001", "10.1111/j.1467-8659.2007.01090.x", "10.1023/A:1008867321648", "10.1111/j.1467-8659.2010.01808.x", "10.1111/j.1467-8659.2004.00782.x", "10.1002/cav.166", "10.1111/j.1467-8659.2009.01404.x", "10.1111/j.1467-8659.2007.01089.x", "10.2307/2786507", "10.2307/2785927", "10.1371/journal.pone.0010047", "10.1017/CBO9780511546877", "10.1038/35035023", "10.1016/j.gmod.2007.09.001", "10.1111/j.1467-8659.2007.01090.x", "10.1023/A:1008867321648", "10.1111/j.1467-8659.2010.01808.x", "10.1111/j.1467-8659.2004.00782.x", "10.1002/cav.166", "10.1111/j.1467-8659.2009.01404.x", "10.1111/j.1467-8659.2007.01089.x", "10.2307/2786507", "10.2307/2785927", "10.1371/journal.pone.0010047", "10.1017/CBO9780511546877", "10.1038/35035023", "10.1016/j.gmod.2007.09.001", "10.1111/j.1467-8659.2007.01090.x", "10.1023/A:1008867321648", "10.1111/j.1467-8659.2010.01808.x", "10.1111/j.1467-8659.2004.00782.x", "10.1002/cav.166"]}, "10.1109/TVCG.2011.78": {"doi": "10.1109/TVCG.2011.78", "author": ["C. Rossl", "H. Theisel"], "title": "Streamline Embedding for 3D Vector Field Exploration", "year": "2012", "abstract": "We propose a new technique for visual exploration of streamlines in 3D vector fields. We construct a map from the space of all streamlines to points in IRn based on the preservation of the Hausdorff metric in streamline space. The image of a vector field under this map is a set of 2-manifolds in IRn with characteristic geometry and topology. Then standard clustering methods applied to the point sets in IRn yield a segmentation of the original vector field. Our approach provides a global analysis of 3D vector fields which incorporates the topological segmentation but yields additional information. In addition to a pure segmentation, the established map provides a natural \"parametrization\u201d visualized by the manifolds. We test our approach on a number of synthetic and real-world data sets.", "keywords": ["computational geometry", "data visualisation", "image segmentation", "pattern clustering", "topology", "vectors", "streamline embedding", "visual exploration", "Hausdorff metric", "streamline space", "clustering methods", "vector field segmentation", "3D vector fields global analysis", "topological segmentation", "map", "natural parametrization", "real-world data sets", "synthetic data sets", "manifold set", "IR", "vector field image", "Manifolds", "Three dimensional displays", "Silicon", "Streaming media", "Measurement", "Topology", "Visualization", "Vector fields", "streamline embedding", "clustering."], "referenced_by": ["10.1109/LDAV.2011.6092316", "10.1109/PACIFICVIS.2016.7465273", "10.1109/PacificVis.2013.6596150", "10.1109/PacificVis.2013.6596153", "10.1109/PacificVis.2014.12", "10.1109/PacificVis.2014.15", "10.1109/TVCG.2013.181", "10.1109/TVCG.2013.2297914", "10.1109/TVCG.2013.236", "10.1109/TVCG.2014.2312009", "10.1109/TVCG.2014.2346416", "10.1109/TVCG.2015.2440252", "10.1109/TVCG.2015.2467204", "10.1109/SIBGRAPI.2015.31", "10.1109/TVCG.2015.2467292", "10.1109/TVCG.2017.2773071", "10.1109/TVCG.2019.2934310", "10.1109/TVCG.2018.2880207", "10.1109/PacificVis48177.2020.1718", "10.1145/2602145", "10.1007/s12650-018-0474-6", "10.1016/j.cag.2015.01.002", "10.1016/j.cag.2015.07.016", "10.1016/j.cag.2016.08.001", "10.1016/j.icheatmasstransfer.2016.04.016", "10.1111/cgf.12933", "10.1111/cgf.12990", "10.1117/12.2038253", "10.4028/www.scientific.net/AMM.571-572.676", "10.1007/s12650-019-00552-x", "10.1016/j.promfg.2019.06.209", "10.1111/cgf.13422", "10.1175/MWR-D-19-0408.1", "10.1007/978-3-030-61864-3_26"], "referencing": ["10.1109/MCG.2008.106", "10.1109/CVPR.2005.332", "10.1109/TVCG.2006.104", "10.1109/VISUAL.2005.1532778", "10.1109/TVCG.2005.59", "10.1109/TVCG.2009.112", "10.1109/VISUAL.2000.885690", "10.1109/VISUAL.2005.1532779", "10.1109/ISBI.2004.1398545", "10.1109/TVCG.2008.52", "10.1109/VISUAL.1999.809863", "10.1109/VISUAL.1999.809865", "10.1109/VISUAL.1999.809904", "10.1109/VISUAL.2004.32", "10.1109/2.35197", "10.1109/38.79452", "10.1109/VISUAL.1997.663858", "10.1109/VISUAL.2003.1250376", "10.1109/34.868688", "10.1109/MCG.2008.106", "10.1109/CVPR.2005.332", "10.1109/TVCG.2006.104", "10.1109/VISUAL.2005.1532778", "10.1109/TVCG.2005.59", "10.1109/TVCG.2009.112", "10.1109/VISUAL.2000.885690", "10.1109/VISUAL.2005.1532779", "10.1109/ISBI.2004.1398545", "10.1109/TVCG.2008.52", "10.1109/VISUAL.1999.809863", "10.1109/VISUAL.1999.809865", "10.1109/VISUAL.1999.809904", "10.1109/VISUAL.2004.32", "10.1109/2.35197", "10.1109/38.79452", "10.1109/VISUAL.1997.663858", "10.1109/VISUAL.2003.1250376", "10.1109/34.868688", "10.1109/MCG.2008.106", "10.1109/CVPR.2005.332", "10.1109/TVCG.2006.104", "10.1109/VISUAL.2005.1532778", "10.1109/TVCG.2005.59", "10.1109/TVCG.2009.112", "10.1109/VISUAL.2000.885690", "10.1109/VISUAL.2005.1532779", "10.1109/ISBI.2004.1398545", "10.1109/TVCG.2008.52", "10.1109/VISUAL.1999.809863", "10.1109/VISUAL.1999.809865", "10.1109/VISUAL.1999.809904", "10.1109/VISUAL.2004.32", "10.1109/2.35197", "10.1109/38.79452", "10.1109/VISUAL.1997.663858", "10.1109/VISUAL.2003.1250376", "10.1109/34.868688", "10.1145/237170.237285", "10.1145/174462.156635", "10.1145/237170.237285", "10.1145/174462.156635", "10.1145/237170.237285", "10.1145/174462.156635", "10.1111/j.1467-8659.2009.01459.x", "10.1007/11566465_18", "10.1111/j.1467-8659.2008.01244.x", "10.1007/978-3-642-55566-4_4", "10.1007/978-3-540-45210-2_47", "10.1017/S0022112095000462", "10.1111/j.1467-8659.2003.00723.x", "10.1017/S0022112004002526", "10.1111/j.1467-8659.2009.01459.x", "10.1007/11566465_18", "10.1111/j.1467-8659.2008.01244.x", "10.1007/978-3-642-55566-4_4", "10.1007/978-3-540-45210-2_47", "10.1017/S0022112095000462", "10.1111/j.1467-8659.2003.00723.x", "10.1017/S0022112004002526", "10.1111/j.1467-8659.2009.01459.x", "10.1007/11566465_18", "10.1111/j.1467-8659.2008.01244.x", "10.1007/978-3-642-55566-4_4", "10.1007/978-3-540-45210-2_47", "10.1017/S0022112095000462", "10.1111/j.1467-8659.2003.00723.x", "10.1017/S0022112004002526"]}, "10.1109/TVCG.2011.54": {"doi": "10.1109/TVCG.2011.54", "author": ["Y. Jang", "D. S. Ebert", "K. Gaither"], "title": "Time-Varying Data Visualization Using Functional Representations", "year": "2012", "abstract": "In many scientific simulations, the temporal variation and analysis of features are important. Visualization and visual analysis of time series data is still a significant challenge because of the large volume of data. Irregular and scattered time series data sets are even more problematic to visualize interactively. Previous work proposed functional representation using basis functions as one solution for interactively visualizing scattered data by harnessing the power of modern PC graphics boards. In this paper, we use the functional representation approach for time-varying data sets and develop an efficient encoding technique utilizing temporal similarity between time steps. Our system utilizes a graduated approach of three methods with increasing time complexity based on the lack of similarity of the evolving data sets. Using this system, we are able to enhance the encoding performance for the time-varying data sets, reduce the data storage by saving only changed or additional basis functions over time, and interactively visualize the time-varying encoding results. Moreover, we present efficient rendering of the functional representations using binary space partitioning tree textures to increase the rendering performance.", "keywords": ["data visualisation", "time series", "time varying data visualization", "functional representations", "temporal variation", "scientific simulations", "time series", "visual analysis", "functional representation", "PC graphics boards", "encoding technique", "time complexity", "data storage", "Encoding", "Rendering (computer graphics)", "Data visualization", "Time varying systems", "Octrees", "Equations", "Feature extraction", "Basis functions", "functional representation", "time-varying data", "volume rendering."], "referenced_by": ["10.1109/TVCG.2016.2599042", "10.1007/978-3-319-77703-0_68", "10.1007/s12530-018-9217-0", "10.1007/s12650-014-0268-4", "10.1016/j.bspc.2017.01.015", "10.1016/j.dss.2015.07.003", "10.1111/cgf.12280", "10.1142/S179396231341002X", "10.15748/jasse.1.171", "10.1007/s12650-018-0526-y", "10.1007/978-3-642-45037-2_13", "10.1007/s12650-020-00654-x"], "referencing": ["10.1109/SMA.2001.923379", "10.1109/PACIFICVIS.2009.4906833", "10.1109/VISUAL.1992.235230", "10.1109/38.180119", "10.1109/38.79453", "10.1109/VISUAL.1999.809910", "10.1109/VISUAL.1994.346321", "10.1109/VISUAL.2002.1183804", "10.1109/PACIFICVIS.2008.4475469", "10.1109/TVCG.2008.186", "10.1109/VISUAL.2003.1250402", "10.1109/VG.2005.194093", "10.1109/2945.597796", "10.1109/VISUAL.1998.745288", "10.1109/SWG.2002.1226514", "10.1109/38.376616", "10.1109/SC.2005.37", "10.1109/MCG.2005.106", "10.1109/VISUAL.1995.480809", "10.1109/SMA.2001.923379", "10.1109/PACIFICVIS.2009.4906833", "10.1109/VISUAL.1992.235230", "10.1109/38.180119", "10.1109/38.79453", "10.1109/VISUAL.1999.809910", "10.1109/VISUAL.1994.346321", "10.1109/VISUAL.2002.1183804", "10.1109/PACIFICVIS.2008.4475469", "10.1109/TVCG.2008.186", "10.1109/VISUAL.2003.1250402", "10.1109/VG.2005.194093", "10.1109/2945.597796", "10.1109/VISUAL.1998.745288", "10.1109/SWG.2002.1226514", "10.1109/38.376616", "10.1109/SC.2005.37", "10.1109/MCG.2005.106", "10.1109/VISUAL.1995.480809", "10.1109/SMA.2001.923379", "10.1109/PACIFICVIS.2009.4906833", "10.1109/VISUAL.1992.235230", "10.1109/38.180119", "10.1109/38.79453", "10.1109/VISUAL.1999.809910", "10.1109/VISUAL.1994.346321", "10.1109/VISUAL.2002.1183804", "10.1109/PACIFICVIS.2008.4475469", "10.1109/TVCG.2008.186", "10.1109/VISUAL.2003.1250402", "10.1109/VG.2005.194093", "10.1109/2945.597796", "10.1109/VISUAL.1998.745288", "10.1109/SWG.2002.1226514", "10.1109/38.376616", "10.1109/SC.2005.37", "10.1109/MCG.2005.106", "10.1109/VISUAL.1995.480809", "10.1145/1028523.1028552", "10.1145/383259.383266", "10.1145/1360612.1360635", "10.1145/571647.571650", "10.1145/1028523.1028552", "10.1145/383259.383266", "10.1145/1360612.1360635", "10.1145/571647.571650", "10.1145/1028523.1028552", "10.1145/383259.383266", "10.1145/1360612.1360635", "10.1145/571647.571650", "10.1016/j.parco.2007.02.015", "10.1111/1467-8659.00497", "10.1016/j.enganabound.2004.06.005", "10.1111/1467-8659.1440181", "10.1080/002077299292038", "10.1098/rsta.1999.0439", "10.1016/S0167-8396(98)00043-0", "10.1007/978-3-662-03567-2_2", "10.1111/j.1467-8659.2005.00855.x", "10.1111/1467-8659.00298", "10.1111/j.1467-8659.2006.00978.x", "10.1016/j.enganabound.2004.01.004", "10.1137/S0036141095289051", "10.1016/j.parco.2007.02.015", "10.1111/1467-8659.00497", "10.1016/j.enganabound.2004.06.005", "10.1111/1467-8659.1440181", "10.1080/002077299292038", "10.1098/rsta.1999.0439", "10.1016/S0167-8396(98)00043-0", "10.1007/978-3-662-03567-2_2", "10.1111/j.1467-8659.2005.00855.x", "10.1111/1467-8659.00298", "10.1111/j.1467-8659.2006.00978.x", "10.1016/j.enganabound.2004.01.004", "10.1137/S0036141095289051", "10.1016/j.parco.2007.02.015", "10.1111/1467-8659.00497", "10.1016/j.enganabound.2004.06.005", "10.1111/1467-8659.1440181", "10.1080/002077299292038", "10.1098/rsta.1999.0439", "10.1016/S0167-8396(98)00043-0", "10.1007/978-3-662-03567-2_2", "10.1111/j.1467-8659.2005.00855.x", "10.1111/1467-8659.00298", "10.1111/j.1467-8659.2006.00978.x", "10.1016/j.enganabound.2004.01.004", "10.1137/S0036141095289051"]}, "10.1109/TVCG.2011.101": {"doi": "10.1109/TVCG.2011.101", "author": ["A. Patel", "W. A. P. Smith"], "title": "Automated Construction of Low-Resolution, Texture-Mapped, Class-Optimal Meshes", "year": "2012", "abstract": "In this paper, we present a framework for the groupwise processing of a set of meshes in dense correspondence. Such sets arise when modeling 3D shape variation or tracking surface motion over time. We extend a number of mesh processing tools to operate in a groupwise manner. Specifically, we present a geodesic-based surface flattening and spectral clustering algorithm which estimates a single class-optimal flattening. We also show how to modify an iterative edge collapse algorithm to perform groupwise simplification while retaining the correspondence of the data. Finally, we show how to compute class-optimal texture coordinates for the simplified meshes. We present alternative algorithms for topologically symmetric data which yield a symmetric flattening and low-resolution mesh topology. We present flattening, simplification, and texture mapping results on three different data sets and show that our approach allows the construction of low-resolution 3D morphable models.", "keywords": ["data acquisition", "differential geometry", "group theory", "iterative methods", "mesh generation", "pattern clustering", "solid modelling", "surface texture", "topology", "groupwise processing", "3D shape variation modeling", "surface motion tracking", "mesh processing tools", "geodesic based surface flattening", "spectral clustering algorithm", "class optimal flattening", "iterative edge collapse algorithm", "groupwise simplification", "class optimal texture coordinate", "topologically symmetric data", "symmetric flattening", "low resolution mesh topology", "texture mapping", "low resolution 3D morphable models", "Shape", "Mesh generation", "Three dimensional displays", "Strain", "Clustering algorithms", "Solid modeling", "Surface texture", "Groupwise processing", "dense correspondence", "surface flattening", "simplification", "texture mapping."], "referenced_by": [], "referencing": ["10.1109/VISUAL.1998.745285", "10.1109/CVPR.2008.4587495", "10.1109/2945.998671", "10.1109/TVCG.2008.54", "10.1109/AVSS.2009.58", "10.1109/CVPR.2009.5206738", "10.1109/CVPR.2007.383165", "10.1109/PCCGA.2003.1238268", "10.1109/VISUAL.1998.745285", "10.1109/CVPR.2008.4587495", "10.1109/2945.998671", "10.1109/TVCG.2008.54", "10.1109/AVSS.2009.58", "10.1109/CVPR.2009.5206738", "10.1109/CVPR.2007.383165", "10.1109/PCCGA.2003.1238268", "10.1109/VISUAL.1998.745285", "10.1109/CVPR.2008.4587495", "10.1109/2945.998671", "10.1109/TVCG.2008.54", "10.1109/AVSS.2009.58", "10.1109/CVPR.2009.5206738", "10.1109/CVPR.2007.383165", "10.1109/PCCGA.2003.1238268", "10.1145/383259.383307", "10.1145/258734.258849", "10.1145/1015706.1015759", "10.1145/1057432.1057439", "10.1145/1061347.1061354", "10.1145/1053427.1053430", "10.1145/311535.311556", "10.1145/1015706.1015811", "10.1145/383259.383277", "10.1145/566570.566590", "10.1145/383259.383307", "10.1145/258734.258849", "10.1145/1015706.1015759", "10.1145/1057432.1057439", "10.1145/1061347.1061354", "10.1145/1053427.1053430", "10.1145/311535.311556", "10.1145/1015706.1015811", "10.1145/383259.383277", "10.1145/566570.566590", "10.1145/383259.383307", "10.1145/258734.258849", "10.1145/1015706.1015759", "10.1145/1057432.1057439", "10.1145/1061347.1061354", "10.1145/1053427.1053430", "10.1145/311535.311556", "10.1145/1015706.1015811", "10.1145/383259.383277", "10.1145/566570.566590", "10.1073/pnas.95.15.8431", "10.1007/3-7643-7384-9_18", "10.1207/s15327906mbr0302_7", "10.1007/s11263-006-6859-3", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1111/j.1467-8659.2008.01142.x", "10.1111/j.1467-8659.2009.01374.x", "10.1016/S1090-5138(03)00084-9", "10.1007/978-3-642-03596-8_10", "10.1111/j.1467-8659.2009.01373.x", "10.1016/S0167-8396(03)00002-5", "10.1111/1467-8659.00236", "10.1073/pnas.95.15.8431", "10.1007/3-7643-7384-9_18", "10.1207/s15327906mbr0302_7", "10.1007/s11263-006-6859-3", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1111/j.1467-8659.2008.01142.x", "10.1111/j.1467-8659.2009.01374.x", "10.1016/S1090-5138(03)00084-9", "10.1007/978-3-642-03596-8_10", "10.1111/j.1467-8659.2009.01373.x", "10.1016/S0167-8396(03)00002-5", "10.1111/1467-8659.00236", "10.1073/pnas.95.15.8431", "10.1007/3-7643-7384-9_18", "10.1207/s15327906mbr0302_7", "10.1007/s11263-006-6859-3", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1111/j.1467-8659.2008.01142.x", "10.1111/j.1467-8659.2009.01374.x", "10.1016/S1090-5138(03)00084-9", "10.1007/978-3-642-03596-8_10", "10.1111/j.1467-8659.2009.01373.x", "10.1016/S0167-8396(03)00002-5", "10.1111/1467-8659.00236"]}, "10.1109/TVCG.2011.35": {"doi": "10.1109/TVCG.2011.35", "author": ["J. Kronander", "D. Jonsson", "J. Low", "P. Ljung", "A. Ynnerman", "J. Unger"], "title": "Efficient Visibility Encoding for Dynamic Illumination in Direct Volume Rendering", "year": "2012", "abstract": "We present an algorithm that enables real-time dynamic shading in direct volume rendering using general lighting, including directional lights, point lights, and environment maps. Real-time performance is achieved by encoding local and global volumetric visibility using spherical harmonic (SH) basis functions stored in an efficient multiresolution grid over the extent of the volume. Our method enables high-frequency shadows in the spatial domain, but is limited to a low-frequency approximation of visibility and illumination in the angular domain. In a first pass, level of detail (LOD) selection in the grid is based on the current transfer function setting. This enables rapid online computation and SH projection of the local spherical distribution of visibility information. Using a piecewise integration of the SH coefficients over the local regions, the global visibility within the volume is then computed. By representing the light sources using their SH projections, the integral over lighting, visibility, and isotropic phase functions can be efficiently computed during rendering. The utility of our method is demonstrated in several examples showing the generality and interactive performance of the approach.", "keywords": ["approximation theory", "encoding", "image coding", "integration", "rendering (computer graphics)", "transfer functions", "visibility encoding", "dynamic illumination", "direct volume rendering", "real-time dynamic shading", "general lighting", "directional lights", "point lights", "environment maps", "spherical harmonic basis functions", "low-frequency approximation", "level of detail selection", "transfer function setting", "piecewise integration", "isotropic phase functions", "Lighting", "Rendering (computer graphics)", "Light sources", "Harmonic analysis", "Scattering", "Real time systems", "Approximation methods", "Volumetric illumination", "precomputed radiance transfer", "volume rendering."], "referenced_by": ["10.1109/ChiCC.2014.6895751", "10.1109/ISICIR.2014.7029497", "10.1109/PACIFICVIS.2015.7156382", "10.1109/TSP.2013.2272289", "10.1109/TVCG.2012.142", "10.1109/TVCG.2012.232", "10.1109/TVCG.2013.129", "10.1109/TVCG.2013.17", "10.1109/TVCG.2013.172", "10.1109/TVCG.2014.2346333", "10.1109/TVCG.2016.2569080", "10.1109/TVCG.2016.2598430", "10.1109/TVCG.2016.2606498", "10.1109/TVCG.2015.2467963", "10.1109/TVCG.2019.2898435", "10.1002/cav.1683", "10.1002/cav.1706", "10.1016/B978-0-12-415873-3.00035-3", "10.1016/j.cag.2015.05.004", "10.1016/j.cag.2016.08.004", "10.1111/cgf.12252", "10.1111/cgf.12916", "10.3745/KTSDE.2016.5.12.677"], "referencing": ["10.1109/VISUAL.2004.62", "10.1109/TVCG.2007.70573", "10.1109/PACIFICVIS.2010.5429594", "10.1109/PG.2007.27", "10.1109/SVV.1998.729583", "10.1109/TVCG.2009.45", "10.1109/TVCG.2003.1196003", "10.1109/SVVG.2004.14", "10.1109/2945.468400", "10.1109/TVCG.2006.33", "10.1109/VISUAL.2004.62", "10.1109/TVCG.2007.70573", "10.1109/PACIFICVIS.2010.5429594", "10.1109/PG.2007.27", "10.1109/SVV.1998.729583", "10.1109/TVCG.2009.45", "10.1109/TVCG.2003.1196003", "10.1109/SVVG.2004.14", "10.1109/2945.468400", "10.1109/TVCG.2006.33", "10.1109/VISUAL.2004.62", "10.1109/TVCG.2007.70573", "10.1109/PACIFICVIS.2010.5429594", "10.1109/PG.2007.27", "10.1109/SVV.1998.729583", "10.1109/TVCG.2009.45", "10.1109/TVCG.2003.1196003", "10.1109/SVVG.2004.14", "10.1109/2945.468400", "10.1109/TVCG.2006.33", "10.1145/1330511.1330515", "10.1145/1360612.1360630", "10.1145/1179352.1141982", "10.1145/566570.566612", "10.1145/882262.882281", "10.1145/1276377.1276411", "10.1145/1356682.1356686", "10.1145/1283900.1283908", "10.1145/1667239.1667241", "10.1145/1321261.1321303", "10.1145/344779.344958", "10.1145/1360612.1360635", "10.1145/1330511.1330515", "10.1145/1360612.1360630", "10.1145/1179352.1141982", "10.1145/566570.566612", "10.1145/882262.882281", "10.1145/1276377.1276411", "10.1145/1356682.1356686", "10.1145/1283900.1283908", "10.1145/1667239.1667241", "10.1145/1321261.1321303", "10.1145/344779.344958", "10.1145/1360612.1360635", "10.1145/1330511.1330515", "10.1145/1360612.1360630", "10.1145/1179352.1141982", "10.1145/566570.566612", "10.1145/882262.882281", "10.1145/1276377.1276411", "10.1145/1356682.1356686", "10.1145/1283900.1283908", "10.1145/1667239.1667241", "10.1145/1321261.1321303", "10.1145/344779.344958", "10.1145/1360612.1360635", "10.1117/12.650849", "10.1561/0600000021", "10.1111/j.1467-8659.2008.01154.x", "10.1111/j.1467-8659.2009.01464.x", "10.1068/p3060", "10.1117/12.650849", "10.1561/0600000021", "10.1111/j.1467-8659.2008.01154.x", "10.1111/j.1467-8659.2009.01464.x", "10.1068/p3060", "10.1117/12.650849", "10.1561/0600000021", "10.1111/j.1467-8659.2008.01154.x", "10.1111/j.1467-8659.2009.01464.x", "10.1068/p3060"]}, "10.1109/TVCG.2011.75": {"doi": "10.1109/TVCG.2011.75", "author": ["M. Dellepiane", "R. Marroquim", "M. Callieri", "P. Cignoni", "R. Scopigno"], "title": "Flow-Based Local Optimization for Image-to-Geometry Projection", "year": "2012", "abstract": "The projection of a photographic data set on a 3D model is a robust and widely applicable way to acquire appearance information of an object. The first step of this procedure is the alignment of the images on the 3D model. While any reconstruction pipeline aims at avoiding misregistration by improving camera calibrations and geometry, in practice a perfect alignment cannot always be reached. Depending on the way multiple camera images are fused on the object surface, remaining misregistrations show up either as ghosting or as discontinuities at transitions from one camera view to another. In this paper we propose a method, based on the computation of Optical Flow between overlapping images, to correct the local misalignment by determining the necessary displacement. The goal is to correct the symptoms of misregistration, instead of searching for a globally consistent mapping, which might not exist. The method scales up well with the size of the data set (both photographic and geometric) and is quite independent of the characteristics of the 3D model (topology cleanliness, parametrization, density). The method is robust and can handle real world cases that have different characteristics: low level geometric details and images that lack enough features for global optimization or manual methods. It can be applied to different mapping strategies, such as texture or per-vertex attribute encoding.", "keywords": ["cameras", "feature extraction", "image colour analysis", "image registration", "image sequences", "optimisation", "photography", "solid modelling", "flow-based local optimization", "image-to-geometry projection", "photographic data set", "3D model", "object information", "pipeline reconstruction", "camera calibration", "multiple camera image", "object surface", "optical flow", "overlapping image", "low level geometric detail", "global optimization", "manual method", "mapping strategy", "per-vertex attribute encoding", "Image color analysis", "Cameras", "Three dimensional displays", "Optical imaging", "Solid modeling", "Adaptive optics", "Geometry", "Computer graphics", "image color analysis."], "referenced_by": ["10.1109/DigitalHeritage.2013.6743711", "10.1109/DigitalHeritage.2013.6743749", "10.1109/ISPA.2013.6703806", "10.1109/TVCG.2014.2312011", "10.1109/3DV.2018.00019", "10.1109/WACV.2019.00155", "10.1109/ICVRV.2018.00009", "10.1109/CVPR42600.2020.00135", "10.1145/2601097.2601134", "10.1145/3072959.3073610", "10.1007/978-3-319-15979-9_20", "10.1007/978-3-319-50518-3_17", "10.1007/978-3-319-70016-8_20", "10.1007/978-3-662-44630-0_2", "10.1007/s00371-012-0743-7", "10.1007/s11263-012-0552-5", "10.1016/j.cag.2015.05.016", "10.1016/j.cag.2016.05.007", "10.1016/j.cag.2017.08.011", "10.1111/cgf.12508", "10.1111/phor.12213", "10.4018/978-1-4666-8379-2.ch001", "10.4018/978-1-5225-5204-8.ch056", "10.1016/j.optcom.2019.01.051", "10.1007/978-3-642-27978-2_2", "10.4995/var.2014.4218"], "referencing": ["10.1109/ICCV.1993.378214", "10.1109/TVCG.2007.1006", "10.1109/3DIM.2005.65", "10.1109/2945.965346", "10.1109/TDPVT.2004.1335288", "10.1109/ICPR.1998.711067", "10.1109/PCCGA.2000.883955", "10.1109/CVPR.2006.204", "10.1109/ICCV.1993.378214", "10.1109/TVCG.2007.1006", "10.1109/3DIM.2005.65", "10.1109/2945.965346", "10.1109/TDPVT.2004.1335288", "10.1109/ICPR.1998.711067", "10.1109/PCCGA.2000.883955", "10.1109/CVPR.2006.204", "10.1109/ICCV.1993.378214", "10.1109/TVCG.2007.1006", "10.1109/3DIM.2005.65", "10.1109/2945.965346", "10.1109/TDPVT.2004.1335288", "10.1109/ICPR.1998.711067", "10.1109/PCCGA.2000.883955", "10.1109/CVPR.2006.204", "10.1145/344779.344855", "10.1145/258734.258885", "10.1145/1276377.1276404", "10.1145/1778765.1778778", "10.1145/311535.311559", "10.1145/280814.280864", "10.1145/1709091.1709092", "10.1145/237170.237191", "10.1145/344779.344855", "10.1145/258734.258885", "10.1145/1276377.1276404", "10.1145/1778765.1778778", "10.1145/311535.311559", "10.1145/280814.280864", "10.1145/1709091.1709092", "10.1145/237170.237191", "10.1145/344779.344855", "10.1145/258734.258885", "10.1145/1276377.1276404", "10.1145/1778765.1778778", "10.1145/311535.311559", "10.1145/280814.280864", "10.1145/1709091.1709092", "10.1145/237170.237191", "10.1007/s11263-007-0039-y", "10.1111/j.1467-8659.2009.01524.x", "10.5244/C.16.38", "10.1117/12.590536", "10.1016/j.cag.2008.05.004", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-540-24673-2_3", "10.1007/3-540-55426-2_72", "10.1007/s00371-005-0309-z", "10.1007/s11263-007-0039-y", "10.1111/j.1467-8659.2009.01524.x", "10.5244/C.16.38", "10.1117/12.590536", "10.1016/j.cag.2008.05.004", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-540-24673-2_3", "10.1007/3-540-55426-2_72", "10.1007/s00371-005-0309-z", "10.1007/s11263-007-0039-y", "10.1111/j.1467-8659.2009.01524.x", "10.5244/C.16.38", "10.1117/12.590536", "10.1016/j.cag.2008.05.004", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-540-24673-2_3", "10.1007/3-540-55426-2_72", "10.1007/s00371-005-0309-z"]}, "10.1109/TVCG.2011.51": {"doi": "10.1109/TVCG.2011.51", "author": ["P. O'Donovan", "A. Hertzmann"], "title": "AniPaint: Interactive Painterly Animation from Video", "year": "2012", "abstract": "This paper presents an interactive system for creating painterly animation from video sequences. Previous approaches to painterly animation typically emphasize either purely automatic stroke synthesis or purely manual stroke key framing. Our system supports a spectrum of interaction between these two approaches which allows the user more direct control over stroke synthesis. We introduce an approach for controlling the results of painterly animation: keyframed Control Strokes can affect automatic stroke's placement, orientation, movement, and color. Furthermore, we introduce a new automatic synthesis algorithm that traces strokes through a video sequence in a greedy manner, but, instead of a vector field, uses an objective function to guide placement. This allows the method to capture fine details, respect region boundaries, and achieve greater temporal coherence than previous methods. All editing is performed with a WYSIWYG interface where the user can directly refine the animation. We demonstrate a variety of examples using both automatic and user-guided results, with a variety of styles and source videos.", "keywords": ["computer animation", "image sequences", "video signal processing", "AniPaint", "interactive painterly animation", "video sequences", "automatic stroke synthesis", "interaction spectrum", "stroke synthesis", "keyframed Control Strokes", "automatic synthesis algorithm", "Animation", "Image color analysis", "Painting", "Rendering (computer graphics)", "Video sequences", "Color", "Integrated optics", "Nonphotorealistic rendering", "painterly animation", "interactive video processing."], "referenced_by": ["10.1109/CADGraphics.2013.97", "10.1109/CVPR.2017.397", "10.1109/ICVRV.2016.47", "10.1109/TVCG.2012.160", "10.1109/TVCG.2017.2700470", "10.1145/2897824.2925968", "10.1145/2484238", "10.1145/2461912.2461929", "10.1007/978-3-319-45886-1_3", "10.1007/s00371-013-0809-1", "10.1007/s00371-013-0881-6", "10.1007/s11042-013-1441-9", "10.1007/s11042-013-1835-8", "10.1007/s11042-017-4882-8", "10.1007/s11432-014-5255-9", "10.1016/j.cag.2014.05.001", "10.1111/cgf.12407", "10.1111/cgf.12729", "10.1111/cgf.13038", "10.1631/jzus.C1400099", "10.3390/sym10100442", "10.1117/12.2081038", "10.1007/s00371-019-01641-6", "10.1007/s42044-019-00034-1", "10.1007/978-1-4471-4519-6_13", "10.1007/978-1-4471-4519-6_1", "10.1007/s00371-016-1256-6", "10.3390/electronics8111277", "10.1007/978-3-030-46032-7_8"], "referencing": ["10.1109/CGI.2001.934657", "10.1109/CVPR.2008.4587845", "10.1109/TVCG.2005.85", "10.1109/TVCG.2010.25", "10.1109/ICCV.2009.5459242", "10.1109/TPAMI.1986.4767851", "10.1109/CGI.2001.934657", "10.1109/CVPR.2008.4587845", "10.1109/TVCG.2005.85", "10.1109/TVCG.2010.25", "10.1109/ICCV.2009.5459242", "10.1109/TPAMI.1986.4767851", "10.1109/CGI.2001.934657", "10.1109/CVPR.2008.4587845", "10.1109/TVCG.2005.85", "10.1109/TVCG.2010.25", "10.1109/ICCV.2009.5459242", "10.1109/TPAMI.1986.4767851", "10.1145/97879.97902", "10.1145/237170.237288", "10.1145/258734.258893", "10.1145/280814.280951", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/1015706.1015764", "10.1145/566570.566648", "10.1145/1242073.1242173", "10.1145/1124728.1124751", "10.1145/1276377.1276507", "10.1145/566570.566650", "10.1145/508530.508545", "10.1145/1186562.1015763", "10.1145/1640443.1640445", "10.1145/1576246.1531376", "10.1145/508530.508546", "10.1145/1274871.1274878", "10.1145/258734.258890", "10.1145/383259.383327", "10.1145/97879.97902", "10.1145/237170.237288", "10.1145/258734.258893", "10.1145/280814.280951", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/1015706.1015764", "10.1145/566570.566648", "10.1145/1242073.1242173", "10.1145/1124728.1124751", "10.1145/1276377.1276507", "10.1145/566570.566650", "10.1145/508530.508545", "10.1145/1186562.1015763", "10.1145/1640443.1640445", "10.1145/1576246.1531376", "10.1145/508530.508546", "10.1145/1274871.1274878", "10.1145/258734.258890", "10.1145/383259.383327", "10.1145/97879.97902", "10.1145/237170.237288", "10.1145/258734.258893", "10.1145/280814.280951", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/1015706.1015764", "10.1145/566570.566648", "10.1145/1242073.1242173", "10.1145/1124728.1124751", "10.1145/1276377.1276507", "10.1145/566570.566650", "10.1145/508530.508545", "10.1145/1186562.1015763", "10.1145/1640443.1640445", "10.1145/1576246.1531376", "10.1145/508530.508546", "10.1145/1274871.1274878", "10.1145/258734.258890", "10.1145/383259.383327", "10.1007/978-3-540-32003-6_44", "10.1016/0734-189X(87)90043-0", "10.1007/978-3-540-32003-6_44", "10.1016/0734-189X(87)90043-0", "10.1007/978-3-540-32003-6_44", "10.1016/0734-189X(87)90043-0"]}, "10.1109/TVCG.2011.48": {"doi": "10.1109/TVCG.2011.48", "author": ["J. Kwon", "I. Lee"], "title": "The Squash-and-Stretch Stylization for Character Motions", "year": "2012", "abstract": "The squash-and-stretch describes the rigidity of the character. This effect is the most important technique in traditional cartoon animation. In this paper, we introduce a method that applies the squash-and-stretch effect to character motion. Our method exaggerates the motion by sequentially applying the spatial exaggeration technique and the temporal exaggeration technique. The spatial exaggeration technique globally deforms the pose in order to make the squashed or stretched pose by modeling it as a covariance matrix of joint positions. Then, the temporal exaggeration technique computes a time-warping function for each joint, and applies it to the position of the joint allowing the character to stretch its links appropriately. The motion stylized by our method is a sequence of squashed and stretched poses with stretching limbs. By performing a user survey, we prove that the motion created using our method is similar to that used in 2D cartoon animation and is funnier than the original motion for human observers who are familiar with 2D cartoon animation.", "keywords": ["computer animation", "covariance matrices", "squash-and-stretch stylization", "character motions", "spatial exaggeration technique", "temporal exaggeration technique", "squashed pose", "stretched pose", "time-warping function", "user survey", "2D cartoon animation", "human observers", "covariance matrix", "joint positions", "Joints", "Animation", "Covariance matrix", "Optimization", "Shape", "Humans", "Kinematics", "Squash-and-stretch", "cartoon stylization", "motion capture", "exaggeration", "covariance matrix", "time warping."], "referenced_by": ["10.1109/ICON-SONICS.2017.8267812", "10.15701/kcgs.2012.18.2.55", "10.1007/978-3-319-01538-5_3"], "referencing": ["10.1109/2945.998665", "10.1109/2945.998665", "10.1109/2945.998665", "10.1145/1531326.1531385", "10.1145/37402.37407", "10.1145/218380.218419", "10.1145/218380.218421", "10.1145/1599470.1599484", "10.1145/508530.508553", "10.1145/1141911.1142010", "10.1145/1179622.1179626", "10.1145/1179622.1179625", "10.1145/1360612.1360627", "10.1145/280814.280946", "10.1145/253284.253321", "10.1145/1281500.1281598", "10.1145/1360612.1360626", "10.1145/566654.566607", "10.1145/1531326.1531385", "10.1145/37402.37407", "10.1145/218380.218419", "10.1145/218380.218421", "10.1145/1599470.1599484", "10.1145/508530.508553", "10.1145/1141911.1142010", "10.1145/1179622.1179626", "10.1145/1179622.1179625", "10.1145/1360612.1360627", "10.1145/280814.280946", "10.1145/253284.253321", "10.1145/1281500.1281598", "10.1145/1360612.1360626", "10.1145/566654.566607", "10.1145/1531326.1531385", "10.1145/37402.37407", "10.1145/218380.218419", "10.1145/218380.218421", "10.1145/1599470.1599484", "10.1145/508530.508553", "10.1145/1141911.1142010", "10.1145/1179622.1179626", "10.1145/1179622.1179625", "10.1145/1360612.1360627", "10.1145/280814.280946", "10.1145/253284.253321", "10.1145/1281500.1281598", "10.1145/1360612.1360626", "10.1145/566654.566607", "10.1006/gmod.2001.0548", "10.1002/cav.132", "10.1007/s00371-007-0125-8", "10.1111/j.1467-8659.2008.01177.x", "10.1007/11784203_61", "10.1006/gmod.2001.0548", "10.1002/cav.132", "10.1007/s00371-007-0125-8", "10.1111/j.1467-8659.2008.01177.x", "10.1007/11784203_61", "10.1006/gmod.2001.0548", "10.1002/cav.132", "10.1007/s00371-007-0125-8", "10.1111/j.1467-8659.2008.01177.x", "10.1007/11784203_61"]}, "10.1109/TVCG.2011.73": {"doi": "10.1109/TVCG.2011.73", "author": ["R. Fan", "S. Xu", "W. Geng"], "title": "Example-Based Automatic Music-Driven Conventional Dance Motion Synthesis", "year": "2012", "abstract": "We introduce a novel method for synthesizing dance motions that follow the emotions and contents of a piece of music. Our method employs a learning-based approach to model the music to motion mapping relationship embodied in example dance motions along with those motions' accompanying background music. A key step in our method is to train a music to motion matching quality rating function through learning the music to motion mapping relationship exhibited in synchronized music and dance motion data, which were captured from professional human dance performance. To generate an optimal sequence of dance motion segments to match with a piece of music, we introduce a constraint-based dynamic programming procedure. This procedure considers both music to motion matching quality and visual smoothness of a resultant dance motion sequence. We also introduce a two-way evaluation strategy, coupled with a GPU-based implementation, through which we can execute the dynamic programming process in parallel, resulting in significant speedup. To evaluate the effectiveness of our method, we quantitatively compare the dance motions synthesized by our method with motion synthesis results by several peer methods using the motions captured from professional human dancers' performance as the gold standard. We also conducted several medium-scale user studies to explore how perceptually our dance motion synthesis method can outperform existing methods in synthesizing dance motions to match with a piece of music. These user studies produced very positive results on our music-driven dance motion synthesis experiments for several Asian dance genres, confirming the advantages of our method.", "keywords": ["dynamic programming", "graphics processing units", "image matching", "image motion analysis", "image sequences", "learning (artificial intelligence)", "music", "example based automatic music driven conventional dance motion synthesis", "learning based approach", "motion mapping relationship", "motion matching quality rating function", "synchronized music", "professional human dance performance", "optimal sequence", "dance motion segments", "constraint based dynamic programming", "visual smoothness", "resultant dance motion sequence", "two-way evaluation strategy", "GPU based implementation", "peer method", "Asian dance genres", "Motion segmentation", "Feature extraction", "Correlation", "Training", "Joints", "Synchronization", "Humans", "Dance motion and music mapping relationship", "music-driven dance motion synthesis", "learning-based dance motion synthesis.", "Analysis of Variance", "Dancing", "Dancing", "Emotions", "Far East", "Female", "Humans", "Image Processing, Computer-Assisted", "Male", "Music", "Pattern Recognition, Automated", "Reproducibility of Results", "Video Recording", "Young Adult"], "referenced_by": ["10.1109/CW.2015.26", "10.1109/ICSIIT.2017.34", "10.1109/TCYB.2014.2305998", "10.1109/ISMS.2014.52", "10.1109/VR.2018.8446498", "10.1109/ISEMANTIC.2018.8549750", "10.1109/TMM.2020.2981989", "10.1145/2663806.2663849", "10.1080/2331186X.2017.1287392", "10.1177/1550147717696083", "10.1587/transinf.2015CYP0010", "10.1016/j.micpro.2020.103743"], "referencing": ["10.1109/ICASSP.1999.758084", "10.1109/TPAMI.2005.159", "10.1109/ICASSP.2010.5494891", "10.1109/EGUK.2002.1011270", "10.1109/ICASSP.1999.758084", "10.1109/TPAMI.2005.159", "10.1109/ICASSP.2010.5494891", "10.1109/EGUK.2002.1011270", "10.1109/ICASSP.1999.758084", "10.1109/TPAMI.2005.159", "10.1109/ICASSP.2010.5494891", "10.1109/EGUK.2002.1011270", "10.1145/218380.218422", "10.1145/545261.545279", "10.1145/566654.566606", "10.1145/258734.258880", "10.1145/1186223.1186384", "10.1145/344779.344865", "10.1145/1187112.1187198", "10.1145/1186562.1015755", "10.1145/1028523.1028534", "10.1145/1186822.1073315", "10.1145/882262.882283", "10.1145/566570.566605", "10.1145/566654.566607", "10.1145/566654.566604", "10.1145/218380.218422", "10.1145/545261.545279", "10.1145/566654.566606", "10.1145/258734.258880", "10.1145/1186223.1186384", "10.1145/344779.344865", "10.1145/1187112.1187198", "10.1145/1186562.1015755", "10.1145/1028523.1028534", "10.1145/1186822.1073315", "10.1145/882262.882283", "10.1145/566570.566605", "10.1145/566654.566607", "10.1145/566654.566604", "10.1145/218380.218422", "10.1145/545261.545279", "10.1145/566654.566606", "10.1145/258734.258880", "10.1145/1186223.1186384", "10.1145/344779.344865", "10.1145/1187112.1187198", "10.1145/1186562.1015755", "10.1145/1028523.1028534", "10.1145/1186822.1073315", "10.1145/882262.882283", "10.1145/566570.566605", "10.1145/566654.566607", "10.1145/566654.566604", "10.1016/0167-6393(90)90021-Z", "10.1007/3-540-36618-0_23", "10.1111/j.1467-8659.2007.01091.x", "10.1023/A:1018628609742", "10.3724/SP.J.1089.2010.10874", "10.1076/jnmr.30.2.159.7114", "10.2307/843561", "10.1007/978-1-4471-0453-7", "10.1002/cav.99", "10.1002/cav.314", "10.1080/09298210701653344", "10.2307/1271436", "10.1007/s10115-004-0154-9", "10.1111/j.1467-8659.2005.00860.x", "10.1139/x98-085", "10.1016/0167-6393(90)90021-Z", "10.1007/3-540-36618-0_23", "10.1111/j.1467-8659.2007.01091.x", "10.1023/A:1018628609742", "10.3724/SP.J.1089.2010.10874", "10.1076/jnmr.30.2.159.7114", "10.2307/843561", "10.1007/978-1-4471-0453-7", "10.1002/cav.99", "10.1002/cav.314", "10.1080/09298210701653344", "10.2307/1271436", "10.1007/s10115-004-0154-9", "10.1111/j.1467-8659.2005.00860.x", "10.1139/x98-085", "10.1016/0167-6393(90)90021-Z", "10.1007/3-540-36618-0_23", "10.1111/j.1467-8659.2007.01091.x", "10.1023/A:1018628609742", "10.3724/SP.J.1089.2010.10874", "10.1076/jnmr.30.2.159.7114", "10.2307/843561", "10.1007/978-1-4471-0453-7", "10.1002/cav.99", "10.1002/cav.314", "10.1080/09298210701653344", "10.2307/1271436", "10.1007/s10115-004-0154-9", "10.1111/j.1467-8659.2005.00860.x", "10.1139/x98-085"]}, "10.1109/TVCG.2011.29": {"doi": "10.1109/TVCG.2011.29", "author": ["K. Bodin", "C. Lacoursiere", "M. Servin"], "title": "Constraint Fluids", "year": "2012", "abstract": "We present a fluid simulation method based on Smoothed Particle Hydrodynamics (SPH) in which incompressibility and boundary conditions are enforced using holonomic kinematic constraints on the density. This formulation enables systematic multiphysics integration in which interactions are modeled via similar constraints between the fluid pseudoparticles and impenetrable surfaces of other bodies. These conditions embody Archimede's principle for solids and thus buoyancy results as a direct consequence. We use a variational time stepping scheme suitable for general constrained multibody systems we call SPOOK. Each step requires the solution of only one Mixed Linear Complementarity Problem (MLCP) with very few inequalities, corresponding to solid boundary conditions. We solve this MLCP with a fast iterative method. Overall stability is vastly improved in comparison to the unconstrained version of SPH, and this allows much larger time steps, and an increase in overall performance by two orders of magnitude. Proof of concept is given for computer graphics applications and interactive simulations.", "keywords": ["computational fluid dynamics", "computer graphics", "digital simulation", "hydrodynamics", "iterative methods", "constraint fluids", "fluid simulation method", "smoothed particle hydrodynamics", "incompressibility conditions", "boundary conditions", "holonomic kinematic constraints", "systematic multiphysics integration", "fluid pseudoparticles", "Archimedes principle", "buoyancy", "SPOOK", "mixed linear complementarity problem", "fast iterative method", "computer graphics applications", "interactive simulations", "Force", "Equations", "Mathematical model", "Computer graphics", "Approximation methods", "Computational modeling", "Stability analysis", "SPH", "incompressible", "constraints", "fluid simulation", "variational integrator.", "Algorithms", "Computer Graphics", "Computer Simulation", "Hydrodynamics", "Models, Theoretical"], "referenced_by": ["10.1109/TVCG.2013.105", "10.1109/TVCG.2016.2578335", "10.1109/ACCESS.2018.2872420", "10.1109/TVCG.2020.3004245", "10.1145/2735627", "10.1145/2766901", "10.1145/2816795.2818117", "10.1145/2366145.2366168", "10.1145/2508363.2508430", "10.1145/2508363.2508395", "10.1145/2185520.2185558", "10.1145/3386569.3392473", "10.1145/2461912.2461984", "10.1002/cav.1568", "10.1007/978-3-319-56397-8_31", "10.1007/s00371-014-1055-x", "10.1007/s00371-018-1488-8", "10.1007/s11390-017-1793-0", "10.1007/s40571-017-0158-3", "10.1111/cgf.12324", "10.1111/cgf.12467", "10.1111/cgf.12490", "10.1111/cgf.12992", "10.1111/cgf.13048", "10.1111/cgf.13292", "10.1119/1.4972493", "10.1016/j.jcp.2018.05.013", "10.1016/j.camwa.2018.06.037", "10.1007/s13319-019-0225-z", "10.1007/s00371-019-01700-y", "10.15748/jasse.2.63", "10.1002/cav.1937", "10.3390/w12071873"], "referencing": ["10.1109/TVCG.2007.70629", "10.1109/38.511", "10.1109/TVCG.2007.70409", "10.1147/rd.112.0215", "10.1109/TVCG.2008.37", "10.1109/TVCG.2007.70629", "10.1109/38.511", "10.1109/TVCG.2007.70409", "10.1147/rd.112.0215", "10.1109/TVCG.2008.37", "10.1109/TVCG.2007.70629", "10.1109/38.511", "10.1109/TVCG.2007.70409", "10.1147/rd.112.0215", "10.1109/TVCG.2008.37", "10.1145/1073368.1073402", "10.1145/1028523.1028542", "10.1145/344779.344940", "10.1145/566570.566584", "10.1145/311535.311600", "10.1145/800059.801167", "10.1145/344779.344936", "10.1145/1576246.1531346", "10.1145/1276377.1276437", "10.1145/1179352.1141960", "10.1145/545261.545289", "10.1145/1073204.1073298", "10.1145/97879.97884", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1275808.1276438", "10.1145/1073368.1073402", "10.1145/1028523.1028542", "10.1145/344779.344940", "10.1145/566570.566584", "10.1145/311535.311600", "10.1145/800059.801167", "10.1145/344779.344936", "10.1145/1576246.1531346", "10.1145/1276377.1276437", "10.1145/1179352.1141960", "10.1145/545261.545289", "10.1145/1073204.1073298", "10.1145/97879.97884", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1275808.1276438", "10.1145/1073368.1073402", "10.1145/1028523.1028542", "10.1145/344779.344940", "10.1145/566570.566584", "10.1145/311535.311600", "10.1145/800059.801167", "10.1145/344779.344936", "10.1145/1576246.1531346", "10.1145/1276377.1276437", "10.1145/1179352.1141960", "10.1145/545261.545289", "10.1145/1073204.1073298", "10.1145/97879.97884", "10.1145/311535.311548", "10.1145/383259.383261", "10.1145/566654.566645", "10.1145/1275808.1276438", "10.1016/0045-7825(72)90018-7", "10.1007/BF00370488", "10.1111/j.1467-8659.2009.01360.x", "10.1086/112164", "10.1093/mnras/181.3.375", "10.1115/1.1431547", "10.1142/9789812564405", "10.1088/0034-4885/68/8/R01", "10.1201/b10635", "10.1006/jcph.1999.6246", "10.1111/1467-8659.00687", "10.1016/j.jcp.2008.06.005", "10.1016/j.jcp.2007.06.019", "10.1016/j.jvcir.2007.01.005", "10.1080/08905459508905232", "10.1002/cpa.3160100103", "10.1137/1.9781611970944", "10.1002/nme.2458", "10.1046/j.1365-8711.2002.05115.x", "10.1016/j.jcp.2006.01.021", "10.1142/6218", "10.1006/gmip.1995.1012", "10.1006/gmip.1996.0039", "10.6028/jres.049.044", "10.1142/9789812564405", "10.1007/s10107-005-0590-7", "10.1016/0045-7825(72)90018-7", "10.1007/BF00370488", "10.1111/j.1467-8659.2009.01360.x", "10.1086/112164", "10.1093/mnras/181.3.375", "10.1115/1.1431547", "10.1142/9789812564405", "10.1088/0034-4885/68/8/R01", "10.1201/b10635", "10.1006/jcph.1999.6246", "10.1111/1467-8659.00687", "10.1016/j.jcp.2008.06.005", "10.1016/j.jcp.2007.06.019", "10.1016/j.jvcir.2007.01.005", "10.1080/08905459508905232", "10.1002/cpa.3160100103", "10.1137/1.9781611970944", "10.1002/nme.2458", "10.1046/j.1365-8711.2002.05115.x", "10.1016/j.jcp.2006.01.021", "10.1142/6218", "10.1006/gmip.1995.1012", "10.1006/gmip.1996.0039", "10.6028/jres.049.044", "10.1142/9789812564405", "10.1007/s10107-005-0590-7", "10.1016/0045-7825(72)90018-7", "10.1007/BF00370488", "10.1111/j.1467-8659.2009.01360.x", "10.1086/112164", "10.1093/mnras/181.3.375", "10.1115/1.1431547", "10.1142/9789812564405", "10.1088/0034-4885/68/8/R01", "10.1201/b10635", "10.1006/jcph.1999.6246", "10.1111/1467-8659.00687", "10.1016/j.jcp.2008.06.005", "10.1016/j.jcp.2007.06.019", "10.1016/j.jvcir.2007.01.005", "10.1080/08905459508905232", "10.1002/cpa.3160100103", "10.1137/1.9781611970944", "10.1002/nme.2458", "10.1046/j.1365-8711.2002.05115.x", "10.1016/j.jcp.2006.01.021", "10.1142/6218", "10.1006/gmip.1995.1012", "10.1006/gmip.1996.0039", "10.6028/jres.049.044", "10.1142/9789812564405", "10.1007/s10107-005-0590-7"]}}