{"10.1109/TVCG.2016.2638558": {"doi": "10.1109/TVCG.2016.2638558", "author": ["L. De Floriani"], "title": "State of the Journal", "year": "2017", "abstract": "Presents the current state of the journal, its scope, and outlook toward the future.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2016.2525771": {"doi": "10.1109/TVCG.2016.2525771", "author": ["Y. Yang", "R. Pintus", "H. Rushmeier", "I. Ivrissimtzis"], "title": "A 3D Steganalytic Algorithm and Steganalysis-Resistant Watermarking", "year": "2017", "abstract": "We propose a simple yet efficient steganalytic algorithm for watermarks embedded by two state-of-the-art 3D watermarking algorithms by Cho et al. The main observation is that while in a clean model the means/variances of Cho et al.\u2019s normalized histogram bins are expected to follow a Gaussian distribution, in a marked model their distribution will be bimodal. The proposed algorithm estimates the number of bins through an exhaustive search and then the presence of a watermark is decided by a tailor made normality test or a $t$ -test. We also propose a modification of Cho et al.\u2019s watermarking algorithms with the watermark embedded by changing the histogram of the radial coordinates of the vertices. Rather than targeting a continuous statistics such as the mean or variance of the values in a bin, the proposed watermarking modifies a discrete statistic, which here is the height of the histogram bin, to achieve watermark embedding. Experimental results demonstrate that the modified algorithm offers not only better resistance against the steganalytic attack we developed, but also an improved robustness/capacity trade-off.", "keywords": ["Watermarking", "Three-dimensional displays", "Robustness", "Histograms", "Solid modeling", "Standards", "Algorithm design and analysis", "Polygonal meshes", "data embedding", "watermarking", "steganalysis"], "referenced_by": ["10.1109/ACCESS.2017.2767072", "10.1109/ICIP.2017.8296333", "10.1109/DESSERT.2018.8409155", "10.1109/ICIP.2018.8451643", "10.1109/MMUL.2018.112142343", "10.1109/ACCESS.2019.2960455", "10.1109/TCYB.2018.2883082", "10.1109/I-SMAC49090.2020.9243381", "10.1145/3382772", "10.1049/iet-ipr.2017.0162", "10.1007/978-3-319-47364-2_51", "10.1007/978-3-319-57699-2_10", "10.1007/978-3-319-68548-9_58", "10.1007/978-3-662-56689-3_5", "10.1007/978-981-10-4154-9_42", "10.1007/s11042-016-3920-2", "10.1007/s11042-017-4483-6", "10.1016/j.ins.2017.06.011", "10.1007/978-3-030-00021-9_21", "10.1007/s11042-018-6721-y", "10.1007/s00034-019-01220-z", "10.1007/s11042-019-08296-4", "10.1016/j.ins.2020.02.061", "10.1088/1742-6596/1570/1/012059", "10.1111/cgf.14065", "10.1016/j.neucom.2020.09.013"], "referencing": ["10.1109/TMM.2005.858412", "10.1109/TVCG.2005.71", "10.1109/TIP.2011.2142004", "10.1109/TMM.2008.2007350", "10.1109/ICIP.2014.7025969", "10.1109/TMM.2005.858377", "10.1109/TIFS.2005.863485", "10.1109/TIFS.2006.890517", "10.1109/TIFS.2014.2364918", "10.1109/TSP.2006.882111", "10.1109/38.736467", "10.1109/TIP.2005.863116", "10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/93.959097", "10.1109/LSP.2005.847889", "10.1109/TIFS.2012.2224108", "10.1109/TMM.2005.858412", "10.1109/TVCG.2005.71", "10.1109/TIP.2011.2142004", "10.1109/TMM.2008.2007350", "10.1109/ICIP.2014.7025969", "10.1109/TMM.2005.858377", "10.1109/TIFS.2005.863485", "10.1109/TIFS.2006.890517", "10.1109/TIFS.2014.2364918", "10.1109/TSP.2006.882111", "10.1109/38.736467", "10.1109/TIP.2005.863116", "10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/93.959097", "10.1109/LSP.2005.847889", "10.1109/TIFS.2012.2224108", "10.1109/TMM.2005.858412", "10.1109/TVCG.2005.71", "10.1109/TIP.2011.2142004", "10.1109/TMM.2008.2007350", "10.1109/ICIP.2014.7025969", "10.1109/TMM.2005.858377", "10.1109/TIFS.2005.863485", "10.1109/TIFS.2006.890517", "10.1109/TIFS.2014.2364918", "10.1109/TSP.2006.882111", "10.1109/38.736467", "10.1109/TIP.2005.863116", "10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/93.959097", "10.1109/LSP.2005.847889", "10.1109/TIFS.2012.2224108", "10.1145/311535.311540", "10.1145/2535555", "10.1145/344779.344924", "10.1145/1022431.1022456", "10.1145/1576246.1531379", "10.1145/311535.311540", "10.1145/2535555", "10.1145/344779.344924", "10.1145/1022431.1022456", "10.1145/1576246.1531379", "10.1145/311535.311540", "10.1145/2535555", "10.1145/344779.344924", "10.1145/1022431.1022456", "10.1145/1576246.1531379", "10.1111/j.1467-8659.2010.01767.x", "10.1016/S0031-3203(03)00086-4", "10.1111/1467-8659.00597", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-007-0147-2", "10.1016/S0031-3203(03)00035-9", "10.2307/2334448", "10.1111/j.1467-8659.2011.02017.x", "10.1111/1467-8659.00236", "10.1111/j.1467-8659.2010.01767.x", "10.1016/S0031-3203(03)00086-4", "10.1111/1467-8659.00597", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-007-0147-2", "10.1016/S0031-3203(03)00035-9", "10.2307/2334448", "10.1111/j.1467-8659.2011.02017.x", "10.1111/1467-8659.00236", "10.1111/j.1467-8659.2010.01767.x", "10.1016/S0031-3203(03)00086-4", "10.1111/1467-8659.00597", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-007-0147-2", "10.1016/S0031-3203(03)00035-9", "10.2307/2334448", "10.1111/j.1467-8659.2011.02017.x", "10.1111/1467-8659.00236"]}, "10.1109/TVCG.2016.2520946": {"doi": "10.1109/TVCG.2016.2520946", "author": ["G. Xiong", "P. Sun", "H. Zhou", "S. Ha", "B. \u00f3. Hartaigh", "Q. A. Truong", "J. K. Min"], "title": "Comprehensive Modeling and Visualization of Cardiac Anatomy and Physiology from CT Imaging and Computer Simulations", "year": "2017", "abstract": "In clinical cardiology, both anatomy and physiology are needed to diagnose cardiac pathologies. CT imaging and computer simulations provide valuable and complementary data for this purpose. However, it remains challenging to gain useful information from the large amount of high-dimensional diverse data. The current tools are not adequately integrated to visualize anatomic and physiologic data from a complete yet focused perspective. We introduce a new computer-aided diagnosis framework, which allows for comprehensive modeling and visualization of cardiac anatomy and physiology from CT imaging data and computer simulations, with a primary focus on ischemic heart disease. The following visual information is presented: (1) Anatomy from CT imaging: geometric modeling and visualization of cardiac anatomy, including four heart chambers, left and right ventricular outflow tracts, and coronary arteries; (2) Function from CT imaging: motion modeling, strain calculation, and visualization of four heart chambers; (3) Physiology from CT imaging: quantification and visualization of myocardial perfusion and contextual integration with coronary artery anatomy; (4) Physiology from computer simulation: computation and visualization of hemodynamics (e.g., coronary blood velocity, pressure, shear stress, and fluid forces on the vessel wall). Substantially, feedback from cardiologists have confirmed the practical utility of integrating these features for the purpose of computer-aided diagnosis of ischemic heart disease.", "keywords": ["cardiology", "data visualisation", "medical image processing", "comprehensive modeling", "physiology", "computer simulations", "clinical cardiology", "cardiac pathologies", "high-dimensional diverse data", "computer-aided diagnosis framework", "CT imaging data", "ischemic heart disease", "visual information", "geometric modeling", "cardiac anatomy visualization", "heart chambers", "ventricular outflow tracts", "coronary arteries", "motion modeling", "strain calculation", "myocardial perfusion", "contextual integration", "coronary artery anatomy", "hemodynamics", "Computed tomography", "Heart", "Arteries", "Physiology", "Computational modeling", "Data visualization", "Cardiac chamber", "coronary artery", "imaging", "anatomy", "physiology", "perfusion", "motion", "strain", "shear stress", "bull\u2019s-eye plot", "image analysis", "geometric modeling", "computational fluid dynamics", "visualization.", "Cardiac Imaging Techniques", "Computer Graphics", "Computer Simulation", "Coronary Vessels", "Humans", "Image Interpretation, Computer-Assisted", "Models, Cardiovascular", "Myocardial Ischemia", "Tomography, X-Ray Computed"], "referenced_by": ["10.1109/EMBC.2018.8513435", "10.1007/978-3-030-20351-1_34", "10.3390/s20030595", "10.1093/jcde/qwaa054", "10.1016/B978-0-12-819295-5.00004-4"], "referencing": ["10.1109/TVCG.2007.70550", "10.1109/TVCG.2008.180", "10.1109/TMI.2008.918330", "10.1109/TMI.2008.2004421", "10.1109/TMI.2008.2011480", "10.1109/TMI.2009.2038908", "10.1109/TMI.2012.2230015", "10.1109/MCG.2007.323435", "10.1109/TVCG.2007.70550", "10.1109/TVCG.2008.180", "10.1109/TMI.2008.918330", "10.1109/TMI.2008.2004421", "10.1109/TMI.2008.2011480", "10.1109/TMI.2009.2038908", "10.1109/TMI.2012.2230015", "10.1109/MCG.2007.323435", "10.1109/TVCG.2007.70550", "10.1109/TVCG.2008.180", "10.1109/TMI.2008.918330", "10.1109/TMI.2008.2004421", "10.1109/TMI.2008.2011480", "10.1109/TMI.2009.2038908", "10.1109/TMI.2012.2230015", "10.1109/MCG.2007.323435", "10.1145/237170.237254", "10.1145/237170.237254", "10.1145/237170.237254", "10.1056/NEJMra1112570", "10.1016/0735-1097(88)90356-7", "10.1016/j.jacc.2011.02.018", "10.1016/j.jacc.2009.08.087", "10.1038/nrcardio.2014.60", "10.1016/j.jcct.2014.11.007", "10.1016/j.nuclcard.2007.01.035", "10.1016/j.jcmg.2011.01.012", "10.1016/j.jcin.2010.01.007", "10.1016/j.jacc.2008.05.024", "10.1001/jama.282.21.2035", "10.1007/s10439-010-0083-6", "10.1016/j.jacc.2012.11.083", "10.1001/2012.jama.11274", "10.1016/j.jacc.2013.11.043", "10.1117/12.771181", "10.1007/s11548-011-0657-2", "10.2967/jnumed.113.119842", "10.1016/j.mri.2012.05.001", "10.1016/j.media.2006.03.004", "10.1118/1.3512795", "10.1118/1.3254077", "10.1117/12.877233", "10.1007/s10554-011-9894-2", "10.1007/978-3-642-40760-4_10", "10.1007/s10554-013-0271-1", "10.1016/j.jcct.2009.09.004", "10.1016/j.media.2015.05.010", "10.1016/j.media.2009.07.011", "10.1007/BF01386390", "10.1016/j.jcmg.2011.09.006", "10.1148/radiol.2231010441", "10.1056/NEJM198702053160604", "10.1007/s00330-008-1019-5", "10.1148/radiol.10100681", "10.1016/S0894-7317(99)70019-2", "10.1016/j.echo.2004.03.027", "10.1002/(SICI)1097-0207(20000430)47:12&lt;2039::AID-NME872&gt;3.0.CO;2-1", "10.1016/j.cma.2004.07.036", "10.1016/j.jcmg.2011.04.017", "10.2214/AJR.14.13546", "10.1016/j.jcmg.2015.09.003", "10.1161/CIRCIMAGING.108.813766", "10.1111/cpf.12163", "10.1007/s10439-010-9901-0", "10.1056/NEJM199606273342604", "10.1161/01.RES.66.4.1045", "10.1016/j.jacc.2004.08.016", "10.1016/j.mric.2014.09.008", "10.1056/NEJMra1112570", "10.1016/0735-1097(88)90356-7", "10.1016/j.jacc.2011.02.018", "10.1016/j.jacc.2009.08.087", "10.1038/nrcardio.2014.60", "10.1016/j.jcct.2014.11.007", "10.1016/j.nuclcard.2007.01.035", "10.1016/j.jcmg.2011.01.012", "10.1016/j.jcin.2010.01.007", "10.1016/j.jacc.2008.05.024", "10.1001/jama.282.21.2035", "10.1007/s10439-010-0083-6", "10.1016/j.jacc.2012.11.083", "10.1001/2012.jama.11274", "10.1016/j.jacc.2013.11.043", "10.1117/12.771181", "10.1007/s11548-011-0657-2", "10.2967/jnumed.113.119842", "10.1016/j.mri.2012.05.001", "10.1016/j.media.2006.03.004", "10.1118/1.3512795", "10.1118/1.3254077", "10.1117/12.877233", "10.1007/s10554-011-9894-2", "10.1007/978-3-642-40760-4_10", "10.1007/s10554-013-0271-1", "10.1016/j.jcct.2009.09.004", "10.1016/j.media.2015.05.010", "10.1016/j.media.2009.07.011", "10.1007/BF01386390", "10.1016/j.jcmg.2011.09.006", "10.1148/radiol.2231010441", "10.1056/NEJM198702053160604", "10.1007/s00330-008-1019-5", "10.1148/radiol.10100681", "10.1016/S0894-7317(99)70019-2", "10.1016/j.echo.2004.03.027", "10.1002/(SICI)1097-0207(20000430)47:12&lt;2039::AID-NME872&gt;3.0.CO;2-1", "10.1016/j.cma.2004.07.036", "10.1016/j.jcmg.2011.04.017", "10.2214/AJR.14.13546", "10.1016/j.jcmg.2015.09.003", "10.1161/CIRCIMAGING.108.813766", "10.1111/cpf.12163", "10.1007/s10439-010-9901-0", "10.1056/NEJM199606273342604", "10.1161/01.RES.66.4.1045", "10.1016/j.jacc.2004.08.016", "10.1016/j.mric.2014.09.008", "10.1056/NEJMra1112570", "10.1016/0735-1097(88)90356-7", "10.1016/j.jacc.2011.02.018", "10.1016/j.jacc.2009.08.087", "10.1038/nrcardio.2014.60", "10.1016/j.jcct.2014.11.007", "10.1016/j.nuclcard.2007.01.035", "10.1016/j.jcmg.2011.01.012", "10.1016/j.jcin.2010.01.007", "10.1016/j.jacc.2008.05.024", "10.1001/jama.282.21.2035", "10.1007/s10439-010-0083-6", "10.1016/j.jacc.2012.11.083", "10.1001/2012.jama.11274", "10.1016/j.jacc.2013.11.043", "10.1117/12.771181", "10.1007/s11548-011-0657-2", "10.2967/jnumed.113.119842", "10.1016/j.mri.2012.05.001", "10.1016/j.media.2006.03.004", "10.1118/1.3512795", "10.1118/1.3254077", "10.1117/12.877233", "10.1007/s10554-011-9894-2", "10.1007/978-3-642-40760-4_10", "10.1007/s10554-013-0271-1", "10.1016/j.jcct.2009.09.004", "10.1016/j.media.2015.05.010", "10.1016/j.media.2009.07.011", "10.1007/BF01386390", "10.1016/j.jcmg.2011.09.006", "10.1148/radiol.2231010441", "10.1056/NEJM198702053160604", "10.1007/s00330-008-1019-5", "10.1148/radiol.10100681", "10.1016/S0894-7317(99)70019-2", "10.1016/j.echo.2004.03.027", "10.1002/(SICI)1097-0207(20000430)47:12&lt;2039::AID-NME872&gt;3.0.CO;2-1", "10.1016/j.cma.2004.07.036", "10.1016/j.jcmg.2011.04.017", "10.2214/AJR.14.13546", "10.1016/j.jcmg.2015.09.003", "10.1161/CIRCIMAGING.108.813766", "10.1111/cpf.12163", "10.1007/s10439-010-9901-0", "10.1056/NEJM199606273342604", "10.1161/01.RES.66.4.1045", "10.1016/j.jacc.2004.08.016", "10.1016/j.mric.2014.09.008"]}, "10.1109/TVCG.2016.2525788": {"doi": "10.1109/TVCG.2016.2525788", "author": ["A. A. Stanley", "A. M. Okamura"], "title": "Deformable Model-Based Methods for Shape Control of a Haptic Jamming Surface", "year": "2017", "abstract": "Haptic Jamming, the approach of simultaneously controlling mechanical properties and surface deformation of a tactile display via particle jamming and pneumatics, shows promise as a tangible, shape-changing human-computer interface. Previous research introduced device design and described the force-displacement interactions for individual jamming cells. The work in this article analyzes the shape output capabilities of a multi-cell array. A spring-mass deformable body simulation combines models of the three actuation inputs of a Haptic Jamming surface: node pinning, chamber pressurization, and cell jamming. Surface measurements of a 12-cell prototype from a depth camera fit the mass and stiffness parameters to the device during pressurization tests and validate the accuracy of the model for various actuation sequences. The simulator is used to develop an algorithm that generates a sequence of actuation inputs for a Haptic Jamming array of any size in order to match a desired surface output shape. Data extracted from topographical maps and three-dimensional solid object models are used to evaluate the shape-matching algorithm and assess the utility of increasing array size and resolution. Results show that a discrete Laplace operator applied to the input is a suitable predictor of the correlation coefficient between the desired shape and the device output.", "keywords": ["computer graphics", "haptic interfaces", "pneumatic control equipment", "shape control", "shape recognition", "deformable model-based methods", "shape control", "haptic jamming surface", "mechanical properties", "tactile display surface deformation", "particle pneumatics", "shape-changing human-computer interface", "force-displacement interactions", "jamming cells", "multicell array", "spring-mass deformable body simulation", "actuation inputs", "node pinning", "chamber pressurization", "cell jamming", "depth camera", "stiffness parameters", "haptic jamming array", "data extraction", "three-dimensional solid object models", "discrete Laplace operator", "correlation coefficient", "Jamming", "Haptic interfaces", "Arrays", "Solid modeling", "Deformable models", "Shape", "Surface topography", "Tactile display", "haptic I/O", "particle jamming", "shape-changing interface"], "referenced_by": ["10.1109/ICRA.2016.7487433", "10.1109/ROBOSOFT.2018.8404916", "10.1109/TMECH.2018.2871176", "10.1109/TIE.2019.2920602", "10.1109/TRO.2020.2980114", "10.1109/TRO.2020.2985583", "10.1145/3130800.3130850", "10.1145/3386569.3392398", "10.1080/01691864.2018.1447393", "10.1115/1.4041913", "10.1089/soro.2018.0086", "10.1016/j.infrared.2019.103174", "10.1007/s12008-020-00656-x", "10.1007/s40997-020-00367-4", "10.3724/SP.J.2096-5796.2019.0008"], "referencing": ["10.1109/TOH.2015.2391093", "10.1109/TBME.2008.2001137", "10.1109/51.391770", "10.1109/TMECH.2014.2341630", "10.1109/HAPTICS.2014.6775477", "10.1109/ICRA.2012.6225278", "10.1109/TBME.2011.2160347", "10.1109/ROBOT.2008.4543353", "10.1109/TOH.2015.2391093", "10.1109/TBME.2008.2001137", "10.1109/51.391770", "10.1109/TMECH.2014.2341630", "10.1109/HAPTICS.2014.6775477", "10.1109/ICRA.2012.6225278", "10.1109/TBME.2011.2160347", "10.1109/ROBOT.2008.4543353", "10.1109/TOH.2015.2391093", "10.1109/TBME.2008.2001137", "10.1109/51.391770", "10.1109/TMECH.2014.2341630", "10.1109/HAPTICS.2014.6775477", "10.1109/ICRA.2012.6225278", "10.1109/TBME.2011.2160347", "10.1109/ROBOT.2008.4543353", "10.1145/1187297.1187329", "10.1145/2501988.2502032", "10.1145/2343483.2343501", "10.1145/1377676.1377725", "10.1145/2380116.2380181", "10.1145/1187297.1187329", "10.1145/2501988.2502032", "10.1145/2343483.2343501", "10.1145/1377676.1377725", "10.1145/2380116.2380181", "10.1145/1187297.1187329", "10.1145/2501988.2502032", "10.1145/2343483.2343501", "10.1145/1377676.1377725", "10.1145/2380116.2380181", "10.1016/S0141-9382(98)00017-1", "10.1016/S1474-6670(17)33418-3", "10.3182/20120905-3-HR-2030.00072", "10.1088/0964-1726/23/9/095007", "10.1016/j.cmpb.2004.11.002", "10.1111/j.1467-8659.2006.01000.x", "10.1007/BF01900346", "10.1016/0734-189X(89)90131-X", "10.1016/j.cag.2009.03.005", "10.1007/978-3-319-10470-6_11", "10.1016/j.pbiomolbio.2010.09.016", "10.1016/j.cma.2010.06.037", "10.1016/S0141-9382(98)00017-1", "10.1016/S1474-6670(17)33418-3", "10.3182/20120905-3-HR-2030.00072", "10.1088/0964-1726/23/9/095007", "10.1016/j.cmpb.2004.11.002", "10.1111/j.1467-8659.2006.01000.x", "10.1007/BF01900346", "10.1016/0734-189X(89)90131-X", "10.1016/j.cag.2009.03.005", "10.1007/978-3-319-10470-6_11", "10.1016/j.pbiomolbio.2010.09.016", "10.1016/j.cma.2010.06.037", "10.1016/S0141-9382(98)00017-1", "10.1016/S1474-6670(17)33418-3", "10.3182/20120905-3-HR-2030.00072", "10.1088/0964-1726/23/9/095007", "10.1016/j.cmpb.2004.11.002", "10.1111/j.1467-8659.2006.01000.x", "10.1007/BF01900346", "10.1016/0734-189X(89)90131-X", "10.1016/j.cag.2009.03.005", "10.1007/978-3-319-10470-6_11", "10.1016/j.pbiomolbio.2010.09.016", "10.1016/j.cma.2010.06.037"]}, "10.1109/TVCG.2016.2532331": {"doi": "10.1109/TVCG.2016.2532331", "author": ["S. R. Gomez", "R. Jianu", "R. Cabeen", "H. Guo", "D. H. Laidlaw"], "title": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks", "year": "2017", "abstract": "We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.", "keywords": ["computer vision", "crowdsourcing", "gaze tracking", "crowdsourcing gaze location estimation", "Fauxvea", "visualization analysis tasks", "user behaviors", "gaze fixations", "cursor interactions", "eye-tracking hardware", "eye-tracking data", "Amazon Mechanical Turk", "MTurk", "crowdsourcing visual analysis tasks", "static information visualizations", "Data visualization", "Visualization", "Crowdsourcing", "Gaze tracking", "Presses", "Hardware", "Layout", "Eye tracking", "crowdsourcing", "focus window", "information visualization", "visual analysis", "user studies", "Adult", "Attention", "Crowdsourcing", "Eye Movement Measurements", "Female", "Fixation, Ocular", "Humans", "Internet", "Male", "Task Performance and Analysis"], "referenced_by": ["10.1109/TVCG.2017.2734659", "10.1109/TCSVT.2019.2940479", "10.1109/BELIV51497.2020.00009", "10.1142/S0218001418500167", "10.1007/s41233-019-0024-6"], "referencing": ["10.1109/TVCG.2011.193", "10.1109/TVCG.2011.185", "10.1109/TVCG.2010.186", "10.1109/TVCG.2012.180", "10.1109/INFVIS.2001.963286", "10.1109/TVCG.2012.215", "10.1109/TVCG.2011.193", "10.1109/TVCG.2011.185", "10.1109/TVCG.2010.186", "10.1109/TVCG.2012.180", "10.1109/INFVIS.2001.963286", "10.1109/TVCG.2012.215", "10.1109/TVCG.2011.193", "10.1109/TVCG.2011.185", "10.1109/TVCG.2010.186", "10.1109/TVCG.2012.180", "10.1109/INFVIS.2001.963286", "10.1109/TVCG.2012.215", "10.1145/634067.634234", "10.1145/1358628.1358797", "10.1145/1753846.1754025", "10.1145/1978942.1979125", "10.1145/2009916.2009967", "10.1145/1753326.1753357", "10.1145/2110192.2110202", "10.1145/634067.634234", "10.1145/1358628.1358797", "10.1145/1753846.1754025", "10.1145/1978942.1979125", "10.1145/2009916.2009967", "10.1145/1753326.1753357", "10.1145/2110192.2110202", "10.1145/634067.634234", "10.1145/1358628.1358797", "10.1145/1753846.1754025", "10.1145/1978942.1979125", "10.1145/2009916.2009967", "10.1145/1753326.1753357", "10.1145/2110192.2110202", "10.1007/3-540-44590-0_17", "10.21236/ADA205963", "10.3758/BF03193158", "10.3758/BF03195573", "10.3758/BF03195497", "10.1016/0010-0285(76)90015-3", "10.1068/p2952", "10.1167/12.4.14", "10.1167/8.7.32", "10.1007/3-540-44590-0_17", "10.21236/ADA205963", "10.3758/BF03193158", "10.3758/BF03195573", "10.3758/BF03195497", "10.1016/0010-0285(76)90015-3", "10.1068/p2952", "10.1167/12.4.14", "10.1167/8.7.32", "10.1007/3-540-44590-0_17", "10.21236/ADA205963", "10.3758/BF03193158", "10.3758/BF03195573", "10.3758/BF03195497", "10.1016/0010-0285(76)90015-3", "10.1068/p2952", "10.1167/12.4.14", "10.1167/8.7.32"]}, "10.1109/TVCG.2016.2525724": {"doi": "10.1109/TVCG.2016.2525724", "author": ["M. Gao", "T. Cao", "T. Tan"], "title": "Flip to Regular Triangulation and Convex Hull", "year": "2017", "abstract": "Flip is a simple and local operation to transform one triangulation to another. It makes changes only to some neighboring simplices, without considering any attribute or configuration global in nature to the triangulation. Thanks to this characteristic, several flips can be independently applied to different small, non-overlapping regions of one triangulation. Such operation is favored when designing algorithms for data-parallel, massively multithreaded hardware, such as the GPU. However, most existing flip algorithms are designed to be executed sequentially, and usually need some restrictions on the execution order of flips, making them hard to be adapted to parallel computation. In this paper, we present an in depth study of flip algorithms in low dimensions, with the emphasis on the flexibility of their execution order. In particular, we propose a series of provably correct flip algorithms for regular triangulation and convex hull in 2D and 3D, with implementations for both CPUs and GPUs. Our experiment shows that our GPU implementation for constructing these structures from a given point set achieves up to two orders of magnitude of speedup over other popular single-threaded CPU implementation of existing algorithms.", "keywords": ["mesh generation", "microprocessor chips", "multi-threading", "regular triangulation", "nonoverlapping regions", "massively multithreaded hardware", "flip algorithms", "parallel computation", "CPU", "GPU", "Three-dimensional displays", "Algorithm design and analysis", "Graphics processing units", "Transforms", "Hardware", "Computational geometry", "Periodic structures", "Graham\u2019s scan", "Lawson\u2019s flip algorithm", "star-shaped polytope", "GPU"], "referenced_by": ["10.1016/j.patrec.2017.12.014", "10.1007/s11431-018-9382-7"], "referencing": ["10.1109/TVCG.2012.307", "10.1109/TVCG.2012.307", "10.1109/TVCG.2012.307", "10.1145/2556700.2556710", "10.1145/142675.142688", "10.1145/2448196.2448203", "10.1145/77635.77639", "10.1145/2556700.2556710", "10.1145/142675.142688", "10.1145/2448196.2448203", "10.1145/77635.77639", "10.1145/2556700.2556710", "10.1145/142675.142688", "10.1145/2448196.2448203", "10.1145/77635.77639", "10.1016/0012-365X(72)90093-3", "10.1137/0910044", "10.1016/j.comgeo.2010.04.008", "10.1016/0020-0190(72)90045-2", "10.1007/978-3-540-39422-8_5", "10.1007/978-1-4612-1098-6", "10.2307/2319703", "10.1007/s00208-005-0643-5", "10.1007/PL00009321", "10.1016/0012-365X(72)90093-3", "10.1137/0910044", "10.1016/j.comgeo.2010.04.008", "10.1016/0020-0190(72)90045-2", "10.1007/978-3-540-39422-8_5", "10.1007/978-1-4612-1098-6", "10.2307/2319703", "10.1007/s00208-005-0643-5", "10.1007/PL00009321", "10.1016/0012-365X(72)90093-3", "10.1137/0910044", "10.1016/j.comgeo.2010.04.008", "10.1016/0020-0190(72)90045-2", "10.1007/978-3-540-39422-8_5", "10.1007/978-1-4612-1098-6", "10.2307/2319703", "10.1007/s00208-005-0643-5", "10.1007/PL00009321"]}, "10.1109/TVCG.2016.2525774": {"doi": "10.1109/TVCG.2016.2525774", "author": ["C. Yao", "S. Hung", "G. Li", "I. Chen", "R. Adhitya", "Y. Lai"], "title": "Manga Vectorization and Manipulation with Procedural Simple Screentone", "year": "2017", "abstract": "Manga are a popular artistic form around the world, and artists use simple line drawing and screentone to create all kinds of interesting productions. Vectorization is helpful to digitally reproduce these elements for proper content and intention delivery on electronic devices. Therefore, this study aims at transforming scanned Manga to a vector representation for interactive manipulation and real-time rendering with arbitrary resolution. Our system first decomposes the patch into rough Manga elements including possible borders and shading regions using adaptive binarization and screentone detector. We classify detected screentone into simple and complex patterns: our system extracts simple screentone properties for refining screentone borders, estimating lighting, compensating missing strokes inside screentone regions, and later resolution independently rendering with our procedural shaders. Our system treats the others as complex screentone areas and vectorizes them with our proposed line tracer which aims at locating boundaries of all shading regions and polishing all shading borders with the curve-based Gaussian refiner. A user can lay down simple scribbles to cluster Manga elements intuitively for the formation of semantic components, and our system vectorizes these components into shading meshes along with embedded B\u00e9zier curves as a unified foundation for consistent manipulation including pattern manipulation, deformation, and lighting addition. Our system can real-time and resolution independently render the shading regions with our procedural shaders and drawing borders with the curve-based shader. For Manga manipulation, the proposed vector representation can be not only magnified without artifacts but also deformed easily to generate interesting results.", "keywords": ["embedded systems", "entertainment", "Gaussian processes", "rendering (computer graphics)", "Manga vectorization", "Manga manipulation", "electronic devices", "vector representation", "interactive manipulation", "real-time rendering", "arbitrary resolution", "adaptive binarization", "screentone detector", "curve-based Gaussian refiner", "embedded Bezier curves", "curve-based shader", "Image color analysis", "Real-time systems", "Rendering (computer graphics)", "Lighting", "Solids", "Semantics", "Color", "Manga", "semantic components", "vectorization", "screentone", "procedural shaders"], "referenced_by": ["10.1109/ICDAR.2017.292", "10.1109/MMUL.2018.2875859", "10.1109/ISM46123.2019.00046", "10.1145/3072959.3073675", "10.1007/s41095-016-0069-x", "10.3390/jimaging4070087", "10.3390/app10010041"], "referencing": ["10.1109/TIP.2010.2046605", "10.1109/ICCV.2003.1238418", "10.1109/34.709593", "10.1109/TVCG.2012.76", "10.1109/TSMC.1979.4310076", "10.1109/TVCG.2009.9", "10.1109/TPAMI.2006.81", "10.1109/TIP.2010.2046605", "10.1109/ICCV.2003.1238418", "10.1109/34.709593", "10.1109/TVCG.2012.76", "10.1109/TSMC.1979.4310076", "10.1109/TVCG.2009.9", "10.1109/TPAMI.2006.81", "10.1109/TIP.2010.2046605", "10.1109/ICCV.2003.1238418", "10.1109/34.709593", "10.1109/TVCG.2012.76", "10.1109/TSMC.1979.4310076", "10.1109/TVCG.2009.9", "10.1109/TPAMI.2006.81", "10.1145/2425836.2425899", "10.1145/2366145.2366159", "10.1145/1531326.1531391", "10.1145/1073204.1073303", "10.1145/2037715.2037756", "10.1145/2421636.2421640", "10.1145/1409060.1409108", "10.1145/1141911.1142017", "10.1145/987657.987677", "10.1145/1618452.1618461", "10.1145/2425836.2425899", "10.1145/2366145.2366159", "10.1145/1531326.1531391", "10.1145/1073204.1073303", "10.1145/2037715.2037756", "10.1145/2421636.2421640", "10.1145/1409060.1409108", "10.1145/1141911.1142017", "10.1145/987657.987677", "10.1145/1618452.1618461", "10.1145/2425836.2425899", "10.1145/2366145.2366159", "10.1145/1531326.1531391", "10.1145/1073204.1073303", "10.1145/2037715.2037756", "10.1145/2421636.2421640", "10.1145/1409060.1409108", "10.1145/1141911.1142017", "10.1145/987657.987677", "10.1145/1618452.1618461", "10.1080/2151237X.2007.10129236", "10.1016/0031-3203(95)00067-4", "10.1016/0167-2789(92)90242-F", "10.1016/S0031-3203(99)00055-2", "10.1080/2151237X.2007.10129236", "10.1016/0031-3203(95)00067-4", "10.1016/0167-2789(92)90242-F", "10.1016/S0031-3203(99)00055-2", "10.1080/2151237X.2007.10129236", "10.1016/0031-3203(95)00067-4", "10.1016/0167-2789(92)90242-F", "10.1016/S0031-3203(99)00055-2"]}, "10.1109/TVCG.2016.2520926": {"doi": "10.1109/TVCG.2016.2520926", "author": ["L. Shuai", "C. Li", "X. Guo", "B. Prabhakaran", "J. Chai"], "title": "Motion Capture With Ellipsoidal Skeleton Using Multiple Depth Cameras", "year": "2017", "abstract": "This paper introduces a novel motion capturing framework which works by minimizing the fitting error between an ellipsoid based skeleton and the input point cloud data captured by multiple depth cameras. The novelty of this method comes from that it uses the ellipsoids equipped with the spherical harmonics encoded displacement and normal functions to capture the geometry details of the tracked object. This method is also integrated with a mechanism to avoid collisions of bones during the motion capturing process. The method is implemented parallelly with CUDA on GPU and has a fast running speed without dedicated code optimization. The errors of the proposed method on the data from Berkeley Multimodal Human Action Database (MHAD) are within a reasonable range compared with the ground truth results. Our experiment shows that this method succeeds on many challenging motions which are failed to be reported by Microsoft Kinect SDK and not tested by existing works. In the comparison with the state-of-art marker-less depth camera based motion tracking work our method shows advantages in both robustness and input data modality.", "keywords": ["cameras", "collision avoidance", "computer graphics", "data handling", "error analysis", "graphics processing units", "image capture", "image motion analysis", "minimisation", "object tracking", "parallel architectures", "ellipsoidal skeleton", "depth cameras", "motion capturing framework", "fitting error minimization", "input point cloud data capture", "object tracking", "collision avoidance", "CUDA", "GPU", "Berkeley multimodal human action database", "MHAD", "Microsoft Kinect SDK", "Bones", "Tracking", "Ellipsoids", "Harmonic analysis", "Cameras", "Motion capture", "skeleton tracking", "depth sensing", "ellipsoidal skeleton"], "referenced_by": ["10.1109/JBHI.2018.2872834", "10.1109/JBHI.2019.2897245", "10.1109/ICMECT.2019.8932143", "10.1109/ACCESS.2020.3027892", "10.1145/3152123", "10.3390/s17061261", "10.3390/s18113865", "10.1007/978-981-13-9917-6_22", "10.1016/B978-0-12-816713-7.00052-0", "10.1007/978-3-030-44070-1_13", "10.1177/1729881420969072", "10.1186/s13673-020-00256-4"], "referencing": ["10.1109/ROBOT.2006.1641949", "10.1109/CVPR.2011.5995316", "10.1109/ROBOT.2006.1641949", "10.1109/CVPR.2011.5995316", "10.1109/ROBOT.2006.1641949", "10.1109/CVPR.2011.5995316", "10.1145/2508363.2508418", "10.1145/2451236.2451246", "10.1145/2508363.2508418", "10.1145/2451236.2451246", "10.1145/2508363.2508418", "10.1145/2451236.2451246", "10.5244/C.25.101", "10.1016/j.cag.2014.09.018", "10.1007/978-3-540-76386-4_38", "10.1007/978-3-540-73040-8_73", "10.1007/978-3-642-33709-3_59", "10.1111/j.1467-8659.2012.03034.x", "10.1111/j.1467-8659.2012.03058.x", "10.1111/j.1467-8659.2007.01055.x", "10.1111/j.1467-8659.2008.01123.x", "10.1016/j.cagd.2011.01.004", "10.1016/j.cag.2014.01.002", "10.5244/C.25.101", "10.1016/j.cag.2014.09.018", "10.1007/978-3-540-76386-4_38", "10.1007/978-3-540-73040-8_73", "10.1007/978-3-642-33709-3_59", "10.1111/j.1467-8659.2012.03034.x", "10.1111/j.1467-8659.2012.03058.x", "10.1111/j.1467-8659.2007.01055.x", "10.1111/j.1467-8659.2008.01123.x", "10.1016/j.cagd.2011.01.004", "10.1016/j.cag.2014.01.002", "10.5244/C.25.101", "10.1016/j.cag.2014.09.018", "10.1007/978-3-540-76386-4_38", "10.1007/978-3-540-73040-8_73", "10.1007/978-3-642-33709-3_59", "10.1111/j.1467-8659.2012.03034.x", "10.1111/j.1467-8659.2012.03058.x", "10.1111/j.1467-8659.2007.01055.x", "10.1111/j.1467-8659.2008.01123.x", "10.1016/j.cagd.2011.01.004", "10.1016/j.cag.2014.01.002"]}, "10.1109/TVCG.2016.2517641": {"doi": "10.1109/TVCG.2016.2517641", "author": ["Y. Liang", "Y. Liu", "D. Gutierrez"], "title": "Objective Quality Prediction of Image Retargeting Algorithms", "year": "2017", "abstract": "Quality assessment of image retargeting results is useful when comparing different methods. However, performing the necessary user studies is a long, cumbersome process. In this paper, we propose a simple yet efficient objective quality assessment method based on five key factors: i) preservation of salient regions; ii) analysis of the influence of artifacts; iii) preservation of the global structure of the image; iv) compliance with well-established aesthetics rules; and v) preservation of symmetry. Experiments on the RetargetMe benchmark, as well as a comprehensive additional user study, demonstrate that our proposed objective quality assessment method outperforms other existing metrics, while correlating better with human judgements. This makes our metric a good predictor of subjective preference.", "keywords": ["image processing", "objective quality prediction", "image retargeting algorithms", "cumbersome process", "objective quality assessment method", "aesthetics rules", "RetargetMe benchmark", "human judgements", "Quality assessment", "Indexes", "Benchmark testing", "Image quality", "Atmospheric measurements", "Particle measurements", "Image retargeting", "quality assessment", "similarity and aesthetic measure", "symmetry"], "referenced_by": ["10.1109/ACCESS.2018.2808322", "10.1109/CVPR.2017.504", "10.1109/TIP.2016.2585884", "10.1109/TIP.2017.2681840", "10.1109/TIP.2017.2736422", "10.1109/TIP.2017.2746260", "10.1109/TIP.2017.2761556", "10.1109/TMM.2016.2557061", "10.1109/TMM.2016.2614187", "10.1109/ACCESS.2019.2927032", "10.1109/TCYB.2018.2864158", "10.1109/TVCG.2018.2866106", "10.1109/TMM.2019.2932620", "10.1109/TIP.2020.2977171", "10.1109/LSP.2020.2977206", "10.1109/TPAMI.2019.2923998", "10.1109/TMM.2019.2959925", "10.1109/TCSVT.2020.2981652", "10.1145/3231598", "10.1007/s11042-017-5543-7", "10.1016/j.image.2018.06.010", "10.1007/978-981-13-3393-4_34", "10.3390/info10030111", "10.1007/s11042-019-7462-2", "10.1007/978-3-030-16657-1_69", "10.1049/iet-ipr.2018.5283", "10.1007/978-3-030-30645-8_24", "10.1016/j.sigpro.2020.107541", "10.1007/s11432-019-2757-1", "10.1007/s11760-020-01736-x"], "referencing": ["10.1109/TMM.2013.2268051", "10.1109/TIP.2003.819861", "10.1109/76.927424", "10.1109/MCG.2012.123", "10.1109/TMM.2012.2228475", "10.1109/TPAMI.2015.2430342", "10.1109/JETCAS.2014.2298919", "10.1109/JSTSP.2014.2311884", "10.1109/TPAMI.2012.89", "10.1109/CVPR.2011.5995344", "10.1109/CVPR.1992.223129", "10.1109/34.1000236", "10.1109/TPAMI.2009.73", "10.1109/TCSVT.2013.2291282", "10.1109/TMM.2013.2268051", "10.1109/TIP.2003.819861", "10.1109/76.927424", "10.1109/MCG.2012.123", "10.1109/TMM.2012.2228475", "10.1109/TPAMI.2015.2430342", "10.1109/JETCAS.2014.2298919", "10.1109/JSTSP.2014.2311884", "10.1109/TPAMI.2012.89", "10.1109/CVPR.2011.5995344", "10.1109/CVPR.1992.223129", "10.1109/34.1000236", "10.1109/TPAMI.2009.73", "10.1109/TCSVT.2013.2291282", "10.1109/TMM.2013.2268051", "10.1109/TIP.2003.819861", "10.1109/76.927424", "10.1109/MCG.2012.123", "10.1109/TMM.2012.2228475", "10.1109/TPAMI.2015.2430342", "10.1109/JETCAS.2014.2298919", "10.1109/JSTSP.2014.2311884", "10.1109/TPAMI.2012.89", "10.1109/CVPR.2011.5995344", "10.1109/CVPR.1992.223129", "10.1109/34.1000236", "10.1109/TPAMI.2009.73", "10.1109/TCSVT.2013.2291282", "10.1145/2077434.2077447", "10.1145/2077451.2077453", "10.1145/1618452.1618471", "10.1145/1882261.1866185", "10.1145/244130.244151", "10.1145/1618452.1618472", "10.1145/2077434.2077447", "10.1145/2077451.2077453", "10.1145/1618452.1618471", "10.1145/1882261.1866185", "10.1145/244130.244151", "10.1145/1618452.1618472", "10.1145/2077434.2077447", "10.1145/2077451.2077453", "10.1145/1618452.1618471", "10.1145/1882261.1866185", "10.1145/244130.244151", "10.1145/1618452.1618472", "10.1111/j.1467-8659.2011.01881.x", "10.1007/978-3-319-10368-6_9", "10.1111/j.1467-8659.2009.01616.x", "10.1111/j.1467-8659.2012.03008.x", "10.1111/j.1467-8659.2012.03067.x", "10.1016/j.cag.2013.06.004", "10.1016/j.cag.2013.10.003", "10.1080/757582976", "10.5244/C.2.23", "10.1023/B:VISI.0000029664.99615.94", "10.5244/C.16.36", "10.2307/2332226", "10.1111/j.1467-8659.2011.01881.x", "10.1007/978-3-319-10368-6_9", "10.1111/j.1467-8659.2009.01616.x", "10.1111/j.1467-8659.2012.03008.x", "10.1111/j.1467-8659.2012.03067.x", "10.1016/j.cag.2013.06.004", "10.1016/j.cag.2013.10.003", "10.1080/757582976", "10.5244/C.2.23", "10.1023/B:VISI.0000029664.99615.94", "10.5244/C.16.36", "10.2307/2332226", "10.1111/j.1467-8659.2011.01881.x", "10.1007/978-3-319-10368-6_9", "10.1111/j.1467-8659.2009.01616.x", "10.1111/j.1467-8659.2012.03008.x", "10.1111/j.1467-8659.2012.03067.x", "10.1016/j.cag.2013.06.004", "10.1016/j.cag.2013.10.003", "10.1080/757582976", "10.5244/C.2.23", "10.1023/B:VISI.0000029664.99615.94", "10.5244/C.16.36", "10.2307/2332226"]}, "10.1109/TVCG.2016.2532333": {"doi": "10.1109/TVCG.2016.2532333", "author": ["T. Torsney-Weir", "S. Bergner", "D. Bingham", "T. M\u00f6ller"], "title": "Predicting the Interactive Rendering Time Threshold of Gaussian Process Models With HyperSlice", "year": "2017", "abstract": "In this paper we present a method for predicting the rendering time to display multi-dimensional data for the analysis of computer simulations using the HyperSlice [36] method with Gaussian process model reconstruction. Our method relies on a theoretical understanding of how the data points are drawn on slices and then fits the formula to a user's machine using practical experiments. We also describe the typical characteristics of data when analyzing deterministic computer simulations as described by the statistics community. We then show the advantage of carefully considering how many data points can be drawn in real time by proposing two approaches of how this predictive formula can be used in a real-world system.", "keywords": ["data handling", "Gaussian processes", "rendering (computer graphics)", "interactive rendering time threshold prediction", "HyperSlice", "multidimensional data", "computer simulations", "Gaussian process model reconstruction", "user machine", "data characteristics", "deterministic computer simulations", "Rendering (computer graphics)", "Computational modeling", "Data visualization", "Gaussian processes", "Kernel", "Numerical models", "Analytical models", "Multi-dimensional visualization", "surrogate modeling", "Gaussian process regression", "interactive rendering"], "referenced_by": [], "referencing": ["10.1109/VISUAL.2002.1183806", "10.1109/TVCG.2013.179", "10.1109/2945.694987", "10.1109/TVCG.2013.125", "10.1109/TVCG.2014.2346321", "10.1109/TMI.2012.2193896", "10.1109/52.329404", "10.1109/TVCG.2011.248", "10.1109/TVCG.2006.17", "10.1109/VISUAL.2002.1183806", "10.1109/TVCG.2013.179", "10.1109/2945.694987", "10.1109/TVCG.2013.125", "10.1109/TVCG.2014.2346321", "10.1109/TMI.2012.2193896", "10.1109/52.329404", "10.1109/TVCG.2011.248", "10.1109/TVCG.2006.17", "10.1109/VISUAL.2002.1183806", "10.1109/TVCG.2013.179", "10.1109/2945.694987", "10.1109/TVCG.2013.125", "10.1109/TVCG.2014.2346321", "10.1109/TMI.2012.2193896", "10.1109/52.329404", "10.1109/TVCG.2011.248", "10.1109/TVCG.2006.17", "10.1145/97924.97933", "10.1145/166117.166149", "10.1145/355588.365104", "10.1145/97924.97933", "10.1145/166117.166149", "10.1145/355588.365104", "10.1145/97924.97933", "10.1145/166117.166149", "10.1145/355588.365104", "10.1002/0470072768", "10.2307/2289444", "10.1029/93JC02564", "10.1002/mmce.20455", "10.1137/S0036144599352836", "10.1007/978-3-642-13800-3_30", "10.1007/BF01898350", "10.1214/11-AOAS489", "10.1198/004017006000000228", "10.1111/cgf.12129", "10.2307/1268522", "10.1016/0022-1694(70)90255-6", "10.1111/j.1467-8659.2009.01684.x", "10.1080/10867651.2003.10487586", "10.1002/0470072768", "10.2307/2289444", "10.1029/93JC02564", "10.1002/mmce.20455", "10.1137/S0036144599352836", "10.1007/978-3-642-13800-3_30", "10.1007/BF01898350", "10.1214/11-AOAS489", "10.1198/004017006000000228", "10.1111/cgf.12129", "10.2307/1268522", "10.1016/0022-1694(70)90255-6", "10.1111/j.1467-8659.2009.01684.x", "10.1080/10867651.2003.10487586", "10.1002/0470072768", "10.2307/2289444", "10.1029/93JC02564", "10.1002/mmce.20455", "10.1137/S0036144599352836", "10.1007/978-3-642-13800-3_30", "10.1007/BF01898350", "10.1214/11-AOAS489", "10.1198/004017006000000228", "10.1111/cgf.12129", "10.2307/1268522", "10.1016/0022-1694(70)90255-6", "10.1111/j.1467-8659.2009.01684.x", "10.1080/10867651.2003.10487586"]}, "10.1109/TVCG.2016.2532327": {"doi": "10.1109/TVCG.2016.2532327", "author": ["J. Lee", "S. Lee", "Y. Kim", "J. Noh"], "title": "ScreenX: Public Immersive Theatres with Uniform Movie Viewing Experiences", "year": "2017", "abstract": "This paper introduces ScreenX, which is a novel movie viewing platform that enables ordinary movie theatres to become multi-projection movie theatres. This enables the general public to enjoy immersive viewing experiences. The left and right side walls are used to form surrounding screens. This surrounding display environment delivers a strong sense of immersion in general movie viewing. However, na\u00efve display of the content on the side walls results in the appearance of distorted images according to the location of the viewer. In addition, the different dimensions in width, height, and depth among theatres may lead to different viewing experiences. Therefore, for successful deployment of this novel platform, an approach to providing similar movie viewing experiences across target theatres is presented. The proposed image representation model ensures minimum average distortion of the images displayed on the side walls when viewed from different locations. Furthermore, the proposed model assists with determining the appropriate variation of the content according to the diverse viewing environments of different theatres. The theatre suitability estimation method excludes outlier theatres that have extraordinary dimensions. In addition, the content production guidelines indicate appropriate regions to place scene elements for the side wall, depending on their importance. The experiments demonstrate that the proposed method improves the movie viewing experiences in ScreenX theatres. Finally, ScreenX and the proposed techniques are discussed with regard to various aspects and the research issues that are relevant to this movie viewing platform are summarized.", "keywords": ["entertainment", "image representation", "screens (display)", "public immersive theatres", "uniform movie viewing experiences", "movie viewing platform", "multiprojection movie theatres", "immersive viewing experiences", "right side walls", "left side walls", "surrounding display environment", "distorted images", "image representation model", "image distortion", "theatre suitability estimation method", "content production guidelines", "ScreenX theatres", "Motion pictures", "Distortion", "Visualization", "Image representation", "Cameras", "Optimization", "Estimation", "Multi-projection", "immersive display system", "image representation", "immersive theatres"], "referenced_by": ["10.1145/3196492", "10.1145/3183794", "10.15701/kcgs.2017.23.5.9"], "referencing": ["10.1109/VR.2004.1310057", "10.1109/JSTSP.2010.2065213", "10.1109/VISUAL.1999.809883", "10.1109/TVCG.2006.121", "10.1109/TVCG.2011.271", "10.1109/MCG.2004.1297011", "10.1109/34.910880", "10.1109/TMM.2011.2116775", "10.1109/VR.2004.1310057", "10.1109/JSTSP.2010.2065213", "10.1109/VISUAL.1999.809883", "10.1109/TVCG.2006.121", "10.1109/TVCG.2011.271", "10.1109/MCG.2004.1297011", "10.1109/34.910880", "10.1109/TMM.2011.2116775", "10.1109/VR.2004.1310057", "10.1109/JSTSP.2010.2065213", "10.1109/VISUAL.1999.809883", "10.1109/TVCG.2006.121", "10.1109/TVCG.2011.271", "10.1109/MCG.2004.1297011", "10.1109/34.910880", "10.1109/TMM.2011.2116775", "10.1145/166117.166134", "10.1145/1015706.1015766", "10.1145/1778765.1778812", "10.1145/2508363.2508376", "10.1145/502348.502354", "10.1145/1187112.1187161", "10.1145/1394622.1394626", "10.1145/2642918.2647383", "10.1145/500141.500192", "10.1145/2461912.2461971", "10.1145/1180639.1180702", "10.1145/1360612.1360615", "10.1145/1661412.1618472", "10.1145/1661412.1618473", "10.1145/2010324.1964983", "10.1145/1057432.1057456", "10.1145/166117.166134", "10.1145/1015706.1015766", "10.1145/1778765.1778812", "10.1145/2508363.2508376", "10.1145/502348.502354", "10.1145/1187112.1187161", "10.1145/1394622.1394626", "10.1145/2642918.2647383", "10.1145/500141.500192", "10.1145/2461912.2461971", "10.1145/1180639.1180702", "10.1145/1360612.1360615", "10.1145/1661412.1618472", "10.1145/1661412.1618473", "10.1145/2010324.1964983", "10.1145/1057432.1057456", "10.1145/166117.166134", "10.1145/1015706.1015766", "10.1145/1778765.1778812", "10.1145/2508363.2508376", "10.1145/502348.502354", "10.1145/1187112.1187161", "10.1145/1394622.1394626", "10.1145/2642918.2647383", "10.1145/500141.500192", "10.1145/2461912.2461971", "10.1145/1180639.1180702", "10.1145/1360612.1360615", "10.1145/1661412.1618472", "10.1145/1661412.1618473", "10.1145/2010324.1964983", "10.1145/1057432.1057456", "10.1006/gmip.1998.0467", "10.1007/s00530-003-0105-4", "10.1007/s11263-012-0593-9", "10.1111/cgf.12541", "10.1006/gmip.1998.0467", "10.1007/s00530-003-0105-4", "10.1007/s11263-012-0593-9", "10.1111/cgf.12541", "10.1006/gmip.1998.0467", "10.1007/s00530-003-0105-4", "10.1007/s11263-012-0593-9", "10.1111/cgf.12541"]}, "10.1109/TVCG.2016.2520920": {"doi": "10.1109/TVCG.2016.2520920", "author": ["T. Ortner", "J. Sorger", "H. Steinlechner", "G. Hesina", "H. Piringer", "E. Gr\u00f6ller"], "title": "Vis-A-Ware: Integrating Spatial and Non-Spatial Visualization for Visibility-Aware Urban Planning", "year": "2017", "abstract": "3D visibility analysis plays a key role in urban planning for assessing the visual impact of proposed buildings on the cityscape. A call for proposals typically yields around 30 candidate buildings that need to be evaluated with respect to selected viewpoints. Current visibility analysis methods are very time-consuming and limited to a small number of viewpoints. Further, analysts neither have measures to evaluate candidates quantitatively, nor to compare them efficiently. The primary contribution of this work is the design study of Vis-A-Ware, a visualization system to qualitatively and quantitatively evaluate, rank, and compare visibility data of candidate buildings with respect to a large number of viewpoints. Vis-A-Ware features a 3D spatial view of an urban scene and non-spatial views of data derived from visibility evaluations, which are tightly integrated by linked interaction. To enable a quantitative evaluation we developed four metrics in accordance with experts from urban planning. We illustrate the applicability of Vis-A-Ware on the basis of a use case scenario and present results from informal feedback sessions with domain experts from urban planning and development. This feedback suggests that Vis-A-Ware is a valuable tool for visibility analysis allowing analysts to answer complex questions more efficiently and objectively.", "keywords": ["data visualisation", "town and country planning", "nonspatial visualization", "visibility-aware urban planning", "3D visibility analysis", "visibility analysis methods", "visibility data", "Vis-A-Ware features", "urban scene", "visibility evaluations", "informal feedback sessions", "Visualization", "Urban planning", "Data visualization", "Buildings", "Three-dimensional displays", "Measurement", "Context", "Geographic/geospatial visualization", "visual analytics", "integrating spatial and non-spatial data visualization", "focus + context techniques", "coordinated and multiple views"], "referenced_by": ["10.1109/TVCG.2017.2744159", "10.1109/VSMM.2017.8346302", "10.1109/MCG.2018.053491730", "10.1109/TVCG.2018.2802945", "10.1016/j.jvlc.2017.08.008", "10.1016/j.visinf.2017.11.002", "10.1007/s00371-016-1257-5", "10.14778/3157794.3157803"], "referencing": ["10.1109/TVCG.2012.175", "10.1109/TVCG.2010.223", "10.1109/TVCG.2014.2346321", "10.1109/TVCG.2010.190", "10.1109/TVCG.2015.2468011", "10.1109/TVCG.2013.173", "10.1109/TVCG.2014.2346898", "10.1109/TVCG.2014.2346893", "10.1109/TVCG.2009.141", "10.1109/TVCG.2007.70574", "10.1109/TVCG.2008.149", "10.1109/TVCG.2011.185", "10.1109/TVCG.2012.175", "10.1109/TVCG.2010.223", "10.1109/TVCG.2014.2346321", "10.1109/TVCG.2010.190", "10.1109/TVCG.2015.2468011", "10.1109/TVCG.2013.173", "10.1109/TVCG.2014.2346898", "10.1109/TVCG.2014.2346893", "10.1109/TVCG.2009.141", "10.1109/TVCG.2007.70574", "10.1109/TVCG.2008.149", "10.1109/TVCG.2011.185", "10.1109/TVCG.2012.175", "10.1109/TVCG.2010.223", "10.1109/TVCG.2014.2346321", "10.1109/TVCG.2010.190", "10.1109/TVCG.2015.2468011", "10.1109/TVCG.2013.173", "10.1109/TVCG.2014.2346898", "10.1109/TVCG.2014.2346893", "10.1109/TVCG.2009.141", "10.1109/TVCG.2007.70574", "10.1109/TVCG.2008.149", "10.1109/TVCG.2011.185", "10.1145/2407336.2407339", "10.1145/2816795.2818134", "10.1145/2407336.2407339", "10.1145/2816795.2818134", "10.1145/2407336.2407339", "10.1145/2816795.2818134", "10.1068/b32142", "10.1111/j.1467-8659.2004.00793.x", "10.7763/IJESD.2010.V1.85", "10.1057/palgrave.ivs.9500058", "10.1111/j.1467-8659.2009.01664.x", "10.1111/j.1467-8659.2009.01429.x", "10.1068/b32142", "10.1111/j.1467-8659.2004.00793.x", "10.7763/IJESD.2010.V1.85", "10.1057/palgrave.ivs.9500058", "10.1111/j.1467-8659.2009.01664.x", "10.1111/j.1467-8659.2009.01429.x", "10.1068/b32142", "10.1111/j.1467-8659.2004.00793.x", "10.7763/IJESD.2010.V1.85", "10.1057/palgrave.ivs.9500058", "10.1111/j.1467-8659.2009.01664.x", "10.1111/j.1467-8659.2009.01429.x"]}, "10.1109/TVCG.2016.2592908": {"doi": "10.1109/TVCG.2016.2592908", "author": ["G. Cirio", "J. Lopez-Moreno", "M. A. Otaduy"], "title": "Yarn-Level Cloth Simulation with Sliding Persistent Contacts", "year": "2017", "abstract": "Cloth is made of yarns that are stitched together forming semi-regular patterns. Due to the complexity of stitches and patterns, the macroscopic behavior of cloth is dictated by the contact interactions between yarns, not by the mechanical properties of yarns alone. The computation of cloth mechanics at the yarn level appears as a computationally complex and costly process at first sight, due to the need to resolve many fine-scale contact interactions. We propose instead an efficient representation of cloth at the yarn level that treats yarn-yarn contacts as persistent, but with the possibility to slide, thereby avoiding expensive contact handling altogether. We introduce a compact representation of yarn geometry and kinematics, capturing the essential deformation modes of yarn crossings, loops, stitches, and stacks, with a minimum cost. Based on this representation, we design force models that reproduce the characteristic macroscopic behavior of yarn-based fabrics. Our approach is suited for both woven and knitted fabrics. We demonstrate the efficiency of our method on simulations with millions of degrees of freedom (hundreds of thousands of yarn loops), almost one order of magnitude faster than previous techniques. We also compare the different macroscopic behavior under woven and knitted patterns with the same yarn density.", "keywords": ["clothing", "computational complexity", "computer graphics", "fabrics", "yarn", "yarn-level cloth simulation", "sliding persistent contacts", "semi-regular patterns", "stitch complexity", "contact interactions", "yarn mechanical properties", "cloth mechanic computation", "fine-scale contact interactions", "yarn-yarn contacts", "yarn geometry compact representation", "yarn kinematic compact representation", "characteristic macroscopic behavior", "yarn-based fabrics", "knitted fabrics", "macroscopic behavior", "Yarn", "Fabrics", "Weaving", "Computational modeling", "Clothing", "Friction", "Deformable models", "Yarns", "knitted cloth", "woven cloth", "physically based simulation"], "referenced_by": ["10.1109/TVCG.2017.2731949", "10.1145/3197517.3201360", "10.1145/3272127.3275105", "10.1145/3306346.3322995", "10.1145/3386569.3392412", "10.1080/00405000.2018.1453635", "10.1007/s11042-018-6063-9", "10.1111/cgf.13521", "10.1115/1.4039046", "10.1016/j.future.2019.02.054", "10.1111/cgf.13885", "10.1177/0040517519894749", "10.1177/1558925020913871", "10.1016/j.cagd.2020.101883", "10.1111/cgf.13915", "10.1111/cgf.14133", "10.1111/cgf.14112"], "referencing": ["10.1145/192161.192259", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2661229.2661279", "10.1145/2786784.2786801", "10.1145/2185520.2335388", "10.1145/2010324.1964934", "10.1145/192161.192259", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2661229.2661279", "10.1145/2786784.2786801", "10.1145/2185520.2335388", "10.1145/2010324.1964934", "10.1145/192161.192259", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2661229.2661279", "10.1145/2786784.2786801", "10.1145/2185520.2335388", "10.1145/2010324.1964934", "10.1080/19447023708658809", "10.1080/00405007308630416", "10.1016/S1359-8368(98)00025-0", "10.1016/S0266-3538(99)00198-0", "10.1016/j.ijmecsci.2005.09.007", "10.1016/j.ijsolstr.2005.05.020", "10.1007/978-94-017-0297-3_18", "10.1016/S0022-5096(03)00038-3", "10.1177/0040517510385171", "10.1533/joti.2005.0005", "10.1016/j.ijsolstr.2004.10.030", "10.1016/j.ijengsci.2010.12.011", "10.1016/j.jmps.2012.05.005", "10.1016/j.compositesa.2005.12.029", "10.1002/app.2261", "10.1080/19447023708658809", "10.1080/00405007308630416", "10.1016/S1359-8368(98)00025-0", "10.1016/S0266-3538(99)00198-0", "10.1016/j.ijmecsci.2005.09.007", "10.1016/j.ijsolstr.2005.05.020", "10.1007/978-94-017-0297-3_18", "10.1016/S0022-5096(03)00038-3", "10.1177/0040517510385171", "10.1533/joti.2005.0005", "10.1016/j.ijsolstr.2004.10.030", "10.1016/j.ijengsci.2010.12.011", "10.1016/j.jmps.2012.05.005", "10.1016/j.compositesa.2005.12.029", "10.1002/app.2261", "10.1080/19447023708658809", "10.1080/00405007308630416", "10.1016/S1359-8368(98)00025-0", "10.1016/S0266-3538(99)00198-0", "10.1016/j.ijmecsci.2005.09.007", "10.1016/j.ijsolstr.2005.05.020", "10.1007/978-94-017-0297-3_18", "10.1016/S0022-5096(03)00038-3", "10.1177/0040517510385171", "10.1533/joti.2005.0005", "10.1016/j.ijsolstr.2004.10.030", "10.1016/j.ijengsci.2010.12.011", "10.1016/j.jmps.2012.05.005", "10.1016/j.compositesa.2005.12.029", "10.1002/app.2261"]}, "10.1109/TVCG.2016.2624266": {"doi": "10.1109/TVCG.2016.2624266", "author": [""], "title": "2016 Index IEEE Transactions on Visualization and Computer Graphics Vol. 22", "year": "2017", "abstract": "Presents the 2016 author/subject index for this publication.", "keywords": [""], "referenced_by": [], "referencing": []}}