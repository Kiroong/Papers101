{"10.1109/TVCG.2015.2481385": {"doi": "10.1109/TVCG.2015.2481385", "author": ["E. Foxlin", "T. Calloway", "H. Zhang"], "title": "Design and Error Analysis of a Vehicular AR System with Auto-Harmonization", "year": "2015", "abstract": "This paper describes the design, development and testing of an AR system that was developed for aerospace and ground vehicles to meet stringent accuracy and robustness requirements. The system uses an optical see-through HMD, and thus requires extremely low latency, high tracking accuracy and precision alignment and calibration of all subsystems in order to avoid mis-registration and \u201cswim\u201d. The paper focuses on the optical/inertial hybrid tracking system and describes novel solutions to the challenges with the optics, algorithms, synchronization, and alignment with the vehicle and HMD systems. Tracker accuracy is presented with simulation results to predict the registration accuracy. A car test is used to create a through-the-eyepiece video demonstrating well-registered augmentations of the road and nearby structures while driving. Finally, a detailed covariance analysis of AR registration error is derived.", "keywords": ["aircraft", "augmented reality", "automobiles", "covariance analysis", "error analysis", "helmet mounted displays", "image registration", "optical tracking", "traffic engineering computing", "error analysis", "vehicular AR system", "auto-harmonization", "aerospace vehicles", "ground vehicles", "optical see-through HMD", "optical-inertial hybrid tracking system", "through-the-eyepiece video", "covariance analysis", "AR registration error", "augmented reality", "car", "aircraft", "Aerospace electronics", "Optical sensors", "Augmented reality", "Virtual reality", "Calibration", "Adaptive optics", "Cameras", "Augmented reality", "registration", "calibration", "hybrid tracking", "inertial", "see-through HMD", "Inertial, augmented reality, calibration, registration, hybrid tracking, see through HMD, image processing, sensor fusion"], "referenced_by": ["IKEY:7524314", "10.1016/j.ijhcs.2016.07.008", "10.4018/978-1-5225-5763-0.ch004", "10.1515/phys-2018-0095"], "referencing": ["IKEY:1191139", "IKEY:6948415", "IKEY:1288525", "IKEY:1544664", "IKEY:4637338", "IKEY:1191139", "IKEY:6948415", "IKEY:1288525", "IKEY:1544664", "IKEY:4637338", "IKEY:1191139", "IKEY:6948415", "IKEY:1288525", "IKEY:1544664", "IKEY:4637338", "10.1145/192161.192199", "10.1145/192161.192199", "10.1145/192161.192199", "10.1016/S0097-8493(99)00105-3", "10.1117/12.45422", "10.1117/12.389141", "10.1177/154193128603000112", "10.1016/j.trc.2011.08.005", "10.1117/12.2015621", "10.1049/PBRA017E", "10.1162/105474601750182289", "10.1016/S0097-8493(99)00105-3", "10.1117/12.45422", "10.1117/12.389141", "10.1177/154193128603000112", "10.1016/j.trc.2011.08.005", "10.1117/12.2015621", "10.1049/PBRA017E", "10.1162/105474601750182289", "10.1016/S0097-8493(99)00105-3", "10.1117/12.45422", "10.1117/12.389141", "10.1177/154193128603000112", "10.1016/j.trc.2011.08.005", "10.1117/12.2015621", "10.1049/PBRA017E", "10.1162/105474601750182289"]}, "10.1109/TVCG.2015.2450745": {"doi": "10.1109/TVCG.2015.2450745", "author": ["J. David Hincapi\u00e9-Ramos", "L. Ivanchuk", "S. K. Sridharan", "P. P. Irani"], "title": "SmartColor: Real-Time Color and Contrast Correction for Optical See-Through Head-Mounted Displays", "year": "2015", "abstract": "Users of optical see-through head-mounted displays (OHMD) perceive color as a blend of the display color and the background. Color-blending is a major usability challenge as it leads to loss of color encodings and poor text legibility. Color correction aims at mitigating color blending by producing an alternative color which, when blended with the background, more closely approximates the color originally intended. In this paper we present an end-to-end approach to the color blending problem addressing the distortions introduced by the transparent material of the display efficiently and in realtime. We also present a user evaluation of correction efficiency. Finally, we present a graphics library called SmartColor showcasing the use of color correction for different types of display content. SmartColor uses color correction to provide three management strategies: correction, contrast, and show-up-on-contrast. Correction determines the alternate color which best preserves the original color. Contrast determines the color which best supports text legibility while preserving as much of the original hue. Show-up-on-contrast makes a component visible when a related component does not have enough contrast to be legible. We describe SmartColor's architecture and illustrate the color strategies for various types of display content.", "keywords": ["colour", "computer graphics", "helmet mounted displays", "optical distortion", "software libraries", "optical see-through head-mounted displays", "real-time color correction", "real-time contrast correction", "color blending problem", "distortions", "transparent display material", "graphics library", "SmartColor", "show-up-on-contrast", "OHMD", "Color", "Image color analysis", "Multimedia communication", "Real-time systems", "Adaptive optics", "Optical distortion", "Virtual reality", "User interfaces", "Information Interfaces and Presentation", "Multimedia Information Systems", "User Interfaces", "Information interfaces and presentation", "multimedia information systems-artificial", "augmented", "and virtual realities", "user interfaces-ergonomics", "evaluation/methodology", "screen design", "style guides", "Algorithms", "Color", "Computer Graphics", "Human Engineering", "Humans", "Image Processing, Computer-Assisted"], "referenced_by": ["IKEY:7820764", "IKEY:8296325", "IKEY:8088457", "IKEY:8998139", "10.1016/j.image.2017.01.004", "10.1117/1.OE.57.12.123104", "10.1364/OE.378489"], "referencing": ["IKEY:6671761", "IKEY:6162883", "IKEY:4362786", "IKEY:6671761", "IKEY:6162883", "IKEY:4362786", "IKEY:6671761", "IKEY:6162883", "IKEY:4362786", "10.1145/2207676.2208547", "10.1145/1242073.1242109", "10.1145/2087756.2087786", "10.1145/2503713.2503716", "10.1145/2207676.2208547", "10.1145/1242073.1242109", "10.1145/2087756.2087786", "10.1145/2503713.2503716", "10.1145/2207676.2208547", "10.1145/1242073.1242109", "10.1145/2087756.2087786", "10.1145/2503713.2503716", "10.1201/b10624", "10.1117/12.2015937", "10.1016/S0097-8493(01)00119-4", "10.1037/0096-3445.121.1.95", "10.1002/col.20070", "10.1007/978-3-642-02710-9_67", "10.1201/b10624", "10.1117/12.2015937", "10.1016/S0097-8493(01)00119-4", "10.1037/0096-3445.121.1.95", "10.1002/col.20070", "10.1007/978-3-642-02710-9_67", "10.1201/b10624", "10.1117/12.2015937", "10.1016/S0097-8493(01)00119-4", "10.1037/0096-3445.121.1.95", "10.1002/col.20070", "10.1007/978-3-642-02710-9_67"]}, "10.1109/TVCG.2015.2450717": {"doi": "10.1109/TVCG.2015.2450717", "author": ["K. Rohmer", "W. B\u00fcschel", "R. Dachselt", "T. Grosch"], "title": "Interactive Near-Field Illumination for Photorealistic Augmented Reality with Varying Materials on Mobile Devices", "year": "2015", "abstract": "At present, photorealistic augmentation is not yet possible since the computational power of mobile devices is insufficient. Even streaming solutions from stationary PCs cause a latency that affects user interactions considerably. Therefore, we introduce a differential rendering method that allows for a consistent illumination of the inserted virtual objects on mobile devices, avoiding delays. The computation effort is shared between a stationary PC and the mobile devices to make use of the capacities available on both sides. The method is designed such that only a minimum amount of data has to be transferred asynchronously between the participants. This allows for an interactive illumination of virtual objects with a consistent appearance under both temporally and spatially varying real illumination conditions. To describe the complex near-field illumination in an indoor scenario, HDR video cameras are used to capture the illumination from multiple directions. In this way, sources of illumination can be considered that are not directly visible to the mobile device because of occlusions and the limited field of view. While our method focuses on Lambertian materials, we also provide some initial approaches to approximate non-diffuse virtual objects and thereby allow for a wider field of application at nearly the same cost.", "keywords": ["augmented reality", "lighting", "mobile computing", "rendering (computer graphics)", "interactive near-field illumination", "photorealistic augmented reality", "differential rendering method", "virtual objects", "mobile devices", "stationary PC", "spatially varying real illumination conditions", "temporally varying real illumination conditions", "HDR video cameras", "Lambertian materials", "Lighting", "Cameras", "Mobile handsets", "Rendering (computer graphics)", "Image reconstruction", "Light sources", "Photography", "Computer Graphics", "Three-Dimensional Graphics and Realism", "Augmented and Virtual Realities", "Computer graphics", "three-dimensional graphics and realism", "augmented and virtual realities"], "referenced_by": ["IKEY:8115406", "IKEY:8007317", "IKEY:8710982", "IKEY:8943754", "IKEY:9020588", "IKEY:9184389", "10.1145/3386496", "10.1007/978-3-030-25999-0_16"], "referencing": ["IKEY:764865", "IKEY:5643556", "IKEY:6671772", "IKEY:6162886", "IKEY:6802044", "IKEY:656788", "IKEY:764865", "IKEY:5643556", "IKEY:6671772", "IKEY:6162886", "IKEY:6802044", "IKEY:656788", "IKEY:764865", "IKEY:5643556", "IKEY:6671772", "IKEY:6162886", "IKEY:6802044", "IKEY:656788", "10.1145/159161.159160", "10.1145/2070781.2024191", "10.1145/1008653.1008665", "10.1145/1187112.1187243", "10.1145/1315184.1315207", "10.1145/2542355.2542380", "10.1145/2503385.2503503", "10.1145/258734.258769", "10.1145/566570.566612", "10.1145/15886.15902", "10.1145/383259.383317", "10.1145/159161.159160", "10.1145/2070781.2024191", "10.1145/1008653.1008665", "10.1145/1187112.1187243", "10.1145/1315184.1315207", "10.1145/2542355.2542380", "10.1145/2503385.2503503", "10.1145/258734.258769", "10.1145/566570.566612", "10.1145/15886.15902", "10.1145/383259.383317", "10.1145/159161.159160", "10.1145/2070781.2024191", "10.1145/1008653.1008665", "10.1145/1187112.1187243", "10.1145/1315184.1315207", "10.1145/2542355.2542380", "10.1145/2503385.2503503", "10.1145/258734.258769", "10.1145/566570.566612", "10.1145/15886.15902", "10.1145/383259.383317", "10.1007/978-3-7091-6858-5_5", "10.1007/978-3-7091-6809-7_29", "10.1111/j.1467-8659.2008.01126.x", "10.1111/j.1467-8659.2008.01268.x", "10.1111/j.1467-8659.2006.00816.x", "10.1007/978-3-7091-6303-0_33", "10.1016/j.cag.2012.04.013", "10.1007/s00371-010-0501-7", "10.1111/j.1467-8659.2012.02093.x", "10.1007/978-3-7091-6858-5_5", "10.1007/978-3-7091-6809-7_29", "10.1111/j.1467-8659.2008.01126.x", "10.1111/j.1467-8659.2008.01268.x", "10.1111/j.1467-8659.2006.00816.x", "10.1007/978-3-7091-6303-0_33", "10.1016/j.cag.2012.04.013", "10.1007/s00371-010-0501-7", "10.1111/j.1467-8659.2012.02093.x", "10.1007/978-3-7091-6858-5_5", "10.1007/978-3-7091-6809-7_29", "10.1111/j.1467-8659.2008.01126.x", "10.1111/j.1467-8659.2008.01268.x", "10.1111/j.1467-8659.2006.00816.x", "10.1007/978-3-7091-6303-0_33", "10.1016/j.cag.2012.04.013", "10.1007/s00371-010-0501-7", "10.1111/j.1467-8659.2012.02093.x"]}, "10.1109/TVCG.2015.2452905": {"doi": "10.1109/TVCG.2015.2452905", "author": ["N. Haouchine", "J. Dequidt", "M. Berger", "S. Cotin"], "title": "Monocular 3D Reconstruction and Augmentation of Elastic Surfaces with Self-Occlusion Handling", "year": "2015", "abstract": "This paper focuses on the 3D shape recovery and augmented reality on elastic objects with self-occlusions handling, using only single view images. Shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or resort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method that uses a mechanical model and able to handle highly elastic objects. The problem is formulated as an energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. In addition, we propose to handle self-occluded regions thanks to the ability of mechanical models to provide appropriate predictions of the shape. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of recovering and augmenting 3D elastic objects. Additionally, experiments in the context of minimally invasive liver surgery are also provided and results on deformations with the presence of self-occlusions are exposed.", "keywords": ["augmented reality", "image reconstruction", "image sequences", "monocular 3D reconstruction", "elastic surfaces augmentation", "self-occlusion handling", "3D shape recovery", "augmented reality", "elastic objects", "single view images", "monocular video sequence", "mechanical model", "energy minimization problem", "nonlinear elastic model", "external image points", "Computational modeling", "Deformable models", "Three-dimensional displays", "Shape analysis", "Context modeling", "Mathematical model", "Virtual reality", "Augmented reality", "Image-guided Simulation", "Physics-based Modeling", "Non-rigid Registration", "Computer Assisted Surgery", "Elastic Augmented Reality", "Image-guided simulation", "physics-based modeling", "non-rigid registration", "computer assisted surgery", "elastic augmented reality", "Algorithms", "Animals", "Computer Graphics", "Humans", "Imaging, Three-Dimensional", "Liver", "Models, Theoretical", "Silicones", "Surface Properties", "Swine"], "referenced_by": ["IKEY:8099864", "IKEY:8000678", "IKEY:8411484", "IKEY:9238703", "IKEY:9207920", "10.1007/978-3-319-46720-7_47", "10.1007/s00464-016-5297-8", "10.1016/j.robot.2016.08.023", "10.1097/SLA.0000000000002448", "10.1016/j.cma.2018.06.011", "10.1016/j.displa.2019.03.001", "10.1007/978-3-319-54057-3_11", "10.1016/j.compmedimag.2020.101702", "10.1002/rcs.2120"], "referencing": ["IKEY:6987340", "IKEY:5648357", "IKEY:4250473", "IKEY:24792", "IKEY:796284", "IKEY:5350717", "IKEY:5557881", "IKEY:4409031", "IKEY:6948432", "IKEY:6264103", "IKEY:764872", "IKEY:6987340", "IKEY:5648357", "IKEY:4250473", "IKEY:24792", "IKEY:796284", "IKEY:5350717", "IKEY:5557881", "IKEY:4409031", "IKEY:6948432", "IKEY:6264103", "IKEY:764872", "IKEY:6987340", "IKEY:5648357", "IKEY:4250473", "IKEY:24792", "IKEY:796284", "IKEY:5350717", "IKEY:5557881", "IKEY:4409031", "IKEY:6948432", "IKEY:6264103", "IKEY:764872", "10.1145/54852.378508", "10.1145/15886.15903", "10.1145/2343483.2343501", "10.1145/1073204.1073300", "10.1145/1477926.1477934", "10.1145/1179352.1141964", "10.1145/54852.378508", "10.1145/15886.15903", "10.1145/2343483.2343501", "10.1145/1073204.1073300", "10.1145/1477926.1477934", "10.1145/1179352.1141964", "10.1145/54852.378508", "10.1145/15886.15903", "10.1145/2343483.2343501", "10.1145/1073204.1073300", "10.1145/1477926.1477934", "10.1145/1179352.1141964", "10.1007/s00371-010-0502-6", "10.1016/j.suronc.2011.07.002", "10.1007/s00464-013-3249-0", "10.1007/978-3-642-15705-9_10", "10.1118/1.4896021", "10.1016/S1361-8415(03)00068-9", "10.1007/978-3-642-04268-3_9", "10.1016/j.media.2013.11.001", "10.1007/BF00133570", "10.1016/j.media.2010.05.009", "10.1006/cviu.1995.1004", "10.1023/B:VISI.0000029666.37597.d3", "10.1007/s10851-007-0062-1", "10.1007/978-3-642-15705-9_33", "10.5244/C.18.92", "10.1007/s11263-011-0452-0", "10.1007/s11263-006-0017-9", "10.1016/j.cag.2010.05.015", "10.1007/s11263-010-0352-8", "10.1111/j.1467-8659.2006.01000.x", "10.1080/10255840500295852", "10.1016/j.cviu.2007.09.014", "10.1007/8415_2012_125", "10.1007/978-3-319-12057-7_26", "10.1016/j.jmbbm.2013.01.013", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-33415-3_7", "10.1016/j.media.2013.04.003", "10.1007/978-3-319-10470-6_5", "10.1007/s00371-015-1123-x", "10.1007/s00371-010-0502-6", "10.1016/j.suronc.2011.07.002", "10.1007/s00464-013-3249-0", "10.1007/978-3-642-15705-9_10", "10.1118/1.4896021", "10.1016/S1361-8415(03)00068-9", "10.1007/978-3-642-04268-3_9", "10.1016/j.media.2013.11.001", "10.1007/BF00133570", "10.1016/j.media.2010.05.009", "10.1006/cviu.1995.1004", "10.1023/B:VISI.0000029666.37597.d3", "10.1007/s10851-007-0062-1", "10.1007/978-3-642-15705-9_33", "10.5244/C.18.92", "10.1007/s11263-011-0452-0", "10.1007/s11263-006-0017-9", "10.1016/j.cag.2010.05.015", "10.1007/s11263-010-0352-8", "10.1111/j.1467-8659.2006.01000.x", "10.1080/10255840500295852", "10.1016/j.cviu.2007.09.014", "10.1007/8415_2012_125", "10.1007/978-3-319-12057-7_26", "10.1016/j.jmbbm.2013.01.013", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-33415-3_7", "10.1016/j.media.2013.04.003", "10.1007/978-3-319-10470-6_5", "10.1007/s00371-015-1123-x", "10.1007/s00371-010-0502-6", "10.1016/j.suronc.2011.07.002", "10.1007/s00464-013-3249-0", "10.1007/978-3-642-15705-9_10", "10.1118/1.4896021", "10.1016/S1361-8415(03)00068-9", "10.1007/978-3-642-04268-3_9", "10.1016/j.media.2013.11.001", "10.1007/BF00133570", "10.1016/j.media.2010.05.009", "10.1006/cviu.1995.1004", "10.1023/B:VISI.0000029666.37597.d3", "10.1007/s10851-007-0062-1", "10.1007/978-3-642-15705-9_33", "10.5244/C.18.92", "10.1007/s11263-011-0452-0", "10.1007/s11263-006-0017-9", "10.1016/j.cag.2010.05.015", "10.1007/s11263-010-0352-8", "10.1111/j.1467-8659.2006.01000.x", "10.1080/10255840500295852", "10.1016/j.cviu.2007.09.014", "10.1007/8415_2012_125", "10.1007/978-3-319-12057-7_26", "10.1016/j.jmbbm.2013.01.013", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-33415-3_7", "10.1016/j.media.2013.04.003", "10.1007/978-3-319-10470-6_5", "10.1007/s00371-015-1123-x"]}, "10.1109/TVCG.2015.2480060": {"doi": "10.1109/TVCG.2015.2480060", "author": ["G. Hough", "I. Williams", "C. Athwal"], "title": "Fidelity and plausibility of bimanual interaction in mixed reality", "year": "2015", "abstract": "When human actors interact with virtual objects the result is often not convincing to a third party viewer, due to incongruities between the actor and object positions. In this study we aim to quantify the magnitude and impact of the errors that occur in a bimanual interaction, that is when an actor attempts to move a virtual object by holding it between both hands. A three stage framework is presented which firstly captures the magnitude of these interaction errors, then quantifies their effect on the relevant third party audience, and thirdly assesses methods to mitigate the impact of the errors. Findings from this work show that the degree of error was dependent on the size of the virtual object and also on the axis of the hand placement with respect to the axis of the interactive motion. In addition, actor hand placement outside and away from the object surface was found to affect the visual plausibility considerably more than when the actor's hands were within the object boundaries. Finally, a method for automatic adaptation of the object size to match the distance between the actor's hands gave a significant improvement in the viewers' assessment of the scene plausibility.", "keywords": ["human computer interaction", "virtual reality", "fidelity", "bimanual interaction", "mixed reality", "human actors", "virtual objects", "third party viewer", "actor positions", "object positions", "interaction errors", "third party audience", "error degree", "interactive motion", "actor hand placement", "object surface", "visual plausibility", "object boundaries", "object size", "viewers assessment", "scene plausibility", "Virtual reality", "Videos", "Human factors", "Interactive systems", "Virtual environments", "Object recognition", "Peformance evaluation", "Interactive Virtual Studios", "Human Performance Measurement", "Interaction Framework", "Mixed Reality", "Interactive Virtual Environment", "Interaction Error", "User Study", "Interactive virtual studios", "human performance measurement", "interaction framework", "mixed reality", "interactive virtual environment", "interaction error", "user study", "Computer Graphics", "Humans", "Image Processing, Computer-Assisted", "Models, Statistical", "Movement", "Task Performance and Analysis", "User-Computer Interface"], "referenced_by": ["IKEY:7899008"], "referencing": ["IKEY:6948414", "IKEY:6948480", "IKEY:1643824", "IKEY:641398", "IKEY:5246946", "IKEY:6948414", "IKEY:6948480", "IKEY:1643824", "IKEY:641398", "IKEY:5246946", "IKEY:6948414", "IKEY:6948480", "IKEY:1643824", "IKEY:641398", "IKEY:5246946", "10.1145/323663.323667", "10.1145/1328202.1328214", "10.1145/280814.280951", "10.1145/169059.169431", "10.1145/501786.501788", "10.1145/882262.882304", "10.1145/323663.323667", "10.1145/1328202.1328214", "10.1145/280814.280951", "10.1145/169059.169431", "10.1145/501786.501788", "10.1145/882262.882304", "10.1145/323663.323667", "10.1145/1328202.1328214", "10.1145/280814.280951", "10.1145/169059.169431", "10.1145/501786.501788", "10.1145/882262.882304", "10.1007/978-3-642-32645-5_37", "10.1037/h0055392", "10.1016/j.intcom.2004.05.001", "10.1080/00222895.1987.10735426", "10.1016/S0141-9382(98)00022-5", "10.1167/9.4.13", "10.1007/978-3-642-32645-5_37", "10.1037/h0055392", "10.1016/j.intcom.2004.05.001", "10.1080/00222895.1987.10735426", "10.1016/S0141-9382(98)00022-5", "10.1167/9.4.13", "10.1007/978-3-642-32645-5_37", "10.1037/h0055392", "10.1016/j.intcom.2004.05.001", "10.1080/00222895.1987.10735426", "10.1016/S0141-9382(98)00022-5", "10.1167/9.4.13"]}, "10.1109/TVCG.2014.2369039": {"doi": "10.1109/TVCG.2014.2369039", "author": ["L. Gao", "Y. Cao", "Y. Lai", "H. Huang", "L. Kobbelt", "S. Hu"], "title": "Active Exploration of Large 3D Model Repositories", "year": "2015", "abstract": "With broader availability of large-scale 3D model repositories, the need for efficient and effective exploration becomes more and more urgent. Existing model retrieval techniques do not scale well with the size of the database since often a large number of very similar objects are returned for a query, and the possibilities to refine the search are quite limited. We propose an interactive approach where the user feeds an active learning procedure by labeling either entire models or parts of them as \u201clike\u201d or \u201cdislike\u201d such that the system can automatically update an active set of recommended models. To provide an intuitive user interface, candidate models are presented based on their estimated relevance for the current query. From the methodological point of view, our main contribution is to exploit not only the similarity between a query and the database models but also the similarities among the database models themselves. We achieve this by an offline pre-processing stage, where global and local shape descriptors are computed for each model and a sparse distance metric is derived that can be evaluated efficiently even for very large databases. We demonstrate the effectiveness of our method by interactively exploring a repository containing over 100 K models.", "keywords": ["computer graphics", "query processing", "user interfaces", "very large databases", "sparse distance metric", "global shape descriptors", "local shape descriptors", "database models", "query", "intuitive user interface", "active learning procedure", "large-scale 3D model repositories", "Computational modeling", "Shape analysis", "Solid modeling", "Three-dimensional displays", "Analytical models", "Semisupervised learning", "semi-supervised", "active learning", "data-driven", "exploration", "Semi-supervised", "active learning", "data-driven", "exploration"], "referenced_by": ["10.1145/3306346.3322944", "10.1007/s00371-015-1193-9", "10.1007/s11432-016-9071-5", "10.1016/j.cag.2017.07.001", "10.1016/j.gmod.2017.01.001", "10.1016/j.gmod.2018.01.003", "10.1016/j.gmod.2018.06.002", "10.1016/j.cag.2018.12.004", "10.1007/s41095-020-0163-y"], "referencing": ["IKEY:4539847", "IKEY:1314504", "IKEY:5674030", "IKEY:4539847", "IKEY:1314504", "IKEY:5674030", "IKEY:4539847", "IKEY:1314504", "IKEY:5674030", "10.1145/571647.571648", "10.1145/2185520.2335382", "10.1145/1015706.1015775", "10.1145/1122501.1122507", "10.1145/2461912.2461968", "10.1145/500156.500159", "10.1145/2508363.2508364", "10.1145/2010324.1964928", "10.1145/2461912.2461933", "10.1145/2185520.2185550", "10.1145/2601097.2601111", "10.1145/2461912.2461954", "10.1145/1618452.1618513", "10.1145/1877808.1877821", "10.1145/2185520.2185526", "10.1145/1360612.1360641", "10.1145/1559755.1559760", "10.1145/571647.571648", "10.1145/2185520.2335382", "10.1145/1015706.1015775", "10.1145/1122501.1122507", "10.1145/2461912.2461968", "10.1145/500156.500159", "10.1145/2508363.2508364", "10.1145/2010324.1964928", "10.1145/2461912.2461933", "10.1145/2185520.2185550", "10.1145/2601097.2601111", "10.1145/2461912.2461954", "10.1145/1618452.1618513", "10.1145/1877808.1877821", "10.1145/2185520.2185526", "10.1145/1360612.1360641", "10.1145/1559755.1559760", "10.1145/571647.571648", "10.1145/2185520.2335382", "10.1145/1015706.1015775", "10.1145/1122501.1122507", "10.1145/2461912.2461968", "10.1145/500156.500159", "10.1145/2508363.2508364", "10.1145/2010324.1964928", "10.1145/2461912.2461933", "10.1145/2185520.2185550", "10.1145/2601097.2601111", "10.1145/2461912.2461954", "10.1145/1618452.1618513", "10.1145/1877808.1877821", "10.1145/2185520.2185526", "10.1145/1360612.1360641", "10.1145/1559755.1559760", "10.1007/s11042-007-0181-0", "10.1111/1467-8659.00669", "10.1111/j.1467-8659.2011.02050.x", "10.1111/j.1467-8659.2009.01515.x", "10.1111/cgf.12200", "10.1007/978-3-7091-6103-6_12", "10.1007/s00371-005-0341-z", "10.1007/s11042-007-0188-6", "10.1007/s11263-009-0294-1", "10.1007/s00371-010-0524-0", "10.1111/cgf.12185", "10.1007/s11042-007-0181-0", "10.1111/1467-8659.00669", "10.1111/j.1467-8659.2011.02050.x", "10.1111/j.1467-8659.2009.01515.x", "10.1111/cgf.12200", "10.1007/978-3-7091-6103-6_12", "10.1007/s00371-005-0341-z", "10.1007/s11042-007-0188-6", "10.1007/s11263-009-0294-1", "10.1007/s00371-010-0524-0", "10.1111/cgf.12185", "10.1007/s11042-007-0181-0", "10.1111/1467-8659.00669", "10.1111/j.1467-8659.2011.02050.x", "10.1111/j.1467-8659.2009.01515.x", "10.1111/cgf.12200", "10.1007/978-3-7091-6103-6_12", "10.1007/s00371-005-0341-z", "10.1007/s11042-007-0188-6", "10.1007/s11263-009-0294-1", "10.1007/s00371-010-0524-0", "10.1111/cgf.12185"]}, "10.1109/TVCG.2015.2414455": {"doi": "10.1109/TVCG.2015.2414455", "author": ["H. Obermaier", "K. I. Joy"], "title": "An Automated Approach for Slicing Plane Placement in Visual Data Analysis", "year": "2015", "abstract": "Effective display and visual analysis of complex 3D data is a challenging task. Occlusions, overlaps, and projective distortions-as frequently caused by typical 3D rendering techniques-can be major obstacles to unambiguous and robust data analysis. Slicing planes are a ubiquitous tool to resolve several of these issues. They act as simple clipping geometry to provide clear cut-away views of the data. We propose to enhance the visualization and analysis process by providing methods for automatic placement of such slicing planes based on local optimization of gradient vector flow. The final obtained slicing planes maximize the total amount of information displayed with respect to a pre-specified importance function. We demonstrate how such automated slicing plane placement is able to support and enrich 3D data visualization and analysis in multiple scenarios, such as volume or surface rendering, and evaluate its performance in several benchmark data sets.", "keywords": ["data analysis", "data visualisation", "visual data analysis", "gradient vector flow local optimization", "automated slicing plane placement", "3D data visualization", "Virtual reality", "Data visualization", "Three-dimensional displays", "Feature extraction", "Data analysis", "Slicing plane", "data analysis", "visualization", "automatic placement", "optimization", "Slicing plane", "data analysis", "visualization", "automatic placement", "optimization"], "referenced_by": ["IKEY:9089663"], "referencing": ["IKEY:4015466", "IKEY:235211", "IKEY:5613471", "IKEY:5290756", "IKEY:4015450", "IKEY:6226391", "IKEY:4015449", "IKEY:1432686", "IKEY:1207438", "IKEY:661186", "IKEY:1631217", "IKEY:4015466", "IKEY:235211", "IKEY:5613471", "IKEY:5290756", "IKEY:4015450", "IKEY:6226391", "IKEY:4015449", "IKEY:1432686", "IKEY:1207438", "IKEY:661186", "IKEY:1631217", "IKEY:4015466", "IKEY:235211", "IKEY:5613471", "IKEY:5290756", "IKEY:4015450", "IKEY:6226391", "IKEY:4015449", "IKEY:1432686", "IKEY:1207438", "IKEY:661186", "IKEY:1631217", "10.1145/99308.99322", "10.1145/325165.325242", "10.1145/142920.134037", "10.1145/99308.99322", "10.1145/325165.325242", "10.1145/142920.134037", "10.1145/99308.99322", "10.1145/325165.325242", "10.1145/142920.134037", "10.1002/0471745790", "10.1006/aima.1997.1650", "10.1007/3-540-30790-7_18", "10.1017/S0022112095000462", "10.1007/BF00133570", "10.1007/11536482_15", "10.1007/978-3-540-85412-8_14", "10.1002/0471745790", "10.1006/aima.1997.1650", "10.1007/3-540-30790-7_18", "10.1017/S0022112095000462", "10.1007/BF00133570", "10.1007/11536482_15", "10.1007/978-3-540-85412-8_14", "10.1002/0471745790", "10.1006/aima.1997.1650", "10.1007/3-540-30790-7_18", "10.1017/S0022112095000462", "10.1007/BF00133570", "10.1007/11536482_15", "10.1007/978-3-540-85412-8_14"]}, "10.1109/TVCG.2015.2440241": {"doi": "10.1109/TVCG.2015.2440241", "author": ["M. Chi", "S. Lin", "S. Chen", "C. Lin", "T. Lee"], "title": "Morphable Word Clouds for Time-Varying Text Data Visualization", "year": "2015", "abstract": "A word cloud is a visual representation of a collection of text documents that uses various font sizes, colors, and spaces to arrange and depict significant words. The majority of previous studies on time-varying word clouds focuses on layout optimization and temporal trend visualization. However, they do not fully consider the spatial shapes and temporal motions of word clouds, which are important factors for attracting people's attention and are also important cues for human visual systems in capturing information from time-varying text data. This paper presents a novel method that uses rigid body dynamics to arrange multi-temporal word-tags in a specific shape sequence under various constraints. Each word-tag is regarded as a rigid body in dynamics. With the aid of geometric, aesthetic, and temporal coherence constraints, the proposed method can generate a temporally morphable word cloud that not only arranges word-tags in their corresponding shapes but also smoothly transforms the shapes of word clouds overtime, thus yielding a pleasing time-varying visualization. Using the proposed frame-by-frame and morphable word clouds, people can observe the overall story of a time-varying text data from the shape transition, and people can also observe the details from the word clouds in frames. Experimental results on various data demonstrate the feasibility and flexibility of the proposed method in morphable word cloud generation. In addition, an application that uses the proposed word clouds in a simulated exhibition demonstrates the usefulness of the proposed method.", "keywords": ["data visualisation", "text analysis", "time-varying systems", "time-varying text data visualization", "text documents visual representation", "rigid body dynamics", "multitemporal word-tag arrangement", "shape sequence", "temporally morphable word cloud", "frame-by-frame word clouds", "Tag clouds", "Virtual reality", "Data visualization", "Text mining", "Interpolation", "Market research", "Data analysis", "Digital systems", "Time-varying systems", "word cloud", "time-varying text data", "digital storytelling", "information visualization", "Word cloud", "time-varying text data", "digital storytelling", "information visualization", "Computer Graphics", "Data Mining", "Humans", "Internet", "Semantics", "Word Processing"], "referenced_by": ["IKEY:8047237", "IKEY:7785428", "IKEY:7465262", "IKEY:8017586", "IKEY:7975911", "IKEY:8564149", "IKEY:8356097", "IKEY:8753913", "IKEY:8807355", "IKEY:9182111", "10.1007/978-3-319-64870-5_10", "10.1007/s00500-017-2536-4", "10.1016/j.engappai.2017.06.023", "10.1080/02664763.2018.1435633", "10.2991/978-94-6239-186-4_5", "10.1089/big.2018.0017", "10.1111/eje.12385", "10.1007/978-981-13-2553-3_6", "10.1088/1742-6596/1314/1/012174", "10.1007/s12650-020-00678-3", "10.1057/s41310-020-00087-w", "10.1007/s12650-020-00689-0"], "referencing": ["IKEY:5333443", "IKEY:5567084", "IKEY:5290722", "IKEY:5290723", "IKEY:6327261", "IKEY:5333443", "IKEY:5567084", "IKEY:5290722", "IKEY:5290723", "IKEY:6327261", "IKEY:5333443", "IKEY:5567084", "IKEY:5290722", "IKEY:5290723", "IKEY:6327261", "10.1145/1242572.1242766", "10.1145/1242572.1242826", "10.1145/1240624.1240775", "10.1145/2010324.1964995", "10.1145/1255438.1255439", "10.1145/344779.344859", "10.1145/964965.808600", "10.1145/1242572.1242766", "10.1145/1242572.1242826", "10.1145/1240624.1240775", "10.1145/2010324.1964995", "10.1145/1255438.1255439", "10.1145/344779.344859", "10.1145/964965.808600", "10.1145/1242572.1242766", "10.1145/1242572.1242826", "10.1145/1240624.1240775", "10.1145/2010324.1964995", "10.1145/1255438.1255439", "10.1145/344779.344859", "10.1145/964965.808600", "10.1111/j.1467-8659.2012.03106.x", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03107.x", "10.1016/S0167-8396(03)00002-5", "10.1177/1473871614534095", "10.1111/j.1467-8659.2012.03106.x", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03107.x", "10.1016/S0167-8396(03)00002-5", "10.1177/1473871614534095", "10.1111/j.1467-8659.2012.03106.x", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03107.x", "10.1016/S0167-8396(03)00002-5", "10.1177/1473871614534095"]}, "10.1109/TVCG.2015.2480087": {"doi": "10.1109/TVCG.2015.2480087", "author": ["U. Eck", "F. Pankratz", "C. Sandor", "G. Klinker", "H. Laga"], "title": "Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality", "year": "2015", "abstract": "Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. PHANToM haptic devices are often employed to provide haptic feedback. Precise co-location of computer-generated graphics and the haptic stylus is necessary to provide a realistic user experience. Previous work has focused on calibration procedures that compensate the non-linear position error caused by inaccuracies in the joint angle sensors. In this article we present a more complete procedure that additionally compensates for errors in the gimbal sensors and improves position calibration. The proposed procedure further includes software-based temporal alignment of sensor data and a method for the estimation of a reference for position calibration, resulting in increased robustness against haptic device initialization and external tracker noise. We designed our procedure to require minimal user input to maximize usability. We conducted an extensive evaluation with two different PHANToMs, two different optical trackers, and a mechanical tracker. Compared to state-of-the-art calibration procedures, our approach significantly improves the co-location of the haptic stylus. This results in higher fidelity visual and haptic augmentations, which are crucial for fine-motor tasks in areas such as medical training simulators, assembly planning tools, or rapid prototyping applications.", "keywords": ["augmented reality", "calibration", "computer graphics", "haptic interfaces", "precise haptic device colocation", "visuo-haptic augmented reality system", "digital information", "PHANToM haptic device", "haptic feedback", "computer-generated graphics", "haptic stylus", "realistic user experience", "nonlinear position error compensation", "joint angle sensor", "gimbal sensor", "position calibration", "software-based temporal alignment", "sensor data", "haptic device initialization", "external tracker noise", "usability maximization", "optical tracker", "mechanical tracker", "higher fidelity visual augmentation", "haptic augmentation", "fine-motor task", "medical training simulator", "assembly planning tool", "rapid prototyping application", "Calibration", "Haptic interfaces", "Sensors", "Phantoms", "Target tracking", "Transforms", "Virtual reality", "Calibration", "orientation calibration", "temporal alignment", "Calibration", "orientation calibration", "temporal alignment", "Computer Graphics", "Equipment Design", "Humans", "Touch", "User-Computer Interface"], "referenced_by": ["IKEY:7836524", "IKEY:7435333", "IKEY:7504708", "IKEY:9089504", "10.1007/978-3-319-43775-0_19", "10.1080/24725854.2018.1493244", "10.3390/app9163382", "10.1007/s12555-018-0882-3", "10.3390/app10155344"], "referencing": ["IKEY:5336492", "IKEY:4492774", "IKEY:5740880", "IKEY:34770", "IKEY:466720", "IKEY:6948475", "IKEY:5336492", "IKEY:4492774", "IKEY:5740880", "IKEY:34770", "IKEY:466720", "IKEY:6948475", "IKEY:5336492", "IKEY:4492774", "IKEY:5740880", "IKEY:34770", "IKEY:466720", "IKEY:6948475", "10.1145/1889863.1889883", "10.1145/769953.769982", "10.1145/1889863.1889883", "10.1145/769953.769982", "10.1145/1889863.1889883", "10.1145/769953.769982", "10.1007/s00268-006-0724-y", "10.1177/027836499601500604", "10.1364/JOSAA.4.000629", "10.2307/2346830", "10.1090/conm/112/1087109", "10.1007/s00268-006-0724-y", "10.1177/027836499601500604", "10.1364/JOSAA.4.000629", "10.2307/2346830", "10.1090/conm/112/1087109", "10.1007/s00268-006-0724-y", "10.1177/027836499601500604", "10.1364/JOSAA.4.000629", "10.2307/2346830", "10.1090/conm/112/1087109"]}, "10.1109/TVCG.2015.2407403": {"doi": "10.1109/TVCG.2015.2407403", "author": ["S. Ohl", "M. Willert", "O. Staadt"], "title": "Latency in Distributed Acquisition and Rendering for Telepresence Systems", "year": "2015", "abstract": "Telepresence systems use 3D techniques to create a more natural human-centered communication over long distances. This work concentrates on the analysis of latency in telepresence systems where acquisition and rendering are distributed. Keeping latency low is important to immerse users in the virtual environment. To better understand latency problems and to identify the source of such latency, we focus on the decomposition of system latency into sub-latencies. We contribute a model of latency and show how it can be used to estimate latencies in a complex telepresence dataflow network. To compare the estimates with real latencies in our prototype, we modify two common latency measurement methods. This presented methodology enables the developer to optimize the design, find implementation issues and gain deeper knowledge about specific sources of latency.", "keywords": ["computer networks", "delays", "rendering (computer graphics)", "teleconferencing", "video communication", "virtual reality", "distributed acquisition", "rendering", "latency analysis", "telepresence systems", "system latency decomposition", "sub-latencies", "complex telepresence dataflow network", "latency measurement methods", "video conferencing", "virtual reality", "Cameras", "Distributed processing", "Rendering (computer graphics)", "Virtual reality", "Logic gates", "Bandwidth", "Data models", "Telepresence systems", "Human computer interaction", "Telepresence", "latency", "distributed rendering", "camera arrays", "Telepresence", "latency", "distributed rendering", "camera arrays", "Algorithms", "Computer Graphics", "Humans", "Imaging, Three-Dimensional", "Software", "Telecommunications", "Time Factors"], "referenced_by": ["IKEY:8466636", "IKEY:8630495", "10.1145/3083187.3084019", "10.1016/j.cag.2017.08.003", "10.1007/978-3-030-05710-7_8"], "referencing": ["IKEY:6461359", "IKEY:1512015", "IKEY:4624254", "IKEY:5154063", "IKEY:799723", "IKEY:913793", "IKEY:1191132", "IKEY:6777458", "IKEY:6461359", "IKEY:1512015", "IKEY:4624254", "IKEY:5154063", "IKEY:799723", "IKEY:913793", "IKEY:1191132", "IKEY:6777458", "IKEY:6461359", "IKEY:1512015", "IKEY:4624254", "IKEY:5154063", "IKEY:799723", "IKEY:913793", "IKEY:1191132", "IKEY:6777458", "10.1145/1450579.1450606", "10.1145/1186562.1015805", "10.1145/1240624.1240846", "10.1145/566570.566639", "10.1145/323663.323670", "10.1145/1450579.1450606", "10.1145/1186562.1015805", "10.1145/1240624.1240846", "10.1145/566570.566639", "10.1145/323663.323670", "10.1145/1450579.1450606", "10.1145/1186562.1015805", "10.1145/1240624.1240846", "10.1145/566570.566639", "10.1145/323663.323670", "10.1007/978-3-540-30541-5_12", "10.1016/j.cag.2012.04.011", "10.1155/2010/247108", "10.1007/978-3-540-89639-5_86", "10.1007/978-3-540-27866-5_65", "10.1007/978-3-540-30541-5_12", "10.1016/j.cag.2012.04.011", "10.1155/2010/247108", "10.1007/978-3-540-89639-5_86", "10.1007/978-3-540-27866-5_65", "10.1007/978-3-540-30541-5_12", "10.1016/j.cag.2012.04.011", "10.1155/2010/247108", "10.1007/978-3-540-89639-5_86", "10.1007/978-3-540-27866-5_65"]}}