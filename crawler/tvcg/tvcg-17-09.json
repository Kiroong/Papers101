{"10.1109/TVCG.2016.2602858": {"doi": "10.1109/TVCG.2016.2602858", "author": ["J. Fan", "J. Yang", "Y. Zhao", "D. Ai", "Y. Liu", "G. Wang", "Y. Wang"], "title": "Convex Hull Aided Registration Method (CHARM)", "year": "2017", "abstract": "Non-rigid registration finds many applications such as photogrammetry, motion tracking, model retrieval, and object recognition. In this paper we propose a novel convex hull aided registration method (CHARM) to match two point sets subject to a non-rigid transformation. First, two convex hulls are extracted from the source and target respectively. Then, all points of the point sets are projected onto the reference plane through each triangular facet of the hulls. From these projections, invariant features are extracted and matched optimally. The matched feature point pairs are mapped back onto the triangular facets of the convex hulls to remove outliers that are outside any relevant triangular facet. The rigid transformation from the source to the target is robustly estimated by the random sample consensus (RANSAC) scheme through minimizing the distance between the matched feature point pairs. Finally, these feature points are utilized as the control points to achieve non-rigid deformation in the form of thin-plate spline of the entire source point set towards the target one. The experimental results based on both synthetic and real data show that the proposed algorithm outperforms several state-of-the-art ones with respect to sampling, rotational angle, and data noise. In addition, the proposed CHARM algorithm also shows higher computational efficiency compared to these methods.", "keywords": ["feature extraction", "image matching", "image registration", "random processes", "sampling methods", "solid modelling", "splines (mathematics)", "convex hull aided registration method", "CHARM", "nonrigid registration", "nonrigid transformation", "invariant feature extraction", "matched feature point pairs", "random sample consensus", "RANSAC scheme", "nonrigid deformation", "thin-plate spline", "rotational angle", "data noise", "3D models", "Three-dimensional displays", "Solid modeling", "Feature extraction", "Iterative closest point algorithm", "Computational modeling", "Deformable models", "Robustness", "Point set", "convex hull", "parallel projection", "invariant feature", "non-rigid registration"], "referenced_by": ["10.1109/RADAR41533.2019.171419"], "referencing": ["10.1109/34.121791", "10.1109/34.24792", "10.1109/CVPR.2011.5995744", "10.1109/TPAMI.2011.248", "10.1109/TPAMI.2010.223", "10.1109/TPAMI.2010.46", "10.1109/34.765655", "10.1109/TPAMI.2005.220", "10.1109/34.993558", "10.1109/TDPVT.2004.1335392", "10.1109/ICCV.2015.236", "10.1109/34.88573", "10.1145/358669.358692", "10.1145/1291233.1291311", "10.1145/1201775.882368", "10.1145/235815.235821", "10.1016/S1077-3142(03)00009-2", "10.1007/3-540-47979-1_28", "10.1007/978-3-540-24672-5_18", "10.1016/j.patcog.2016.02.023", "10.1016/j.neucom.2016.01.078", "10.1023/B:VISI.0000029664.99615.94"]}, "10.1109/TVCG.2016.2609429": {"doi": "10.1109/TVCG.2016.2609429", "author": ["J. Kim", "J. Lee", "S. Cha", "C. Kim"], "title": "Efficient Representation of Detailed Foam Waves by Incorporating Projective Space", "year": "2017", "abstract": "We propose an efficient framework to realistically simulate foam effects in which 3D water particles from a base water solver are first projected onto 2D screen space in order to reduce computational complexity of finding foam particles. Because foam effects are often created primarily in fast and complicated water flows, we analyze acceleration and curvature values to identify the areas exhibiting such flow patterns. Identified foam particles are emitted in 3D simulation space, and each foam particle is advected by its classified type based on its velocity, thereby capturing the essential characteristics of foam wave motions (e.g., floating waves or scattering bubbles). In addition, we provide an intuitive and flexible mechanism (e.g., user sketch or image) to customize parameters and control the appearance of foam effects while minimizing the occurrence of popping artifacts. Experiments convincingly demonstrate that the proposed approach is efficient and easy to use while delivering high-quality results.", "keywords": ["computational complexity", "digital simulation", "flow simulation", "foams", "realistic images", "rendering (computer graphics)", "foam waves representation", "projective space", "foam effects simulation", "3D water particles", "base water solver", "2D screen space", "computational complexity", "acceleration values", "curvature values", "flow patterns", "3D simulation space", "realistic rendering", "Three-dimensional displays", "Acceleration", "Two dimensional displays", "Computational modeling", "Surface waves", "Rendering (computer graphics)", "Solid modeling", "Foam effects", "projective space", "foam wave patterns"], "referenced_by": ["10.1109/ACCESS.2019.2934163"], "referencing": ["10.1109/TVCG.2008.37", "10.1145/1507149.1507164", "10.1145/1399504.1360647", "10.1145/2485895.2485912", "10.1145/1833349.1778807", "10.1145/1275808.1276500", "10.1145/2407746.2407747", "10.1145/1028523.1028562", "10.1145/2461912.2461918", "10.1145/1275808.1276499", "10.1145/37401.37422", "10.1145/1186822.1073298", "10.1145/1661412.1618456", "10.1145/1399504.1360649", "10.1145/1661412.1618467", "10.1145/1882262.1866196", "10.1145/2185520.2335463", "10.1145/2508363.2508429", "10.1145/1275808.1276502", "10.1145/2019406.2019408", "10.1111/j.1467-8659.2010.01734.x", "10.1007/s00371-012-0696-x", "10.1007/s00371-009-0339-z", "10.1007/s00371-009-0338-0", "10.1111/cgf.12057", "10.1111/1467-8659.00686", "10.21236/ADA479067", "10.1111/j.1467-8659.2009.01362.x", "10.1007/s00371-012-0697-9", "10.1007/s00371-013-0849-6", "10.1016/j.gmod.2009.08.001", "10.1017/CBO9780511529566"]}, "10.1109/TVCG.2016.2606498": {"doi": "10.1109/TVCG.2016.2606498", "author": ["P. von Radziewsky", "T. Kroes", "M. Eisemann", "E. Eisemann"], "title": "Efficient Stochastic Rendering of Static and Animated Volumes Using Visibility Sweeps", "year": "2017", "abstract": "Stochastically solving the rendering integral (particularly visibility) is the de-facto standard for physically-based light transport but it is computationally expensive, especially when displaying heterogeneous volumetric data. In this work, we present efficient techniques to speed-up the rendering process via a novel visibility-estimation method in concert with an unbiased importance sampling (involving environmental lighting and visibility inside the volume), filtering, and update techniques for both static and animated scenes. Our major contributions include a progressive estimate of partial occlusions based on a fast sweeping-plane algorithm. These occlusions are stored in an octahedral representation, which can be conveniently transformed into a quadtree-based hierarchy suited for a joint importance sampling. Further, we propose sweep-space filtering, which suppresses the occurrence of fireflies and investigate different update schemes for animated scenes. Our technique is unbiased, requires little precomputation, is highly parallelizable, and is applicable to a various volume data sets, dynamic transfer functions, animated volumes and changing environmental lighting.", "keywords": ["computer animation", "quadtrees", "rendering (computer graphics)", "sampling methods", "stochastic rendering", "animated volumes", "visibility sweeps", "static volumes", "rendering integral", "de-facto standard", "physically-based light transport", "heterogeneous volumetric data", "visibility-estimation method", "unbiased importance sampling", "static scenes", "animated scenes", "partial occlusions", "fast sweeping-plane algorithm", "octahedral representation", "quadtree-based hierarchy", "joint importance sampling", "sweep-space filtering", "little precomputation", "volume data sets", "dynamic transfer functions", "environmental lighting", "Lighting", "Rendering (computer graphics)", "Monte Carlo methods", "Transfer functions", "Scattering", "Light sources", "Visibility", "raytracing", "volume rendering", "stochastic rendering", "importance sampling"], "referenced_by": [], "referencing": ["10.1109/TVCG.2011.35", "10.1109/TVCG.2011.211", "10.1145/280814.280864", "10.1145/1073204.1073328", "10.1145/965139.807402", "10.1145/344779.344958", "10.1145/1283900.1283908", "10.1145/1477926.1477933", "10.1145/2661229.2661292", "10.1145/882262.882314", "10.1145/1015706.1015750", "10.1145/218380.218498", "10.1145/2508363.2508411", "10.1145/882262.882274", "10.1145/237170.237199", "10.1111/cgf.12252", "10.1111/j.1467-8659.2008.01154.x", "10.1007/978-3-7091-6242-2_16", "10.1117/12.650849", "10.1371/journal.pone.0038586", "10.1111/j.1467-8659.2008.01166.x", "10.1007/s00371-008-0227-y", "10.1137/1.9781611970081", "10.1086/144246", "10.1016/j.cag.2014.08.003"]}, "10.1109/TVCG.2016.2605092": {"doi": "10.1109/TVCG.2016.2605092", "author": ["F. Hou", "Y. He", "H. Qin", "A. Hao"], "title": "Knot Optimization for Biharmonic B-splines on Manifold Triangle Meshes", "year": "2017", "abstract": "Biharmonic B-splines, proposed by Feng and Warren, are an elegant generalization of univariate B-splines to planar and curved domains with fully irregular knot configuration. Despite the theoretic breakthrough, certain technical difficulties are imperative, including the necessity of Voronoi tessellation, the lack of analytical formulation of bases on general manifolds, expensive basis re-computation during knot refinement/removal, being applicable for simple domains only (e.g., such as euclidean planes, spherical and cylindrical domains, and tori). To ameliorate, this paper articulates a new biharmonic B-spline computing paradigm with a simple formulation. We prove that biharmonic B-splines have an equivalent representation, which is solely based on a linear combination of Green's functions of the bi-Laplacian operator. Consequently, without explicitly computing their bases, biharmonic B-splines can bypass the Voronoi partitioning and the discretization of bi-Laplacian, enable the computational utilities on any compact 2-manifold. The new representation also facilitates optimization-driven knot selection for constructing biharmonic B-splines on manifold triangle meshes. We develop algorithms for spline evaluation, data interpolation and hierarchical data decomposition. Our results demonstrate that biharmonic B-splines, as a new type of spline functions with theoretic and application appeal, afford progressive update of fully irregular knots, free of singularity, without the need of explicit parameterization, making it ideal for a host of graphics tasks on manifolds.", "keywords": ["computational geometry", "Green's function methods", "mesh generation", "optimisation", "splines (mathematics)", "knot optimization", "biharmonic B-splines", "manifold triangle meshes", "univariate B-splines", "Green's functions", "bi-Laplacian operator", "2-manifold", "spline evaluation", "data interpolation", "hierarchical data decomposition", "spline functions", "Splines (mathematics)", "Manifolds", "Green's function methods", "Laplace equations", "Optimization", "Interpolation", "Geometry", "Biharmonic B-splines", "green\u2019s functions", "manifold triangle meshes", "implicit representation", "knot optimization"], "referenced_by": [], "referencing": ["10.1109/SMI.2004.1314506", "10.1109/TVCG.2005.33", "10.1109/TVCG.2012.173", "10.1145/2185520.2185611", "10.1145/1508044.1508063", "10.1145/1015706.1015772", "10.1145/2766952", "10.1145/2661229.2661255", "10.1145/882262.882295", "10.1145/1183287.1183295", "10.1145/2461912.2461935", "10.1111/cgf.12516", "10.1111/j.1467-8659.2009.01515.x", "10.1111/j.1467-8659.2011.02025.x", "10.1111/j.1467-8659.2010.01765.x", "10.1111/j.1467-8659.2011.02026.x", "10.1198/1061860031284", "10.1016/j.cad.2014.08.022", "10.1111/cgf.12589", "10.1007/s00041-008-9045-x", "10.2307/2152982", "10.1016/j.gmod.2006.03.004", "10.1111/j.1467-8659.2010.01759.x"]}, "10.1109/TVCG.2016.2608828": {"doi": "10.1109/TVCG.2016.2608828", "author": ["A. Baldacci", "F. Ganovelli", "M. Corsini", "R. Scopigno"], "title": "Presentation of 3D Scenes Through Video Example", "year": "2017", "abstract": "Using synthetic videos to present a 3D scene is a common requirement for architects, designers, engineers or Cultural Heritage professionals however it is usually time consuming and, in order to obtain high quality results, the support of a film maker/computer animation expert is necessary. We introduce an alternative approach that takes the 3D scene of interest and an example video as input, and automatically produces a video of the input scene that resembles the given video example. In other words, our algorithm allows the user to \u201creplicate\u201d an existing video, on a different 3D scene. We build on the intuition that a video sequence of a static environment is strongly characterized by its optical flow, or, in other words, that two videos are similar if their optical flows are similar. We therefore recast the problem as producing a video of the input scene whose optical flow is similar to the optical flow of the input video. Our intuition is supported by a user-study specifically designed to verify this statement. We have successfully tested our approach on several scenes and input videos, some of which are reported in the accompanying material of this paper.", "keywords": ["computer animation", "image sequences", "multimedia computing", "solid modelling", "3D scene presentation", "video example", "synthetic videos", "film maker", "computer animation expert", "video scene", "video replication", "video sequence", "optical flow", "multimedia content production", "Three-dimensional displays", "Cameras", "Databases", "Solid modeling", "Video sequences", "Optical imaging", "Visualization", "Multimedia content production", "video similarity", "2D vector field comparison", "computer animation"], "referenced_by": [], "referencing": ["10.1109/TIP.2002.999674", "10.1109/RT.2007.4342600", "10.1109/TPAMI.2014.2321376", "10.1145/237170.237259", "10.1145/641480.641491", "10.1145/1599470.1599478", "10.1145/2019627.2019628", "10.1145/1186822.1073244", "10.1145/1401843.1401875", "10.1145/1631272.1631280", "10.1145/2501654.2501658", "10.1145/973264.973304", "10.1111/j.1467-8659.2004.00781.x", "10.1111/cgf.12334", "10.1007/s11263-010-0390-2", "10.1023/A:1026543900054", "10.1007/11744078_26", "10.1364/JOSAA.4.000629", "10.1016/0031-3203(81)90009-1"]}, "10.1109/TVCG.2016.2617872": {"doi": "10.1109/TVCG.2016.2617872", "author": ["J. Guo", "J. Qian", "Y. Guo", "J. Pan"], "title": "Rendering Thin Transparent Layers with Extended Normal Distribution Functions", "year": "2017", "abstract": "Realistic Rendering of thin transparent layers bounded by rough surfaces involves substantial expense of computation time to account for multiple internal reflections. Resorting to Monte Carlo rendering for such material is usually impractical since recursive importance sampling is inevitable. To reduce the burden of sampling for simulating subsurface scattering and hence improve rendering performance, we adapt the microfacet model to the material with a single thin layer by introducing the extended normal distribution function (ENDF), a new representation of this model, to express visually perceived roughness due to multiple bounces of reflections and refractions. With such a representation, both surface reflection and subsurface scattering can be treated in the same microfacet framework, and the sampling process can be reduced to only once for each bounce of scattering. We derive analytical expressions of the ENDF for several cases using joint spherical warping. We also show how to choose proper shadowing-masking and Fresnel terms to make the proposed bidirectional scattering distribution function (BSDF) model energy-conserving. Experiments demonstrate that our model can be easily incorporated into a Monte Carlo path tracer with little extra computational and storage overhead, enabling some real-time applications.", "keywords": ["light reflection", "light scattering", "normal distribution", "rendering (computer graphics)", "sampling methods", "microfacet model", "extended normal distribution function", "refractions", "surface reflection", "subsurface scattering", "sampling process", "ENDF", "joint spherical warping", "shadowing-masking", "Fresnel terms", "bidirectional scattering distribution function", "BSDF model", "rendering", "Scattering", "Rendering (computer graphics)", "Computational modeling", "Rough surfaces", "Surface roughness", "Gaussian distribution", "Monte Carlo methods", "Microfacet", "layered material", "normal distribution function", "BSDF"], "referenced_by": ["10.1109/TVCG.2018.2872709"], "referencing": ["10.1109/TVCG.2012.73", "10.1109/TAP.1967.1138991", "10.1109/TVCG.2011.282", "10.1109/PCCGA.2002.1167838", "10.1145/1321261.1321292", "10.1145/166117.166139", "10.1145/357290.357293", "10.1145/2776880.2787670", "10.1145/344779.344814", "10.1145/1618452.1618479", "10.1145/2070781.2024179", "10.1145/2366145.2366163", "10.1145/344779.344824", "10.1145/383259.383319", "10.1145/1073204.1073308", "10.1145/237170.237278", "10.1145/2601097.2601155", "10.1145/2508363.2508386", "10.1145/2897824.2925943", "10.1364/JOSA.57.001105", "10.1007/978-3-7091-6303-0_17", "10.1111/j.1467-8659.2012.03147.x", "10.1111/cgf.12417", "10.1111/j.1467-8659.2009.01570.x", "10.1111/cgf.12420", "10.1007/s00371-014-0967-9", "10.1111/1467-8659.00515", "10.6028/jres.029.017"]}, "10.1109/TVCG.2016.2616404": {"doi": "10.1109/TVCG.2016.2616404", "author": ["G. Andrienko", "N. Andrienko", "G. Fuchs", "J. Wood"], "title": "Revealing Patterns and Trends of Mass Mobility Through Spatial and Temporal Abstraction of Origin-Destination Movement Data", "year": "2017", "abstract": "Origin-destination (OD) movement data describe moves or trips between spatial locations by specifying the origins, destinations, start, and end times, but not the routes travelled. For studying the spatio-temporal patterns and trends of mass mobility, individual OD moves of many people are aggregated into flows (collective moves) by time intervals. Time-variant flow data pose two difficult challenges for visualization and analysis. First, flows may connect arbitrary locations (not only neighbors), thus making a graph with numerous edge intersections, which is hard to visualize in a comprehensible way. Even a single spatial situation consisting of flows in one time step is hard to explore. The second challenge is the need to analyze long time series consisting of numerous spatial situations. We present an approach facilitating exploration of long-term flow data by means of spatial and temporal abstraction. It involves a special way of data aggregation, which allows representing spatial situations by diagram maps instead of flow maps, thus reducing the intersections and occlusions pertaining to flow maps. The aggregated data are used for clustering of time intervals by similarity of the spatial situations. Temporal and spatial displays of the clustering results facilitate the discovery of periodic patterns and longer-term trends in the mass mobility behavior.", "keywords": ["data aggregation", "data analysis", "data visualisation", "graph theory", "pattern clustering", "time series", "mass mobility", "spatial abstraction", "temporal abstraction", "origin-destination movement data", "OD movement data", "spatio-temporal patterns", "time-variant flow data", "data visualization", "data analysis", "graph", "time series", "data aggregation", "diagram maps", "flow maps", "clustering", "periodic pattern discovery", "Time series analysis", "Market research", "Data visualization", "Complexity theory", "Electronic mail", "Visualization", "Clutter", "Movement data", "mobility behavior", "spatial flow situation", "flow map"], "referenced_by": ["10.1109/TITS.2017.2683539", "10.1109/ACCESS.2018.2864662", "10.1109/TVCG.2018.2864811", "10.1109/TVCG.2018.2864503", "10.1109/TVCG.2018.2865018", "10.1109/ACCESS.2019.2897070", "10.1109/MCG.2019.2926242", "10.1109/ACCESS.2019.2942844", "10.1109/TVCG.2019.2934671", "10.1109/ACCESS.2019.2959907", "10.1109/ACCESS.2019.2963107", "10.1109/ACCESS.2020.2983052", "10.1109/TITS.2019.2924796", "10.1109/ACCESS.2020.3040852", "10.1109/ACCESS.2020.3045182"], "referencing": []}, "10.1109/TVCG.2016.2601915": {"doi": "10.1109/TVCG.2016.2601915", "author": ["J. Wang", "K. Xu"], "title": "Shape Detection from Raw LiDAR Data with Subspace Modeling", "year": "2017", "abstract": "LiDAR scanning has become a prevalent technique for digitalizing large-scale outdoor scenes. However, the raw LiDAR data often contain imperfections, e.g., missing large regions, anisotropy of sampling density, and contamination of noise and outliers, which are the major obstacles that hinder its more ambitious and higher level applications in digital city modeling. Observing that 3D urban scenes can be locally described with several low dimensional subspaces, we propose to locally classify the neighborhoods of the scans to model the substructures of the scenes. The key enabler is the adaptive kernel-scale scoring, filtering and clustering of substructures, making it possible to recover the local structures at all points simultaneously, even in the presence of severe data imperfections. Integrating the local analyses leads to robust shape detection from raw LiDAR data. On this basis, we develop several urban scene applications and verify them on a number of LiDAR scans with various complexities and styles, which demonstrates the effectiveness and robustness of our methods.", "keywords": ["optical radar", "raw LiDAR data", "subspace modeling", "robust shape detection", "large-scale outdoor scenes", "digital city modeling", "adaptive kernel-scale scoring", "robust local classification technique", "Laser radar", "Three-dimensional displays", "Robustness", "Shape", "Data models", "Computational modeling", "Solid modeling", "Urban building", "raw LiDAR scan", "modeling", "reconstruction", "substructure modeling"], "referenced_by": ["10.1109/ICASSP.2018.8461475", "10.1109/ICCSP.2018.8524455", "10.1109/ICASSP.2019.8683804", "10.1109/TGRS.2018.2889335", "10.1109/CESYS.2018.8724044", "10.1109/LRA.2020.2969936", "10.1109/JSTARS.2020.2969119", "10.1109/TVCG.2018.2889944"], "referencing": ["10.1109/VISUAL.2001.964489", "10.1109/ICCV.2007.4408978", "10.1109/TPAMI.2011.216", "10.1145/1273496.1273511", "10.1145/358669.358692", "10.1145/2461912.2461969", "10.1145/882262.882293", "10.1145/1007730.1007731", "10.1145/2766995", "10.1007/978-1-4612-1106-8", "10.1007/s11263-011-0474-7", "10.1111/j.1467-8659.2005.00852.x", "10.1111/cgf.12042", "10.1111/cgf.12077", "10.1111/j.1467-8659.2009.01388.x", "10.1111/j.1467-8659.2007.01016.x", "10.1007/978-1-4899-4493-1", "10.1016/0167-8655(90)90042-Z", "10.1111/cgf.12187", "10.1016/S0924-2716(99)00011-8", "10.1111/cgf.12720"]}, "10.1109/TVCG.2016.2614803": {"doi": "10.1109/TVCG.2016.2614803", "author": ["M. Brehmer", "B. Lee", "B. Bach", "N. H. Riche", "T. Munzner"], "title": "Timelines Revisited: A Design Space and Considerations for Expressive Storytelling", "year": "2017", "abstract": "There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.", "keywords": ["computer aided instruction", "data analysis", "data structures", "data visualisation", "interactive systems", "expressive storytelling", "event sequence visualization", "timeline designs", "data analysis", "representation dimension", "scale dimension", "layout dimension", "animated transitions", "story presentation", "interactive storytelling", "data videos", "Data visualization", "Visualization", "Context", "Layout", "Videos", "History", "Biographies", "Timelines", "storytelling", "narrative visualization", "design space", "animated transitions"], "referenced_by": ["10.1109/TVCG.2017.2764459", "10.1109/BDVA.2018.8534029", "10.1109/TVCG.2018.2864899", "10.1109/BELIV.2018.8634297", "10.1109/PacificVis.2019.00038", "10.1109/VAST.2018.8802509", "10.1109/TVCG.2019.2934275", "10.1109/TVCG.2019.2934398", "10.1109/TVCG.2019.2934655", "10.1109/TVCG.2019.2934810", "10.1109/TVCG.2018.2889054"], "referencing": ["10.1109/TVCG.2013.119", "10.1109/TVCG.2010.179", "10.1109/TVCG.2015.2413786", "10.1109/TVCG.2015.2467531", "10.1109/TVCG.2013.200", "10.1109/TVCG.2011.255", "10.1109/MC.2013.36", "10.1109/MCG.2015.99", "10.1109/TVCG.2007.70539", "10.1109/TVCG.2013.254", "10.1109/INFVIS.2002.1173148", "10.1109/TVCG.2014.2346424", "10.1109/TVCG.2015.2467732", "10.1109/INFVIS.2001.963273", "10.1109/TVCG.2015.2502587", "10.1109/TVCG.2015.2467851", "10.1109/TVCG.2012.212", "10.1109/TVCG.2011.185", "10.1145/2702123.2702431", "10.1145/1294211.1294229", "10.1145/238386.238493", "10.1145/2254556.2254639", "10.1145/1357054.1357129", "10.1145/2702123.2702476", "10.1145/1978942.1979233", "10.1145/972648.972652", "10.1145/288392.288399", "10.1145/506443.506505", "10.1145/37401.37407", "10.1111/cgf.12392", "10.1111/j.1467-8659.2009.01687.x", "10.1016/0933-3657(91)90005-V", "10.1057/palgrave.ivs.9500165", "10.2307/2288400", "10.1167/7.13.14"]}, "10.1109/TVCG.2016.2607204": {"doi": "10.1109/TVCG.2016.2607204", "author": ["L. Liu", "A. P. Boone", "I. T. Ruginski", "L. Padilla", "M. Hegarty", "S. H. Creem-Regehr", "W. B. Thompson", "C. Yuksel", "D. H. House"], "title": "Uncertainty Visualization by Representative Sampling from Prediction Ensembles", "year": "2017", "abstract": "Data ensembles are often used to infer statistics to be used for a summary display of an uncertain prediction. In a spatial context, these summary displays have the drawback that when uncertainty is encoded via a spatial spread, display glyph area increases in size with prediction uncertainty. This increase can be easily confounded with an increase in the size, strength or other attribute of the phenomenon being presented. We argue that by directly displaying a carefully chosen subset of a prediction ensemble, so that uncertainty is conveyed implicitly, such misinterpretations can be avoided. Since such a display does not require uncertainty annotation, an information channel remains available for encoding additional information about the prediction. We demonstrate these points in the context of hurricane prediction visualizations, showing how we avoid occlusion of selected ensemble elements while preserving the spatial statistics of the original ensemble, and how an explicit encoding of uncertainty can also be constructed from such a selection. We conclude with the results of a cognitive experiment demonstrating that the approach can be used to construct storm prediction displays that significantly reduce the confounding of uncertainty with storm size, and thus improve viewers' ability to estimate potential for storm damage.", "keywords": ["geophysics computing", "storms", "uncertainty handling", "uncertainty visualization", "representative sampling", "prediction ensembles", "data ensembles", "information encoding", "hurricane prediction visualizations", "spatial statistics", "explicit uncertainty encoding", "storm prediction", "storm size", "Uncertainty", "Hurricanes", "Storms", "Data visualization", "Visualization", "Predictive models", "Numerical models", "Implicit uncertainty presentation", "ensembles", "ensemble visualization", "sampling", "uncertainty", "hurricane prediction"], "referenced_by": ["10.1109/MCG.2017.3621220", "10.1109/TVCG.2018.2864503", "10.1109/TVCG.2018.2865051", "10.1109/TVCG.2018.2865193", "10.1109/BELIV.2018.8634267", "10.1109/TVCG.2018.2853721", "10.1109/ACCESS.2019.2935471", "10.1109/TVCG.2019.2934806"], "referencing": ["10.1109/TVCG.2013.143", "10.1109/TVCG.2014.2346455", "10.1109/TVCG.2014.2346594", "10.1145/1833349.1778816", "10.1145/383259.383266", "10.1145/1141911.1141915", "10.3402/tellusa.v52i3.12267", "10.1007/978-3-540-71158-2_12", "10.1080/13875868.2015.1137577", "10.1111/cgf.12649", "10.1175/BAMS-88-5-651", "10.1615/Int.J.UncertaintyQuantification.2012003966", "10.1080/00207179608921659", "10.1111/cgf.12538", "10.1214/aos/1176347507", "10.1037/a0025813"]}, "10.1109/TVCG.2016.2610422": {"doi": "10.1109/TVCG.2016.2610422", "author": ["P. Federico", "F. Heimerl", "S. Koch", "S. Miksch"], "title": "A Survey on Visual Approaches for Analyzing Scientific Literature and Patents", "year": "2017", "abstract": "The increasingly large number of available writings describing technical and scientific progress, calls for advanced analytic tools for their efficient analysis. This is true for many application scenarios in science and industry and for different types of writings, comprising patents and scientific articles. Despite important differences between patents and scientific articles, both have a variety of common characteristics that lead to similar search and analysis tasks. However, the analysis and visualization of these documents is not a trivial task due to the complexity of the documents as well as the large number of possible relations between their multivariate attributes. In this survey, we review interactive analysis and visualization approaches of patents and scientific articles, ranging from exploration tools to sophisticated mining methods. In a bottom-up approach, we categorize them according to two aspects: (a) data type (text, citations, authors, metadata, and combinations thereof), and (b) task (finding and comparing single entities, seeking elementary relations, finding complex patterns, and in particular temporal patterns, and investigating connections between multiple behaviours). Finally, we identify challenges and research directions in this area that ask for future investigations.", "keywords": ["data mining", "data visualisation", "document handling", "information analysis", "patents", "scientific information systems", "visual approaches", "scientific literature", "patents", "writings", "advanced analytic tools", "scientific articles", "document visualization", "multivariate attributes", "interactive analysis", "sophisticated mining methods", "data type", "Patents", "Data visualization", "Visualization", "Metadata", "Law", "Writing", "Visualization", "scientific literature", "patents", "documents", "survey"], "referenced_by": ["10.1109/TVCG.2018.2865022", "10.1109/iV.2018.00033", "10.1109/TVCG.2018.2830759", "10.1109/TVCG.2018.2834341", "10.1109/ACCESS.2019.2929754", "10.1109/TVCG.2019.2934667", "10.1109/ICITBS49701.2020.00191"], "referencing": ["10.1109/PACIFICVIS.2015.7156366", "10.1109/INFVIS.2001.963287", "10.1109/TVCG.2013.212", "10.1109/TVCG.2010.85", "10.1109/IV.2011.95", "10.1109/TVCG.2009.139", "10.1109/VAST.2010.5652940", "10.1109/72.846729", "10.1109/PacificVis.2014.47", "10.1109/38.974518", "10.1109/ICDIM.2009.5356798", "10.1109/IV.2007.86", "10.1109/TKDE.2015.2453957", "10.1109/TVCG.2009.202", "10.1109/TVCG.2013.109", "10.1109/TVCG.2007.70582", "10.1109/IV.2005.35", "10.1109/IV.2004.1320261", "10.1109/TVCG.2014.2383380", "10.1109/TVCG.2013.254", "10.1109/VAST.2011.6102441", "10.1109/TVCG.2015.2467757", "10.1109/TVCG.2013.184", "10.1109/TVCG.2013.167", "10.1109/TVCG.2009.122", "10.1109/IV.2004.1320189", "10.1109/VAST.2011.6102443", "10.1109/2.910895", "10.1109/VAST.2011.6102442", "10.1109/TVCG.2015.2467621", "10.1109/TVCG.2011.186", "10.1109/TVCG.2012.252", "10.1109/TVCG.2014.2346578", "10.1109/TVCG.2014.2346481", "10.1145/2207676.2207738", "10.1145/223904.223912", "10.1145/243199.243214", "10.1145/122860.122873", "10.1145/1056808.1057069", "10.1145/2254556.2254572", "10.1145/2089094.2089099", "10.1145/223904.223913", "10.1145/2212776.2212796", "10.1145/1255175.1255179", "10.1145/2254556.2254654", "10.1145/2254556.2254672", "10.1145/336597.336637", "10.1145/2212776.2212830", "10.1145/1518701.1518896", "10.1145/1273496.1273526", "10.1016/j.wpi.2009.05.008", "10.1016/j.wpi.2013.12.006", "10.1016/j.wpi.2008.01.007", "10.1007/978-3-642-19231-9", "10.1007/978-1-4471-5128-9", "10.1017/CBO9781139644082", "10.1002/widm.1071", "10.1017/CBO9780511809071", "10.1016/0306-4573(93)90024-8", "10.1007/978-3-642-15384-6_45", "10.1111/cgf.12618", "10.1007/978-3-642-41335-3_29", "10.1023/A:1008690008856", "10.1111/j.1467-8659.2011.01923.x", "10.1007/s12650-015-0323-9", "10.1111/j.1467-8659.2012.03108.x", "10.1111/cgf.12376", "10.1073/pnas.0307654100", "10.1073/pnas.0307626100", "10.1016/j.joi.2014.07.006", "10.1371/journal.pone.0008694", "10.1002/asi.22680", "10.1177/0165551504042802", "10.1080/10447310701702402", "10.1002/asi.21160", "10.1007/s00799-004-0110-z", "10.1002/(SICI)1097-4571(1999)50:9&lt;799::AID-ASI9&gt;3.3.CO;2-7", "10.1073/pnas.0706851105", "10.1057/palgrave.ivs.9500049", "10.1007/s11192-009-0146-3", "10.1073/pnas.0307513100", "10.1002/asi.10229", "10.1177/030631277700700202", "10.1016/j.technovation.2008.03.009", "10.1002/asi.20317", "10.1057/palgrave.ivs.9500143", "10.1057/ivs.2009.31", "10.1007/978-3-540-85565-1_108", "10.1007/978-3-642-24469-8_24", "10.1007/978-3-642-17277-9_55", "10.1016/j.joi.2007.03.002", "10.1016/S0306-4573(02)00037-7", "10.1007/s11051-006-9194-2", "10.5220/0005360002680273", "10.1371/journal.pone.0018209", "10.1007/978-3-642-21566-7_25", "10.1002/asi.10227", "10.1073/pnas.0307630100", "10.1016/S0360-8352(02)00143-2", "10.1111/j.1467-8659.2011.01921.x", "10.1016/S0306-4573(98)00068-5", "10.1007/978-3-540-30230-8_12", "10.1057/palgrave.ivs.9500156", "10.1007/978-3-642-41939-3_2", "10.1002/asi.22652", "10.1016/j.ijhcs.2005.09.004", "10.1016/j.cag.2013.11.002"]}, "10.1109/TVCG.2016.2615308": {"doi": "10.1109/TVCG.2016.2615308", "author": ["P. Isenberg", "F. Heimerl", "S. Koch", "T. Isenberg", "P. Xu", "C. D. Stolper", "M. Sedlmair", "J. Chen", "T. M\u00f6ller", "J. Stasko"], "title": "Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications", "year": "2017", "abstract": "We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.", "keywords": ["data visualisation", "meta data", "text analysis", "Vispubdata.org", "metadata collection", "IEEE Visualization", "VIS Publications", "InfoVis", "SciVis", "VAST", "conference series", "broad data visualization community", "document collection", "text data visualization research", "Data visualization", "Conferences", "Portable document format", "Metadata", "History", "Terminology", "Libraries", "Visualization", "publication data", "citation data"], "referenced_by": ["10.1109/BDVA.2017.8114622", "10.1109/TVCG.2017.2723393", "10.1109/BDVA.2018.8533894", "10.1109/BDVA.2018.8533897", "10.1109/TVCG.2018.2865149", "10.1109/BELIV.2018.8634297", "10.1109/TVCG.2018.2834341", "10.1109/ACCESS.2019.2929754", "10.1109/ACCESS.2019.2932051", "10.1109/TVCG.2019.2934667", "10.1109/VISUAL.2019.8933542", "10.1109/ISMAR-Adjunct.2019.00-44", "10.1109/DSAA.2019.00063", "10.1109/MCG.2020.2973945", "10.1109/PacificVis48177.2020.1010", "10.1109/ACCESS.2020.2997907", "10.1109/VIS4DH51463.2020.00009", "10.1109/BELIV51497.2020.00009", "10.1109/BELIV51497.2020.00016"], "referencing": ["10.1109/TVCG.2012.324", "10.1109/TVCG.2015.2467621", "10.1109/TVCG.2016.2598827", "10.1109/PACIFICVIS.2015.7156366", "10.1109/TVCG.2007.70412", "10.1177/1473871613490678", "10.1007/978-3-319-45880-9_21", "10.2312/eurovisshort.20161169"]}}