{"10.1109/TVCG.2012.31": {"doi": "10.1109/TVCG.2012.31", "author": ["L. Liu", "R. Van Liere"], "title": "Modeling Object Pursuit for Desktop Virtual Reality", "year": "2012", "abstract": "Models of interaction tasks are quantitative descriptions of relationships between human temporal performance and the spatial characteristics of the interactive tasks. Examples include Fitts' law for modeling the pointing task and Accot and Zhai's steering law for the path steering task. Interaction models can be used as guidelines to design efficient user interfaces and quantitatively evaluate interaction techniques and input devices. In this paper, we introduce and experimentally verify an interaction model for a 3D object-pursuit interaction task. Object pursuit requires that a user continuously tracks an object that moves with constant velocities in a desktop virtual environment. For modeling purposes, we divide the total object-pursuit movement into a tracking phase and a correction phase. Following a two-step modeling methodology that is originally proposed in this paper, the time for the correction phase is modeled as a function of path length, path curvature, target width, and target velocity. The object-pursuit model can be used to quantitatively evaluate the efficiency of user interfaces that involve 3D interaction with moving objects.", "keywords": ["user interfaces", "virtual reality", "desktop virtual reality", "human temporal performance", "spatial characteristics", "Fitts law", "pointing task", "Accot", "path steering task", "Zhai steering law", "user interfaces", "quantitatively evaluate interaction techniques", "input devices", "3D object-pursuit interaction task", "tracking phase", "correction phase", "two-step modeling methodology", "path length", "path curvature", "target width", "target velocity", "moving objects", "Mathematical model", "Equations", "Solid modeling", "Target tracking", "Computational modeling", "Virtual environments", "3D interaction", "interaction modeling", "object pursuit.", "Adult", "Analysis of Variance", "Computer Graphics", "Computers", "Feedback, Sensory", "Female", "Humans", "Male", "Models, Theoretical", "Psychomotor Performance", "Regression Analysis", "Statistics, Nonparametric", "User-Computer Interface", "Video Games"], "referenced_by": ["10.1109/ACCESS.2019.2891132", "10.1109/AIVR.2018.00009", "10.1016/j.cag.2018.10.003"], "referencing": ["10.1109/MCG.2009.121", "10.1109/MCG.2009.121", "10.1109/MCG.2009.121", "10.1145/258549.258760", "10.1145/108844.108868", "10.1145/302979.303133", "10.1145/985692.985749", "10.1145/1450579.1450606", "10.1145/258549.258760", "10.1145/108844.108868", "10.1145/302979.303133", "10.1145/985692.985749", "10.1145/1450579.1450606", "10.1145/258549.258760", "10.1145/108844.108868", "10.1145/302979.303133", "10.1145/985692.985749", "10.1145/1450579.1450606", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1146/annurev.ne.10.030187.000525", "10.1016/0042-6989(83)90171-2", "10.1523/JNEUROSCI.4198-06.2007", "10.1093/brain/120.8.1325", "10.1016/j.ijhcs.2004.09.001", "10.1080/14640748308402133", "10.1162/1054746041382393", "10.1016/j.ijhcs.2010.11.006", "10.1037/h0046162", "10.1007/978-94-011-7801-3", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1146/annurev.ne.10.030187.000525", "10.1016/0042-6989(83)90171-2", "10.1523/JNEUROSCI.4198-06.2007", "10.1093/brain/120.8.1325", "10.1016/j.ijhcs.2004.09.001", "10.1080/14640748308402133", "10.1162/1054746041382393", "10.1016/j.ijhcs.2010.11.006", "10.1037/h0046162", "10.1007/978-94-011-7801-3", "10.1037/h0055392", "10.1207/s15327051hci0701_3", "10.1146/annurev.ne.10.030187.000525", "10.1016/0042-6989(83)90171-2", "10.1523/JNEUROSCI.4198-06.2007", "10.1093/brain/120.8.1325", "10.1016/j.ijhcs.2004.09.001", "10.1080/14640748308402133", "10.1162/1054746041382393", "10.1016/j.ijhcs.2010.11.006", "10.1037/h0046162", "10.1007/978-94-011-7801-3"]}, "10.1109/TVCG.2011.278": {"doi": "10.1109/TVCG.2011.278", "author": ["K. Petkov", "C. Papadopoulos", "M. Zhang", "A. E. Kaufman", "X. Gu"], "title": "Interactive Visibility Retargeting in VR Using Conformal Visualization", "year": "2012", "abstract": "In Virtual Reality, immersive systems such as the CAVE provide an important tool for the collaborative exploration of large 3D data. Unlike head-mounted displays, these systems are often only partially immersive due to space, access, or cost constraints. The resulting loss of visual information becomes a major obstacle for critical tasks that need to utilize the users' entire field of vision. We have developed a conformal visualization technique that establishes a conformal mapping between the full 360^\\circ field of view and the display geometry of a given visualization system. The mapping is provably angle-preserving and has the desirable property of preserving shapes locally, which is important for identifying shape-based features in the visual data. We apply the conformal visualization to both forward and backward rendering pipelines in a variety of retargeting scenarios, including CAVEs and angled arrangements of flat panel displays. In contrast to image-based retargeting approaches, our technique constructs accurate stereoscopic images that are free of resampling artifacts. Our user study shows that on the visual polyp detection task in Immersive Virtual Colonoscopy, conformal visualization leads to imprrenderingoved sensitivity at comparable examination times against the traditional rendering approach. We also develop a novel user interface based on the interactive recreation of the conformal mapping and the real-time regeneration of the view direction correspondence.", "keywords": ["conformal mapping", "data visualisation", "user interfaces", "virtual reality", "interactive visibility", "virtual reality", "visual information", "conformal visualization technique", "conformal mapping", "shape-based features", "backward rendering pipelines", "forward rendering pipelines", "stereoscopic images", "visual polyp detection task", "immersive virtual colonoscopy", "user interface", "real-time regeneration", "Measurement", "Data visualization", "Rendering (computer graphics)", "Conformal mapping", "Geometry", "Shape", "Visualization", "Virtual reality", "conformal visualization", "Ricci flow", "GPU", "immersive cabin", "CAVE", "partially immersive.", "Algorithms", "Colonic Polyps", "Colonoscopy", "Computer Graphics", "Depth Perception", "Humans", "Models, Theoretical", "Sensitivity and Specificity", "User-Computer Interface"], "referenced_by": ["10.1109/3DUI.2016.7460052", "10.1109/JSYST.2015.2410533", "10.1109/VR.2014.6802042", "10.1109/VR.2016.7504702", "10.1007/s11227-018-2360-3", "10.1007/s12555-012-0431-4", "10.1016/j.proeng.2017.09.811"], "referencing": ["10.1109/TVCG.2008.57", "10.1109/TVCG.2008.54", "10.1109/MCG.2005.29", "10.1109/TVCG.2009.98", "10.1109/TVCG.2007.70565", "10.1109/TVCG.2008.57", "10.1109/TVCG.2008.54", "10.1109/MCG.2005.29", "10.1109/TVCG.2009.98", "10.1109/TVCG.2007.70565", "10.1109/TVCG.2008.57", "10.1109/TVCG.2008.54", "10.1109/MCG.2005.29", "10.1109/TVCG.2009.98", "10.1109/TVCG.2007.70565", "10.1145/129888.129892", "10.1145/769953.769974", "10.1145/1364901.1364920", "10.1145/566570.566590", "10.1145/1360612.1360676", "10.1145/1198555.1198715", "10.1145/258734.258750", "10.1145/1174429.1174480", "10.1145/129888.129892", "10.1145/769953.769974", "10.1145/1364901.1364920", "10.1145/566570.566590", "10.1145/1360612.1360676", "10.1145/1198555.1198715", "10.1145/258734.258750", "10.1145/1174429.1174480", "10.1145/129888.129892", "10.1145/769953.769974", "10.1145/1364901.1364920", "10.1145/566570.566590", "10.1145/1360612.1360676", "10.1145/1198555.1198715", "10.1145/258734.258750", "10.1145/1174429.1174480", "10.1007/978-3-540-89639-5_85", "10.1148/radiology.216.2.r00au47331", "10.1111/j.1467-8659.2007.01064.x", "10.1007/978-3-540-89639-5_85", "10.1148/radiology.216.2.r00au47331", "10.1111/j.1467-8659.2007.01064.x", "10.1007/978-3-540-89639-5_85", "10.1148/radiology.216.2.r00au47331", "10.1111/j.1467-8659.2007.01064.x"]}, "10.1109/TVCG.2011.275": {"doi": "10.1109/TVCG.2011.275", "author": ["C. T. Neth", "J. L. Souman", "D. Engel", "U. Kloos", "H. H. Bulthoff", "B. J. Mohler"], "title": "Velocity-Dependent Dynamic Curvature Gain for Redirected Walking", "year": "2012", "abstract": "Redirected walking techniques allow people to walk in a larger virtual space than the physical extents of the laboratory. We describe two experiments conducted to investigate human sensitivity to walking on a curved path and to validate a new redirected walking technique. In a psychophysical experiment, we found that sensitivity to walking on a curved path was significantly lower for slower walking speeds (radius of 10 m versus 22 m). In an applied study, we investigated the influence of a velocity-dependent dynamic gain controller and an avatar controller on the average distance that participants were able to freely walk before needing to be reoriented. The mean walked distance was significantly greater in the dynamic gain controller condition, as compared to the static controller (22 m versus 15 m). Our results demonstrate that perceptually motivated dynamic redirected walking techniques, in combination with reorientation techniques, allow for unaided exploration of a large virtual city model.", "keywords": ["avatars", "gain control", "gait analysis", "psychology", "velocity-dependent dynamic curvature gain", "redirected walking techniques", "virtual space", "human walking sensitivity", "curved path", "psychophysical experiment", "velocity-dependent dynamic gain controller", "avatar controller", "dynamic gain controller condition", "static controller", "reorientation techniques", "virtual city model", "Legged locomotion", "Sensitivity", "Trajectory", "Virtual environments", "Particle measurements", "Atmospheric measurements", "Games", "Virtual reality", "redirected walking", "virtual locomotion", "curvature sensitivity", "avatars.", "Adult", "Algorithms", "Analysis of Variance", "Computer Graphics", "Female", "Humans", "Male", "Middle Aged", "Orientation", "User-Computer Interface", "Walking"], "referenced_by": ["10.1109/3DUI.2014.6798851", "10.1109/TVCG.2013.28", "10.1109/TVCG.2013.88", "10.1109/TVCG.2014.34", "10.1109/TVCG.2017.2657038", "10.1109/TVCG.2017.2657220", "10.1109/TVCG.2018.2793671", "10.1109/TVCG.2018.2793679", "10.1109/MCG.2018.111125628", "10.1109/VR.2018.8446216", "10.1109/VR.2018.8448288", "10.1109/ISMAR.2018.00041", "10.1109/TVCG.2019.2899228", "10.1109/TVCG.2019.2898764", "10.1109/VR.2019.8797989", "10.1109/VR.2019.8797983", "10.1109/VR.2019.8797994", "10.1109/VR.2019.8798286", "10.1109/VR.2019.8797818", "10.1109/ACCESS.2020.2977363", "10.1109/TVCG.2018.2887379", "10.1109/VRW50115.2020.00227", "10.1109/VR46266.2020.00035", "10.1109/VR46266.2020.00082", "10.1109/ISMAR50242.2020.00088", "10.1109/ISMAR50242.2020.00099", "10.1145/3197517.3201335", "10.1145/2964284.2964293", "10.1145/3345554", "10.1145/3414685.3417773", "10.1007/978-3-319-08234-9_253-1", "10.1016/j.procir.2016.02.086", "10.1007/978-3-662-57876-6_6", "10.1038/s41598-018-36035-6", "10.1007/978-3-030-29390-1_19", "10.1016/j.cag.2019.09.005", "10.1007/978-3-030-31908-3_14", "10.1016/j.cortex.2020.01.018", "10.1007/978-3-030-62655-6_2"], "referencing": ["10.1109/TVCG.2009.62", "10.1109/TVCG.2008.191", "10.1109/TVCG.2009.62", "10.1109/TVCG.2008.191", "10.1109/TVCG.2009.62", "10.1109/TVCG.2008.191", "10.1145/1450579.1450611", "10.1145/1450579.1450612", "10.1145/1272582.1272590", "10.1145/1670671.1670675", "10.1145/1857893.1857896", "10.1145/1190036.1190041", "10.1145/1450579.1450611", "10.1145/1450579.1450612", "10.1145/1272582.1272590", "10.1145/1670671.1670675", "10.1145/1857893.1857896", "10.1145/1190036.1190041", "10.1145/1450579.1450611", "10.1145/1450579.1450612", "10.1145/1272582.1272590", "10.1145/1670671.1670675", "10.1145/1857893.1857896", "10.1145/1190036.1190041", "10.1037/0096-1523.33.1.183", "10.1016/j.cub.2009.07.053", "10.1007/s00221-008-1525-3", "10.1207/s15327108ijap0303_3", "10.3758/BF03194544", "10.1007/s00421-003-0906-3", "10.1186/1743-0003-2-28", "10.1037/0096-1523.18.4.906", "10.3758/APP.71.6.1284", "10.1177/0146167203029007002", "10.1037/0096-1523.33.1.183", "10.1016/j.cub.2009.07.053", "10.1007/s00221-008-1525-3", "10.1207/s15327108ijap0303_3", "10.3758/BF03194544", "10.1007/s00421-003-0906-3", "10.1186/1743-0003-2-28", "10.1037/0096-1523.18.4.906", "10.3758/APP.71.6.1284", "10.1177/0146167203029007002", "10.1037/0096-1523.33.1.183", "10.1016/j.cub.2009.07.053", "10.1007/s00221-008-1525-3", "10.1207/s15327108ijap0303_3", "10.3758/BF03194544", "10.1007/s00421-003-0906-3", "10.1186/1743-0003-2-28", "10.1037/0096-1523.18.4.906", "10.3758/APP.71.6.1284", "10.1177/0146167203029007002"]}, "10.1109/TVCG.2011.289": {"doi": "10.1109/TVCG.2011.289", "author": ["T. C. Peck", "H. Fuchs", "M. C. Whitton"], "title": "The Design and Evaluation of a Large-Scale Real-Walking Locomotion Interface", "year": "2012", "abstract": "Redirected Free Exploration with Distractors (RFEDs) is a large-scale real-walking locomotion interface developed to enable people to walk freely in Virtual Environments (VEs) that are larger than the tracked space in their facility. This paper describes the RFED system in detail and reports on a user study that evaluated RFED by comparing it to Walking-in-Place (WIP) and Joystick (JS) interfaces. The RFED system is composed of two major components, redirection and distractors. This paper discusses design challenges, implementation details, and lessons learned during the development of two working RFED systems. The evaluation study examined the effect of the locomotion interface on users' cognitive performance on navigation and wayfinding measures. The results suggest that participants using RFED were significantly better at navigating and wayfinding through virtual mazes than participants using walking-in-place and joystick interfaces. Participants traveled shorter distances, made fewer wrong turns, pointed to hidden targets more accurately and more quickly, and were able to place and label targets on maps more accurately, and more accurately estimate the virtual environment size.", "keywords": ["cognition", "interactive devices", "user interfaces", "virtual reality", "large-scale real-walking locomotion interface", "redirected free exploration with distractors", "virtual environments", "RFED system", "walking-in-place interfaces", "joystick interfaces", "user cognitive performance", "wayfinding measures", "navigation measures", "virtual mazes", "Prediction algorithms", "Legged locomotion", "Navigation", "Vectors", "Virtual environments", "Target tracking", "Virtual reality", "locomotion", "navigation", "redirection", "distractors", "wayfinding.", "Algorithms", "Analysis of Variance", "Computer Graphics", "Female", "Humans", "Male", "Psychomotor Performance", "Statistics, Nonparametric", "User-Computer Interface", "Walking"], "referenced_by": ["10.1109/3DUI.2014.6798851", "10.1109/3DUI.2015.7131717", "10.1109/3DUI.2015.7131718", "10.1109/CW.2012.10", "10.1109/VR.2015.7223354", "10.1109/VR.2015.7223357", "10.1109/VR.2015.7223404", "10.1109/VR.2016.7504715", "10.1109/WEVR.2016.7859537", "10.1109/TVCG.2016.2518298", "10.1109/VR.2019.8797777", "10.1109/VR.2019.8797751", "10.1109/TOH.2020.2965937", "10.1109/TVCG.2018.2887379", "10.1109/ACCESS.2020.3027985", "10.1145/2720020", "10.1145/3130800.3130893", "10.1145/3345554", "10.1007/978-3-319-08234-9_186-1", "10.1007/s10055-015-0267-3", "10.1016/j.apacoust.2016.04.007", "10.1016/j.future.2018.02.029", "10.1080/10447318.2013.796441", "10.3390/mti2020020", "10.1007/978-1-4419-8432-6_11", "10.1007/978-3-642-38803-3_10", "10.1007/978-1-4471-5445-7_4", "10.1007/978-3-030-22514-8_17", "10.1007/978-3-030-20476-1_28", "10.1007/978-3-030-22649-7_4", "10.1016/j.cag.2019.09.005", "10.1117/12.2545154"], "referencing": ["10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/VR.2008.4480761", "10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/VR.2008.4480761", "10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/VR.2008.4480761", "10.1145/238386.238459", "10.1145/210079.210084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/263407.263550", "10.1145/147156.147201", "10.1145/1140491.1140495", "10.1145/1272582.1272590", "10.1145/1394281.1394310", "10.1145/1836248.1836260", "10.1145/238386.238459", "10.1145/210079.210084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/263407.263550", "10.1145/147156.147201", "10.1145/1140491.1140495", "10.1145/1272582.1272590", "10.1145/1394281.1394310", "10.1145/1836248.1836260", "10.1145/238386.238459", "10.1145/210079.210084", "10.1145/311535.311589", "10.1145/1502800.1502805", "10.1145/263407.263550", "10.1145/147156.147201", "10.1145/1140491.1140495", "10.1145/1272582.1272590", "10.1145/1394281.1394310", "10.1145/1836248.1836260", "10.1518/001872098779591296", "10.1007/BF00896880", "10.1162/105474698565659", "10.1162/105474604774048225", "10.1162/pres.16.4.385", "10.1007/s00221-001-0983-7", "10.1162/105474698565631", "10.1006/ijhc.1996.0060", "10.1162/105474600566925", "10.1207/s15327108ijap0303_3", "10.1518/001872098779591296", "10.1007/BF00896880", "10.1162/105474698565659", "10.1162/105474604774048225", "10.1162/pres.16.4.385", "10.1007/s00221-001-0983-7", "10.1162/105474698565631", "10.1006/ijhc.1996.0060", "10.1162/105474600566925", "10.1207/s15327108ijap0303_3", "10.1518/001872098779591296", "10.1007/BF00896880", "10.1162/105474698565659", "10.1162/105474604774048225", "10.1162/pres.16.4.385", "10.1007/s00221-001-0983-7", "10.1162/105474698565631", "10.1006/ijhc.1996.0060", "10.1162/105474600566925", "10.1207/s15327108ijap0303_3"]}, "10.1109/TVCG.2011.274": {"doi": "10.1109/TVCG.2011.274", "author": ["G. Bruder", "F. Steinicke", "P. Wieland", "M. Lappe"], "title": "Tuning Self-Motion Perception in Virtual Reality with Visual Illusions", "year": "2012", "abstract": "Motion perception in immersive virtual environments significantly differs from the real world. For example, previous work has shown that users tend to underestimate travel distances in virtual environments (VEs). As a solution to this problem, researchers proposed to scale the mapped virtual camera motion relative to the tracked real-world movement of a user until real and virtual motion are perceived as equal, i.e., real-world movements could be mapped with a larger gain to the VE in order to compensate for the underestimation. However, introducing discrepancies between real and virtual motion can become a problem, in particular, due to misalignments of both worlds and distorted space cognition. In this paper, we describe a different approach that introduces apparent self-motion illusions by manipulating optic flow fields during movements in VEs. These manipulations can affect self-motion perception in VEs, but omit a quantitative discrepancy between real and virtual motions. In particular, we consider to which regions of the virtual view these apparent self-motion illusions can be applied, i.e., the ground plane or peripheral vision. Therefore, we introduce four illusions and show in experiments that optic flow manipulation can significantly affect users' self-motion judgments. Furthermore, we show that with such manipulations of optic flow fields the underestimation of travel distances can be compensated.", "keywords": ["image sensors", "image sequences", "motion estimation", "virtual reality", "visual perception", "self-motion perception tuning", "virtual reality", "visual illusions", "virtual environments", "travel distance underestimation", "mapped virtual camera motion", "underestimation compensation", "distorted space cognition", "self-motion illusions", "optic flow fields", "ground plane vision", "peripheral vision", "Visualization", "Optical sensors", "Cameras", "Optical distortion", "Blindness", "Stimulated emission", "Detectors", "Self-motion perception", "virtual environments", "visual illusions", "optic flow.", "Adult", "Computer Graphics", "Female", "Humans", "Illusions", "Male", "Motion Perception", "Optic Flow", "User-Computer Interface"], "referenced_by": ["10.1109/3DUI.2013.6550200", "10.1109/ISMAR.2013.6671765", "10.1109/MCG.2013.13", "10.1109/TVCG.2015.2391851", "10.1109/TVCG.2018.2793679", "10.1109/VR.2013.6549382", "10.1109/MCG.2018.111125628", "10.1109/TVCG.2017.2771520", "10.1109/ACCESS.2019.2898324", "10.1109/VR46266.2020.00055", "10.1145/2671015.2671026", "10.1145/2984511.2984545", "10.1145/3414685.3417773", "10.1016/j.displa.2012.10.007", "10.1155/2015/151702", "10.12688/f1000research.3557.2", "10.1007/978-3-642-39405-8_15", "10.1016/j.gaitpost.2020.04.018", "10.3389/fpsyg.2020.01604", "10.1007/978-3-030-62655-6_2", "10.3389/frvir.2020.581132"], "referencing": ["10.1109/TVCG.2009.62", "10.1109/VR.2002.996517", "10.1109/3DUI.2009.4811208", "10.1109/TVCG.2009.62", "10.1109/VR.2002.996517", "10.1109/3DUI.2009.4811208", "10.1109/TVCG.2009.62", "10.1109/VR.2002.996517", "10.1109/3DUI.2009.4811208", "10.1145/1152399.1152451", "10.1145/127719.122721", "10.1145/1227134.1227137", "10.1145/1152399.1152451", "10.1145/127719.122721", "10.1145/1227134.1227137", "10.1145/1152399.1152451", "10.1145/127719.122721", "10.1145/1227134.1227137", "10.1007/s100550200008", "10.1007/s00221-006-0835-6", "10.1016/S1364-6613(99)01364-9", "10.1007/s00221-008-1435-4", "10.3758/BF03193534", "10.1007/s002210050625", "10.1007/s00221-006-0598-0", "10.1162/105474605774785262", "10.1068/p241247", "10.1364/JOSAA.2.000284", "10.1016/j.visres.2005.12.022", "10.1016/S0042-6989(98)00191-6", "10.1016/0042-6989(93)90141-I", "10.1068/p150627", "10.1098/rspb.1980.0057", "10.1111/j.1467-9280.1997.tb00427.x", "10.1016/S0079-6123(02)40046-5", "10.1162/105474600566989", "10.1207/s15327108ijap0303_3", "10.1098/rspb.1999.0944", "10.1007/s100550200008", "10.1007/s00221-006-0835-6", "10.1016/S1364-6613(99)01364-9", "10.1007/s00221-008-1435-4", "10.3758/BF03193534", "10.1007/s002210050625", "10.1007/s00221-006-0598-0", "10.1162/105474605774785262", "10.1068/p241247", "10.1364/JOSAA.2.000284", "10.1016/j.visres.2005.12.022", "10.1016/S0042-6989(98)00191-6", "10.1016/0042-6989(93)90141-I", "10.1068/p150627", "10.1098/rspb.1980.0057", "10.1111/j.1467-9280.1997.tb00427.x", "10.1016/S0079-6123(02)40046-5", "10.1162/105474600566989", "10.1207/s15327108ijap0303_3", "10.1098/rspb.1999.0944", "10.1007/s100550200008", "10.1007/s00221-006-0835-6", "10.1016/S1364-6613(99)01364-9", "10.1007/s00221-008-1435-4", "10.3758/BF03193534", "10.1007/s002210050625", "10.1007/s00221-006-0598-0", "10.1162/105474605774785262", "10.1068/p241247", "10.1364/JOSAA.2.000284", "10.1016/j.visres.2005.12.022", "10.1016/S0042-6989(98)00191-6", "10.1016/0042-6989(93)90141-I", "10.1068/p150627", "10.1098/rspb.1980.0057", "10.1111/j.1467-9280.1997.tb00427.x", "10.1016/S0079-6123(02)40046-5", "10.1162/105474600566989", "10.1207/s15327108ijap0303_3", "10.1098/rspb.1999.0944"]}, "10.1109/TVCG.2011.114": {"doi": "10.1109/TVCG.2011.114", "author": ["M. Liao", "J. Gao", "R. Yang", "M. Gong"], "title": "Video Stereolization: Combining Motion Analysis with User Interaction", "year": "2012", "abstract": "We present a semiautomatic system that converts conventional videos into stereoscopic videos by combining motion analysis with user interaction, aiming to transfer as much as possible labeling work from the user to the computer. In addition to the widely used structure from motion (SFM) techniques, we develop two new methods that analyze the optical flow to provide additional qualitative depth constraints. They remove the camera movement restriction imposed by SFM so that general motions can be used in scene depth estimation-the central problem in mono-to-stereo conversion. With these algorithms, the user's labeling task is significantly simplified. We further developed a quadratic programming approach to incorporate both quantitative depth and qualitative depth (such as these from user scribbling) to recover dense depth maps for all frames, from which stereoscopic view can be synthesized. In addition to visual results, we present user study results showing that our approach is more intuitive and less labor intensive, while producing 3D effect comparable to that from current state-of-the-art interactive algorithms.", "keywords": ["image motion analysis", "image sensors", "image sequences", "interactive systems", "quadratic programming", "stereo image processing", "user interfaces", "video signal processing", "video stereolization", "motion analysis", "user interaction", "semiautomatic system", "stereoscopic videos", "structure-from-motion techniques", "SFM techniques", "optical flow analysis", "qualitative depth constraints", "camera movement restriction", "scene depth estimation", "mono-to-stereo conversion", "user labeling task", "quadratic programming", "quantitative depth", "user scribbling", "3D effect", "interactive algorithms", "Three dimensional displays", "Cameras", "Pixel", "Labeling", "Quadratic programming", "Image segmentation", "Image sequences", "Semiautomatic 2D-3D conversion", "stereo/3D video/movie", "motion analysis", "user labeling.", "Computer Graphics", "Depth Perception", "Humans", "Imaging, Three-Dimensional", "Motion", "Questionnaires", "Task Performance and Analysis", "User-Computer Interface", "Video Recording"], "referenced_by": ["10.1109/3DTV.2013.6676648", "10.1109/3DV.2013.18", "10.1109/3DV.2014.55", "10.1109/ICAECCT.2016.7942584", "10.1109/ICCIC.2015.7435781", "10.1109/ICCUBEA.2015.192", "10.1109/LSP.2014.2359643", "10.1109/TCE.2016.7838096", "10.1109/TIP.2013.2270375", "10.1109/TIP.2014.2315958", "10.1109/TMM.2014.2341599", "10.1109/TMM.2017.2748458", "10.1109/TPAMI.2014.2316835", "10.23919/ChiCC.2017.8029101", "10.1109/TIP.2018.2813093", "10.1109/TIP.2015.2495261", "10.1109/TIP.2018.2836318", "10.1109/IC3D.2018.8657860", "10.1002/9781118583593.ch14", "10.1007/s00371-013-0904-3", "10.1007/s00371-014-0961-2", "10.1007/s00371-014-0994-6", "10.1007/s00530-014-0375-z", "10.1007/s10586-017-1470-7", "10.1007/s11760-015-0833-x", "10.1007/s41095-017-0089-1", "10.1016/j.cviu.2015.01.001", "10.1016/j.image.2013.07.008", "10.1016/j.jvcir.2016.05.004", "10.1111/cgf.12249", "10.1111/cgf.12536", "10.1111/cgf.12757", "10.1111/j.1467-8659.2012.03214.x", "10.4018/978-1-4666-9685-3.ch004", "10.1117/1.OE.54.7.073103", "10.1007/978-3-642-33715-4_56", "10.1007/978-3-319-16631-5_23", "10.1007/978-3-319-23048-1_9"], "referencing": ["10.1109/TPAMI.2009.52", "10.1109/TCSVT.2005.852401", "10.1049/cp:20070061", "10.1109/ICCV.2007.4408828", "10.1109/TVCG.2009.47", "10.1109/TVCG.2007.1032", "10.1109/TPAMI.2009.52", "10.1109/TCSVT.2005.852401", "10.1049/cp:20070061", "10.1109/ICCV.2007.4408828", "10.1109/TVCG.2009.47", "10.1109/TVCG.2007.1032", "10.1109/TPAMI.2009.52", "10.1109/TCSVT.2005.852401", "10.1049/cp:20070061", "10.1109/ICCV.2007.4408828", "10.1109/TVCG.2009.47", "10.1109/TVCG.2007.1032", "10.1145/1186562.1015719", "10.1145/1276377.1276497", "10.1145/1186562.1015719", "10.1145/1276377.1276497", "10.1145/1186562.1015719", "10.1145/1276377.1276497", "10.1007/BF00129684", "10.1016/0004-3702(81)90024-2", "10.1006/cviu.1996.0006", "10.1111/j.1467-8659.2009.01631.x", "10.1007/BF00129684", "10.1016/0004-3702(81)90024-2", "10.1006/cviu.1996.0006", "10.1111/j.1467-8659.2009.01631.x", "10.1007/BF00129684", "10.1016/0004-3702(81)90024-2", "10.1006/cviu.1996.0006", "10.1111/j.1467-8659.2009.01631.x"]}, "10.1109/TVCG.2011.143": {"doi": "10.1109/TVCG.2011.143", "author": ["P. Rodgers", "L. Zhang", "H. Purchase"], "title": "Wellformedness Properties in Euler Diagrams: Which Should Be Used?", "year": "2012", "abstract": "Euler diagrams are often used to visualize intersecting data sets in applications such as criminology; genetics, medicine, and computer file systems. One interesting aspect of these diagrams is that some data sets cannot be drawn without breaking one or more \"wellformedness properties,\u201d which are considered to reduce the user comprehension of the diagrams. However, it is possible to draw the same data with different diagrams, each of which breaks different wellformedness properties. Hence, some properties are \"swappable,\u201d so motivating the study of which of the alternatives would be best to use. This paper reports on the two empirical studies to determine how wellformedness properties affect comprehension. One study was with abstract data, the other was with concrete data that visualized students' enrollment on university modules. We have results from both studies that imply that diagrams with concurrency or disconnected zones perform less well than other some other properties. Further, we have no results that imply that diagrams with brushing points adversely affect performance. Our data also indicate that nonsimple curves are preferred less than diagrams with other properties. These results will inform both human diagram designers and the developers of automated drawing systems on the best way to visualize data using Euler diagrams.", "keywords": ["data visualisation", "wellformedness properties", "Euler diagrams", "intersecting data sets visualization", "criminology", "genetics", "medicine", "computer file systems", "comprehension", "student enrollment visualization", "university modules", "brushing points", "human diagram designers", "automated drawing systems", "Handheld computers", "Decision support systems", "Euler diagrams", "Venn diagrams", "empirical studies", "information visualization.", "Color", "Computer Graphics", "Databases, Factual", "Humans", "Models, Theoretical", "Research Design", "Statistics as Topic"], "referenced_by": ["10.1109/COMSNETS.2017.7945455", "10.1109/ICGCIoT.2015.7380712", "10.1109/TVCG.2012.199", "10.1109/TVCG.2013.104", "10.1109/TVCG.2014.2315995", "10.1109/TVCG.2014.2346422", "10.1109/VLHCC.2013.6645262", "10.1109/COMSNETS.2016.7439961", "10.1109/ICTAI50040.2020.00185", "10.1145/2810012", "10.1007/s11225-017-9711-6", "10.1007/s11334-017-0298-x", "10.1016/j.ins.2015.05.020", "10.1016/j.ins.2016.05.045", "10.1016/j.jvlc.2013.08.006", "10.1016/j.jvlc.2013.08.007", "10.1016/j.jvlc.2013.08.009", "10.1016/j.jvlc.2014.09.002", "10.1111/cgf.12722", "10.1371/journal.pone.0101717", "10.1007/978-3-662-44043-8_18", "10.1007/978-3-662-44043-8_16", "10.1007/978-3-662-44043-8_15", "10.1007/978-3-030-54249-8_25"], "referencing": ["10.1109/IV.2003.1217967", "10.1109/MCG.2010.83", "10.1109/IV.2003.1217967", "10.1109/TVCG.2010.210", "10.1109/MCG.2010.83", "10.1109/IV.2003.1217967", "10.1109/TVCG.2010.210", "10.1109/MCG.2010.83", "10.1016/j.jvlc.2011.01.002", "10.1007/3-540-46037-3_6", "10.1093/bioinformatics/bti169", "10.1007/978-3-540-87730-1_6", "10.1111/j.1467-8659.2009.01452.x", "10.1378/chest.124.2.474", "10.1037/0096-3445.114.3.285", "10.1080/14786448008626877", "10.1016/j.jvlc.2011.01.002", "10.1007/3-540-46037-3_6", "10.1093/bioinformatics/bti169", "10.1007/978-3-540-87730-1_6", "10.1111/j.1467-8659.2009.01452.x", "10.1378/chest.124.2.474", "10.1037/0096-3445.114.3.285", "10.1080/14786448008626877", "10.1016/j.jvlc.2011.01.002", "10.1007/3-540-46037-3_6", "10.1093/bioinformatics/bti169", "10.1007/978-3-540-87730-1_6", "10.1111/j.1467-8659.2009.01452.x", "10.1378/chest.124.2.474", "10.1037/0096-3445.114.3.285", "10.1080/14786448008626877"]}, "10.1109/TVCG.2011.132": {"doi": "10.1109/TVCG.2011.132", "author": ["A. Kotranza", "D. S. Lind", "B. Lok"], "title": "Real-Time Evaluation and Visualization of Learner Performance in a Mixed-Reality Environment for Clinical Breast Examination", "year": "2012", "abstract": "We investigate the efficacy of incorporating real-time feedback of user performance within mixed-reality environments (MREs) for training real-world tasks with tightly coupled cognitive and psychomotor components. This paper presents an approach to providing real-time evaluation and visual feedback of learner performance in an MRE for training clinical breast examination (CBE). In a user study of experienced and novice CBE practitioners (n = 69), novices receiving real-time feedback performed equivalently or better than more experienced practitioners in the completeness and correctness of the exam. A second user study (n = 8) followed novices through repeated practice of CBE in the MRE. Results indicate that skills improvement in the MRE transfers to the real-world task of CBE of human patients. This initial case study demonstrates the efficacy of MREs incorporating real-time feedback for training real-world cognitive-psychomotor tasks.", "keywords": ["biomedical education", "computer based training", "data visualisation", "medical computing", "patient diagnosis", "virtual reality", "real-time evaluation", "learner performance visualization", "mixed-reality environment", "clinical breast examination", "real-time user performance feedback", "real-world tasks training", "tightly coupled cognitive components", "psychomotor components", "visual feedback", "MRE", "human patients", "real-world cognitive-psychomotor tasks", "Breast", "Sensors", "Real time systems", "Visualization", "Training", "Data models", "Humans", "Mixed and augmented reality", "information visualization", "life and medical sciences.", "Breast", "Computer Graphics", "Computer-Assisted Instruction", "Feedback, Sensory", "Female", "Humans", "Medical Informatics Applications", "Models, Anatomic", "Palpation", "Pressure", "User-Computer Interface"], "referenced_by": ["10.1109/CBMS.2016.74", "10.1007/978-3-319-74310-3_25", "10.1016/j.compedu.2018.03.018", "10.1016/j.ijhcs.2015.08.006", "10.1111/resp.13074", "10.1007/s12008-019-00546-x", "10.1007/s11596-019-1992-8", "10.1002/rcs.2120", "10.3389/frvir.2020.577534"], "referencing": ["10.1109/ISMAR.2005.57", "10.1109/MMUL.2007.79", "10.1109/TVCG.2008.195", "10.1109/ISMAR.2005.5", "10.1109/ISMAR.2009.5336486", "10.1109/ISMAR.2005.31", "10.1109/ISMAR.2009.5336485", "10.1109/ISMAR.2005.57", "10.1109/MMUL.2007.79", "10.1109/TVCG.2008.195", "10.1109/ISMAR.2005.5", "10.1109/ISMAR.2009.5336486", "10.1109/ISMAR.2005.31", "10.1109/ISMAR.2009.5336485", "10.1109/ISMAR.2005.57", "10.1109/MMUL.2007.79", "10.1109/TVCG.2008.195", "10.1109/ISMAR.2005.5", "10.1109/ISMAR.2009.5336486", "10.1109/ISMAR.2005.31", "10.1109/ISMAR.2009.5336485", "10.1145/642625.642626", "10.1145/1518701.1518724", "10.1145/642625.642626", "10.1145/1518701.1518724", "10.1145/642625.642626", "10.1145/1518701.1518724", "10.1007/BF02598289", "10.1080/00222899709600829", "10.3322/canjclin.54.6.327", "10.1001/jama.282.13.1270", "10.1089/cpb.2005.8.187", "10.2307/1169513", "10.1162/1054746053967094", "10.1016/S0002-9610(02)01094-2", "10.1016/j.amjsurg.2006.12.033", "10.3322/canjclin.54.6.345", "10.1097/00001888-199808000-00017", "10.1007/BF02303525", "10.1016/S0002-9610(98)00075-0", "10.1016/j.amjsurg.2008.11.025", "10.1152/advan.00045.2005", "10.7326/0003-4819-112-10-772", "10.1037/a0015085", "10.1518/107118193784162254", "10.1097/00001888-200209000-00022", "10.1007/BF02598289", "10.1080/00222899709600829", "10.3322/canjclin.54.6.327", "10.1001/jama.282.13.1270", "10.1089/cpb.2005.8.187", "10.2307/1169513", "10.1162/1054746053967094", "10.1016/S0002-9610(02)01094-2", "10.1016/j.amjsurg.2006.12.033", "10.3322/canjclin.54.6.345", "10.1097/00001888-199808000-00017", "10.1007/BF02303525", "10.1016/S0002-9610(98)00075-0", "10.1016/j.amjsurg.2008.11.025", "10.1152/advan.00045.2005", "10.7326/0003-4819-112-10-772", "10.1037/a0015085", "10.1518/107118193784162254", "10.1097/00001888-200209000-00022", "10.1007/BF02598289", "10.1080/00222899709600829", "10.3322/canjclin.54.6.327", "10.1001/jama.282.13.1270", "10.1089/cpb.2005.8.187", "10.2307/1169513", "10.1162/1054746053967094", "10.1016/S0002-9610(02)01094-2", "10.1016/j.amjsurg.2006.12.033", "10.3322/canjclin.54.6.345", "10.1097/00001888-199808000-00017", "10.1007/BF02303525", "10.1016/S0002-9610(98)00075-0", "10.1016/j.amjsurg.2008.11.025", "10.1152/advan.00045.2005", "10.7326/0003-4819-112-10-772", "10.1037/a0015085", "10.1518/107118193784162254", "10.1097/00001888-200209000-00022"]}, "10.1109/TVCG.2011.117": {"doi": "10.1109/TVCG.2011.117", "author": ["H. Yu", "T. Lee", "I. Yeh", "X. Yang", "W. Li", "J. J. Zhang"], "title": "An RBF-Based Reparameterization Method for Constrained Texture Mapping", "year": "2012", "abstract": "Texture mapping has long been used in computer graphics to enhance the realism of virtual scenes. However, to match the 3D model feature points with the corresponding pixels in a texture image, surface parameterization must satisfy specific positional constraints. However, despite numerous research efforts, the construction of a mathematically robust, foldover-free parameterization that is subject to positional constraints continues to be a challenge. In the present paper, this foldover problem is addressed by developing radial basis function (RBF)-based reparameterization. Given initial 2D embedding of a 3D surface, the proposed method can reparameterize 2D embedding into a foldover-free 2D mesh, satisfying a set of user-specified constraint points. In addition, this approach is mesh free. Therefore, generating smooth texture mapping results is possible without extra smoothing optimization.", "keywords": ["feature extraction", "image enhancement", "image matching", "image texture", "iterative methods", "mesh generation", "radial basis function networks", "solid modelling", "RBF-based reparameterization method", "constrained texture mapping", "computer graphics", "virtual scenes", "3D model feature point matching", "texture image", "surface parameterization", "positional constraints", "foldover-free parameterization", "radial basis function-based reparameterization", "2D embedding", "3D surface", "foldover-free 2D mesh", "user specified constraint points", "smooth texture mapping", "Three dimensional displays", "Solid modeling", "Approximation methods", "Mesh generation", "Computational modeling", "Equations", "Smoothing methods", "Foldover", "constrained texture mapping", "reparameterization."], "referenced_by": ["10.1109/CADGraphics.2013.37", "10.1109/TVCG.2014.2366101", "10.1109/PIC.2018.8706136", "10.1145/2461912.2462014", "10.1007/s10916-015-0397-x", "10.1007/s12008-015-0276-1", "10.1016/j.gmod.2013.11.001", "10.1039/C3FD00145H", "10.1080/13658816.2015.1127377", "10.1111/cgf.12452", "10.1007/978-3-319-27863-6_70", "10.3390/math7080753", "10.1016/j.cagd.2020.101909"], "referencing": ["10.1109/TVCG.2007.70432", "10.1109/2945.856998", "10.1109/SMI.2004.1314507", "10.1109/SMI.2004.1314506", "10.1109/TVCG.2003.1175099", "10.1109/TVCG.2007.70432", "10.1109/2945.856998", "10.1109/SMI.2004.1314507", "10.1109/SMI.2004.1314506", "10.1109/TVCG.2003.1175099", "10.1109/TVCG.2007.70432", "10.1109/2945.856998", "10.1109/SMI.2004.1314507", "10.1109/SMI.2004.1314506", "10.1109/TVCG.2003.1175099", "10.1145/882262.882271", "10.1145/383259.383308", "10.1145/383259.383307", "10.1145/1073204.1073325", "10.1145/218380.218440", "10.1145/566570.566590", "10.1145/280814.280930", "10.1145/344779.344990", "10.1145/1061347.1061354", "10.1145/383259.383266", "10.1145/1037957.1037958", "10.1145/882262.882271", "10.1145/383259.383308", "10.1145/383259.383307", "10.1145/1073204.1073325", "10.1145/218380.218440", "10.1145/566570.566590", "10.1145/280814.280930", "10.1145/344779.344990", "10.1145/1061347.1061354", "10.1145/383259.383266", "10.1145/1037957.1037958", "10.1145/882262.882271", "10.1145/383259.383308", "10.1145/383259.383307", "10.1145/1073204.1073325", "10.1145/218380.218440", "10.1145/566570.566590", "10.1145/280814.280930", "10.1145/344779.344990", "10.1145/1061347.1061354", "10.1145/383259.383266", "10.1145/1037957.1037958", "10.1111/1467-8659.00580", "10.1016/S0167-8396(96)00031-3", "10.1111/1467-8659.00502", "10.1111/1467-8659.00580", "10.1111/j.1467-8659.2009.01386.x", "10.1016/j.cag.2005.09.013", "10.1016/S0097-8493(03)00036-0", "10.1007/3-540-26808-1_9", "10.1007/PL00013391", "10.1006/gmip.1998.0454", "10.1016/S0097-8493(00)00107-2", "10.1215/S0012-7094-63-03008-4", "10.1111/1467-8659.00580", "10.1016/S0167-8396(96)00031-3", "10.1111/1467-8659.00502", "10.1111/1467-8659.00580", "10.1111/j.1467-8659.2009.01386.x", "10.1016/j.cag.2005.09.013", "10.1016/S0097-8493(03)00036-0", "10.1007/3-540-26808-1_9", "10.1007/PL00013391", "10.1006/gmip.1998.0454", "10.1016/S0097-8493(00)00107-2", "10.1215/S0012-7094-63-03008-4", "10.1111/1467-8659.00580", "10.1016/S0167-8396(96)00031-3", "10.1111/1467-8659.00502", "10.1111/1467-8659.00580", "10.1111/j.1467-8659.2009.01386.x", "10.1016/j.cag.2005.09.013", "10.1016/S0097-8493(03)00036-0", "10.1007/3-540-26808-1_9", "10.1007/PL00013391", "10.1006/gmip.1998.0454", "10.1016/S0097-8493(00)00107-2", "10.1215/S0012-7094-63-03008-4"]}, "10.1109/TVCG.2011.131": {"doi": "10.1109/TVCG.2011.131", "author": ["O. Kin-Chung Au", "Y. Zheng", "M. Chen", "P. Xu", "C. Tai"], "title": "Mesh Segmentation with Concavity-Aware Fields", "year": "2012", "abstract": "This paper presents a simple and efficient automatic mesh segmentation algorithm that solely exploits the shape concavity information. The method locates concave creases and seams using a set of concavity-sensitive scalar fields. These fields are computed by solving a Laplacian system with a novel concavity-sensitive weighting scheme. Isolines sampled from the concavity-aware fields naturally gather at concave seams, serving as good cutting boundary candidates. In addition, the fields provide sufficient information allowing efficient evaluation of the candidate cuts. We perform a summarization of all field gradient magnitudes to define a score for each isoline and employ a score-based greedy algorithm to select the best cuts. Extensive experiments and quantitative analysis have shown that the quality of our segmentations are better than or comparable with existing state-of-the-art more complex approaches.", "keywords": ["gradient methods", "greedy algorithms", "mesh generation", "solid modelling", "concavity-aware fields", "automatic mesh segmentation algorithm", "shape concavity information", "concave creases", "concave seams", "concavity-sensitive scalar fields", "Laplacian system", "concavity-sensitive weighting scheme", "isolines", "cutting boundary candidates", "field gradient magnitudes", "score-based greedy algorithm", "3D models", "Shape", "Solid modeling", "Computational modeling", "Face", "Laplace equations", "Extremities", "Boundary conditions", "Concavity-aware field", "mesh segmentation", "isolines."], "referenced_by": ["10.1109/3DV.2016.29", "10.1109/ICCSNT.2013.6967065", "10.1109/ICCV.2015.204", "10.1109/IS3C.2016.88", "10.1109/TASE.2014.2317497", "10.1109/TMI.2012.2216542", "10.1109/TPAMI.2016.2544311", "10.1109/TVCG.2011.140", "10.1109/TVCG.2013.60", "10.1109/TVCG.2015.2498557", "10.1109/WACV.2016.7477626", "10.1109/TVCG.2017.2767047", "10.1109/TVCG.2018.2839685", "10.1109/ICVRV.2017.00028", "10.1109/TVCG.2018.2882212", "10.1109/ACCESS.2020.2976847", "10.1109/TIP.2020.2980962", "10.1109/TVCG.2019.2892076", "10.1109/TVCG.2019.2896310", "10.1145/2611811", "10.1145/3272127.3275009", "10.1145/3272127.3275029", "10.1007/978-3-319-54042-9_59", "10.1007/s00371-015-1198-4", "10.1007/s00371-017-1434-1", "10.1007/s11042-017-5287-4", "10.1007/s41095-016-0071-3", "10.1016/j.cad.2016.05.015", "10.1016/j.cad.2018.02.001", "10.1016/j.cag.2013.05.021", "10.1016/j.cag.2015.05.012", "10.1016/j.cag.2015.07.018", "10.1016/j.cag.2018.01.004", "10.1016/j.cagd.2016.02.015", "10.1016/j.cam.2017.05.007", "10.1016/j.cmpb.2017.03.005", "10.1016/j.compbiomed.2014.10.013", "10.1016/j.cviu.2014.12.008", "10.1016/j.gmod.2014.04.009", "10.1016/j.neucom.2015.06.115", "10.1016/j.sigpro.2014.05.025", "10.1108/RPJ-07-2015-0091", "10.1111/cgf.13323", "10.1155/2015/187173", "10.1371/journal.pone.0161159", "10.1016/j.gmod.2018.07.003", "10.1007/s00371-017-1419-0", "10.1117/1.JEI.25.3.033011", "10.1111/cgf.13641"], "referencing": ["10.1109/TMM.2006.886344", "10.1109/SMI.2007.33", "10.1109/TVCG.2007.19", "10.1109/TMM.2006.886344", "10.1109/SMI.2007.33", "10.1109/TVCG.2007.19", "10.1109/TMM.2006.886344", "10.1109/SMI.2007.33", "10.1109/TVCG.2007.19", "10.1145/882262.882369", "10.1145/1409060.1409098", "10.1145/1833349.1778839", "10.1145/1531326.1531379", "10.1145/311535.311576", "10.1145/1661412.1618482", "10.1145/882262.882369", "10.1145/1409060.1409098", "10.1145/1833349.1778839", "10.1145/1531326.1531379", "10.1145/311535.311576", "10.1145/1661412.1618482", "10.1145/882262.882369", "10.1145/1409060.1409098", "10.1145/1833349.1778839", "10.1145/1531326.1531379", "10.1145/311535.311576", "10.1145/1661412.1618482", "10.1007/s00371-005-0344-9", "10.1016/j.cagd.2008.09.007", "10.1016/0010-0277(84)90022-2", "10.1016/S0010-0277(96)00791-3", "10.1111/1467-8659.00581", "10.1007/s00371-006-0375-x", "10.1111/j.1467-8659.2007.01061.x", "10.1007/s00371-007-0197-5", "10.1111/j.1467-8659.2009.01379.x", "10.1111/j.1467-8659.2009.01621.x", "10.1111/j.1467-8659.2009.01622.x", "10.1111/j.1467-8659.2008.01283.x", "10.1111/j.1467-8659.2008.01271.x", "10.1007/s00371-005-0344-9", "10.1016/j.cagd.2008.09.007", "10.1016/0010-0277(84)90022-2", "10.1016/S0010-0277(96)00791-3", "10.1111/1467-8659.00581", "10.1007/s00371-006-0375-x", "10.1111/j.1467-8659.2007.01061.x", "10.1007/s00371-007-0197-5", "10.1111/j.1467-8659.2009.01379.x", "10.1111/j.1467-8659.2009.01621.x", "10.1111/j.1467-8659.2009.01622.x", "10.1111/j.1467-8659.2008.01283.x", "10.1111/j.1467-8659.2008.01271.x", "10.1007/s00371-005-0344-9", "10.1016/j.cagd.2008.09.007", "10.1016/0010-0277(84)90022-2", "10.1016/S0010-0277(96)00791-3", "10.1111/1467-8659.00581", "10.1007/s00371-006-0375-x", "10.1111/j.1467-8659.2007.01061.x", "10.1007/s00371-007-0197-5", "10.1111/j.1467-8659.2009.01379.x", "10.1111/j.1467-8659.2009.01621.x", "10.1111/j.1467-8659.2009.01622.x", "10.1111/j.1467-8659.2008.01283.x", "10.1111/j.1467-8659.2008.01271.x"]}, "10.1109/TVCG.2011.134": {"doi": "10.1109/TVCG.2011.134", "author": ["S. Lee", "T. Park", "J. Kim", "C. Kim"], "title": "Adaptive Synthesis of Distance Fields", "year": "2012", "abstract": "We address the computational resource requirements of 3D example-based synthesis with an adaptive synthesis technique that uses a tree-based synthesis map. A signed-distance field (SDF) is determined for the 3D exemplars, and then new models can be synthesized as SDFs by neighborhood matching. Unlike voxel synthesis approach, our input is posed in the real domain to preserve maximum detail. In comparison to straightforward extensions to the existing volume texture synthesis approach, we made several improvements in terms of memory requirements, computation times, and synthesis quality. The inherent parallelism in this method makes it suitable for a multicore CPU. Results show that computation times and memory requirements are very much reduced, and large synthesized scenes exhibit fine details which mimic the exemplars.", "keywords": ["computer games", "image matching", "image texture", "multiprocessing systems", "resource allocation", "trees (mathematics)", "distance fields adaptive synthesis", "computational resource requirements", "3D example-based synthesis", "tree-based synthesis map", "signed-distance field", "SDF", "neighborhood matching", "texture synthesis approach", "memory requirements", "computation times", "synthesis quality", "multicore CPU", "Three dimensional displays", "Shape", "Octrees", "Optimization", "Adaptation models", "Memory management", "Jitter", "3D shape synthesis", "example-based synthesis."], "referenced_by": ["10.1007/s41095-016-0064-2"], "referencing": ["10.1109/SMI.2005.24", "10.1109/TIP.2004.833105", "10.1109/TVCG.2005.49", "10.1109/SMI.2005.24", "10.1109/TIP.2004.833105", "10.1109/TVCG.2005.49", "10.1109/SMI.2005.24", "10.1109/TIP.2004.833105", "10.1109/TVCG.2005.49", "10.1145/1073204.1073261", "10.1145/1276377.1276390", "10.1145/1015706.1015814", "10.1145/1057432.1057437", "10.1145/1141911.1141942", "10.1145/1409060.1409111", "10.1145/364338.364405", "10.1145/882262.882293", "10.1145/344779.344899", "10.1145/1399504.1360642", "10.1145/1833349.1778841", "10.1145/1111411.1111432", "10.1145/566570.566634", "10.1145/383259.383295", "10.1145/566570.566636", "10.1145/1186562.1015816", "10.1145/566570.566586", "10.1145/1465482.1465560", "10.1145/1073204.1073261", "10.1145/1276377.1276390", "10.1145/1015706.1015814", "10.1145/1057432.1057437", "10.1145/1141911.1141942", "10.1145/1409060.1409111", "10.1145/364338.364405", "10.1145/882262.882293", "10.1145/344779.344899", "10.1145/1399504.1360642", "10.1145/1833349.1778841", "10.1145/1111411.1111432", "10.1145/566570.566634", "10.1145/383259.383295", "10.1145/566570.566636", "10.1145/1186562.1015816", "10.1145/566570.566586", "10.1145/1465482.1465560", "10.1145/1073204.1073261", "10.1145/1276377.1276390", "10.1145/1015706.1015814", "10.1145/1057432.1057437", "10.1145/1141911.1141942", "10.1145/1409060.1409111", "10.1145/364338.364405", "10.1145/882262.882293", "10.1145/344779.344899", "10.1145/1399504.1360642", "10.1145/1833349.1778841", "10.1145/1111411.1111432", "10.1145/566570.566634", "10.1145/383259.383295", "10.1145/566570.566636", "10.1145/1186562.1015816", "10.1145/566570.566586", "10.1145/1465482.1465560", "10.1111/j.1467-8659.2008.01254.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2007.01062.x", "10.1111/j.1467-8659.2008.01254.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2007.01062.x", "10.1111/j.1467-8659.2008.01254.x", "10.1111/j.1467-8659.2004.00787.x", "10.1111/j.1467-8659.2007.01062.x"]}, "10.1109/TVCG.2011.120": {"doi": "10.1109/TVCG.2011.120", "author": ["X. Zhang", "Y. J. Kim"], "title": "Simple Culling Methods for Continuous Collision Detection of Deforming Triangles", "year": "2012", "abstract": "We present a simple and efficient approach for continuous collision detection of deforming triangles based on conservative advancement. The efficiency of our approach is due to a sequence of simple collision-free conditions for deforming triangles. In our experiment, we show that our CCD algorithm achieves 2-30 times performance improvement over existing algorithms for triangle primitives.", "keywords": ["computer graphics", "simple culling methods", "continuous collision detection", "Triangle deformation", "conservative advancement", "CCD algorithm", "Charge coupled devices", "Mathematical model", "Equations", "Face", "Heuristic algorithms", "Acceleration", "Solid modeling", "Continuous collision detection", "conservative advancement", "distance computation."], "referenced_by": ["10.1109/CADGRAPHICS.2015.32", "10.1109/ICMA.2015.7237581", "10.1145/2668956.2668959", "10.1145/2856317", "10.1145/2766907", "10.1145/2461912.2461951", "10.1007/s00371-014-1056-9", "10.1016/j.gmod.2015.06.001", "10.1111/cgf.12284", "10.1111/cgf.12736", "10.1002/nla.2323"], "referencing": ["10.1109/TPAMI.1986.4767773", "10.1109/TRO.2005.862479", "10.1109/2945.722297", "10.1109/TPAMI.1986.4767773", "10.1109/TRO.2005.862479", "10.1109/2945.722297", "10.1109/TPAMI.1986.4767773", "10.1109/TRO.2005.862479", "10.1109/2945.722297", "10.1145/1275808.1276396", "10.1145/1730804.1730806", "10.1145/781606.781612", "10.1145/336154.336219", "10.1145/566570.566623", "10.1145/1342250.1342260", "10.1145/1364901.1364908", "10.1145/1186822.1073301", "10.1145/1833349.1778818", "10.1145/1833349.1778817", "10.1145/1198555.1198788", "10.1145/1275808.1276396", "10.1145/1730804.1730806", "10.1145/781606.781612", "10.1145/336154.336219", "10.1145/566570.566623", "10.1145/1342250.1342260", "10.1145/1364901.1364908", "10.1145/1186822.1073301", "10.1145/1833349.1778818", "10.1145/1833349.1778817", "10.1145/1198555.1198788", "10.1145/1275808.1276396", "10.1145/1730804.1730806", "10.1145/781606.781612", "10.1145/336154.336219", "10.1145/566570.566623", "10.1145/1342250.1342260", "10.1145/1364901.1364908", "10.1145/1186822.1073301", "10.1145/1833349.1778818", "10.1145/1833349.1778817", "10.1145/1198555.1198788", "10.1007/s00371-006-0060-0", "10.1111/j.1467-8659.2009.01556.x", "10.1007/s00371-008-0235-y", "10.1002/cav.173", "10.1016/j.cag.2006.09.005", "10.1111/1467-8659.00543", "10.1007/s00371-006-0060-0", "10.1111/j.1467-8659.2009.01556.x", "10.1007/s00371-008-0235-y", "10.1002/cav.173", "10.1016/j.cag.2006.09.005", "10.1111/1467-8659.00543", "10.1007/s00371-006-0060-0", "10.1111/j.1467-8659.2009.01556.x", "10.1007/s00371-008-0235-y", "10.1002/cav.173", "10.1016/j.cag.2006.09.005", "10.1111/1467-8659.00543"]}, "10.1109/TVCG.2011.111": {"doi": "10.1109/TVCG.2011.111", "author": ["L. Zhang", "H. Huang", "H. Fu"], "title": "EXCOL: An EXtract-and-COmplete Layering Approach to Cartoon Animation Reusing", "year": "2012", "abstract": "We introduce the EXtract-and-COmplete Layering method (EXCOL)-a novel cartoon animation processing technique to convert a traditional animated cartoon video into multiple semantically meaningful layers. Our technique is inspired by vision-based layering techniques but focuses on shape cues in both the extraction and completion steps to reflect the unique characteristics of cartoon animation. For layer extraction, we define a novel similarity measure incorporating both shape and color of automatically segmented regions within individual frames and propagate a small set of user-specified layer labels among similar regions across frames. By clustering regions with the same labels, each frame is appropriately partitioned into different layers, with each layer containing semantically meaningful content. Then, a warping-based approach is used to fill missing parts caused by occlusion within the extracted layers to achieve a complete representation. EXCOL provides a flexible way to effectively reuse traditional cartoon animations with only a small amount of user interaction. It is demonstrated that our EXCOL method is effective and robust, and the layered representation benefits a variety of applications in cartoon animation processing.", "keywords": ["computer animation", "extract-and-complete layering approach", "EXCOL", "cartoon animation reusing", "cartoon animation processing technique", "animated cartoon video", "vision based layering techniques", "shape cues", "Animation", "Shape", "Image color analysis", "Color", "Pixel", "Image segmentation", "Feature extraction", "Cartoon animation", "layer extraction", "layer completion", "label propagation."], "referenced_by": ["10.1109/CSE.2014.314", "10.1109/TVCG.2013.2297931", "10.1109/TVCG.2014.2360406", "10.1109/TVCG.2016.2574705", "10.1109/TVCG.2017.2705182", "10.1145/3197517.3201326", "10.1145/2897824.2925872", "10.1007/s41095-016-0074-0", "10.1016/j.neucom.2013.03.042", "10.1016/j.sigpro.2012.01.028"], "referencing": ["10.1109/TVCG.2009.9", "10.1109/TPAMI.2005.202", "10.1109/83.334981", "10.1109/34.993558", "10.1109/TPAMI.2004.1262177", "10.1109/34.121791", "10.1109/TVCG.2009.9", "10.1109/TPAMI.2005.202", "10.1109/83.334981", "10.1109/34.993558", "10.1109/TPAMI.2004.1262177", "10.1109/34.121791", "10.1109/TVCG.2009.9", "10.1109/TPAMI.2005.202", "10.1109/83.334981", "10.1109/34.993558", "10.1109/TPAMI.2004.1262177", "10.1109/34.121791", "10.1145/566654.566595", "10.1145/1015706.1015763", "10.1145/1572614.1572619", "10.1145/1028523.1028559", "10.1145/1809939.1809955", "10.1145/1073204.1073233", "10.1145/1073204.1073234", "10.1145/1531326.1531376", "10.1145/1531326.1531390", "10.1145/882262.882267", "10.1145/1015706.1015719", "10.1145/358669.358692", "10.1145/566654.566595", "10.1145/1015706.1015763", "10.1145/1572614.1572619", "10.1145/1028523.1028559", "10.1145/1809939.1809955", "10.1145/1073204.1073233", "10.1145/1073204.1073234", "10.1145/1531326.1531376", "10.1145/1531326.1531390", "10.1145/882262.882267", "10.1145/1015706.1015719", "10.1145/358669.358692", "10.1145/566654.566595", "10.1145/1015706.1015763", "10.1145/1572614.1572619", "10.1145/1028523.1028559", "10.1145/1809939.1809955", "10.1145/1073204.1073233", "10.1145/1073204.1073234", "10.1145/1531326.1531376", "10.1145/1531326.1531390", "10.1145/882262.882267", "10.1145/1015706.1015719", "10.1145/358669.358692", "10.1016/j.imavis.2005.05.010", "10.1007/s00371-005-0313-3", "10.1111/j.1467-8659.2009.01400.x", "10.1007/s11263-006-7934-5", "10.1016/S0262-8856(03)00137-9", "10.1111/j.1467-8659.2009.01631.x", "10.1016/j.imavis.2005.05.010", "10.1007/s00371-005-0313-3", "10.1111/j.1467-8659.2009.01400.x", "10.1007/s11263-006-7934-5", "10.1016/S0262-8856(03)00137-9", "10.1111/j.1467-8659.2009.01631.x", "10.1016/j.imavis.2005.05.010", "10.1007/s00371-005-0313-3", "10.1111/j.1467-8659.2009.01400.x", "10.1007/s11263-006-7934-5", "10.1016/S0262-8856(03)00137-9", "10.1111/j.1467-8659.2009.01631.x"]}, "10.1109/TVCG.2011.127": {"doi": "10.1109/TVCG.2011.127", "author": ["C. Healey", "J. Enns"], "title": "Attention and Visual Memory in Visualization and Computer Graphics", "year": "2012", "abstract": "A fundamental goal of visualization is to produce images of data that support visual analysis, exploration, and discovery of novel insights. An important consideration during visualization design is the role of human visual perception. How we \"see\u201d details in an image can directly impact a viewer's efficiency and effectiveness. This paper surveys research on attention and visual perception, with a specific focus on results that have direct relevance to visualization and visual analytics. We discuss theories of low-level visual perception, then show how these findings form a foundation for more recent work on visual memory and visual attention. We conclude with a brief overview of how knowledge of visual attention and visual memory is being applied in visualization and graphics. We also discuss how challenges in visualization are motivating research in psychophysics.", "keywords": ["data analysis", "data visualisation", "visual perception", "visual memory", "attention", "computer graphics", "visual analysis", "visualization design", "human visual perception", "visual analytics", "visual attention", "psychophysics", "Visualization", "Feature extraction", "Humans", "Bars", "Visual perception", "Data visualization", "Attention", "color", "motion", "nonphotorealism", "texture", "visual memory", "visual perception", "visualization.", "Attention", "Computer Graphics", "Humans", "Memory", "Models, Theoretical", "Pattern Recognition, Visual", "Psychophysics", "Research", "Visual Perception"], "referenced_by": ["10.1109/BDVA.2016.7787041", "10.1109/BDVA.2017.8114627", "10.1109/CBI.2016.18", "10.1109/CCDC.2017.7978845", "10.1109/ICACCI.2017.8125876", "10.1109/ICCWAMTIP.2016.8079893", "10.1109/ICIP.2016.7532325", "10.1109/ICIP.2017.8296864", "10.1109/ICPR.2014.411", "10.1109/IMMERSIVE.2016.7932381", "10.1109/ISC2.2017.8090852", "10.1109/IV.2016.28", "10.1109/PACIFICVIS.2015.7156363", "10.1109/SMC.2016.7844371", "10.1109/TASE.2012.2214772", "10.1109/TVCG.2014.2346352", "10.1109/TVCG.2015.2467321", "10.1109/TVCG.2016.2598898", "10.1109/TVCG.2017.2744686", "10.1109/TVCG.2017.2745105", "10.1109/VIZSEC.2015.7312766", "10.1109/VLHCC.2013.6645262", "10.1109/iV.2017.44", "10.1109/TVCG.2015.2467759", "10.4108/icst.pervasivehealth.2013.252130", "10.1109/ICIP.2018.8451660", "10.1109/ICCI-CC.2018.8482037", "10.1109/TVCG.2018.2864498", "10.1109/MIPR.2019.00041", "10.1109/VIZSEC.2018.8709181", "10.1007/978-3-319-39396-4_29", "10.1007/978-3-319-39567-8_7", "10.1007/978-3-319-58706-6_9", "10.1007/978-3-319-59424-8_21", "10.1007/978-3-319-59424-8_22", "10.1007/978-3-319-66435-4_6", "10.1007/s00267-015-0615-9", "10.1007/s00799-016-0168-4", "10.1007/s11225-017-9711-6", "10.1016/B978-0-12-809715-1.00007-9", "10.1016/j.aei.2017.03.007", "10.1016/j.cag.2014.02.005", "10.1016/j.cag.2014.03.002", "10.1016/j.cageo.2017.07.006", "10.1016/j.conengprac.2018.03.008", "10.1016/j.dss.2017.04.004", "10.1016/j.eswa.2017.10.003", "10.1016/j.ipl.2018.03.004", "10.1016/j.jvlc.2017.09.007", "10.1016/j.neucom.2015.04.055", "10.1016/j.neucom.2015.10.111", "10.1061/(ASCE)CO.1943-7862.0000687", "10.1080/10618600.2014.951547", "10.1080/23729333.2017.1301346", "10.1093/bioinformatics/btw525", "10.1111/cgf.12092", "10.1111/cgf.12877", "10.1111/cgf.12927", "10.1111/cgf.12929", "10.1111/cgf.12936"], "referencing": ["10.1109/2945.773807", "10.1109/38.7760", "10.1109/VISUAL.1996.568118", "10.1109/TVCG.2008.112", "10.1109/MCG.2000.888001", "10.1109/INFVIS.2001.963274", "10.1145/217853.217855", "10.1145/200972.200974", "10.1145/383745.383748", "10.1145/987657.987669", "10.1145/1559755.1559757", "10.1145/604471.604483", "10.1145/1274871.1274886", "10.1145/966131.966135", "10.1145/217853.217855", "10.1145/200972.200974", "10.1145/383745.383748", "10.1145/987657.987669", "10.1145/1559755.1559757", "10.1145/604471.604483", "10.1145/1274871.1274886", "10.1145/966131.966135", "10.1145/217853.217855", "10.1145/200972.200974", "10.1145/383745.383748", "10.1145/987657.987669", "10.1145/1559755.1559757", "10.1145/604471.604483", "10.1145/1274871.1274886", "10.1145/966131.966135", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(71)90213-6", "10.1038/35058500", "10.1016/j.visres.2009.09.014", "10.1037/10037-000", "10.1016/S0022-5371(64)80015-3", "10.1037/0033-295X.87.3.272", "10.3758/BF03198811", "10.1002/j.1538-7305.1983.tb03502.x", "10.1037/0096-1523.18.1.34", "10.1126/science.4001937", "10.1037/0033-295X.95.1.15", "10.1016/0010-0285(80)90005-5", "10.1037/0033-295X.101.1.80", "10.1364/JOSAA.7.001209", "10.1016/0042-6989(91)90203-H", "10.1016/0042-6989(95)00207-3", "10.1016/B978-0-12-084320-6.50007-4", "10.3758/BF03207480", "10.1126/science.2300824", "10.1038/320264a0", "10.1080/17470215508416674", "10.1126/science.121.3136.173", "10.3758/BF03205076", "10.1016/0042-6989(82)90167-5", "10.1016/S0042-6989(98)00014-5", "10.1111/j.1467-9280.1990.tb00227.x", "10.1068/p5418", "10.1146/annurev.ne.13.030190.000325", "10.1037/0096-1523.17.3.652", "10.1037/0096-1523.12.1.3", "10.1016/S0734-189X(85)80004-9", "10.1037/0096-1523.15.3.419", "10.1007/BF00335367", "10.1038/scientificamerican0475-34", "10.1038/290091a0", "10.1016/S0166-2236(84)80275-1", "10.3758/BF03203039", "10.1068/p180457", "10.1037/0033-295X.96.3.433", "10.1073/pnas.92.24.11155", "10.3758/BF03212096", "10.1111/j.1467-9280.1990.tb00067.x", "10.3758/BF03200774", "10.1037/0096-1523.16.4.879", "10.1037/0033-295X.114.3.599", "10.1126/science.1143515", "10.1111/1467-9280.00327", "10.1038/89532", "10.1016/j.visres.2004.10.004", "10.1037/a0013899", "10.1016/S0042-6989(02)00596-5", "10.3758/BF03206351", "10.3758/BF03204984", "10.1037/0096-1523.24.5.1354", "10.1016/j.visres.2010.09.012", "10.1016/0042-6989(91)90183-6", "10.3758/BF03200441", "10.1037/0096-1523.15.4.635", "10.1037/0096-1523.26.2.693", "10.1080/13506280902758044", "10.4249/scholarpedia.3649", "10.1093/acprof:oso/9780198566816.003.0009", "10.1068/p2935", "10.1093/acprof:oso/9780198566175.001.0001", "10.1126/science.171.3968.308", "10.1016/0028-3932(95)00014-T", "10.1037/0033-2909.85.3.618", "10.1006/cogp.1998.0681", "10.1016/S1364-6613(00)01476-5", "10.1016/S1364-6613(00)01452-2", "10.1016/j.tics.2008.06.001", "10.1111/j.1467-9280.2005.01596.x", "10.3758/BF03193936", "10.3758/APP.71.4.681", "10.1146/annurev.psych.48.1.269", "10.1016/S0042-6989(00)00003-1", "10.1080/135062800394658", "10.1111/j.1467-9280.1997.tb00427.x", "10.3758/BF03214339", "10.1111/j.1467-9280.1996.tb00378.x", "10.1016/j.tics.2004.11.006", "10.1167/3.1.9", "10.1037/0096-3445.131.1.48", "10.1111/1467-9280.t01-1-01425", "10.1068/p2952", "10.3758/BF03210498", "10.1037/0096-1523.18.3.849", "10.1037/0033-2909.125.4.458", "10.1038/369313a0", "10.1057/palgrave.ivs.9500005", "10.1117/12.135952", "10.1016/j.tics.2010.12.001", "10.1016/S0042-6989(99)00163-7", "10.1016/j.cub.2006.05.056", "10.1016/S1364-6613(97)89058-4", "10.4159/harvard.9780674734470", "10.1111/1467-8721.00003", "10.1511/2006.3.247", "10.1111/j.1467-9280.2006.01785.x", "10.1016/j.tics.2007.02.003", "10.1098/rspb.2007.0368", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(71)90213-6", "10.1038/35058500", "10.1016/j.visres.2009.09.014", "10.1037/10037-000", "10.1016/S0022-5371(64)80015-3", "10.1037/0033-295X.87.3.272", "10.3758/BF03198811", "10.1002/j.1538-7305.1983.tb03502.x", "10.1037/0096-1523.18.1.34", "10.1126/science.4001937", "10.1037/0033-295X.95.1.15", "10.1016/0010-0285(80)90005-5", "10.1037/0033-295X.101.1.80", "10.1364/JOSAA.7.001209", "10.1016/0042-6989(91)90203-H", "10.1016/0042-6989(95)00207-3", "10.1016/B978-0-12-084320-6.50007-4", "10.3758/BF03207480", "10.1126/science.2300824", "10.1038/320264a0", "10.1080/17470215508416674", "10.1126/science.121.3136.173", "10.3758/BF03205076", "10.1016/0042-6989(82)90167-5", "10.1016/S0042-6989(98)00014-5", "10.1111/j.1467-9280.1990.tb00227.x", "10.1068/p5418", "10.1146/annurev.ne.13.030190.000325", "10.1037/0096-1523.17.3.652", "10.1037/0096-1523.12.1.3", "10.1016/S0734-189X(85)80004-9", "10.1037/0096-1523.15.3.419", "10.1007/BF00335367", "10.1038/scientificamerican0475-34", "10.1038/290091a0", "10.1016/S0166-2236(84)80275-1", "10.3758/BF03203039", "10.1068/p180457", "10.1037/0033-295X.96.3.433", "10.1073/pnas.92.24.11155", "10.3758/BF03212096", "10.1111/j.1467-9280.1990.tb00067.x", "10.3758/BF03200774", "10.1037/0096-1523.16.4.879", "10.1037/0033-295X.114.3.599", "10.1126/science.1143515", "10.1111/1467-9280.00327", "10.1038/89532", "10.1016/j.visres.2004.10.004", "10.1037/a0013899", "10.1016/S0042-6989(02)00596-5", "10.3758/BF03206351", "10.3758/BF03204984", "10.1037/0096-1523.24.5.1354", "10.1016/j.visres.2010.09.012", "10.1016/0042-6989(91)90183-6", "10.3758/BF03200441", "10.1037/0096-1523.15.4.635", "10.1037/0096-1523.26.2.693", "10.1080/13506280902758044", "10.4249/scholarpedia.3649", "10.1093/acprof:oso/9780198566816.003.0009", "10.1068/p2935", "10.1093/acprof:oso/9780198566175.001.0001", "10.1126/science.171.3968.308", "10.1016/0028-3932(95)00014-T", "10.1037/0033-2909.85.3.618", "10.1006/cogp.1998.0681", "10.1016/S1364-6613(00)01476-5", "10.1016/S1364-6613(00)01452-2", "10.1016/j.tics.2008.06.001", "10.1111/j.1467-9280.2005.01596.x", "10.3758/BF03193936", "10.3758/APP.71.4.681", "10.1146/annurev.psych.48.1.269", "10.1016/S0042-6989(00)00003-1", "10.1080/135062800394658", "10.1111/j.1467-9280.1997.tb00427.x", "10.3758/BF03214339", "10.1111/j.1467-9280.1996.tb00378.x", "10.1016/j.tics.2004.11.006", "10.1167/3.1.9", "10.1037/0096-3445.131.1.48", "10.1111/1467-9280.t01-1-01425", "10.1068/p2952", "10.3758/BF03210498", "10.1037/0096-1523.18.3.849", "10.1037/0033-2909.125.4.458", "10.1038/369313a0", "10.1057/palgrave.ivs.9500005", "10.1117/12.135952", "10.1016/j.tics.2010.12.001", "10.1016/S0042-6989(99)00163-7", "10.1016/j.cub.2006.05.056", "10.1016/S1364-6613(97)89058-4", "10.4159/harvard.9780674734470", "10.1111/1467-8721.00003", "10.1511/2006.3.247", "10.1111/j.1467-9280.2006.01785.x", "10.1016/j.tics.2007.02.003", "10.1098/rspb.2007.0368", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(71)90213-6", "10.1038/35058500", "10.1016/j.visres.2009.09.014", "10.1037/10037-000", "10.1016/S0022-5371(64)80015-3", "10.1037/0033-295X.87.3.272", "10.3758/BF03198811", "10.1002/j.1538-7305.1983.tb03502.x", "10.1037/0096-1523.18.1.34", "10.1126/science.4001937", "10.1037/0033-295X.95.1.15", "10.1016/0010-0285(80)90005-5", "10.1037/0033-295X.101.1.80", "10.1364/JOSAA.7.001209", "10.1016/0042-6989(91)90203-H", "10.1016/0042-6989(95)00207-3", "10.1016/B978-0-12-084320-6.50007-4", "10.3758/BF03207480", "10.1126/science.2300824", "10.1038/320264a0", "10.1080/17470215508416674", "10.1126/science.121.3136.173", "10.3758/BF03205076", "10.1016/0042-6989(82)90167-5", "10.1016/S0042-6989(98)00014-5", "10.1111/j.1467-9280.1990.tb00227.x", "10.1068/p5418", "10.1146/annurev.ne.13.030190.000325", "10.1037/0096-1523.17.3.652", "10.1037/0096-1523.12.1.3", "10.1016/S0734-189X(85)80004-9", "10.1037/0096-1523.15.3.419", "10.1007/BF00335367", "10.1038/scientificamerican0475-34", "10.1038/290091a0", "10.1016/S0166-2236(84)80275-1", "10.3758/BF03203039", "10.1068/p180457", "10.1037/0033-295X.96.3.433", "10.1073/pnas.92.24.11155", "10.3758/BF03212096", "10.1111/j.1467-9280.1990.tb00067.x", "10.3758/BF03200774", "10.1037/0096-1523.16.4.879", "10.1037/0033-295X.114.3.599", "10.1126/science.1143515", "10.1111/1467-9280.00327", "10.1038/89532", "10.1016/j.visres.2004.10.004", "10.1037/a0013899", "10.1016/S0042-6989(02)00596-5", "10.3758/BF03206351", "10.3758/BF03204984", "10.1037/0096-1523.24.5.1354", "10.1016/j.visres.2010.09.012", "10.1016/0042-6989(91)90183-6", "10.3758/BF03200441", "10.1037/0096-1523.15.4.635", "10.1037/0096-1523.26.2.693", "10.1080/13506280902758044", "10.4249/scholarpedia.3649", "10.1093/acprof:oso/9780198566816.003.0009", "10.1068/p2935", "10.1093/acprof:oso/9780198566175.001.0001", "10.1126/science.171.3968.308", "10.1016/0028-3932(95)00014-T", "10.1037/0033-2909.85.3.618", "10.1006/cogp.1998.0681", "10.1016/S1364-6613(00)01476-5", "10.1016/S1364-6613(00)01452-2", "10.1016/j.tics.2008.06.001", "10.1111/j.1467-9280.2005.01596.x", "10.3758/BF03193936", "10.3758/APP.71.4.681", "10.1146/annurev.psych.48.1.269", "10.1016/S0042-6989(00)00003-1", "10.1080/135062800394658", "10.1111/j.1467-9280.1997.tb00427.x", "10.3758/BF03214339", "10.1111/j.1467-9280.1996.tb00378.x", "10.1016/j.tics.2004.11.006", "10.1167/3.1.9", "10.1037/0096-3445.131.1.48", "10.1111/1467-9280.t01-1-01425", "10.1068/p2952", "10.3758/BF03210498", "10.1037/0096-1523.18.3.849", "10.1037/0033-2909.125.4.458", "10.1038/369313a0", "10.1057/palgrave.ivs.9500005", "10.1117/12.135952", "10.1016/j.tics.2010.12.001", "10.1016/S0042-6989(99)00163-7", "10.1016/j.cub.2006.05.056", "10.1016/S1364-6613(97)89058-4", "10.4159/harvard.9780674734470", "10.1111/1467-8721.00003", "10.1511/2006.3.247", "10.1111/j.1467-9280.2006.01785.x", "10.1016/j.tics.2007.02.003", "10.1098/rspb.2007.0368"]}}