{"10.1109/TVCG.2016.2601607": {"doi": "10.1109/TVCG.2016.2601607", "author": ["E. D. Ragan", "S. Scerbo", "F. Bacim", "D. A. Bowman"], "title": "Amplified Head Rotation in Virtual Reality and the Effects on 3D Search, Training Transfer, and Spatial Orientation", "year": "2017", "abstract": "Many types of virtual reality (VR) systems allow users to use natural, physical head movements to view a 3D environment. In some situations, such as when using systems that lack a fully surrounding display or when opting for convenient low-effort interaction, view control can be enabled through a combination of physical and virtual turns to view the environment, but the reduced realism could potentially interfere with the ability to maintain spatial orientation. One solution to this problem is to amplify head rotations such that smaller physical turns are mapped to larger virtual turns, allowing trainees to view the entire surrounding environment with small head movements. This solution is attractive because it allows semi-natural physical view control rather than requiring complete physical rotations or a fully-surrounding display. However, the effects of amplified head rotations on spatial orientation and many practical tasks are not well understood. In this paper, we present an experiment that evaluates the influence of amplified head rotation on 3D search, spatial orientation, and cybersickness. In the study, we varied the amount of amplification and also varied the type of display used (head-mounted display or surround-screen CAVE) for the VR search task. By evaluating participants first with amplification and then without, we were also able to study training transfer effects. The findings demonstrate the feasibility of using amplified head rotation to view 360 degrees of virtual space, but noticeable problems were identified when using high amplification with a head-mounted display. In addition, participants were able to more easily maintain a sense of spatial orientation when using the CAVE version of the application, which suggests that visibility of the user's body and awareness of the CAVE's physical environment may have contributed to the ability to use the amplification technique while keeping track of orientation.", "keywords": ["helmet mounted displays", "virtual reality", "amplified head rotation", "virtual reality", "3D search", "training transfer", "spatial orientation", "VR systems", "head movements", "3D environment", "virtual turns", "seminatural physical view control", "physical rotations", "cybersickness", "head-mounted display", "surround-screen CAVE", "VR search task", "Training", "Legged locomotion", "Three-dimensional displays", "Games", "Navigation", "Visualization", "Virtual reality", "Virtual reality", "spatial orientation", "rotation amplification", "3D interaction", "search", "cybersickness"], "referenced_by": ["10.1109/VR.2017.7892228", "10.1109/VR.2017.7892227", "10.1109/TVCG.2019.2898798", "10.1109/TVCG.2019.2898782", "10.1109/VR.2019.8797722", "10.1109/VR.2019.8797994", "10.1109/VS-Games.2019.8864589", "10.1109/ISMAR.2019.00-20", "10.1109/TVCG.2018.2884468", "10.1109/TVCG.2018.2887379", "10.1109/VR46266.2020.00066", "10.1109/VRW50115.2020.00098", "10.1109/ISMAR50242.2020.00102"], "referencing": ["10.1109/MC.2005.297", "10.1109/MC.2007.257", "10.1109/TVCG.2012.163", "10.1109/TVCG.2015.2403312", "10.1109/TVCG.2009.62", "10.1109/TVCG.2016.2518298", "10.1109/VR.2002.996517", "10.1109/TVCG.2013.41", "10.1109/TVCG.2015.2391864", "10.1109/TVCG.2005.92", "10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/TVCG.2012.47", "10.1109/TVCG.2015.2391851", "10.1162/PRES_a_00195", "10.1109/MC.2005.297", "10.1109/MC.2007.257", "10.1109/TVCG.2012.163", "10.1109/TVCG.2015.2403312", "10.1109/TVCG.2009.62", "10.1109/TVCG.2016.2518298", "10.1109/VR.2002.996517", "10.1109/TVCG.2013.41", "10.1109/TVCG.2015.2391864", "10.1109/TVCG.2005.92", "10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/TVCG.2012.47", "10.1109/TVCG.2015.2391851", "10.1162/PRES_a_00195", "10.1109/MC.2005.297", "10.1109/MC.2007.257", "10.1109/TVCG.2012.163", "10.1109/TVCG.2015.2403312", "10.1109/TVCG.2009.62", "10.1109/TVCG.2016.2518298", "10.1109/VR.2002.996517", "10.1109/TVCG.2013.41", "10.1109/TVCG.2015.2391864", "10.1109/TVCG.2005.92", "10.1109/TVCG.2008.191", "10.1109/VR.2011.5759455", "10.1109/TVCG.2012.47", "10.1109/TVCG.2015.2391851", "10.1162/PRES_a_00195", "10.1145/2330667.2330687", "10.1145/364338.364339", "10.1145/1080402.1080411", "10.1145/985692.985748", "10.1145/632716.632874", "10.1145/1272582.1272590", "10.1145/1402236.1402241", "10.1145/1889863.1889865", "10.1145/1450579.1450611", "10.1145/1278240.1278243", "10.1145/333329.333344", "10.1145/2492494.2492514", "10.1145/2330667.2330687", "10.1145/364338.364339", "10.1145/1080402.1080411", "10.1145/985692.985748", "10.1145/632716.632874", "10.1145/1272582.1272590", "10.1145/1402236.1402241", "10.1145/1889863.1889865", "10.1145/1450579.1450611", "10.1145/1278240.1278243", "10.1145/333329.333344", "10.1145/2492494.2492514", "10.1145/2330667.2330687", "10.1145/364338.364339", "10.1145/1080402.1080411", "10.1145/985692.985748", "10.1145/632716.632874", "10.1145/1272582.1272590", "10.1145/1402236.1402241", "10.1145/1889863.1889865", "10.1145/1450579.1450611", "10.1145/1278240.1278243", "10.1145/333329.333344", "10.1145/2492494.2492514", "10.1162/105474603765879521", "10.1207/s15327108ijap0803_4", "10.1162/105474699566143", "10.1162/pres.1997.6.1.73", "10.1162/105474698565659", "10.1146/annurev.psych.52.1.471", "10.1002/bjs.4407", "10.1007/s00268-007-9307-9", "10.1177/001872089403600301", "10.1518/107118192786749450", "10.1162/pres_a_00016", "10.1016/j.future.2008.07.015", "10.1016/S0003-6870(98)00039-8", "10.1207/s15327108ijap0303_3", "10.1162/PRES_a_00152", "10.1162/105474603765879521", "10.1207/s15327108ijap0803_4", "10.1162/105474699566143", "10.1162/pres.1997.6.1.73", "10.1162/105474698565659", "10.1146/annurev.psych.52.1.471", "10.1002/bjs.4407", "10.1007/s00268-007-9307-9", "10.1177/001872089403600301", "10.1518/107118192786749450", "10.1162/pres_a_00016", "10.1016/j.future.2008.07.015", "10.1016/S0003-6870(98)00039-8", "10.1207/s15327108ijap0303_3", "10.1162/PRES_a_00152", "10.1162/105474603765879521", "10.1207/s15327108ijap0803_4", "10.1162/105474699566143", "10.1162/pres.1997.6.1.73", "10.1162/105474698565659", "10.1146/annurev.psych.52.1.471", "10.1002/bjs.4407", "10.1007/s00268-007-9307-9", "10.1177/001872089403600301", "10.1518/107118192786749450", "10.1162/pres_a_00016", "10.1016/j.future.2008.07.015", "10.1016/S0003-6870(98)00039-8", "10.1207/s15327108ijap0303_3", "10.1162/PRES_a_00152"]}, "10.1109/TVCG.2016.2582174": {"doi": "10.1109/TVCG.2016.2582174", "author": ["G. Aldrich", "J. D. Hyman", "S. Karra", "C. W. Gable", "N. Makedonska", "H. Viswanathan", "J. Woodring", "B. Hamann"], "title": "Analysis and Visualization of Discrete Fracture Networks Using a Flow Topology Graph", "year": "2017", "abstract": "We present an analysis and visualization prototype using the concept of a flow topology graph (FTG) for characterization of flow in constrained networks, with a focus on discrete fracture networks (DFN), developed collaboratively by geoscientists and visualization scientists. Our method allows users to understand and evaluate flow and transport in DFN simulations by computing statistical distributions, segment paths of interest, and cluster particles based on their paths. The new approach enables domain scientists to evaluate the accuracy of the simulations, visualize features of interest, and compare multiple realizations over a specific domain of interest. Geoscientists can simulate complex transport phenomena modeling large sites for networks consisting of several thousand fractures without compromising the geometry of the network. However, few tools exist for performing higher-level analysis and visualization of simulated DFN data. The prototype system we present addresses this need. We demonstrate its effectiveness for increasingly complex examples of DFNs, covering two distinct use cases - hydrocarbon extraction from unconventional resources and transport of dissolved contaminant from a spent nuclear fuel repository.", "keywords": ["data visualisation", "graph theory", "statistical distributions", "discrete fracture networks", "flow topology graph", "visualization prototype", "FTG", "geoscientists", "visualization scientists", "statistical distributions", "simulated DFN data", "hydrocarbon extraction", "nuclear fuel repository", "Data visualization", "Topology", "Analytical models", "Network topology", "Trajectory", "Computational modeling", "Geometry", "Fracture network flow analysis and visualization", "flow topology graph", "topological path analysis", "topological trace clustering", "flow in fractured rock", "discrete fracture network"], "referenced_by": ["10.1109/MCSE.2019.2908104"], "referencing": ["10.1109/TVCG.2010.218", "10.1109/TVCG.2007.70602", "10.1109/TVCG.2011.219", "10.1109/LDAV.2013.6675154", "10.1109/TVCG.2007.1021", "10.1109/TVCG.2008.33", "10.1109/TVCG.2011.107", "10.1109/TVCG.2012.150", "10.1016/j.apenergy.2015.03.023", "10.1007/s10040-004-0397-2", "10.1007/978-3-540-70823-0_1", "10.1111/j.1467-8659.2011.01901.x", "10.1029/2012JB009461", "10.1137/080729244", "10.1002/2015WR017151", "10.1002/2014WR016829", "10.1137/060653482", "10.1016/j.cageo.2015.08.001", "10.1137/130942541", "10.2172/1168703", "10.1007/s10596-012-9307-1", "10.1007/s10596-015-9525-4", "10.1029/2001WR001009", "10.1090/qam/253822", "10.1093/comjnl/20.4.364", "10.1016/B978-012387582-2/50038-1"]}, "10.1109/TVCG.2016.2569084": {"doi": "10.1109/TVCG.2016.2569084", "author": ["X. Xu", "L. Zhong", "M. Xie", "X. Liu", "J. Qin", "T. Wong"], "title": "ASCII Art Synthesis from Natural Photographs", "year": "2017", "abstract": "While ASCII art is a worldwide popular art form, automatic generating structure-based ASCII art from natural photographs remains challenging. The major challenge lies on extracting the perception-sensitive structure from the natural photographs so that a more concise ASCII art reproduction can be produced based on the structure. However, due to excessive amount of texture in natural photos, extracting perception-sensitive structure is not easy, especially when the structure may be weak and within the texture region. Besides, to fit different target text resolutions, the amount of the extracted structure should also be controllable. To tackle these challenges, we introduce a visual perception mechanism of non-classical receptive field modulation (non-CRF modulation) from physiological findings to this ASCII art application, and propose a new model of non-CRF modulation which can better separate the weak structure from the crowded texture, and also better control the scale of texture suppression. Thanks to our non-CRF model, more sensible ASCII art reproduction can be obtained. In addition, to produce more visually appealing ASCII arts, we propose a novel optimization scheme to obtain the optimal placement of proportional-font characters. We apply our method on a rich variety of images, and visually appealing ASCII art can be obtained in all cases.", "keywords": ["art", "data visualisation", "feature extraction", "image resolution", "image texture", "modulation", "optimisation", "photography", "visual perception", "ASCII art synthesis", "natural photographs", "perception-sensitive structure extraction", "texture region", "target text resolutions", "visual perception mechanism", "nonclassical receptive field modulation", "nonCRF modulation", "optimization scheme", "Art", "Modulation", "Visualization", "Computational modeling", "Optimization", "Detectors", "Smoothing methods", "ASCII art synthesis", "non-classical receptive field modulation", "texture suppression"], "referenced_by": [], "referencing": ["10.1109/TPAMI.2010.161", "10.1109/TIP.2003.814250", "10.1049/cp:20080660", "10.1109/TIP.2014.2361210", "10.1109/34.24792", "10.1145/1778765.1778789", "10.1145/2366145.2366158", "10.1145/2601097.2601188", "10.1145/1274871.1274878", "10.1145/2508363.2508403", "10.1016/j.patcog.2010.08.013", "10.1016/j.neucom.2010.12.022", "10.1016/j.neucom.2012.09.027", "10.1111/cgf.12597", "10.1113/jphysiol.2007.130294", "10.1016/j.patcog.2007.02.009", "10.1016/j.patcog.2007.10.011"]}, "10.1109/TVCG.2016.2598570": {"doi": "10.1109/TVCG.2016.2598570", "author": ["Y. Zhang", "Y. Tong", "K. Zhou"], "title": "Coloring 3D Printed Surfaces by Thermoforming", "year": "2017", "abstract": "Decorating the surfaces of 3D printed objects with color textures is still not readily available in most consumer-level or even high-end 3D printers. Existing techniques such as hydrographics color transfer suffer from the issues of air pockets in concave regions and discoloration in overly stretched regions. We propose a novel thermoforming-based coloring technique to alleviate these problems as well as to simplify the overall procedure. Thermoforming is a widely used technique in industry for plastic thin shell product manufacturing by pressing heated plastic sheets onto molds using atmospheric pressure. We attach on the transparent plastic sheet a precomputed color pattern decal prior to heating, and adhere it to 3D printed models treated as the molds in thermoforming. The 3D models are thus decorated with the desired color texture, as well as a thin, polished protective cover. The precomputation involves a physical simulation of the thermoforming process to compute the correct color pattern on the plastic sheet, and the vent hole layout on the 3D model for air pocket elimination. We demonstrate the effectiveness and accuracy of our computational model and our prototype thermoforming surface coloring system through physical experiments.", "keywords": ["heat treatment", "plastic products", "polishing", "pressing", "production engineering computing", "solid modelling", "thermoforming", "three-dimensional printing", "3D printed surfaces", "thermoforming-based coloring technique", "plastic thin shell product manufacturing", "plastic sheets pressing", "atmospheric pressure", "3D printed models", "protective cover polishing", "Solid modeling", "Atmospheric modeling", "Computational modeling", "Three-dimensional displays", "Thermoforming", "Plastics", "Color", "3D printing", "thermoforming", "thermoplastic sheet simulation", "texture mapping"], "referenced_by": ["10.1109/MCG.2017.37"], "referencing": ["10.1145/2185520.2185543", "10.1145/2366145.2366149", "10.1145/2508363.2508400", "10.1145/2601097.2601143", "10.1145/2461912.2461979", "10.1145/2766937", "10.1145/1531326.1531338", "10.1145/1618452.1618474", "10.1145/2461912.2461989", "10.1145/1833349.1778799", "10.1145/2167076.2167078", "10.1145/1276377.1276397", "10.1145/1360612.1360646", "10.1145/2601097.2601132", "10.1145/2751541", "10.1145/566654.566623", "10.1145/2816795.2818112", "10.1111/cgf.12697", "10.2200/S00184ED1V01Y200904MRE001", "10.1007/978-1-84996-432-6_83", "10.5244/C.17.39", "10.1002/pen.760302009", "10.1002/pen.11355", "10.1016/S0378-4754(02)00077-0", "10.1177/0021998311429882"]}, "10.1109/TVCG.2016.2592906": {"doi": "10.1109/TVCG.2016.2592906", "author": ["A. Prouzeau", "A. Bezerianos", "O. Chapuis"], "title": "Evaluating Multi-User Selection for Exploring Graph Topology on Wall-Displays", "year": "2017", "abstract": "Wall-displays allow multiple users to simultaneously view and analyze large amounts of information, such as the increasingly complex graphs present in domains like biology or social network analysis. We focus on how pairs explore graphs on a touch enabled wall-display using two techniques, both adapted for collaboration: a basic localized selection, and a propagation selection technique that uses the idea of diffusion/transmission from an origin node. We assess in a controlled experiment the impact of selection technique on a shortest path identification task. Pairs consistently divided space even if the task is not spatially divisible, and for the basic selection technique that has a localized visual effect, it led to parallel work that negatively impacted accuracy. The large visual footprint of the propagation technique led to close coordination, improving speed and accuracy for complex graphs only. We then observed the use of propagation on additional graph topology tasks, confirming pair strategies on spatial division and coordination.", "keywords": ["computer displays", "data visualisation", "graph theory", "multiuser selection", "graph topology", "wall displays", "basic localized selection", "propagation selection", "graph visualization", "Visualization", "Collaboration", "Topology", "Navigation", "Data visualization", "Keyboards", "Mice", "Wall-displays", "multi-user interaction", "graph visualization", "selection techniques", "co-located collaboration"], "referenced_by": ["10.1109/TVCG.2018.2865235", "10.1109/PacificVis.2019.00010", "10.1109/VR.2019.8797733"], "referencing": ["10.1109/PACIFICVIS.2009.4906845", "10.1109/TVCG.2013.254", "10.1109/MC.2012.110", "10.1109/TVCG.2012.251", "10.1109/TVCG.2012.252", "10.1109/TVCG.2013.151", "10.1109/2945.841119", "10.1109/TVCG.2007.70568", "10.1109/MCG.2009.78", "10.1109/TVCG.2013.163", "10.1109/TVCG.2013.170", "10.1109/TVCG.2014.2315995", "10.1109/TVCG.2009.151", "10.1109/38.946632", "10.1109/TVCG.2006.184", "10.1145/330572.330576", "10.1145/1240624.1240656", "10.1145/1054972.1055023", "10.1145/2556288.2556956", "10.1145/143457.143468", "10.1145/1357054.1357203", "10.1145/2254556.2254652", "10.1145/2556288.2557130", "10.1145/2576099", "10.1145/2702123.2702312", "10.1145/2207676.2208691", "10.1145/2817721.2817726", "10.1145/1168149.1168168", "10.1145/2858036.2858039", "10.1145/2556288.2557020", "10.1145/1518701.1519056", "10.1145/1996461.1996518", "10.1145/2817721.2817735", "10.1145/2702123.2702406", "10.1145/2468356.2479653", "10.1145/1936652.1936673", "10.1145/1294211.1294221", "10.1145/642611.642650", "10.1145/1124772.1124950", "10.1145/2702123.2702129", "10.1145/1031607.1031647", "10.1145/1279640.1279642", "10.1016/j.ic.2014.12.005", "10.1002/spe.4380211102", "10.1023/A:1021271517844", "10.1007/978-3-540-70956-5_5", "10.1177/1473871611412817", "10.1111/j.1467-8659.2009.01444.x", "10.1007/978-3-642-24028-7_46", "10.1007/978-3-662-45803-7_10", "10.1016/S1045-926X(02)90232-6", "10.1111/j.1467-8659.2011.01898.x", "10.1057/palgrave.ivs.9500090"]}, "10.1109/TVCG.2016.2586071": {"doi": "10.1109/TVCG.2016.2586071", "author": ["P. Punpongsanon", "E. Guy", "D. Iwai", "K. Sato", "T. Boubekeur"], "title": "Extended LazyNav: Virtual 3D Ground Navigation for Large Displays and Head-Mounted Displays", "year": "2017", "abstract": "This paper presents the extended work on LazyNav, a head-free, eyes-free and hands-free mid-air ground navigation control model presented at the IEEE 3D User Interfaces (3DUI) 2015, in particular with a new application to the head-mounted display (HMD). Our mid-air interaction metaphor makes use of only a single pair of the remaining tracked body elements to tailor the navigation. Therefore, the user can navigate in the scene while still being able to perform other interactions with her hands and head, e.g., carrying a bag, grasping a cup of coffee, or observing the content by moving her eyes and locally rotating her head. We design several body motions for navigation by considering the use of non-critical body parts and develop assumptions about ground navigation techniques. Through the user studies, we investigate the motions that are easy to discover, easy to control, socially acceptable, accurate and not tiring. Finally, we evaluate the desired ground navigation features with a prototype application in both a large display (LD) and a HMD navigation scenarios. We highlight several recommendations for designing a particular mid-air ground navigation technique for a LD and a HMD.", "keywords": ["helmet mounted displays", "human computer interaction", "navigation", "user interfaces", "virtual reality", "LazyNav", "virtual 3D ground navigation", "head-mounted displays", "head-free mid-air ground navigation control model", "eyes-free mid-air ground navigation control model", "hands-free mid-air ground navigation control model", "IEEE 3D user interfaces", "3DUI", "HMD", "mid-air interaction metaphor", "body motions", "noncritical body parts", "motion control", "large display", "Navigation", "Three-dimensional displays", "Hip", "Tracking", "Space vehicles", "Legged locomotion", "Sensors", "3D user interface", "spatial interaction", "virtual reality", "navigation"], "referenced_by": ["10.1109/VR.2017.7892228", "10.1109/VR.2019.8797777", "10.1109/ISMAR.2019.000-6"], "referencing": ["10.1109/MCG.2008.109", "10.1109/3DTV.2014.6874711", "10.1145/2614066.2614105", "10.1145/1520340.1520717", "10.1145/2073370.2073391", "10.1145/1889863.1889867", "10.1145/2556288.2557130", "10.1162/105474601750182342", "10.1016/j.chb.2011.06.014", "10.1007/978-1-4419-8432-6", "10.1007/978-1-4419-8432-6_6", "10.1007/978-3-642-02115-2_7", "10.1016/j.compenvurbsys.2013.10.003"]}, "10.1109/TVCG.2016.2597827": {"doi": "10.1109/TVCG.2016.2597827", "author": ["J. Zhang", "J. Fan", "Z. Luo"], "title": "Generating Multi-Destination Maps", "year": "2017", "abstract": "Multi-destination maps are a kind of navigation maps aimed to guide visitors to multiple destinations within a region, which can be of great help to urban visitors. However, they have not been developed in the current online map service. To address this issue, we introduce a novel layout model designed especially for generating multi-destination maps, which considers the global and local layout of a multi-destination map. We model the layout problem as a graph drawing that satisfies a set of hard and soft constraints. In the global layout phase, we balance the scale factor between ROIs. In the local layout phase, we make all edges have good visibility and optimize the map layout to preserve the relative length and angle of roads. We also propose a perturbation-based optimization method to find an optimal layout in the complex solution space. The multi-destination maps generated by our system are potential feasible on the modern mobile devices and our result can show an overview and a detail view of the whole map at the same time. In addition, we perform a user study to evaluate the effectiveness of our method, and the results prove that the multi-destination maps achieve our goals well.", "keywords": ["cartography", "data visualisation", "graph theory", "mobile computing", "optimisation", "multidestination maps", "online map service", "graph drawing", "global layout phase", "layout optimization", "mobile devices", "visualization", "Roads", "Layout", "Visualization", "Navigation", "Trajectory", "Optimization methods", "Multi-destination maps", "visualization", "layout optimization", "urban network", "traffic visualization", "geographic/geospatial visualization"], "referenced_by": ["10.1109/ACCESS.2019.2942282"], "referencing": ["10.1109/TVCG.2009.65", "10.1109/TVCG.2014.2346265", "10.1109/MCG.2008.99", "10.1109/TVCG.2005.66", "10.1109/TVCG.2011.205", "10.1109/TVCG.2011.191", "10.1109/TVCG.2015.2467771", "10.1109/TVCG.2015.2468111", "10.1109/TVCG.2011.116", "10.1145/383259.383286", "10.1145/1882261.1866184", "10.1145/1360612.1360699", "10.1145/502348.502358", "10.1145/2557500.2557514", "10.1111/j.1467-8659.2011.02055.x", "10.1111/cgf.12333", "10.1016/0010-0285(81)90016-5", "10.1007/978-1-4613-3724-9_43", "10.1007/3-540-48384-5_4"]}, "10.1109/TVCG.2016.2607714": {"doi": "10.1109/TVCG.2016.2607714", "author": ["E. Zgraggen", "A. Galakatos", "A. Crotty", "J. Fekete", "T. Kraska"], "title": "How Progressive Visualizations Affect Exploratory Analysis", "year": "2017", "abstract": "The stated goal for visual data exploration is to operate at a rate that matches the pace of human data analysts, but the ever increasing amount of data has led to a fundamental problem: datasets are often too large to process within interactive time frames. Progressive analytics and visualizations have been proposed as potential solutions to this issue. By processing data incrementally in small chunks, progressive systems provide approximate query answers at interactive speeds that are then refined over time with increasing precision. We study how progressive visualizations affect users in exploratory settings in an experiment where we capture user behavior and knowledge discovery through interaction logs and think-aloud protocols. Our experiment includes three visualization conditions and different simulated dataset sizes. The visualization conditions are: (1) blocking, where results are displayed only after the entire dataset has been processed; (2) instantaneous, a hypothetical condition where results are shown almost immediately; and (3) progressive, where approximate results are displayed quickly and then refined over time. We analyze the data collected in our experiment and observe that users perform equally well with either instantaneous or progressive visualizations in key metrics, such as insight discovery rates and dataset coverage, while blocking visualizations have detrimental effects.", "keywords": ["data analysis", "data mining", "data visualisation", "progressive visualizations", "exploratory analysis", "visual data exploration", "interactive time frames", "progressive analytics", "progressive systems", "knowledge discovery", "interaction logs", "think-aloud protocols", "instantaneous visualizations", "blocking visualizations", "Data visualization", "Prefetching", "Visualization", "Measurement", "Data analysis", "Histograms", "Time factors", "Exploratory analysis", "interactive visualization", "progressive visualization", "scalability", "insight-based evaluation"], "referenced_by": ["10.1109/ICDM.2018.00057", "10.1109/ACCESS.2018.2882244", "10.1109/TVCG.2018.2859973", "10.1109/TVCG.2019.2934556", "10.1109/TVCG.2019.2934800", "10.1109/TVCG.2019.2934537", "10.1109/TVCG.2018.2869149", "10.1109/VDS48975.2019.8973384", "10.1109/ICDE48307.2020.00069", "10.1109/VizSec48167.2019.9161633", "10.1109/TKDE.2019.2913651", "10.1109/BDCAT50828.2020.00020"], "referencing": ["10.1109/TVCG.2014.2346574", "10.1109/LDAV.2011.6092320", "10.1109/MCG.2012.48", "10.1109/INFVIS.2005.1532136", "10.1109/TVCG.2014.2346293", "10.1109/2945.981851", "10.1109/TVCG.2013.179", "10.1109/DASFAA.2003.1192383", "10.1109/ICDE.2014.6816674", "10.1109/2.781635", "10.1109/TVCG.2014.2346452", "10.1109/TVCG.2015.2467613", "10.1109/TVCG.2014.2346298", "10.1109/VAST.2014.7042482", "10.1145/108844.108874", "10.1145/2514.2517", "10.1145/2207676.2208294", "10.1145/2133416.2146416", "10.1145/2168931.2168943", "10.1145/2207676.2208293", "10.1145/248603.248616", "10.1145/2588555.2593666", "10.1145/2702123.2702590", "10.1145/2465351.2465355", "10.1145/2556288.2557131", "10.1145/1753326.1753557", "10.1145/304181.304208", "10.1145/1189769.1189775", "10.1145/2213836.2213902", "10.1145/2702123.2702300", "10.1145/1016540.1016556", "10.1117/12.810501", "10.14778/2824032.2824127", "10.1111/cgf.12129", "10.1002/sim.3471", "10.14778/1687627.1687675"]}, "10.1109/TVCG.2016.2582158": {"doi": "10.1109/TVCG.2016.2582158", "author": ["A. Bhattacharya", "J. Weissenb\u00f6ck", "R. Wenger", "A. Amirkhanov", "J. Kastner", "C. Heinzl"], "title": "Interactive Exploration and Visualization Using MetaTracts extracted from Carbon Fiber Reinforced Composites", "year": "2017", "abstract": "This work introduces a tool for interactive exploration and visualization using MetaTracts. MetaTracts is a novel method for extraction and visualization of individual fiber bundles and weaving patterns from X-ray computed tomography (XCT) scans of endless carbon fiber reinforced polymers (CFRPs). It is designed specifically to handle XCT scans of low resolutions where the individual fibers are barely visible, which makes extraction of fiber bundles a challenging problem. The proposed workflow is used to analyze unit cells of CFRP materials integrating a recurring weaving pattern. First, a coarse version of integral curves is used to trace sections of the individual fiber bundles in the woven CFRP materials. We call these sections MetaTracts. In the second step, these extracted fiber bundle sections are clustered using a two-step approach: first by orientation, then by proximity. The tool can generate volumetric representations as well as surface models of the extracted fiber bundles to be exported for further analysis. In addition a custom interactive tool for exploration and visual analysis of MetaTracts is designed. We evaluate the proposed workflow on a number of real world datasets and demonstrate that MetaTracts effectively and robustly identifies and extracts fiber bundles.", "keywords": ["carbon fibre reinforced composites", "computerised tomography", "data analysis", "data visualisation", "interactive systems", "materials science computing", "polymers", "weaving", "interactive exploration", "interactive visualization", "MetaTracts", "carbon fiber reinforced composites", "weaving patterns", "X-ray computed tomography", "endless carbon fiber reinforced polymers", "XCT scans", "integral curves", "woven CFRP materials", "fiber bundle sections", "two-step approach", "volumetric representations", "surface models", "visual analysis", "Carbon", "Diffusion tensor imaging", "Weaving", "Visualization", "X-ray imaging", "Fabrics", "Three-dimensional displays", "MetaTracts", "fiber bundle extraction", "analysis and visualization", "carbon fiber reinforced polymers", "X-ray computed tomography", "interactive visual exploration and analysis"], "referenced_by": [], "referencing": ["10.1109/PACIFICVIS.2015.7156377", "10.1109/TVCG.2008.52", "10.1109/VISUAL.2005.1532779", "10.1109/TMI.2007.906785", "10.1109/TVCG.2009.115", "10.1109/PacificVis.2014.52", "10.1145/37402.37422", "10.1016/j.ijmachtools.2012.01.001", "10.1016/j.compscitech.2005.05.014", "10.1002/1531-8249(199902)45:2&lt;265::AID-ANA21&gt;3.0.CO;2-3", "10.1002/nbm.781", "10.1002/1522-2594(200010)44:4&lt;625::AID-MRM17&gt;3.0.CO;2-O", "10.1007/978-3-540-45210-2_47", "10.1007/978-3-540-30135-6_45", "10.1016/S1361-8415(02)00053-1", "10.3389/fnins.2012.00175", "10.1016/j.media.2004.06.026", "10.1007/BFb0029240", "10.3139/217.2441", "10.1111/cgf.12088"]}, "10.1109/TVCG.2016.2597830": {"doi": "10.1109/TVCG.2016.2597830", "author": ["H. Huang", "E. Kalogerakis", "E. Yumer", "R. Mech"], "title": "Shape Synthesis from Sketches via Procedural Models and Convolutional Networks", "year": "2017", "abstract": "Procedural modeling techniques can produce high quality visual content through complex rule sets. However, controlling the outputs of these techniques for design purposes is often notoriously difficult for users due to the large number of parameters involved in these rule sets and also their non-linear relationship to the resulting content. To circumvent this problem, we present a sketch-based approach to procedural modeling. Given an approximate and abstract hand-drawn 2D sketch provided by a user, our algorithm automatically computes a set of procedural model parameters, which in turn yield multiple, detailed output shapes that resemble the user's input sketch. The user can then select an output shape, or further modify the sketch to explore alternative ones. At the heart of our approach is a deep Convolutional Neural Network (CNN) that is trained to map sketches to procedural model parameters. The network is trained by large amounts of automatically generated synthetic line drawings. By using an intuitive medium, i.e., freehand sketching as input, users are set free from manually adjusting procedural model parameters, yet they are still able to create high quality content. We demonstrate the accuracy and efficacy of our method in a variety of procedural modeling scenarios including design of man-made and organic shapes.", "keywords": ["convolution", "data visualisation", "learning (artificial intelligence)", "shape recognition", "shape synthesis", "procedural models", "high quality visual content", "complex rule sets", "hand-drawn 2D sketch", "deep convolutional neural network", "CNN", "network training", "automatically generated synthetic line drawings", "intuitive medium", "Shape", "Computational modeling", "Three-dimensional displays", "Two dimensional displays", "Neural networks", "Solid modeling", "Computer architecture", "Shape synthesis", "convolutional neural networks", "procedural modeling", "sketch-based modeling"], "referenced_by": ["10.1109/CVPR.2018.00578"], "referencing": ["10.1109/38.736469", "10.1145/383259.383292", "10.1145/1179849.1179969", "10.1145/1141911.1141931", "10.1145/2185520.2185540", "10.1145/2185520.2335382", "10.1145/2661229.2661231", "10.1145/192161.192254", "10.1145/1882261.1866203", "10.1145/2629573", "10.1145/2366145.2366187", "10.1145/1944846.1944851", "10.1145/2766895", "10.1145/1618452.1618513", "10.1145/2807442.2807448", "10.1145/588272.588279", "10.1145/882262.882354", "10.1145/1961189.1961199", "10.1145/1449715.1449740", "10.1145/2647868.2654889", "10.1111/cgf.12276", "10.1016/0022-5193(68)90079-9", "10.1111/j.1467-8659.2009.01351.x", "10.1111/cgf.12282", "10.1111/cgf.12317", "10.1080/16864360.2005.10738335", "10.1007/s11263-015-0816-y", "10.1111/cgf.12809"]}, "10.1109/TVCG.2016.2600594": {"doi": "10.1109/TVCG.2016.2600594", "author": ["W. Wang", "J. Shen", "Y. Yu", "K. Ma"], "title": "Stereoscopic Thumbnail Creation via Efficient Stereo Saliency Detection", "year": "2017", "abstract": "In this paper, we propose a framework for automatically producing thumbnails from stereo image pairs. It has two components focusing respectively on stereo saliency detection and stereo thumbnail generation. The first component analyzes stereo saliency through various saliency stimuli, stereoscopic perception and the relevance between two stereo views. The second component uses stereo saliency to guide stereo thumbnail generation. We develop two types of thumbnail generation methods, both changing image size automatically. The first method is called content-persistent cropping (CPC), which aims at cropping stereo images for display devices with different aspect ratios while preserving as much content as possible. The second method is an object-aware cropping method (OAC) for generating the smallest possible thumbnail pair that retains the most important content only and facilitates quick visual exploration of a stereo image database. Quantitative and qualitative experimental evaluations demonstrate promising performance of our thumbnail generation methods in comparison to state-of-the-art algorithms.", "keywords": ["stereo image processing", "visual databases", "stereoscopic thumbnail creation", "stereo saliency detection", "stereo image pairs", "stereo thumbnail generation", "stereoscopic perception", "content-persistent cropping", "CPC", "object-aware cropping", "OAC", "quick visual exploration", "stereo image database", "Stereo image processing", "Detection algorithms", "Image edge detection", "Estimation", "Visualization", "Distortion", "Computer science", "Stereoscopic thumbnails", "stereo saliency", "image cropping"], "referenced_by": ["10.1109/TIP.2017.2754941", "10.1109/TIP.2017.2722691", "10.1109/TPAMI.2017.2662005", "10.1109/TIP.2016.2616302", "10.1109/TCSVT.2016.2645661", "10.1109/TMM.2017.2767784", "10.1109/TCSVT.2017.2701279", "10.1109/TCYB.2017.2771488", "10.1109/CVPR.2018.00514", "10.1109/CVPR.2018.00184", "10.1109/ACCESS.2019.2892098", "10.1109/ACCESS.2019.2896918", "10.1109/TIP.2018.2887029", "10.1109/TPAMI.2018.2840724", "10.1109/TIP.2018.2843680", "10.1109/TCYB.2018.2846361", "10.23919/CISTI.2019.8760988", "10.1109/TIP.2019.2910377", "10.1109/TIP.2019.2916769", "10.1109/TCSVT.2018.2870832", "10.1109/TGRS.2019.2925070", "10.1109/TMM.2019.2918730", "10.1109/CVPR.2019.00154", "10.1109/ACCESS.2020.2966628", "10.1109/APSIPAASC47483.2019.9023009", "10.1109/TIP.2020.2985531", "10.1109/TMM.2019.2939707", "10.1109/TPAMI.2019.2905607", "10.1109/TIP.2020.3036749", "10.1049/iet-cvi.2018.5591"], "referencing": ["10.1109/TVCG.2013.75", "10.1109/TMM.2011.2116775", "10.1109/TMM.2012.2186122", "10.1109/TMM.2016.2545409", "10.1109/34.730558", "10.1109/TIP.2014.2302892", "10.1109/CVPR.2011.5995344", "10.1109/TIP.2015.2438550", "10.1109/TIP.2013.2260166", "10.1109/TCYB.2015.2453091", "10.1109/TPAMI.2012.28", "10.1109/TPAMI.2007.1166", "10.1109/TIP.2015.2460013", "10.1109/TPAMI.2012.120", "10.1109/TIP.2015.2505184", "10.1145/964696.964707", "10.1145/1095034.1095061", "10.1145/1360612.1360615", "10.1145/957013.957045", "10.1145/1631272.1631384", "10.1145/1124772.1124886", "10.1145/321992.321993", "10.1007/s00530-003-0105-4", "10.1007/s11263-013-0618-z", "10.1007/s11263-009-0215-3", "10.1007/s41095-015-0028-y"]}, "10.1109/TVCG.2016.2603178": {"doi": "10.1109/TVCG.2016.2603178", "author": ["T. von Landesberger", "D. W. Fellner", "R. A. Ruddle"], "title": "Visualization System Requirements for Data Processing Pipeline Design and Optimization", "year": "2017", "abstract": "The rising quantity and complexity of data creates a need to design and optimize data processing pipelines-the set of data processing steps, parameters and algorithms that perform operations on the data. Visualization can support this process but, although there are many examples of systems for visual parameter analysis, there remains a need to systematically assess users' requirements and match those requirements to exemplar visualization methods. This article presents a new characterization of the requirements for pipeline design and optimization. This characterization is based on both a review of the literature and first-hand assessment of eight application case studies. We also match these requirements with exemplar functionality provided by existing visualization tools. Thus, we provide end-users and visualization developers with a way of identifying functionality that addresses data processing problems in an application. We also identify seven future challenges for visualization research that are not met by the capabilities of today's systems.", "keywords": ["data visualisation", "design", "formal specification", "formal verification", "optimisation", "systems analysis", "visualization system requirements", "data processing pipeline design", "data processing pipeline optimization", "Pipelines", "Data visualization", "Optimization", "Data processing", "Visualization", "Computational modeling", "Algorithm design and analysis", "Visualization systems", "requirement analysis", "data processing pipelines"], "referenced_by": ["10.1109/TVCG.2017.2682865"], "referencing": ["10.1109/BioVis.2013.6664351", "10.1109/TVCG.2014.2346321", "10.1109/VAST.2010.5652392", "10.1109/TVCG.2010.190", "10.1109/TVCG.2011.253", "10.1109/TVCG.2013.61", "10.1109/VISUAL.2005.1532788", "10.1109/TVCG.2008.145", "10.1109/TVCG.2010.223", "10.1109/VL.1996.545307", "10.1109/TVCG.2012.219", "10.1109/TVCG.2013.124", "10.1109/TVCG.2009.111", "10.1109/TMI.2013.2242901", "10.1109/VAST.2011.6102439", "10.1109/LDAV.2013.6675159", "10.1109/TVCG.2011.248", "10.1109/TVCG.2013.147", "10.1109/VAST.2011.6102457", "10.1109/TVCG.2006.99", "10.1109/TVCG.2013.222", "10.1109/TVCG.2014.2346626", "10.1109/TVCG.2009.170", "10.1109/TVCG.2012.285", "10.1109/TVCG.2012.190", "10.1109/MCG.2004.20", "10.1109/TVCG.2012.110", "10.1109/TVCG.2014.2346319", "10.1109/TVCG.2015.2462356", "10.1109/TVCG.2014.2346744", "10.1109/TVCG.2014.2346578", "10.1109/TVCG.2014.2346574", "10.1109/2.781635", "10.1109/TVCG.2009.195", "10.1109/TVCG.2008.174", "10.1109/INFVIS.1995.528680", "10.1109/TVCG.2010.214", "10.1145/2016904.2016907", "10.1145/2637748.2638410", "10.1145/258734.258887", "10.1145/1520340.1520616", "10.1145/1240624.1240656", "10.1145/2207676.2207741", "10.1016/j.scitotenv.2011.06.022", "10.1111/j.1467-8659.2011.01940.x", "10.1016/j.cag.2014.02.004", "10.1186/1471-2105-16-S11-S9", "10.1111/j.1467-8659.2012.03116.x", "10.1021/op200279m", "10.2514/6.2008-5802", "10.1002/prot.23144", "10.1016/j.cag.2013.09.004", "10.1111/j.1467-8659.2009.01684.x", "10.1016/j.eswa.2013.03.006", "10.1080/03081078908935060", "10.1038/429241a", "10.1111/j.1467-8659.2011.01898.x", "10.1007/978-1-4471-2804-5_6", "10.1101/gr.4086505", "10.1057/ivs.2008.29", "10.1007/s00371-013-0852-y", "10.1016/S0167-739X(98)00047-8", "10.1177/1473871611415997"]}}