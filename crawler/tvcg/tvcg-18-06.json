{"10.1109/TVCG.2018.2816198": {"doi": "10.1109/TVCG.2018.2816198", "author": ["S. Bruckner", "K. Koyamada", "B. Lee"], "title": "Guest Editors\u2019 Introduction: Special Section on IEEE PacificVis 2018", "year": "2018", "abstract": "The four papers in this special section were presented at the 2018 IEEE Pacific Visualization Symposium (IEEE PacificVis\u201918), which was held at the Integrated Research Center of Kobe University, Kobe,Japan from April 10 to 13, 2018. ", "keywords": ["Special issues and sections", "Meetings", "Data visualization", "VIsual analytics", "Visualization"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2018.2816219": {"doi": "10.1109/TVCG.2018.2816219", "author": ["R. Kr\u00fcger", "G. Simeonov", "F. Beck", "T. Ertl"], "title": "Visual Interactive Map Matching", "year": "2018", "abstract": "Map matching is the process of assigning observed geographic positions of vehicles and their trajectories to the actual road links in a road network. In this paper, we present Visual Interactive Map Matching, a visual analytics approach to fine-tune the data preprocessing and matching process. It is based on ST-matching, a state-of-the-art and easy-to-understand map matching algorithm. Parameters of the preprocessing step and algorithm can be optimized with immediate visual feedback. Visualizations show current matching issues and performance metrics on a map and in diagrams. Manual and computer-supported editing of the road network model leads to a refined alignment of trajectories and roads. We demonstrate our approach with large-scale taxi trajectory data. We show that optimizing the matching on a subsample results in considerably improved matching quality, also when later scaled to the full dataset. An optimized matching ensures data faithfulness and prevents misinterpretation when the matched data might be investigated in follow-up analysis.", "keywords": ["cartography", "data analysis", "data visualisation", "geographic information systems", "Global Positioning System", "image matching", "optimisation", "road traffic", "traffic information systems", "visual analytics approach", "data preprocessing", "visualizations", "road network model", "large-scale taxi trajectory data", "road links", "visual interactive map matching", "ST-matching algorithm", "visual feedback", "matching optimization", "vehicles geographic positions", "performance metrics", "computer-supported editing", "manual supported editing", "follow-up analysis", "Trajectory", "Roads", "Data visualization", "Visual analytics", "Data preprocessing", "Global Positioning System", "Map matching", "data cleaning", "data transformation and representation", "geographic visualization"], "referenced_by": ["IKEY:8950169", "IKEY:9015964", "IKEY:9159926"], "referencing": ["IKEY:7120975", "IKEY:5432167", "IKEY:7847429", "IKEY:6634127", "IKEY:6634174", "IKEY:7192687", "IKEY:7850970", "IKEY:6065001", "IKEY:7120975", "IKEY:5432167", "IKEY:7847429", "IKEY:6634127", "IKEY:6634174", "IKEY:7192687", "IKEY:7850970", "IKEY:6065001", "IKEY:7120975", "IKEY:5432167", "IKEY:7847429", "IKEY:6634127", "IKEY:6634174", "IKEY:7192687", "IKEY:7850970", "IKEY:6065001", "10.1145/1653771.1653820", "10.1145/1978942.1979444", "10.1145/2637748.2638423", "10.1145/1653771.1653820", "10.1145/1978942.1979444", "10.1145/2637748.2638423", "10.1145/1653771.1653820", "10.1145/1978942.1979444", "10.1145/2637748.2638423", "10.1007/978-3-642-37583-5", "10.1007/978-3-540-75177-9", "10.1179/000870409X12525737905042", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12107", "10.1111/cgf.12132", "10.1016/j.trc.2007.05.002", "10.1080/17489725.2010.537449", "10.1007/978-981-10-2398-9_27", "10.1007/978-3-642-37583-5", "10.1007/978-3-540-75177-9", "10.1179/000870409X12525737905042", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12107", "10.1111/cgf.12132", "10.1016/j.trc.2007.05.002", "10.1080/17489725.2010.537449", "10.1007/978-981-10-2398-9_27", "10.1007/978-3-642-37583-5", "10.1007/978-3-540-75177-9", "10.1179/000870409X12525737905042", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12107", "10.1111/cgf.12132", "10.1016/j.trc.2007.05.002", "10.1080/17489725.2010.537449", "10.1007/978-981-10-2398-9_27"]}, "10.1109/TVCG.2018.2816208": {"doi": "10.1109/TVCG.2018.2816208", "author": ["M. Reckziegel", "M. F. Cheema", "G. Scheuermann", "S. J\u00e4nicke"], "title": "Predominance Tag Maps", "year": "2018", "abstract": "A predominance map expresses the predominant data category for each geographical entity and colors are used to differentiate a small number of data categories. In tag maps, many data categories are present in the form of different tags, but related tag map approaches do not account for predominance, as tags are either displaced from their respective geographical locations or visual clutter occurs. We propose predominance tag maps, a layout algorithm that accounts for predominance for arbitrary aggregation granularities. The algorithm is able to utilize the font sizes of the tags as visual variable and it is further configurable to implement aggregation strategies beyond visualizing predominance. We introduce various measures to evaluate numerically the qualitative aspects of tag maps regarding local predominance, global features, and layout stability and we comparatively analyze our method to the tag map approach by Thom et al. [1] on the basis of real world data sets.", "keywords": ["cartography", "data analysis", "data visualisation", "predominance tag maps", "predominant data category", "geographical entity", "colors", "related tag map approaches", "local predominance", "predominance visualization", "geographical locations", "visual clutter", "arbitrary aggregation granularities", "visual variable", "aggregation strategies", "Tag clouds", "Geospatial analysis", "Layout", "Image color analysis", "Data visualization", "Shape", "Visualization", "Geospatial visualization", "point-based data", "data aggregation"], "referenced_by": [], "referencing": ["IKEY:6183572", "IKEY:7539286", "IKEY:4658131", "IKEY:6787132", "IKEY:6787165", "IKEY:4438863", "IKEY:4287373", "IKEY:4376131", "IKEY:5290722", "IKEY:6875959", "IKEY:6065008", "IKEY:6875938", "IKEY:5333443", "IKEY:7192666", "IKEY:6327261", "IKEY:6634195", "IKEY:4658135", "IKEY:6183572", "IKEY:7539286", "IKEY:4658131", "IKEY:6787132", "IKEY:6787165", "IKEY:4438863", "IKEY:4287373", "IKEY:4376131", "IKEY:5290722", "IKEY:6875959", "IKEY:6065008", "IKEY:6875938", "IKEY:5333443", "IKEY:7192666", "IKEY:6327261", "IKEY:6634195", "IKEY:4658135", "IKEY:6183572", "IKEY:7539286", "IKEY:4658131", "IKEY:6787132", "IKEY:6787165", "IKEY:4438863", "IKEY:4287373", "IKEY:4376131", "IKEY:5290722", "IKEY:6875959", "IKEY:6065008", "IKEY:6875938", "IKEY:5333443", "IKEY:7192666", "IKEY:6327261", "IKEY:6634195", "IKEY:4658135", "10.1145/1178677.1178692", "10.1145/1374489.1374501", "10.1145/1518701.1519010", "10.1145/1255175.1255177", "10.1145/1178677.1178692", "10.1145/1374489.1374501", "10.1145/1518701.1519010", "10.1145/1255175.1255177", "10.1145/1178677.1178692", "10.1145/1374489.1374501", "10.1145/1518701.1519010", "10.1145/1255175.1255177", "10.14358/PERS.74.11.1379", "10.1559/152304075784313304", "10.1007/s10708-007-9111-y", "10.4324/9780203468029", "10.1111/cgf.12886", "10.1177/0165551506078083", "10.1007/978-3-642-03655-2_43", "10.5220/0006548000400051", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03106.x", "10.1017/CBO9780511779633", "10.1214/aoms/1177728190", "10.1214/aoms/1177704472", "10.14358/PERS.74.11.1379", "10.1559/152304075784313304", "10.1007/s10708-007-9111-y", "10.4324/9780203468029", "10.1111/cgf.12886", "10.1177/0165551506078083", "10.1007/978-3-642-03655-2_43", "10.5220/0006548000400051", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03106.x", "10.1017/CBO9780511779633", "10.1214/aoms/1177728190", "10.1214/aoms/1177704472", "10.14358/PERS.74.11.1379", "10.1559/152304075784313304", "10.1007/s10708-007-9111-y", "10.4324/9780203468029", "10.1111/cgf.12886", "10.1177/0165551506078083", "10.1007/978-3-642-03655-2_43", "10.5220/0006548000400051", "10.1111/j.1467-8659.2011.01923.x", "10.1111/j.1467-8659.2012.03106.x", "10.1017/CBO9780511779633", "10.1214/aoms/1177728190", "10.1214/aoms/1177704472"]}, "10.1109/TVCG.2018.2816223": {"doi": "10.1109/TVCG.2018.2816223", "author": ["J. Wang", "L. Gou", "H. Yang", "H. Shen"], "title": "GANViz: A Visual Analytics Approach to Understand the Adversarial Game", "year": "2018", "abstract": "Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.", "keywords": ["data analysis", "data visualisation", "unsupervised learning", "GANViz", "visual analytics approach", "adversarial game", "generative models", "data representations", "unsupervised fashion", "deep learning", "Generative Adversarial Nets", "in-depth understanding", "adversarial training process", "domain experts", "long-time training process", "visual analytics system", "GAN models", "Gallium nitride", "Training", "Feature extraction", "Visual analytics", "Analytical models", "Neurons", "Neural networks", "Generative adversarial nets", "deep learning", "model interpretation", "visual analytics"], "referenced_by": ["IKEY:8454905", "IKEY:8440049", "IKEY:8667661", "IKEY:8371286", "IKEY:8884176", "IKEY:8805426", "IKEY:8805457", "IKEY:8805421", "IKEY:8812988", "IKEY:8933677", "IKEY:9086289", "IKEY:9086290", "IKEY:9288163", "IKEY:9308631"], "referencing": ["IKEY:7536654", "IKEY:8017583", "IKEY:8019879", "IKEY:1284395", "IKEY:545307", "IKEY:6065010", "IKEY:5613429", "IKEY:7539323", "IKEY:7536654", "IKEY:8017583", "IKEY:8019879", "IKEY:1284395", "IKEY:545307", "IKEY:6065010", "IKEY:5613429", "IKEY:7539323", "IKEY:7536654", "IKEY:8017583", "IKEY:8019879", "IKEY:1284395", "IKEY:545307", "IKEY:6065010", "IKEY:5613429", "IKEY:7539323", "10.1145/2470654.2466441", "10.1145/2470654.2466441", "10.1145/2470654.2466441", "10.1214/aoms/1177729694", "10.1057/palgrave.ivs.9500061", "10.1214/aoms/1177729694", "10.1057/palgrave.ivs.9500061", "10.1214/aoms/1177729694", "10.1057/palgrave.ivs.9500061"]}, "10.1109/TVCG.2018.2816203": {"doi": "10.1109/TVCG.2018.2816203", "author": ["Y. Shi", "C. Bryan", "S. Bhamidipati", "Y. Zhao", "Y. Zhang", "K. Ma"], "title": "MeetingVis: Visual Narratives to Assist in Recalling Meeting Context and Content", "year": "2018", "abstract": "In team-based workplaces, reviewing and reflecting on the content from a previously held meeting can lead to better planning and preparation. However, ineffective meeting summaries can impair this process, especially when participants have difficulty remembering what was said and what its context was. To assist with this process, we introduce MeetingVis, a visual narrative-based approach to meeting summarization. MeetingVis is composed of two primary components: (1) a data pipeline that processes the spoken audio from a group discussion, and (2) a visual-based interface that efficiently displays the summarized content. To design MeetingVis, we create a taxonomy of relevant meeting data points, identifying salient elements to promote recall and reflection. These are mapped to an augmented storyline visualization, which combines the display of participant activities, topic evolutions, and task assignments. For evaluation, we conduct a qualitative user study with five groups. Feedback from the study indicates that MeetingVis effectively triggers the recall of subtle details from prior meetings: all study participants were able to remember new details, points, and tasks compared to an unaided, memory-only baseline. This visual-based approaches can also potentially enhance the productivity of both individuals and the whole team.", "keywords": ["audio signal processing", "augmented reality", "business data processing", "data visualisation", "human computer interaction", "productivity", "speech recognition", "user interfaces", "MeetingVis", "augmented storyline visualization", "visual narrative", "team-based workplaces", "meeting summarization", "data pipeline", "visual-based interface", "summarized content", "spoken audio processing", "individual productivity", "team productivity", "business meeting context", "speaker recognition", "Visualization", "Data visualization", "Taxonomy", "Task analysis", "Pipelines", "Communication channels", "Design study", "information visualization", "meeting summarization", "natural language processing", "visual narrative", "voice recognition"], "referenced_by": ["IKEY:8440829", "IKEY:8781569", "IKEY:8801911", "IKEY:8807351", "IKEY:9237998"], "referencing": ["IKEY:6327274", "IKEY:5613452", "IKEY:7539294", "IKEY:981848", "IKEY:6875938", "IKEY:8017641", "IKEY:6875986", "IKEY:7374750", "IKEY:6327274", "IKEY:5613452", "IKEY:7539294", "IKEY:981848", "IKEY:6875938", "IKEY:8017641", "IKEY:6875986", "IKEY:7374750", "IKEY:6327274", "IKEY:5613452", "IKEY:7539294", "IKEY:981848", "IKEY:6875938", "IKEY:8017641", "IKEY:6875986", "IKEY:7374750", "10.1145/2531602.2531709", "10.1145/1026653.1026665", "10.1145/1322192.1322210", "10.1145/2733373.2806258", "10.1145/1978942.1979432", "10.1145/1378773.1378810", "10.1145/1150402.1150450", "10.1145/2187836.2187957", "10.1145/2810012", "10.1145/2702123.2702419", "10.1145/2531602.2531709", "10.1145/1026653.1026665", "10.1145/1322192.1322210", "10.1145/2733373.2806258", "10.1145/1978942.1979432", "10.1145/1378773.1378810", "10.1145/1150402.1150450", "10.1145/2187836.2187957", "10.1145/2810012", "10.1145/2702123.2702419", "10.1145/2531602.2531709", "10.1145/1026653.1026665", "10.1145/1322192.1322210", "10.1145/2733373.2806258", "10.1145/1978942.1979432", "10.1145/1378773.1378810", "10.1145/1150402.1150450", "10.1145/2187836.2187957", "10.1145/2810012", "10.1145/2702123.2702419", "10.1007/s10462-016-9475-9", "10.1057/palgrave.ivs.9500139", "10.1007/978-3-540-85853-9_23", "10.1111/j.1467-8535.2008.00906.x", "10.1037/h0035640", "10.1007/s00779-003-0242-y", "10.1111/cgf.12919", "10.1111/cgf.13181", "10.1086/448619", "10.1093/acprof:oso/9780195140057.003.0004", "10.1016/j.sbspro.2010.03.986", "10.1108/MRR-03-2013-0067", "10.1080/09658211.2015.1031679", "10.1016/j.bica.2016.04.002", "10.1007/s10462-016-9475-9", "10.1057/palgrave.ivs.9500139", "10.1007/978-3-540-85853-9_23", "10.1111/j.1467-8535.2008.00906.x", "10.1037/h0035640", "10.1007/s00779-003-0242-y", "10.1111/cgf.12919", "10.1111/cgf.13181", "10.1086/448619", "10.1093/acprof:oso/9780195140057.003.0004", "10.1016/j.sbspro.2010.03.986", "10.1108/MRR-03-2013-0067", "10.1080/09658211.2015.1031679", "10.1016/j.bica.2016.04.002", "10.1007/s10462-016-9475-9", "10.1057/palgrave.ivs.9500139", "10.1007/978-3-540-85853-9_23", "10.1111/j.1467-8535.2008.00906.x", "10.1037/h0035640", "10.1007/s00779-003-0242-y", "10.1111/cgf.12919", "10.1111/cgf.13181", "10.1086/448619", "10.1093/acprof:oso/9780195140057.003.0004", "10.1016/j.sbspro.2010.03.986", "10.1108/MRR-03-2013-0067", "10.1080/09658211.2015.1031679", "10.1016/j.bica.2016.04.002"]}, "10.1109/TVCG.2017.2704119": {"doi": "10.1109/TVCG.2017.2704119", "author": ["X. Hu", "X. Fu", "L. Liu"], "title": "Advanced Hierarchical Spherical Parameterizations", "year": "2018", "abstract": "Computing spherical parameterizations for genus-zero closed surfaces is a fundamental task for geometric processing and computer graphics. Existing methods usually suffer from a lack of practical robustness or poor quality. In this paper, we present a practically robust method to compute high-quality spherical parameterizations with bijection and low isometric distortion. Our method is based on the hierarchical scheme containing mesh decimation and parameterization refinement. The practical robustness of our method relies on two novel techniques. The first one is a flat-to-extrusive decimation strategy, which contains two decimation error metrics to alleviate the difficulty of further mesh refinement. The second is a flexible group refinement technique that consists of flexible vertex insertion and efficient volumetric distortion minimization to control the maximum distortion. We convert the task of volumetric distortion minimization to one of tetrahedral mesh improvement to make the vertices distribute uniformly for efficient refinement. Compared with state-of-the-art methods, our method is more practically robust and possesses better mapping qualities. We demonstrate the efficacy of our method in spherical parameterization computations on a data set containing over five thousand complex models.", "keywords": ["computational geometry", "computer graphics", "distortion", "mesh generation", "minimisation", "geometric processing", "computer graphics", "low isometric distortion", "parameterization refinement", "decimation error metrics", "mapping qualities", "spherical parameterization computations", "genus-zero closed surfaces", "robust method", "volumetric distortion minimization", "tetrahedral mesh", "hierarchical spherical parameterizations", "bijection parameter", "mesh decimation", "Nonlinear distortion", "Mesh generation", "Computational modeling", "Robustness", "Surface treatment", "Optimization", "Spherical parameterizations", "bijection", "low isometric distortion", "flat-to-extrusive decimation", "volumetric distortion optimization", "bijective surface correspondence"], "referenced_by": ["IKEY:8425577"], "referencing": ["IKEY:749341", "IKEY:962858", "IKEY:749341", "IKEY:962858", "IKEY:749341", "IKEY:962858", "10.1145/882262.882274", "10.1145/566654.566589", "10.1145/2766921", "10.1145/2980179.2982412", "10.1145/237170.237216", "10.1145/1073204.1073207", "10.1145/258734.258849", "10.1145/2766938", "10.1145/2185520.2335459", "10.1145/2766947", "10.1145/2980179.2980231", "10.1145/882262.882276", "10.1145/311535.311586", "10.1145/383259.383277", "10.1145/604471.604502", "10.1145/2601097.2601158", "10.1145/1015706.1015811", "10.1145/1360612.1360698", "10.1145/1015706.1015812", "10.1145/2661229.2661235", "10.1145/1015706.1015810", "10.1145/882262.882274", "10.1145/566654.566589", "10.1145/2766921", "10.1145/2980179.2982412", "10.1145/237170.237216", "10.1145/1073204.1073207", "10.1145/258734.258849", "10.1145/2766938", "10.1145/2185520.2335459", "10.1145/2766947", "10.1145/2980179.2980231", "10.1145/882262.882276", "10.1145/311535.311586", "10.1145/383259.383277", "10.1145/604471.604502", "10.1145/2601097.2601158", "10.1145/1015706.1015811", "10.1145/1360612.1360698", "10.1145/1015706.1015812", "10.1145/2661229.2661235", "10.1145/1015706.1015810", "10.1145/882262.882274", "10.1145/566654.566589", "10.1145/2766921", "10.1145/2980179.2982412", "10.1145/237170.237216", "10.1145/1073204.1073207", "10.1145/258734.258849", "10.1145/2766938", "10.1145/2185520.2335459", "10.1145/2766947", "10.1145/2980179.2980231", "10.1145/882262.882276", "10.1145/311535.311586", "10.1145/383259.383277", "10.1145/604471.604502", "10.1145/2601097.2601158", "10.1145/1015706.1015811", "10.1145/1360612.1360698", "10.1145/1015706.1015812", "10.1145/2661229.2661235", "10.1145/1015706.1015810", "10.1016/j.cag.2016.06.001", "10.1007/978-3-319-46466-4_14", "10.1080/2151237X.2007.10129230", "10.1016/j.gmod.2014.03.016", "10.1007/978-3-642-34263-9_22", "10.1007/3-540-26808-1_9", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1016/j.cag.2016.05.005", "10.1111/cgf.12179", "10.1007/978-3-642-38868-2_41", "10.1088/1367-2630/10/8/083006", "10.1111/cgf.13007", "10.1016/j.cag.2016.06.001", "10.1007/978-3-319-46466-4_14", "10.1080/2151237X.2007.10129230", "10.1016/j.gmod.2014.03.016", "10.1007/978-3-642-34263-9_22", "10.1007/3-540-26808-1_9", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1016/j.cag.2016.05.005", "10.1111/cgf.12179", "10.1007/978-3-642-38868-2_41", "10.1088/1367-2630/10/8/083006", "10.1111/cgf.13007", "10.1016/j.cag.2016.06.001", "10.1007/978-3-319-46466-4_14", "10.1080/2151237X.2007.10129230", "10.1016/j.gmod.2014.03.016", "10.1007/978-3-642-34263-9_22", "10.1007/3-540-26808-1_9", "10.1561/0600000011", "10.1111/j.1467-8659.2008.01290.x", "10.1016/j.cag.2016.05.005", "10.1111/cgf.12179", "10.1007/978-3-642-38868-2_41", "10.1088/1367-2630/10/8/083006", "10.1111/cgf.13007"]}, "10.1109/TVCG.2017.2697948": {"doi": "10.1109/TVCG.2017.2697948", "author": ["H. Q. Phan", "H. Fu", "A. B. Chan"], "title": "Color Orchestra: Ordering Color Palettes for Interpolation and Prediction", "year": "2018", "abstract": "Color theme or color palette can deeply influence the quality and the feeling of a photograph or a graphical design. Although color palettes may come from different sources such as online crowd-sourcing, photographs and graphical designs, in this paper, we consider color palettes extracted from fine art collections, which we believe to be an abundant source of stylistic and unique color themes. We aim to capture color styles embedded in these collections by means of statistical models and to build practical applications upon these models. As artists often use their personal color themes in their paintings, making these palettes appear frequently in the dataset, we employed density estimation to capture the characteristics of palette data. Via density estimation, we carried out various predictions and interpolations on palettes, which led to promising applications such as photo-style exploration, real-time color suggestion, and enriched photo recolorization. It was, however, challenging to apply density estimation to palette data as palettes often come as unordered sets of colors, which make it difficult to use conventional metrics on them. To this end, we developed a divide-and-conquer sorting algorithm to rearrange the colors in the palettes in a coherent order, which allows meaningful interpolation between color palettes. To confirm the performance of our model, we also conducted quantitative experiments on datasets of digitized paintings collected from the Internet and received favorable results.", "keywords": ["art", "divide and conquer methods", "image colour analysis", "interpolation", "sorting", "statistical analysis", "color orchestra", "graphical design", "stylistic color themes", "density estimation", "color palettes", "Interpolation", "Prediction", "statistical model", "photo-style exploration", "enriched photo recolorization", "divide-and-conquer sorting algorithm", "fine art collection", "Image color analysis", "Interpolation", "Estimation", "Painting", "Art", "Real-time systems", "Color", "Image color analysis", "machine learning", "color palette", "colorization"], "referenced_by": ["IKEY:8938867", "IKEY:9046053"], "referencing": ["IKEY:5995539", "IKEY:946629", "IKEY:1297012", "IKEY:6205760", "IKEY:5995539", "IKEY:946629", "IKEY:1297012", "IKEY:6205760", "IKEY:5995539", "IKEY:946629", "IKEY:1297012", "IKEY:6205760", "10.1145/2010324.1964958", "10.1145/2461912.2461988", "10.1145/1882261.1866172", "10.1145/2010324.1964959", "10.1145/1141911.1141933", "10.1145/2630099.2630100", "10.1145/2461912.2461939", "10.1145/2766978", "10.1145/2070781.2024190", "10.1145/2699645", "10.1145/2207676.2208547", "10.1145/258734.258887", "10.1145/2366145.2366151", "10.1145/1015706.1015780", "10.1145/2010324.1964958", "10.1145/2461912.2461988", "10.1145/1882261.1866172", "10.1145/2010324.1964959", "10.1145/1141911.1141933", "10.1145/2630099.2630100", "10.1145/2461912.2461939", "10.1145/2766978", "10.1145/2070781.2024190", "10.1145/2699645", "10.1145/2207676.2208547", "10.1145/258734.258887", "10.1145/2366145.2366151", "10.1145/1015706.1015780", "10.1145/2010324.1964958", "10.1145/2461912.2461988", "10.1145/1882261.1866172", "10.1145/2010324.1964959", "10.1145/1141911.1141933", "10.1145/2630099.2630100", "10.1145/2461912.2461939", "10.1145/2766978", "10.1145/2070781.2024190", "10.1145/2699645", "10.1145/2207676.2208547", "10.1145/258734.258887", "10.1145/2366145.2366151", "10.1145/1015706.1015780", "10.1111/cgf.12498", "10.3758/s13414-010-0027-0", "10.1002/col.20208", "10.1111/cgf.12409", "10.1111/j.1467-8659.2008.01203.x", "10.1111/j.1467-8659.2009.01403.x", "10.1111/cgf.12549", "10.1002/nav.3800020109", "10.1126/science.1127647", "10.1007/BF02289565", "10.1111/cgf.12498", "10.3758/s13414-010-0027-0", "10.1002/col.20208", "10.1111/cgf.12409", "10.1111/j.1467-8659.2008.01203.x", "10.1111/j.1467-8659.2009.01403.x", "10.1111/cgf.12549", "10.1002/nav.3800020109", "10.1126/science.1127647", "10.1007/BF02289565", "10.1111/cgf.12498", "10.3758/s13414-010-0027-0", "10.1002/col.20208", "10.1111/cgf.12409", "10.1111/j.1467-8659.2008.01203.x", "10.1111/j.1467-8659.2009.01403.x", "10.1111/cgf.12549", "10.1002/nav.3800020109", "10.1126/science.1127647", "10.1007/BF02289565"]}, "10.1109/TVCG.2017.2703853": {"doi": "10.1109/TVCG.2017.2703853", "author": ["L. Liu", "H. Zhang", "G. Jing", "Y. Guo", "Z. Chen", "W. Wang"], "title": "Correlation-Preserving Photo Collage", "year": "2018", "abstract": "A new method is presented for producing photo collages that preserve content correlation of photos. We use deep learning techniques to find correlation among given photos to facilitate their embedding on the canvas, and develop an efficient combinatorial optimization technique to make correlated photos stay close to each other. To make efficient use of canvas space, our method first extracts salient regions of photos and packs only these salient regions. We allow the salient regions to have arbitrary shapes, therefore yielding informative, yet more compact collages than by other similar collage methods based on salient regions. We present extensive experimental results, user study results, and comparisons against the state-of-the-art methods to show the superiority of our method.", "keywords": ["combinatorial mathematics", "correlation methods", "feature extraction", "learning (artificial intelligence)", "optimisation", "correlation-preserving photo collage", "deep learning techniques", "canvas space", "salient regions", "combinatorial optimization technique", "photos content correlation", "Correlation", "Shape", "Optimization", "Semantics", "Machine learning", "Computer science", "Feature extraction", "Photo collage", "image saliency", "irregular shaped packing", "image classification"], "referenced_by": [], "referencing": ["IKEY:5210167", "IKEY:4408863", "IKEY:7226841", "IKEY:7321831", "IKEY:5210167", "IKEY:4408863", "IKEY:7226841", "IKEY:7321831", "IKEY:5210167", "IKEY:4408863", "IKEY:7226841", "IKEY:7321831", "10.1145/1015706.1015718", "10.1145/1141911.1141965", "10.1145/1631272.1631533", "10.1145/1991996.1992000", "10.1145/1135777.1135911", "10.1145/566654.566633", "10.1145/1618452.1618470", "10.1145/1015706.1015718", "10.1145/1141911.1141965", "10.1145/1631272.1631533", "10.1145/1991996.1992000", "10.1145/1135777.1135911", "10.1145/566654.566633", "10.1145/1618452.1618470", "10.1145/1015706.1015718", "10.1145/1141911.1141965", "10.1145/1631272.1631533", "10.1145/1991996.1992000", "10.1145/1135777.1135911", "10.1145/566654.566633", "10.1145/1618452.1618470", "10.1007/s00371-009-0346-0", "10.1016/j.cag.2012.02.010", "10.1111/j.1467-8659.2012.03210.x", "10.1111/j.1467-8659.2009.01615.x", "10.1113/jphysiol.1962.sp006837", "10.1023/B:VISI.0000013087.49260.fb", "10.1007/s00371-009-0346-0", "10.1016/j.cag.2012.02.010", "10.1111/j.1467-8659.2012.03210.x", "10.1111/j.1467-8659.2009.01615.x", "10.1113/jphysiol.1962.sp006837", "10.1023/B:VISI.0000013087.49260.fb", "10.1007/s00371-009-0346-0", "10.1016/j.cag.2012.02.010", "10.1111/j.1467-8659.2012.03210.x", "10.1111/j.1467-8659.2009.01615.x", "10.1113/jphysiol.1962.sp006837", "10.1023/B:VISI.0000013087.49260.fb"]}, "10.1109/TVCG.2017.2702620": {"doi": "10.1109/TVCG.2017.2702620", "author": ["N. Lv", "Z. Jiang", "Y. Huang", "X. Meng", "G. Meenakshisundaram", "J. Peng"], "title": "Generic Content-Based Retrieval of Marker-Based Motion Capture Data", "year": "2018", "abstract": "In this work, we propose an original scheme for generic content-based retrieval of marker-based motion capture data. It works on motion capture data of arbitrary subject types and arbitrary marker attachment and labelling conventions. Specifically, we propose a novel motion signature to statistically describe both the high-level and the low-level morphological and kinematic characteristics of a motion capture sequence, and conduct the content-based retrieval by computing and ordering the motion signature distance between the query and every item in the database. The distance between two motion signatures is computed by a weighted sum of differences in separate features contained in them. For maximum retrieval performance, we propose a method to pre-learn an optimal set of weights for each type of motion in the database through biased discriminant analysis, and adaptively choose a good set of weights for any given query at the run time. Excellence of the proposed scheme is experimentally demonstrated on various data sets and performance metrics.", "keywords": ["content-based retrieval", "feature extraction", "image motion analysis", "image retrieval", "learning (artificial intelligence)", "generic content", "motion capture data", "arbitrary subject types", "arbitrary marker attachment", "labelling conventions", "low-level", "kinematic characteristics", "motion capture sequence", "motion signature distance", "retrieval performance", "generic content-based retrieval", "Feature extraction", "Content-based retrieval", "Labeling", "Two dimensional displays", "Indexes", "Kinematics", "Motion capture", "content-based retrieval", "minimal motion spanning tree", "motion signature", "biased discriminant analysis"], "referenced_by": ["IKEY:9023336", "IKEY:9220910"], "referencing": ["10.1145/1507149.1507181", "10.1145/2448196.2448199", "10.1145/1925101.1925104", "10.1145/1073204.1073247", "10.1145/1073368.1073377", "10.1145/1015706.1015760", "10.1145/2019406.2019427", "10.1145/1180495.1180528", "10.1145/1276377.1276421", "10.1145/1077534.1077546", "10.1145/571647.571648", "10.1145/1507149.1507181", "10.1145/2448196.2448199", "10.1145/1925101.1925104", "10.1145/1073204.1073247", "10.1145/1073368.1073377", "10.1145/1015706.1015760", "10.1145/2019406.2019427", "10.1145/1180495.1180528", "10.1145/1276377.1276421", "10.1145/1077534.1077546", "10.1145/571647.571648", "10.1145/1507149.1507181", "10.1145/2448196.2448199", "10.1145/1925101.1925104", "10.1145/1073204.1073247", "10.1145/1073368.1073377", "10.1145/1015706.1015760", "10.1145/2019406.2019427", "10.1145/1180495.1180528", "10.1145/1276377.1276421", "10.1145/1077534.1077546", "10.1145/571647.571648", "10.1016/j.cviu.2003.06.001", "10.1007/s00371-009-0345-1", "10.1111/j.1467-8659.2011.02048.x", "10.1016/j.cag.2013.11.008", "10.1007/s00500-014-1237-5", "10.1002/cav.1505", "10.1111/j.1467-8659.2012.03198.x", "10.1016/j.sigpro.2015.01.004", "10.1016/j.sigpro.2014.11.015", "10.1007/s11263-012-0550-7", "10.1007/s00371-014-0923-8", "10.5244/C.16.73", "10.1007/3-540-48482-5_14", "10.1080/03610927908827842", "10.1111/j.1467-8659.2008.01135.x", "10.1002/cav.1597", "10.1016/j.cviu.2003.06.001", "10.1007/s00371-009-0345-1", "10.1111/j.1467-8659.2011.02048.x", "10.1016/j.cag.2013.11.008", "10.1007/s00500-014-1237-5", "10.1002/cav.1505", "10.1111/j.1467-8659.2012.03198.x", "10.1016/j.sigpro.2015.01.004", "10.1016/j.sigpro.2014.11.015", "10.1007/s11263-012-0550-7", "10.1007/s00371-014-0923-8", "10.5244/C.16.73", "10.1007/3-540-48482-5_14", "10.1080/03610927908827842", "10.1111/j.1467-8659.2008.01135.x", "10.1002/cav.1597", "10.1016/j.cviu.2003.06.001", "10.1007/s00371-009-0345-1", "10.1111/j.1467-8659.2011.02048.x", "10.1016/j.cag.2013.11.008", "10.1007/s00500-014-1237-5", "10.1002/cav.1505", "10.1111/j.1467-8659.2012.03198.x", "10.1016/j.sigpro.2015.01.004", "10.1016/j.sigpro.2014.11.015", "10.1007/s11263-012-0550-7", "10.1007/s00371-014-0923-8", "10.5244/C.16.73", "10.1007/3-540-48482-5_14", "10.1080/03610927908827842", "10.1111/j.1467-8659.2008.01135.x", "10.1002/cav.1597"]}, "10.1109/TVCG.2017.2704078": {"doi": "10.1109/TVCG.2017.2704078", "author": ["E. Medeiros", "M. Siqueira"], "title": "Good Random Multi-Triangulation of Surfaces", "year": "2018", "abstract": "We introduce the Hierarchical Poisson Disk Sampling Multi-Triangulation (HPDS-MT) of surfaces, a novel structure that combines the power of multi-triangulation (MT) with the benefits of Hierarchical Poisson Disk Sampling (HPDS). MT is a general framework for representing surfaces through variable resolution triangle meshes, while HPDS is a well-spaced random distribution with blue noise characteristics. The distinguishing feature of the HPDS-MT is its ability to extract adaptive meshes whose triangles are guaranteed to have good shape quality. The key idea behind the HPDS-MT is a preprocessed hierarchy of points, which is used in the construction of a MT via incremental simplification. In addition to proving theoretical properties on the shape quality of the triangle meshes extracted by the HPDS-MT, we provide an implementation that computes the HPDS-MT with high accuracy. Our results confirm the theoretical guarantees and outperform similar methods. We also prove that the Hausdorff distance between the original surface and any (extracted) adaptive mesh is bounded by the sampling distribution of the radii of Poisson-disks over the surface. Finally, we illustrate the advantages of the HPDS-MT in some typical problems of geometry processing.", "keywords": ["computational geometry", "computer graphics", "mesh generation", "optimisation", "stochastic processes", "Hierarchical Poisson Disk Sampling MultiTriangulation", "HPDS-MT", "geometry processing", "sampling distribution", "Hausdorff distance", "random multitriangulation", "Shape", "Surface treatment", "Feature extraction", "Face", "Three-dimensional displays", "Adaptation models", "Computational modeling", "Multiresolution", "poisson disk sampling", "triangulation"], "referenced_by": [], "referencing": ["10.1145/1057432.1057457", "10.1145/280814.280831", "10.1145/237170.237217", "10.1145/177424.178010", "10.1145/2516971.2516973", "10.1145/882262.882275", "10.1145/280814.280947", "10.1145/1137856.1137906", "10.1145/1073204.1073238", "10.1145/1015706.1015817", "10.1145/280814.280828", "10.1145/218380.218440", "10.1145/1057432.1057457", "10.1145/280814.280831", "10.1145/237170.237217", "10.1145/177424.178010", "10.1145/2516971.2516973", "10.1145/882262.882275", "10.1145/280814.280947", "10.1145/1137856.1137906", "10.1145/1073204.1073238", "10.1145/1015706.1015817", "10.1145/280814.280828", "10.1145/218380.218440", "10.1145/1057432.1057457", "10.1145/280814.280831", "10.1145/237170.237217", "10.1145/177424.178010", "10.1145/2516971.2516973", "10.1145/882262.882275", "10.1145/280814.280947", "10.1145/1137856.1137906", "10.1145/1073204.1073238", "10.1145/1015706.1015817", "10.1145/280814.280828", "10.1145/218380.218440", "10.1016/S0925-7721(98)00029-7", "10.1016/j.gmod.2013.10.004", "10.1111/j.1467-8659.2007.01100.x", "10.1007/BFb0018533", "10.1016/j.cad.2011.08.012", "10.21236/ADA210101", "10.1016/j.gmod.2005.01.004", "10.1137/060665889", "10.1111/1467-8659.00457", "10.1007/s11390-015-1535-0", "10.1142/S0218654309001215", "10.1111/cgf.12461", "10.1007/978-1-4612-1098-6", "10.1111/j.1467-8659.2010.01781.x", "10.1007/978-3-642-34364-3", "10.1016/j.comgeo.2005.08.002", "10.1016/j.cag.2014.09.015", "10.1007/s00366-009-0162-1", "10.1007/PL00009475", "10.1016/j.cag.2009.03.025", "10.1016/S0925-7721(99)00030-9", "10.1016/j.gmod.2006.05.001", "10.1016/S0925-7721(98)00029-7", "10.1016/j.gmod.2013.10.004", "10.1111/j.1467-8659.2007.01100.x", "10.1007/BFb0018533", "10.1016/j.cad.2011.08.012", "10.21236/ADA210101", "10.1016/j.gmod.2005.01.004", "10.1137/060665889", "10.1111/1467-8659.00457", "10.1007/s11390-015-1535-0", "10.1142/S0218654309001215", "10.1111/cgf.12461", "10.1007/978-1-4612-1098-6", "10.1111/j.1467-8659.2010.01781.x", "10.1007/978-3-642-34364-3", "10.1016/j.comgeo.2005.08.002", "10.1016/j.cag.2014.09.015", "10.1007/s00366-009-0162-1", "10.1007/PL00009475", "10.1016/j.cag.2009.03.025", "10.1016/S0925-7721(99)00030-9", "10.1016/j.gmod.2006.05.001", "10.1016/S0925-7721(98)00029-7", "10.1016/j.gmod.2013.10.004", "10.1111/j.1467-8659.2007.01100.x", "10.1007/BFb0018533", "10.1016/j.cad.2011.08.012", "10.21236/ADA210101", "10.1016/j.gmod.2005.01.004", "10.1137/060665889", "10.1111/1467-8659.00457", "10.1007/s11390-015-1535-0", "10.1142/S0218654309001215", "10.1111/cgf.12461", "10.1007/978-1-4612-1098-6", "10.1111/j.1467-8659.2010.01781.x", "10.1007/978-3-642-34364-3", "10.1016/j.comgeo.2005.08.002", "10.1016/j.cag.2014.09.015", "10.1007/s00366-009-0162-1", "10.1007/PL00009475", "10.1016/j.cag.2009.03.025", "10.1016/S0925-7721(99)00030-9", "10.1016/j.gmod.2006.05.001"]}, "10.1109/TVCG.2017.2698041": {"doi": "10.1109/TVCG.2017.2698041", "author": ["L. Zhou", "D. Weiskopf"], "title": "Indexed-Points Parallel Coordinates Visualization of Multivariate Correlations", "year": "2018", "abstract": "We address the problem of visualizing multivariate correlations in parallel coordinates. We focus on multivariate correlation in the form of linear relationships between multiple variables. Traditional parallel coordinates are well prepared to show negative correlations between two attributes by distinct visual patterns. However, it is difficult to recognize positive correlations in parallel coordinates. Furthermore, there is no support to highlight multivariate correlations in parallel coordinates. In this paper, we exploit the indexed point representation of p -flats (planes in multidimensional data) to visualize local multivariate correlations in parallel coordinates. Our method yields clear visual signatures for negative and positive correlations alike, and it supports large datasets. All information is shown in a unified parallel coordinates framework, which leads to easy and familiar user interactions for analysts who have experience with traditional parallel coordinates. The usefulness of our method is demonstrated through examples of typical multidimensional datasets.", "keywords": ["data visualisation", "parallel processing", "multivariate correlation", "negative correlations", "distinct visual patterns", "positive correlations", "local multivariate correlations", "unified parallel coordinates framework", "indexed-points parallel coordinates", "visual signatures", "multidimensional datasets", "Data visualization", "Correlation", "Visualization", "Two dimensional displays", "Geometry", "Brushes", "Shape", "Multidimensional data visualization", "multivariate correlations", "parallel coordinates"], "referenced_by": ["IKEY:8638183", "IKEY:8811901"], "referencing": ["IKEY:5290705", "IKEY:5613448", "IKEY:6064997", "IKEY:1382894", "IKEY:4015444", "IKEY:5290770", "IKEY:346302", "IKEY:485139", "IKEY:1173157", "IKEY:6464263", "IKEY:4658159", "IKEY:6634155", "IKEY:856994", "IKEY:5290704", "IKEY:6064985", "IKEY:7194847", "IKEY:5290705", "IKEY:5613448", "IKEY:6064997", "IKEY:1382894", "IKEY:4015444", "IKEY:5290770", "IKEY:346302", "IKEY:485139", "IKEY:1173157", "IKEY:6464263", "IKEY:4658159", "IKEY:6634155", "IKEY:856994", "IKEY:5290704", "IKEY:6064985", "IKEY:7194847", "IKEY:5290705", "IKEY:5613448", "IKEY:6064997", "IKEY:1382894", "IKEY:4015444", "IKEY:5290770", "IKEY:346302", "IKEY:485139", "IKEY:1173157", "IKEY:6464263", "IKEY:4658159", "IKEY:6634155", "IKEY:856994", "IKEY:5290704", "IKEY:6064985", "IKEY:7194847", "10.1145/361002.361007", "10.1145/1497577.1497578", "10.1145/846170.846172", "10.1145/2590349", "10.1145/361002.361007", "10.1145/1497577.1497578", "10.1145/846170.846172", "10.1145/2590349", "10.1145/361002.361007", "10.1145/1497577.1497578", "10.1145/846170.846172", "10.1145/2590349", "10.1007/BF01898350", "10.1057/ivs.2008.13", "10.1007/978-1-4757-1951-2", "10.1080/01621459.1990.10474926", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2009.01476.x", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1002/wics.101", "10.1016/j.dss.2009.05.016", "10.1179/000870403235002042", "10.1016/j.ijepes.2014.02.027", "10.1007/BF01898350", "10.1057/ivs.2008.13", "10.1007/978-1-4757-1951-2", "10.1080/01621459.1990.10474926", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2009.01476.x", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1002/wics.101", "10.1016/j.dss.2009.05.016", "10.1179/000870403235002042", "10.1016/j.ijepes.2014.02.027", "10.1007/BF01898350", "10.1057/ivs.2008.13", "10.1007/978-1-4757-1951-2", "10.1080/01621459.1990.10474926", "10.1111/j.1467-8659.2009.01666.x", "10.1111/j.1467-8659.2009.01476.x", "10.1111/j.1467-8659.2009.01477.x", "10.5220/0005717500600071", "10.1002/wics.101", "10.1016/j.dss.2009.05.016", "10.1179/000870403235002042", "10.1016/j.ijepes.2014.02.027"]}, "10.1109/TVCG.2017.2705184": {"doi": "10.1109/TVCG.2017.2705184", "author": ["R. A. Borsoi", "G. H. Costa"], "title": "On the Performance and Implementation of Parallax Free Video See-Through Displays", "year": "2018", "abstract": "In see-through systems an observer watches a (background) scene partially occluded by a display. In this display, usually positioned close to the observer, a region of the background scene is shown, yielding the sensation that the display is transparent. To achieve the transparency effect, it is very important to compensate the parallax error and other distortions caused by the image acquisition system. In this paper a detailed study of a video see-through methodology with parallax correction is performed. In a system composed by two cameras-one directed to the user and another to the background scene-and a display, the relative position between the user, the display and the scene is estimated using a feature detection algorithm and the parallax error is compensated assuming a planar scene model. The application of the proposed methodology on Driver Assistance Systems (DAS) is proposed. A theoretical assessment of the algorithm shows that although approximations are proposed to simplify the methodology and reduce the computational cost, such as the planar scene model and fixed working distance, on some practical situations their effects can be neglected without noticeable impact on the perceptual quality of the solution.", "keywords": ["computer vision", "driver information systems", "feature extraction", "image reconstruction", "video signal processing", "observer", "transparency effect", "parallax error", "image acquisition system", "parallax correction", "relative position", "feature detection algorithm", "planar scene model", "Driver Assistance Systems", "background scene", "video see-through methodology", "parallax free video see-through displays", "Cameras", "Geometry", "Computational modeling", "Vehicles", "Distortion", "Approximation algorithms", "Computational efficiency", "See-through", "virtual transparency", "DAS", "parallax", "augmented reality", "user-perspective rendering", "dual-view"], "referenced_by": [], "referencing": ["IKEY:930982", "IKEY:6170893", "IKEY:6162897", "IKEY:6550226", "IKEY:683770", "IKEY:879797", "IKEY:4497208", "IKEY:930982", "IKEY:6170893", "IKEY:6162897", "IKEY:6550226", "IKEY:683770", "IKEY:879797", "IKEY:4497208", "IKEY:930982", "IKEY:6170893", "IKEY:6162897", "IKEY:6550226", "IKEY:683770", "IKEY:879797", "IKEY:4497208", "10.1145/2470654.2470680", "10.1145/2207676.2208405", "10.1145/503376.503423", "10.1145/2019627.2019631", "10.1145/2582051.2582103", "10.1145/2522848.2522885", "10.1145/2459236.2459275", "10.1145/2559184.2559198", "10.1145/2470654.2470680", "10.1145/2207676.2208405", "10.1145/503376.503423", "10.1145/2019627.2019631", "10.1145/2582051.2582103", "10.1145/2522848.2522885", "10.1145/2459236.2459275", "10.1145/2559184.2559198", "10.1145/2470654.2470680", "10.1145/2207676.2208405", "10.1145/503376.503423", "10.1145/2019627.2019631", "10.1145/2582051.2582103", "10.1145/2522848.2522885", "10.1145/2459236.2459275", "10.1145/2559184.2559198", "10.1162/pres.1997.6.4.355", "10.1162/105474600566808", "10.1007/BF00547132", "10.1023/B:VISI.0000013087.49260.fb", "10.1162/pres.1997.6.4.355", "10.1162/105474600566808", "10.1007/BF00547132", "10.1023/B:VISI.0000013087.49260.fb", "10.1162/pres.1997.6.4.355", "10.1162/105474600566808", "10.1007/BF00547132", "10.1023/B:VISI.0000013087.49260.fb"]}, "10.1109/TVCG.2017.2702738": {"doi": "10.1109/TVCG.2017.2702738", "author": ["Q. Guo", "S. Gao", "X. Zhang", "Y. Yin", "C. Zhang"], "title": "Patch-Based Image Inpainting via Two-Stage Low Rank Approximation", "year": "2018", "abstract": "To recover the corrupted pixels, traditional inpainting methods based on low-rank priors generally need to solve a convex optimization problem by an iterative singular value shrinkage algorithm. In this paper, we propose a simple method for image inpainting using low rank approximation, which avoids the time-consuming iterative shrinkage. Specifically, if similar patches of a corrupted image are identified and reshaped as vectors, then a patch matrix can be constructed by collecting these similar patch-vectors. Due to its columns being highly linearly correlated, this patch matrix is low-rank. Instead of using an iterative singular value shrinkage scheme, the proposed method utilizes low rank approximation with truncated singular values to derive a closed-form estimate for each patch matrix. Depending upon an observation that there exists a distinct gap in the singular spectrum of patch matrix, the rank of each patch matrix is empirically determined by a heuristic procedure. Inspired by the inpainting algorithms with component decomposition, a two-stage low rank approximation (TSLRA) scheme is designed to recover image structures and refine texture details of corrupted images. Experimental results on various inpainting tasks demonstrate that the proposed method is comparable and even superior to some state-of-the-art inpainting algorithms.", "keywords": ["approximation theory", "convex programming", "image denoising", "image restoration", "image texture", "iterative methods", "singular value decomposition", "patch matrix", "two-stage low rank approximation scheme", "corrupted image", "state-of-the-art inpainting algorithms", "image inpainting", "iterative singular value shrinkage algorithm", "time-consuming iterative shrinkage", "iterative singular value shrinkage scheme", "truncated singular values", "Approximation algorithms", "Matrix decomposition", "Convex functions", "Iterative algorithms", "Component decomposition", "image inpainting", "low rank approximation", "self-similarity", "singular value decomposition"], "referenced_by": ["IKEY:8304249", "IKEY:8439932", "IKEY:8547106", "IKEY:8723371", "IKEY:8700607", "IKEY:8772040", "IKEY:8434353", "IKEY:8951161", "IKEY:9007419", "IKEY:9078737", "IKEY:9096341", "IKEY:9133422", "IKEY:9133299", "IKEY:9141413", "IKEY:9190055", "IKEY:8730533", "IKEY:9044621"], "referencing": ["IKEY:7056453", "IKEY:6319405", "IKEY:935036", "IKEY:6824802", "IKEY:7414488", "IKEY:1323101", "IKEY:4069262", "IKEY:6853394", "IKEY:7180400", "IKEY:6714519", "IKEY:6918459", "IKEY:5898409", "IKEY:5742677", "IKEY:6873296", "IKEY:6814320", "IKEY:5452187", "IKEY:7293148", "IKEY:7067415", "IKEY:1217265", "IKEY:7272134", "IKEY:7056453", "IKEY:6319405", "IKEY:935036", "IKEY:6824802", "IKEY:7414488", "IKEY:1323101", "IKEY:4069262", "IKEY:6853394", "IKEY:7180400", "IKEY:6714519", "IKEY:6918459", "IKEY:5898409", "IKEY:5742677", "IKEY:6873296", "IKEY:6814320", "IKEY:5452187", "IKEY:7293148", "IKEY:7067415", "IKEY:1217265", "IKEY:7272134", "IKEY:7056453", "IKEY:6319405", "IKEY:935036", "IKEY:6824802", "IKEY:7414488", "IKEY:1323101", "IKEY:4069262", "IKEY:6853394", "IKEY:7180400", "IKEY:6714519", "IKEY:6918459", "IKEY:5898409", "IKEY:5742677", "IKEY:6873296", "IKEY:6814320", "IKEY:5452187", "IKEY:7293148", "IKEY:7067415", "IKEY:1217265", "IKEY:7272134", "10.1145/2508363.2508403", "10.1145/344779.344972", "10.1145/2508363.2508381", "10.1145/1073204.1073274", "10.1145/1531326.1531330", "10.1145/2766934", "10.1145/2601097.2601205", "10.1145/2185520.2185578", "10.1145/2508363.2508403", "10.1145/344779.344972", "10.1145/2508363.2508381", "10.1145/1073204.1073274", "10.1145/1531326.1531330", "10.1145/2766934", "10.1145/2601097.2601205", "10.1145/2185520.2185578", "10.1145/2508363.2508403", "10.1145/344779.344972", "10.1145/2508363.2508381", "10.1145/1073204.1073274", "10.1145/1531326.1531330", "10.1145/2766934", "10.1145/2601097.2601205", "10.1145/2185520.2185578", "10.1111/cgf.12521", "10.1007/s10208-009-9045-5", "10.1137/080738970", "10.1007/s11263-006-5631-z", "10.1137/S0036139900368844", "10.1137/S0036139901390088", "10.1007/s41095-016-0064-2", "10.1007/s11263-010-0418-7", "10.5201/ipol.2015.136", "10.1016/j.acha.2005.03.005", "10.1007/s11390-016-1645-3", "10.1007/s41095-016-0056-2", "10.1007/s41095-015-0005-5", "10.1111/cgf.13019", "10.1007/978-3-642-19318-7_25", "10.1137/1.9780898719697", "10.1137/1.9781611971408", "10.1007/s10851-011-0294-y", "10.1007/BF01937276", "10.1137/110854989", "10.1137/S1064827597327309", "10.1007/978-3-642-15558-1_3", "10.1111/cgf.12521", "10.1007/s10208-009-9045-5", "10.1137/080738970", "10.1007/s11263-006-5631-z", "10.1137/S0036139900368844", "10.1137/S0036139901390088", "10.1007/s41095-016-0064-2", "10.1007/s11263-010-0418-7", "10.5201/ipol.2015.136", "10.1016/j.acha.2005.03.005", "10.1007/s11390-016-1645-3", "10.1007/s41095-016-0056-2", "10.1007/s41095-015-0005-5", "10.1111/cgf.13019", "10.1007/978-3-642-19318-7_25", "10.1137/1.9780898719697", "10.1137/1.9781611971408", "10.1007/s10851-011-0294-y", "10.1007/BF01937276", "10.1137/110854989", "10.1137/S1064827597327309", "10.1007/978-3-642-15558-1_3", "10.1111/cgf.12521", "10.1007/s10208-009-9045-5", "10.1137/080738970", "10.1007/s11263-006-5631-z", "10.1137/S0036139900368844", "10.1137/S0036139901390088", "10.1007/s41095-016-0064-2", "10.1007/s11263-010-0418-7", "10.5201/ipol.2015.136", "10.1016/j.acha.2005.03.005", "10.1007/s11390-016-1645-3", "10.1007/s41095-016-0056-2", "10.1007/s41095-015-0005-5", "10.1111/cgf.13019", "10.1007/978-3-642-19318-7_25", "10.1137/1.9780898719697", "10.1137/1.9781611971408", "10.1007/s10851-011-0294-y", "10.1007/BF01937276", "10.1137/110854989", "10.1137/S1064827597327309", "10.1007/978-3-642-15558-1_3"]}, "10.1109/TVCG.2017.2703612": {"doi": "10.1109/TVCG.2017.2703612", "author": ["T. Leimk\u00fchler", "P. Kellnhofer", "T. Ritschel", "K. Myszkowski", "H. Seidel"], "title": "Perceptual Real-Time 2D-to-3D Conversion Using Cue Fusion", "year": "2018", "abstract": "We propose a system to infer binocular disparity from a monocular video stream in real-time. Different from classic reconstruction of physical depth in computer vision, we compute perceptually plausible disparity, that is numerically inaccurate, but results in a very similar overall depth impression with plausible overall layout, sharp edges, fine details and agreement between luminance and disparity. We use several simple monocular cues to estimate disparity maps and confidence maps of low spatial and temporal resolution in real-time. These are complemented by spatially-varying, appearance-dependent and class-specific disparity prior maps, learned from example stereo images. Scene classification selects this prior at runtime. Fusion of prior and cues is done by means of robust MAP inference on a dense spatio-temporal conditional random field with high spatial and temporal resolution. Using normal distributions allows this in constant-time, parallel per-pixel work. We compare our approach to previous 2D-to-3D conversion systems in terms of different metrics, as well as a user study and validate our notion of perceptually plausible disparity.", "keywords": ["computer vision", "image classification", "image fusion", "image reconstruction", "image resolution", "normal distribution", "stereo image processing", "video streaming", "cue fusion", "binocular disparity", "monocular video stream", "computer vision", "perceptually plausible disparity", "simple monocular cues", "confidence maps", "temporal resolution", "class-specific disparity prior maps", "robust MAP inference", "dense spatio-temporal conditional random field", "spatial resolution", "Perceptual Real-Time 2D-to-3D Conversion", "classic physical depth reconstruction", "spatially-varying disparity prior maps", "appearance-dependent disparity prior maps", "stereo images", "scene classification", "normal distributions", "Real-time systems", "Streaming media", "Image edge detection", "Three-dimensional displays", "Image reconstruction", "Spatial resolution", "Runtime", "Depth cues", "stereo", "image-based rendering", "perceptual reasoning", "video analysis", "viewing algorithms", "pixel classification", "real-time systems"], "referenced_by": [], "referencing": ["IKEY:5739557", "IKEY:5590261", "IKEY:4531745", "IKEY:6787109", "IKEY:5156848", "IKEY:4293013", "IKEY:4767940", "IKEY:6566029", "IKEY:855433", "IKEY:710815", "IKEY:5739557", "IKEY:5590261", "IKEY:4531745", "IKEY:6787109", "IKEY:5156848", "IKEY:4293013", "IKEY:4767940", "IKEY:6566029", "IKEY:855433", "IKEY:710815", "IKEY:5739557", "IKEY:5590261", "IKEY:4531745", "IKEY:6787109", "IKEY:5156848", "IKEY:4293013", "IKEY:4767940", "IKEY:6566029", "IKEY:855433", "IKEY:710815", "10.1145/1778765.1778812", "10.1145/2366145.2366203", "10.1145/2185520.2335385", "10.1145/1073204.1073232", "10.1145/2508363.2508396", "10.1145/1276377.1276497", "10.1145/1360612.1360671", "10.1145/1276377.1276506", "10.1145/253284.253292", "10.1145/2804408.2804409", "10.1145/2980179.2980230", "10.1145/1778765.1778812", "10.1145/2366145.2366203", "10.1145/2185520.2335385", "10.1145/1073204.1073232", "10.1145/2508363.2508396", "10.1145/1276377.1276497", "10.1145/1360612.1360671", "10.1145/1276377.1276506", "10.1145/253284.253292", "10.1145/2804408.2804409", "10.1145/2980179.2980230", "10.1145/1778765.1778812", "10.1145/2366145.2366203", "10.1145/2185520.2335385", "10.1145/1073204.1073232", "10.1145/2508363.2508396", "10.1145/1276377.1276497", "10.1145/1360612.1360671", "10.1145/1276377.1276506", "10.1145/253284.253292", "10.1145/2804408.2804409", "10.1145/2980179.2980230", "10.1038/nn1059", "10.1016/j.image.2008.10.010", "10.1523/JNEUROSCI.1652-13.2014", "10.1093/acprof:oso/9780199764143.001.0001", "10.1111/j.1467-8659.2007.01083.x", "10.1889/1.1833914", "10.1007/s11263-015-0806-0", "10.1111/j.1467-8659.2012.03003.x", "10.1016/0042-6989(94)00176-M", "10.1017/CBO9780511984037", "10.1167/13.11.3", "10.1167/9.1.8", "10.1017/S0952523808080930", "10.1117/12.474113", "10.1117/12.807147", "10.1016/S0004-3702(83)80021-6", "10.1007/3-540-57956-7_10", "10.1007/BF00126502", "10.1038/nn1059", "10.1016/j.image.2008.10.010", "10.1523/JNEUROSCI.1652-13.2014", "10.1093/acprof:oso/9780199764143.001.0001", "10.1111/j.1467-8659.2007.01083.x", "10.1889/1.1833914", "10.1007/s11263-015-0806-0", "10.1111/j.1467-8659.2012.03003.x", "10.1016/0042-6989(94)00176-M", "10.1017/CBO9780511984037", "10.1167/13.11.3", "10.1167/9.1.8", "10.1017/S0952523808080930", "10.1117/12.474113", "10.1117/12.807147", "10.1016/S0004-3702(83)80021-6", "10.1007/3-540-57956-7_10", "10.1007/BF00126502", "10.1038/nn1059", "10.1016/j.image.2008.10.010", "10.1523/JNEUROSCI.1652-13.2014", "10.1093/acprof:oso/9780199764143.001.0001", "10.1111/j.1467-8659.2007.01083.x", "10.1889/1.1833914", "10.1007/s11263-015-0806-0", "10.1111/j.1467-8659.2012.03003.x", "10.1016/0042-6989(94)00176-M", "10.1017/CBO9780511984037", "10.1167/13.11.3", "10.1167/9.1.8", "10.1017/S0952523808080930", "10.1117/12.474113", "10.1117/12.807147", "10.1016/S0004-3702(83)80021-6", "10.1007/3-540-57956-7_10", "10.1007/BF00126502"]}, "10.1109/TVCG.2017.2700470": {"doi": "10.1109/TVCG.2017.2700470", "author": ["C. Lu", "Y. Xiao", "C. Tang"], "title": "Real-Time Video Stylization Using Object Flows", "year": "2018", "abstract": "We present a real-time video stylization system and demonstrate a variety of painterly styles rendered on real video inputs. The key technical contribution lies on the object flow, which is robust to inaccurate optical flow, unknown object transformation and partial occlusion as well. Since object flows relate regions of the same object across frames, shower-door effect can be effectively reduced where painterly strokes and textures are rendered on video objects. The construction of object flows is performed in real time and automatically after applying metric learning. To reduce temporal flickering, we extend the bilateral filtering into motion bilateral filtering. We propose quantitative metrics to measure the temporal coherence on structures and textures of our stylized videos, and perform extensive experiments to compare our stylized results with baseline systems and prior works specializing in watercolor and abstraction.", "keywords": ["image colour analysis", "image sequences", "image texture", "learning (artificial intelligence)", "object detection", "video signal processing", "object flow", "real-time video stylization system", "video inputs", "inaccurate optical flow", "unknown object transformation", "video objects", "Streaming media", "Real-time systems", "Coherence", "Integrated optics", "Optical imaging", "Measurement", "Animation", "Video stylization", "video processing", "metric learning", "video flows", "bilateral filtering"], "referenced_by": ["IKEY:9106365", "IKEY:8624466"], "referencing": ["IKEY:6243138", "IKEY:5406517", "IKEY:5728804", "IKEY:1000236", "IKEY:6104061", "IKEY:6243138", "IKEY:5406517", "IKEY:5728804", "IKEY:1000236", "IKEY:6104061", "IKEY:6243138", "IKEY:5406517", "IKEY:5728804", "IKEY:1000236", "IKEY:6104061", "10.1145/237170.237288", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/280814.280951", "10.1145/2024676.2024690", "10.1145/566654.566650", "10.1145/1141911.1142018", "10.1145/1015706.1015764", "10.1145/1015706.1015763", "10.1145/1809939.1809948", "10.1145/1531326.1531376", "10.1145/1276377.1276507", "10.1145/2010324.1964925", "10.1145/1124728.1124735", "10.1145/237170.237288", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/280814.280951", "10.1145/2024676.2024690", "10.1145/566654.566650", "10.1145/1141911.1142018", "10.1145/1015706.1015764", "10.1145/1015706.1015763", "10.1145/1809939.1809948", "10.1145/1531326.1531376", "10.1145/1276377.1276507", "10.1145/2010324.1964925", "10.1145/1124728.1124735", "10.1145/237170.237288", "10.1145/340916.340917", "10.1145/987657.987676", "10.1145/280814.280951", "10.1145/2024676.2024690", "10.1145/566654.566650", "10.1145/1141911.1142018", "10.1145/1015706.1015764", "10.1145/1015706.1015763", "10.1145/1809939.1809948", "10.1145/1531326.1531376", "10.1145/1276377.1276507", "10.1145/2010324.1964925", "10.1145/1124728.1124735", "10.1111/j.1467-8659.2011.02075.x", "10.1007/0-387-28831-7", "10.1111/1467-8659.1130227", "10.1007/s11263-010-0390-2", "10.1111/j.1467-8659.2011.02075.x", "10.1007/0-387-28831-7", "10.1111/1467-8659.1130227", "10.1007/s11263-010-0390-2", "10.1111/j.1467-8659.2011.02075.x", "10.1007/0-387-28831-7", "10.1111/1467-8659.1130227", "10.1007/s11263-010-0390-2"]}}