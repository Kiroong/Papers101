{"10.1109/TVCG.2017.2738998": {"doi": "10.1109/TVCG.2017.2738998", "author": ["L. De Floriani", "D. Schmalstieg"], "title": "Message from the Editor-in-Chief and from the Associate Editor-in-Chief", "year": "2017", "abstract": "Welcome the November 2017 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR), held this year in Nantes, France, from September 9 to September 13, 2017.", "keywords": ["Special issues and sections", "Meetings", "Augmented reality"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2734327": {"doi": "10.1109/TVCG.2017.2734327", "author": ["H. Roodaki", "N. Navab", "A. Eslami", "C. Stapleton", "N. Navab"], "title": "SonifEye: Sonification of Visual Information Using Physical Modeling Sound Synthesis", "year": "2017", "abstract": "Sonic interaction as a technique for conveying information has advantages over conventional visual augmented reality methods specially when augmenting the visual field with extra information brings distraction. Sonification of knowledge extracted by applying computational methods to sensory data is a well-established concept. However, some aspects of sonic interaction design such as aesthetics, the cognitive effort required for perceiving information, and avoiding alarm fatigue are not well studied in literature. In this work, we present a sonification scheme based on employment of physical modeling sound synthesis which targets focus demanding tasks requiring extreme precision. Proposed mapping techniques are designed to require minimum training for users to adapt to and minimum mental effort to interpret the conveyed information. Two experiments are conducted to assess the feasibility of the proposed method and compare it against visual augmented reality in high precision tasks. The observed quantitative results suggest that utilizing sound patches generated by physical modeling achieve the desired goal of improving the user experience and general task performance with minimal training.", "keywords": ["augmented reality", "data visualisation", "SonifEye", "visual information", "sound patches", "conveyed information", "minimum mental effort", "mapping techniques", "sonification scheme", "cognitive effort", "sonic interaction design", "sensory data", "computational methods", "visual field", "conventional visual augmented reality methods", "physical modeling sound synthesis", "Visualization", "Mathematical model", "Augmented reality", "Load modeling", "Auditory displays", "Acceleration", "Computational modeling", "Aural augmented reality", "sonification", "sonic interaction", "auditory feedback", "Computer Graphics", "Feedback, Sensory", "Humans", "Models, Neurological", "Psychomotor Performance", "Software", "Virtual Reality"], "referenced_by": ["10.1109/TVCG.2018.2868591", "10.1109/CogInfoCom.2018.8639902", "10.1109/ISMAR.2019.000-3", "10.1007/s11548-018-1827-2", "10.1016/B978-0-12-814245-5.00036-0"], "referencing": ["10.1109/TMM.2016.2531978", "10.1109/JDT.2008.2001575", "10.1371/journal.pone.0082491", "10.1007/978-3-319-46720-7_44", "10.1007/978-3-642-04076-4_6", "10.3758/BF03206843", "10.3758/BF03195786"]}, "10.1109/TVCG.2017.2735078": {"doi": "10.1109/TVCG.2017.2735078", "author": ["Y. Gaffary", "B. Le Gouis", "M. Marchal", "F. Argelaguet", "B. Arnaldi", "A. L\u00e9cuyer"], "title": "AR Feels \u201cSofter\u201d than VR: Haptic Perception of Stiffness in Augmented versus Virtual Reality", "year": "2017", "abstract": "Does it feel the same when you touch an object in Augmented Reality (AR) or in Virtual Reality (VR)? In this paper we study and compare the haptic perception of stiffness of a virtual object in two situations: (1) a purely virtual environment versus (2) a real and augmented environment. We have designed an experimental setup based on a Microsoft HoloLens and a haptic force-feedback device, enabling to press a virtual piston, and compare its stiffness successively in either Augmented Reality (the virtual piston is surrounded by several real objects all located inside a cardboard box) or in Virtual Reality (the same virtual piston is displayed in a fully virtual scene composed of the same other objects). We have conducted a psychophysical experiment with 12 participants. Our results show a surprising bias in perception between the two conditions. The virtual piston is on average perceived stiffer in the VR condition compared to the AR condition. For instance, when the piston had the same stiffness in AR and VR, participants would select the VR piston as the stiffer one in 60% of cases. This suggests a psychological effect as if objects in AR would feel \u201dsofter\u201d than in pure VR. Taken together, our results open new perspectives on perception in AR versus VR, and pave the way to future studies aiming at characterizing potential perceptual biases.", "keywords": ["augmented reality", "force feedback", "haptic interfaces", "pistons", "psychology", "VR piston", "pure VR", "haptic perception", "Virtual Reality", "Augmented Reality", "virtual object", "purely virtual environment", "real environment", "augmented environment", "haptic force-feedback device", "virtual piston", "fully virtual scene", "VR condition", "Microsoft HoloLens", "psychological effect", "Pistons", "Haptic interfaces", "Visualization", "Augmented reality", "Virtual reality", "Virtual environments", "Physiology", "Psychology", "Augmented Reality", "Virtual Reality", "Haptic", "Perception", "Stiffness", "Psychophysical Study", "Adult", "Computer Graphics", "Equipment Design", "Female", "Humans", "Male", "Psychophysics", "Touch", "Virtual Reality", "Young Adult"], "referenced_by": ["10.1109/ACCESS.2018.2875558", "10.1109/TOH.2019.2961883"], "referencing": ["10.1109/HAPTICS.2008.4479918", "10.1109/WHC.2009.4810845", "10.1109/TOH.2012.74", "10.1109/ISMAR.2009.5336501", "10.1109/TVCG.2015.2459792", "10.1162/pres.15.3.353", "10.1007/978-3-540-69057-3_78", "10.1007/BF00228884", "10.1162/pres.18.1.39", "10.1177/0278364907082611", "10.1162/pres.1995.4.1.24", "10.1007/978-3-662-44193-0_60", "10.3758/APP.71.5.1096"]}, "10.1109/TVCG.2017.2735098": {"doi": "10.1109/TVCG.2017.2735098", "author": ["J. Baumeister", "S. Y. Ssin", "N. A. M. ElSayed", "J. Dorrian", "D. P. Webb", "J. A. Walsh", "T. M. Simon", "A. Irlitti", "R. T. Smith", "M. Kohler", "B. H. Thomas"], "title": "Cognitive Cost of Using Augmented Reality Displays", "year": "2017", "abstract": "This paper presents the results of two cognitive load studies comparing three augmented reality display technologies: spatial augmented reality, the optical see-through Microsoft HoloLens, and the video see-through Samsung Gear VR. In particular, the two experiments focused on isolating the cognitive load cost of receiving instructions for a button-pressing procedural task. The studies employed a self-assessment cognitive load methodology, as well as an additional dual-task cognitive load methodology. The results showed that spatial augmented reality led to increased performance and reduced cognitive load. Additionally, it was discovered that a limited field of view can introduce increased cognitive load requirements. The findings suggest that some of the inherent restrictions of head-mounted displays materialize as increased user cognitive load.", "keywords": ["augmented reality", "cognition", "helmet mounted displays", "wearable computers", "cognitive load cost", "button-pressing procedural task", "self-assessment cognitive load methodology", "spatial augmented reality", "increased cognitive load requirements", "augmented reality displays", "Microsoft HoloLens", "Samsung Gear VR", "dual-task cognitive load methodology", "user cognitive load", "head-mounted displays", "Human computer interaction", "Augmented reality", "Cognition", "Monitoring", "Mobile communication", "Training", "Visual perception", "Augmented reality", "human computer interaction", "cognitive load", "Adult", "Cognition", "Computer Graphics", "Equipment Design", "Female", "Humans", "Male", "Middle Aged", "Reaction Time", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": ["10.1109/SYSMC.2018.8509744", "10.1109/TVCG.2018.2868587", "10.1109/TVCG.2018.2868591", "10.1109/VR.2019.8798312", "10.1109/TVCG.2019.2932173", "10.1109/THMS.2019.2944384", "10.1109/ISMAR.2019.000-3", "10.1109/TVCG.2019.2892415", "10.1109/TVCG.2020.3023605", "10.1109/IETC47856.2020.9249161", "10.1109/ISMAR50242.2020.00069", "10.1109/ISMAR-Adjunct51615.2020.00068"], "referencing": ["10.1109/TVCG.2016.2518133", "10.1109/TVCG.2013.86", "10.1109/ISMAR.2009.5336486", "10.1109/ISMAR.2011.6092386", "10.1109/ISMAR.2014.6948425", "10.1109/ISMAR.2013.6671762", "10.1109/2.989929", "10.1109/ISMAR.2008.4637331", "10.1145/2808435.2808455", "10.1145/2971648.2971706", "10.1145/2371574.2371610", "10.1145/2856400.2856416", "10.1207/S15326985EP3801_7", "10.1007/BF01682023", "10.1080/10494820.2013.815221", "10.1016/S0166-4115(08)62386-9", "10.1061/(ASCE)CP.1943-5487.0000184", "10.1348/000709904X19254", "10.4018/978-1-59904-945-8.ch022", "10.1007/978-1-4614-4205-9_3", "10.1037/h0043158", "10.1207/S15326985EP3801_8", "10.1037//0022-0663.84.4.429", "10.1037/0033-2909.116.2.220", "10.1162/105474600566808", "10.1007/978-3-540-27780-4_26", "10.1007/s40436-015-0131-4", "10.1177/0018720815623431", "10.1007/978-3-319-22723-8_59"]}, "10.1109/TVCG.2017.2734458": {"doi": "10.1109/TVCG.2017.2734458", "author": ["Z. Yan", "M. Ye", "L. Ren"], "title": "Dense Visual SLAM with Probabilistic Surfel Map", "year": "2017", "abstract": "Visual SLAM is one of the key technologies to align the virtual and real world together in Augmented Reality applications. RGBD dense Visual SLAM approaches have shown their advantages in robustness and accuracy in recent years. However, there are still several challenges such as the inconsistencies in RGBD measurements across multiple frames that could jeopardize the accuracy of both camera trajectory and scene reconstruction. In this paper, we propose a novel map representation called Probabilistic Surfel Map (PSM) for dense visual SLAM. The main idea is to maintain a globally consistent map with both photometric and geometric uncertainties encoded in order to address the inconsistency issue. The key of our PSM is proper modeling and updating of sensor measurement uncertainties, as well as the strategies to apply them for improving both the front-end pose estimation and the back-end optimization. Experimental results on publicly available datasets demonstrate major improvements with our approach over the state-of-the-art methods. Specifically, comparing with \u03c3-DVO, we achieve a 40% reduction in absolute trajectory error and an 18% reduction in relative pose error in visual odometry, as well as an 8.5% reduction in absolute trajectory error in complete SLAM. Moreover, our PSM enables generation of a high quality dense point cloud with comparable accuracy as the state-of-the-art approach.", "keywords": ["augmented reality", "cameras", "distance measurement", "image reconstruction", "image representation", "pose estimation", "robot vision", "SLAM (robots)", "Augmented Reality applications", "RGBD dense Visual SLAM approaches", "RGBD measurements", "scene reconstruction", "Probabilistic Surfel Map", "PSM", "globally consistent map", "photometric uncertainties", "geometric uncertainties", "sensor measurement uncertainties", "absolute trajectory error", "visual odometry", "high quality dense point cloud", "camera trajectory", "map representation", "front-end pose estimation", "back-end optimization", "simultaneously localization and mapping", "Visualization", "Simultaneous localization and mapping", "Three-dimensional displays", "Cameras", "Image reconstruction", "Trajectory", "Augmented reality", "Object tracking", "Visual SLAM", "Dense Visual Odometry", "RGBD 6-DoF Tracking", "3D Reconstruction", "Augmented Reality", "Computer Graphics", "Databases, Factual", "Imaging, Three-Dimensional", "Virtual Reality"], "referenced_by": ["10.1109/TVCG.2018.2868591", "10.1109/ISMAR-Adjunct.2018.00034", "10.1109/CVPR.2019.00022", "10.1109/TVCG.2020.2990315", "10.1109/TVCG.2018.2889944", "10.1109/TPAMI.2019.2947048"], "referencing": ["10.1109/ICRA.2012.6225199", "10.1109/ISMAR.2011.6092378", "10.1109/ISMAR.2014.6948420", "10.1109/ICCVW.2011.6130500", "10.1023/B:VISI.0000011205.11775.fd", "10.1007/978-3-319-10605-2_54", "10.1007/978-3-319-46484-8_30", "10.1007/978-3-540-77457-0_17", "10.1016/j.jvcir.2013.02.008", "10.1177/0278364914551008", "10.15607/RSS.2015.XI.001"]}, "10.1109/TVCG.2017.2734539": {"doi": "10.1109/TVCG.2017.2734539", "author": ["D. J. Tan", "N. Navab", "F. Tombari"], "title": "Looking Beyond the Simple Scenarios: Combining Learners and Optimizers in 3D Temporal Tracking", "year": "2017", "abstract": "3D object temporal trackers estimate the 3D rotation and 3D translation of a rigid object by propagating the transformation from one frame to the next. To confront this task, algorithms either learn the transformation between two consecutive frames or optimize an energy function to align the object to the scene. The motivation behind our approach stems from a consideration on the nature of learners and optimizers. Throughout the evaluation of different types of objects and working conditions, we observe their complementary nature - on one hand, learners are more robust when undergoing challenging scenarios, while optimizers are prone to tracking failures due to the entrapment at local minima; on the other, optimizers can converge to a better accuracy and minimize jitter. Therefore, we propose to bridge the gap between learners and optimizers to attain a robust and accurate RGB-D temporal tracker that runs at approximately 2 ms per frame using one CPU core. Our work is highly suitable for Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) applications due to its robustness, accuracy, efficiency and low latency. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in the absence of clutter, averting to touch the object to prevent close-range partial occlusion or selecting brightly colored objects to easily segment them individually, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions.", "keywords": ["augmented reality", "image segmentation", "image sequences", "learning (artificial intelligence)", "motion estimation", "object tracking", "virtual reality", "3D temporal tracking", "rigid object", "consecutive frames", "optimizers", "tracking failures", "robust RGB-D temporal tracker", "close-range partial occlusion", "learners", "energy function", "CPU core", "augmented reality", "mixed reality", "virtual reality", "Robustness", "Three-dimensional displays", "Tracking", "Cameras", "Solid modeling", "Pose estimation", "Iterative closest point algorithm", "3D Tracking", "Random Forest", "6D Pose Estimation"], "referenced_by": ["10.1109/WIO.2018.8643463", "10.1109/TPAMI.2018.2884990", "10.1109/TIP.2020.2973512", "10.1109/LRA.2020.3003866", "10.1109/SIBGRAPI51738.2020.00052"], "referencing": ["10.1109/34.121791", "10.1109/TPAMI.2002.1017625", "10.1109/ISMAR.2011.6092378", "10.1109/ISMAR.2008.4637336", "10.1109/3DV.2014.39", "10.1109/ICRA.2011.5980567", "10.1109/ISMAR.2014.6948422", "10.1109/TPAMI.2004.92", "10.1145/2380116.2380170", "10.1023/B:VISI.0000011205.11775.fd", "10.1023/A:1010933404324", "10.1016/0262-8856(92)90066-C", "10.1016/S0262-8856(02)00013-6", "10.5244/C.4.15", "10.1007/978-3-642-37431-9_2", "10.1007/978-3-642-33718-5_34", "10.1023/B:VISI.0000029664.99615.94", "10.1175/1520-0493(1968)096&lt;0351:IOTNBV&gt;2.0.CO;2", "10.1007/978-3-319-46493-0_26"]}, "10.1109/TVCG.2017.2734599": {"doi": "10.1109/TVCG.2017.2734599", "author": ["M. Garon", "J. Lalonde"], "title": "Deep 6-DOF Tracking", "year": "2017", "abstract": "We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.", "keywords": ["computer vision", "image sequences", "learning (artificial intelligence)", "object tracking", "pose estimation", "real-time performance", "robust tracking", "tracking method", "deep learning", "state-of-the-art performance", "datasets", "RGBD sequences", "Robustness", "Cameras", "Real-time systems", "Rendering (computer graphics)", "Three-dimensional displays", "Machine learning", "Neural networks", "Augmented reality", "Tracking", "Tracking", "Deep Learning", "Augmented Reality"], "referenced_by": ["10.1109/3DV.2018.00032", "10.1109/TVCG.2018.2868591", "10.1109/IROS.2018.8594187", "10.1109/IROS.2018.8593776", "10.1109/ISMAR.2018.00028", "10.1109/WIO.2018.8643463", "10.1109/ISMAR-Adjunct.2018.00080", "10.1109/TIP.2020.2973512", "10.1109/UR49135.2020.9144789", "10.1109/ICRA40945.2020.9197155", "10.1109/ICRA40945.2020.9196966", "10.1109/SIBGRAPI51738.2020.00052", "10.1109/ISMAR50242.2020.00034"], "referencing": ["10.1109/ICCVW.2015.29", "10.1109/ICCV.2015.499", "10.1109/ICCV.2015.336", "10.1109/ICCV.2015.115", "10.1109/ISMAR.2011.6092378", "10.1109/ICCV.2015.379", "10.1109/ICRA.2017.7989233", "10.1109/TPAMI.2016.2577031", "10.1109/TIP.2003.819861", "10.1007/978-3-319-48881-3_56", "10.1023/A:1010933404324", "10.18637/jss.v008.i17", "10.1016/j.patcog.2014.01.005", "10.1007/978-3-319-46487-9_13", "10.1007/978-3-319-16817-3_25", "10.1017/S0263574707003529"]}, "10.1109/TVCG.2017.2734478": {"doi": "10.1109/TVCG.2017.2734478", "author": ["Y. Kitajima", "D. Iwai", "K. Sato"], "title": "Simultaneous Projection and Positioning of Laser Projector Pixels", "year": "2017", "abstract": "This paper presents a novel projected pixel localization principle for online geometric registration in dynamic projection mapping applications. We propose applying a time measurement of a laser projector raster-scanning beam using a photosensor to estimate its position while the projector displays meaningful visual information to human observers. Based on this principle, we develop two types of position estimation techniques. One estimates the position of a projected beam when it directly illuminates a photosensor. The other localizes a beam by measuring the reflection from a retro-reflective marker with the photosensor placed in the optical path of the projector. We conduct system evaluations using prototypes to validate this method as well as to confirm the applicability of our principle. In addition, we discuss the technical limitations of the prototypes based on the evaluation results. Finally, we build several dynamic projection mapping applications to demonstrate the feasibility of our principle.", "keywords": ["cameras", "computer vision", "data visualisation", "image reconstruction", "image registration", "image resolution", "optical projectors", "retro-reflective marker", "photosensor", "dynamic projection mapping applications", "projected pixel localization principle", "online geometric registration", "time measurement", "laser projector raster-scanning beam", "human observers", "position estimation techniques", "projected beam", "beam localization", "visual information", "simultaneous projection", "laser projector pixels positioning", "Measurement by laser beam", "Laser beams", "Position measurement", "Surface texture", "Cameras", "Calibration", "Lasers", "Augmented reality", "Dynamic projection mapping", "spatial augmented reality", "laser projector", "light pen", "geometric registration"], "referenced_by": ["10.1109/VR.2018.8446049", "10.1109/TVCG.2018.2868591", "10.1109/ISMAR-Adjunct.2019.00034", "10.1109/TVCG.2020.2973444"], "referencing": ["10.1109/CVPRW.2006.172", "10.1109/ISAR.2001.970539", "10.1109/TIP.2015.2478388", "10.1109/GCCE.2014.7031234", "10.1109/TVCG.2015.2391861", "10.1109/IWAR.1999.803809", "10.1109/TVCG.2015.2459905", "10.1109/34.888718", "10.1145/1709886.1709897", "10.1145/1836821.1836830", "10.1145/2047196.2047255", "10.1145/2984511.2984547", "10.1145/1029632.1029653", "10.1145/1095034.1095045", "10.1145/2816795.2818133", "10.1145/1015706.1015738", "10.1145/1276377.1276422", "10.1145/882262.882349", "10.1111/cgf.13128", "10.1364/OPN.20.5.000028", "10.1016/j.infrared.2009.11.001", "10.1007/s10055-010-0168-4", "10.1007/s10055-014-0256-y"]}, "10.1109/TVCG.2017.2734598": {"doi": "10.1109/TVCG.2017.2734598", "author": ["P. Pjanic", "S. Willi", "A. Grundh\u00f6fer"], "title": "Geometric and Photometric Consistency in a Mixed Video and Galvanoscopic Scanning Laser Projection Mapping System", "year": "2017", "abstract": "We present a geometric calibration method to accurately register a galvanoscopic scanning laser projection system (GLP) based on 2D vector input data onto an arbitrarily complex 3D-shaped projection surface. This method allows for accurate merging of 3D vertex data displayed on the laser projector with geometrically calibrated standard rasterization-based video projectors that are registered to the same geometry. Because laser projectors send out a laser light beam via galvanoscopic mirrors, a standard pinhole model calibration procedure that is normally used for pixel raster displays projecting structured light patterns, such as Gray codes, cannot be carried out directly with sufficient accuracy as the rays do not converge into a single point. To overcome the complications of accurately registering the GLP while still enabling a treatment equivalent to a standard pinhole device, an adapted version is applied to enable straightforward content generation. Besides the geometrical calibration, we also present a photometric calibration to unify the color appearance of GLPs and standard video projectors maximizing the advantages of the large color gamut of the GLP and optimizing its color appearance to smoothly fade into the significantly smaller gamut of the video projector. The proposed algorithms were evaluated on a prototypical mixed video projector and GLP projection mapping setup.", "keywords": ["calibration", "computational geometry", "Gray codes", "image colour analysis", "image reconstruction", "optical projectors", "video signal processing", "photometric calibration", "color appearance", "standard video projectors", "prototypical mixed video projector", "GLP projection mapping setup", "photometric consistency", "galvanoscopic scanning laser projection mapping system", "geometric calibration method", "2D vector input data", "3D-shaped projection surface", "3D vertex data", "laser projector", "geometrically calibrated standard rasterization-based video projectors", "laser light beam", "galvanoscopic mirrors", "standard pinhole model calibration procedure", "pixel raster displays", "structured light patterns", "standard pinhole device", "geometrical calibration", "Calibration", "Image color analysis", "Three-dimensional displays", "Image registration", "Lasers", "Cameras", "Optical distortion", "Stereoscopy", "Entertainment", "Broadcasting", "Projector-camera systems", "Calibration and registration of sensing systems", "Display hardware", "including 3D", "stereoscopic and multi-user Entertainment", "broadcast"], "referenced_by": ["10.1109/ISMAR-Adjunct51615.2020.00058"], "referencing": ["10.1109/CVPRW.2006.172", "10.1109/VR.2006.34", "10.1109/VISUAL.2002.1183793", "10.1109/URAI.2015.7358918", "10.1109/TCI.2017.2652844", "10.1109/MC.2012.154", "10.1109/DigitalHeritage.2013.6744835", "10.1109/TVCG.2016.2593766", "10.1109/34.888718", "10.1145/2614066.2614078", "10.1145/2754391", "10.1145/2816795.2818070", "10.1145/585740.585765", "10.1145/1037957.1037964", "10.1145/769953.769988", "10.1007/978-3-319-16199-0_20", "10.1007/3-540-47977-5_2", "10.1007/BFb0086566", "10.1016/j.jvcir.2016.05.014", "10.1016/j.patcog.2014.01.005", "10.1016/j.patcog.2015.09.023", "10.1016/j.displa.2009.03.003", "10.3390/sym7010182", "10.1016/S0377-0427(00)00367-8", "10.3756/artsci.7.155", "10.1364/AO.49.005914", "10.1111/j.1467-8659.2009.01608.x", "10.1007/3-540-44480-7_21", "10.1016/j.optlaseng.2016.12.012", "10.3390/s17010164"]}, "10.1109/TVCG.2017.2734428": {"doi": "10.1109/TVCG.2017.2734428", "author": ["C. Siegl", "V. Lange", "M. Stamminger", "F. Bauer", "J. Thies"], "title": "FaceForge: Markerless Non-Rigid Face Multi-Projection Mapping", "year": "2017", "abstract": "Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target geometries. This constrains the applicability and quality for live performances with multiple spectators. Given the cost and complexity of current systems, we present a low-cost easy-to-use markerless non-rigid face multi-projection system. It is based on a non-rigid, dense face tracker and a real-time multi-projection solver adapted to imprecise tracking, geometry and calibration. Using this novel system we produce compelling results with only consumer-grade hardware.", "keywords": ["face recognition", "object detection", "object tracking", "target tracking", "markerless nonrigid face multiprojection", "projection mapping", "nonrigid target geometries", "complexity", "nonrigid face multiprojection system", "dense face tracker", "real-time multiprojection", "FaceForge", "calibration", "consumer-grade hardware", "Face recognition", "Cameras", "Target tracking", "Computational modeling", "Augmented reality", "Object tracking", "Image color analysis", "Face Projection", "Mixed Reality", "Multi-Projection Mapping", "Non-Rigid Face Tracking"], "referenced_by": ["10.1109/TVCG.2018.2868530"], "referencing": ["10.1109/MC.2012.154", "10.1145/1667239.1667251", "10.1145/2508363.2508416", "10.1145/1401132.1401239", "10.1145/311535.311556", "10.1145/2461912.2461976", "10.1145/2766943", "10.1145/2601097.2601204", "10.1145/2897824.2925873", "10.1145/2806173.2806188", "10.1145/2461912.2462019", "10.1145/383259.383317", "10.1145/2816795.2818111", "10.1145/2816795.2818056", "10.1145/2929464.2929475", "10.1145/2010324.1964972", "10.1111/cgf.13128", "10.1201/b10624", "10.1007/s10055-010-0175-5", "10.1007/978-3-7091-6242-2_9", "10.1007/s11263-010-0380-4"]}, "10.1109/TVCG.2017.2734425": {"doi": "10.1109/TVCG.2017.2734425", "author": ["A. Meka", "G. Fox", "M. Zollh\u00f6fer", "C. Richardt", "C. Theobalt"], "title": "Live User-Guided Intrinsic Video for Static Scenes", "year": "2017", "abstract": "We present a novel real-time approach for user-guided intrinsic decomposition of static scenes captured by an RGB-D sensor. In the first step, we acquire a three-dimensional representation of the scene using a dense volumetric reconstruction framework. The obtained reconstruction serves as a proxy to densely fuse reflectance estimates and to store user-provided constraints in three-dimensional space. User constraints, in the form of constant shading and reflectance strokes, can be placed directly on the real-world geometry using an intuitive touch-based interaction metaphor, or using interactive mouse strokes. Fusing the decomposition results and constraints in three-dimensional space allows for robust propagation of this information to novel views by re-projection. We leverage this information to improve on the decomposition quality of existing intrinsic video decomposition techniques by further constraining the ill-posed decomposition problem. In addition to improved decomposition quality, we show a variety of live augmented reality applications such as recoloring of objects, relighting of scenes and editing of material appearance.", "keywords": ["augmented reality", "geometry", "image colour analysis", "image reconstruction", "interactive systems", "video signal processing", "dense volumetric reconstruction framework", "three-dimensional space", "user constraints", "reflectance strokes", "real-world geometry", "interactive mouse strokes", "decomposition results", "intrinsic video decomposition techniques", "improved decomposition quality", "live augmented reality applications", "live user", "static scenes", "intrinsic decomposition", "RGB-D sensor", "three-dimensional representation", "densely fuse reflectance estimation", "intuitive touch-based interaction metaphor", "Streaming media", "Image reconstruction", "Real-time systems", "Geometry", "Three-dimensional displays", "Image color analysis", "Cameras", "Intrinsic video decomposition", "reflectance fusion", "user-guided shading refinement", "Algorithms", "Humans", "Imaging, Three-Dimensional", "Video Recording"], "referenced_by": ["10.1109/CVPR.2018.00661", "10.1109/3DV.2019.00083", "10.1109/TVCG.2018.2869326", "10.1109/TVCG.2020.3023565", "10.1109/ACCESS.2020.3035213"], "referencing": ["10.1109/TPAMI.2014.2377712", "10.1109/VR.2015.7223334", "10.1109/ICCV.2015.99", "10.1109/3DV.2014.62", "10.1109/ICCV.2015.57", "10.1109/ISMAR.2011.6092378", "10.1109/IM.2001.924423", "10.1109/CVPR.2011.5995507", "10.1109/CVPR.2011.5995738", "10.1109/TPAMI.2013.136", "10.1109/TPAMI.2005.185", "10.1109/TPAMI.2012.77", "10.1145/2601097.2601206", "10.1145/2766946", "10.1145/2661229.2661253", "10.1145/1618452.1618476", "10.1145/2461912.2461940", "10.1145/237170.237269", "10.1145/2601097.2601163", "10.1145/2897824.2925907", "10.1145/2751556", "10.1145/2601097.2601135", "10.1145/2461912.2461919", "10.1145/2766887", "10.1111/j.1467-8659.2012.03137.x", "10.1007/978-3-319-10605-2_24", "10.1364/JOSA.61.000001", "10.1007/978-3-642-33783-3_24", "10.1007/978-3-540-24671-8_22", "10.1111/j.1467-8659.2012.03003.x", "10.5244/C.26.112", "10.1016/j.gmod.2012.09.002"]}, "10.1109/TVCG.2017.2734578": {"doi": "10.1109/TVCG.2017.2734578", "author": ["T. Sch\u00f6ps", "M. R. Oswald", "P. Speciale", "S. Yang", "M. Pollefeys"], "title": "Real-Time View Correction for Mobile Devices", "year": "2017", "abstract": "We present a real-time method for rendering novel virtual camera views from given RGB-D (color and depth) data of a different viewpoint. Missing color and depth information due to incomplete input or disocclusions is efficiently inpainted in a temporally consistent way. The inpainting takes the location of strong image gradients into account as likely depth discontinuities. We present our method in the context of a view correction system for mobile devices, and discuss how to obtain a screen-camera calibration and options for acquiring depth input. Our method has use cases in both augmented and virtual reality applications. We demonstrate the speed of our system and the visual quality of its results in multiple experiments in the paper as well as in the supplementary video.", "keywords": ["augmented reality", "calibration", "cameras", "image colour analysis", "image sensors", "image sequences", "rendering (computer graphics)", "mobile devices", "screen-camera calibration", "augmented reality applications", "virtual reality applications", "depth discontinuities", "image gradients", "real-time view correction system", "virtual camera views", "RGB-D", "Cameras", "Real-time systems", "Mobile handsets", "Interpolation", "Augmented reality", "Rendering (computer graphics)", "Image color analysis", "View Correction", "Depth Image Based Rendering (DIBR)", "Mobile Devices", "Augmented Reality (AR)"], "referenced_by": ["10.1109/ISMAR-Adjunct.2018.00084", "10.1109/TVCG.2020.3003768", "10.1109/TPAMI.2019.2947048"], "referencing": ["10.1109/ICIP.2015.7351624", "10.1109/TIP.2016.2619263", "10.1109/TIP.2004.833105", "10.1109/3DV.2014.102", "10.1109/TVCG.2015.2459891", "10.1109/TVCG.2015.2462368", "10.1109/ICIP.2014.7025583", "10.1145/166117.166153", "10.1145/237170.237269", "10.1145/1836845.1836983", "10.1145/1599301.1599394", "10.1145/37401.37422", "10.1145/237170.237196", "10.1145/1015706.1015766", "10.1007/s10851-007-0002-0", "10.1007/s10851-010-0251-1", "10.15607/RSS.2015.XI.037", "10.15607/RSS.2015.XI.008"]}, "10.1109/TVCG.2017.2734427": {"doi": "10.1109/TVCG.2017.2734427", "author": ["Y. Itoh", "T. Hamasaki", "M. Sugimoto"], "title": "Occlusion Leak Compensation for Optical See-Through Displays Using a Single-Layer Transmissive Spatial Light Modulator", "year": "2017", "abstract": "We propose an occlusion compensation method for optical see-through head-mounted displays (OST-HMDs) equipped with a singlelayer transmissive spatial light modulator (SLM), in particular, a liquid crystal display (LCD). Occlusion is an important depth cue for 3D perception, yet realizing it on OST-HMDs is particularly difficult due to the displays' semitransparent nature. A key component for the occlusion support is the SLM-a device that can selectively interfere with light rays passing through it. For example, an LCD is a transmissive SLM that can block or pass incoming light rays by turning pixels black or transparent. A straightforward solution places an LCD in front of an OST-HMD and drives the LCD to block light rays that could pass through rendered virtual objects at the viewpoint. This simple approach is, however, defective due to the depth mismatch between the LCD panel and the virtual objects, leading to blurred occlusion. This led existing OST-HMDs to employ dedicated hardware such as focus optics and multi-stacked SLMs. Contrary to these viable, yet complex and/or computationally expensive solutions, we return to the single-layer LCD approach for the hardware simplicity while maintaining fine occlusion-we compensate for a degraded occlusion area by overlaying a compensation image. We compute the image based on the HMD parameters and the background scene captured by a scene camera. The evaluation demonstrates that the proposed method reduced the occlusion leak error by 61.4% and the occlusion error by 85.7%.", "keywords": ["cameras", "helmet mounted displays", "liquid crystal displays", "rendering (computer graphics)", "spatial light modulators", "virtual reality", "occlusion leak error", "occlusion leak compensation", "single-layer transmissive spatial light modulator", "occlusion compensation method", "OST-HMD", "liquid crystal display", "occlusion support", "transmissive SLM", "rendered virtual objects", "depth mismatch", "LCD panel", "blurred occlusion", "focus optics", "single-layer LCD approach", "degraded occlusion area", "compensation image", "depth cue", "optical see-through head-mounted displays", "virtual objects", "multi-stacked SLM", "fine occlusion", "Liquid crystal displays", "Cameras", "Optical imaging", "Lenses", "Image color analysis", "Occlusion support", "optical see-through HMD", "occlusion leak", "spatial light modulator", "depth cue"], "referenced_by": ["10.1109/VR.2018.8446052", "10.1109/TVCG.2019.2899249", "10.1109/TVCG.2019.2899229", "10.1109/VR.2019.8798001", "10.1109/TVCG.2019.2933120", "10.1109/TVCG.2020.2973443", "10.1109/TVCG.2020.3023569", "10.1109/ISMAR50242.2020.00056", "10.1049/htl.2018.5065", "10.1049/htl.2019.0082"], "referencing": ["10.1109/TVCG.2017.2657058", "10.1109/TBME.2005.863952", "10.1109/TVCG.2016.2593781", "10.1109/TVCG.2016.2518038", "10.1109/ISMAR.2013.6671761", "10.1145/1186562.1015804", "10.1145/3041164.3041178", "10.1145/2766922", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2614066.2614080", "10.1145/1394669.1394675", "10.1145/315762.315813", "10.1145/199404.199405", "10.1364/OE.18.015223", "10.1007/978-3-7091-6805-9_18", "10.1117/12.2015937", "10.1201/9781315367408", "10.1364/3D.2017.JTu1F.2", "10.1364/OE.22.024491", "10.1007/978-3-540-79567-4_140", "10.1016/S0097-8493(01)00119-4", "10.1016/j.cag.2013.10.003", "10.1364/OE.22.006526", "10.1073/pnas.1617251114", "10.1162/pres.1992.1.1.45", "10.1143/JJAP.39.480", "10.1364/AO.55.00A144"]}, "10.1109/TVCG.2017.2734426": {"doi": "10.1109/TVCG.2017.2734426", "author": ["K. Rohmer", "J. Jendersie", "T. Grosch"], "title": "Natural Environment Illumination: Coherent Interactive Augmented Reality for Mobile and Non-Mobile Devices", "year": "2017", "abstract": "Augmented Reality offers many applications today, especially on mobile devices. Due to the lack of mobile hardware for illumination measurements, photorealistic rendering with consistent appearance of virtual objects is still an area of active research. In this paper, we present a full two-stage pipeline for environment acquisition and augmentation of live camera images using a mobile device with a depth sensor. We show how to directly work on a recorded 3D point cloud of the real environment containing high dynamic range color values. For unknown and automatically changing camera settings, a color compensation method is introduced. Based on this, we show photorealistic augmentations using variants of differential light simulation techniques. The presented methods are tailored for mobile devices and run at interactive frame rates. However, our methods are scalable to trade performance for quality and can produce quality renderings on desktop hardware.", "keywords": ["augmented reality", "cameras", "image colour analysis", "lighting", "mobile computing", "rendering (computer graphics)", "solid modelling", "natural environment illumination", "coherent interactive augmented Reality", "mobile hardware", "illumination measurements", "environment acquisition", "live camera images", "high dynamic range color values", "photorealistic augmentations", "differential light simulation techniques", "rendering", "Cameras", "Image color analysis", "Three-dimensional displays", "Rendering (computer graphics)", "Lighting", "Image reconstruction", "Augmented reality", "Augmented Reality", "Mixed Reality", "Differential Rendering", "Color Compensation", "Impostors Tracing", "GPU-Importance Sampling", "Mobile AR", "Scene Reconstruction", "Light Estimation", "Material Estimation", "Depth-Sensing", "Point Clouds", "Global Illumination"], "referenced_by": ["10.1109/CVPR.2018.00661", "10.1109/ISMAR.2019.00-25"], "referencing": ["10.1109/ISMAR.2013.6671772", "10.1109/TPAMI.2010.55", "10.1109/VR.2015.7223334", "10.1109/TPAMI.2007.70732", "10.1109/ISMAR.2010.5643556", "10.1109/ISMAR.2011.6092382", "10.1109/ISMAR.2011.6092384", "10.1109/ISMAR.2014.6948406", "10.1109/TVCG.2015.2450717", "10.1145/1944745.1944787", "10.1145/1053427.1053460", "10.1145/3054739", "10.1145/1315184.1315207", "10.1145/1899950.1900004", "10.1145/2047196.2047270", "10.1145/15922.15902", "10.1145/97879.97901", "10.1145/2980179.2982432", "10.1111/j.1467-8659.2009.01602.x", "10.1111/cgf.12591", "10.1111/j.1467-8659.2003.00716.x", "10.1111/j.1467-8659.2005.0m894.x", "10.1177/0278364914551008", "10.1177/0278364916669237"]}, "10.1109/TVCG.2017.2734538": {"doi": "10.1109/TVCG.2017.2734538", "author": ["A. Morgand", "M. Tamaazousti", "A. Bartoli"], "title": "A Multiple-View Geometric Model of Specularities on Non-Planar Shapes with Application to Dynamic Retexturing", "year": "2017", "abstract": "Predicting specularities in images, given the camera pose and scene geometry from SLAM, forms a challenging and open problem. It is nonetheless essential in several applications such as retexturing. A recent geometric model called JOLIMAS partially answers this problem, under the assumptions that the specularities are elliptical and the scene is planar. JOLIMAS models a moving specularity as the image of a fixed 3D quadric. We propose dual JOLIMAS, a new model which raises the planarity assumption. It uses the fact that specularities remain elliptical on convex surfaces and that every surface can be divided in convex parts. The geometry of dual JOLIMAS then uses a 3D quadric per convex surface part and light source, and predicts the specularities by a means of virtual cameras, allowing it to cope with surface's unflatness. We assessed the efficiency and precision of dual JOLIMAS on multiple synthetic and real videos with various objects and lighting conditions. We give results of a retexturing application. Further results are presented as supplementary video material.", "keywords": ["cameras", "computational geometry", "computer vision", "image reconstruction", "image sequences", "image texture", "SLAM (robots)", "stereo image processing", "scene geometry", "specularities", "planarity assumption", "convex surfaces", "multiple-view geometric model", "nonplanar shapes", "dynamic retexturing", "camera pose", "JOLIMAS", "fixed 3D quadric image", "light source", "virtual cameras", "surface unflatness", "lighting conditions", "JOint LIght-MAterial Specularity", "Light sources", "Cameras", "Three-dimensional displays", "Surface reconstruction", "Shape analysis", "Geometry", "Image reconstruction", "Augmented reality", "Specularity Prediction", "Augmented Reality", "Retexturing", "Quadric", "Multiple Light Sources"], "referenced_by": [], "referencing": ["10.1109/CCV.1988.590016", "10.1109/ICPR.2006.1156", "10.1109/IROS.2015.7354240", "10.1109/34.161352", "10.1109/TPAMI.2012.127", "10.1109/ISMAR.2011.6092378", "10.1109/CVPR.2011.5995358", "10.1145/563858.563893", "10.1145/15922.15901", "10.1145/15922.15902", "10.1145/360825.360839", "10.1111/j.1467-8659.2011.01971.x", "10.1038/343165a0", "10.5244/C.27.105", "10.5244/C.29.43", "10.1007/BF00137441", "10.1007/s11263-005-1086-x", "10.1007/978-3-540-76390-1_77", "10.15607/RSS.2015.XI.001", "10.1177/0278364916669237", "10.1007/978-3-540-88682-2_48"]}}