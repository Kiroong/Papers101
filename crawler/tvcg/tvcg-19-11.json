{"10.1109/TVCG.2019.2934698": {"doi": "10.1109/TVCG.2019.2934698", "author": ["K. Mueller", "D. Bowman"], "title": "Message from the Editor-in-Chief and from the Associate Editor-in-Chief", "year": "2019", "abstract": "Welcome to the November 2019 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). This issue contains selected papers accepted at the IEEE International Symposium on Mixed and Augmented Reality (ISMAR), held this year in Beijing, China from October 14 to October 18, 2019.", "keywords": ["Visualization", "Augmented reality", "Rendering (computer graphics)", "Haptic interfaces", "Special issues and sections", "Production"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2934813": {"doi": "10.1109/TVCG.2019.2934813", "author": ["J. L. Gabbard", "J. Grubert", "S. Hu", "S. Zollmann"], "title": "Message from the ISMAR 2019 Science and Technology Program Chairs and TVCG Guest Editors", "year": "2019", "abstract": "In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the TVCG papers from the 18th IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2019), held October 14\u201318 in Beijing, China. ISMAR continues the over 20-year long tradition of IWAR, ISMR, and ISAR, and is undoubtedly the premier conference for mixed and augmented reality in the world.", "keywords": ["Special issues and sections", "Computer graphics", "Visualization", "Meetings"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2019.2932216": {"doi": "10.1109/TVCG.2019.2932216", "author": ["J. Zhang", "M. Gui", "Q. Wang", "R. Liu", "J. Xu", "S. Chen"], "title": "Hierarchical Topic Model Based Object Association for Semantic SLAM", "year": "2019", "abstract": "Object-based simultaneous localization and mapping (SLAM) is a more natural and robust way for agents to interact with their surrounding environment. However, it introduces a problem of semantic objects association. Correct object association is the key factor to achieve a successful object SLAM system because object association and SLAM are inherently coupled and have not been well tackled yet. A novel formulation of the object association problem based on a hierarchical Dirichlet process (HDP) is proposed. Through the HDP, we can hierarchically associate the grouped object measurements. This can improve the object association accuracy and computation efficiency. Thanks to the novel formulation, the proposed method is also able to correct failure object associations according to its sampling inference algorithm. Furthermore, we introduce object poses to the processing of pose optimization. The object association and pose optimization are then solved in a tightly coupled way, by which both aspects can promote each other. The proposed method is evaluated on indoor and outdoor datasets and the experimental results show a very impressive improvement with respect to the traditional SLAM.", "keywords": ["inference mechanisms", "SLAM (robots)", "semantic SLAM", "semantic objects association", "grouped object measurements", "computation efficiency", "failure object associations", "hierarchical topic model based object association", "object-based simultaneous localization and mapping", "object SLAM system", "sampling inference algorithm", "Simultaneous localization and mapping", "Semantics", "Optimization", "Cameras", "Image reconstruction", "Atmospheric modeling", "Computer science", "Visual Semantic SLAM", "Object Association", "Hierarchical Dirichlet Process"], "referenced_by": ["IKEY:9060874", "IKEY:9284738"], "referencing": ["IKEY:7989203", "IKEY:5354410", "IKEY:6906584", "IKEY:6912003", "IKEY:6618864", "IKEY:4538852", "IKEY:1389727", "IKEY:7368948", "IKEY:7989538", "IKEY:7759677", "IKEY:7219438", "IKEY:7946260", "IKEY:976019", "IKEY:6162880", "IKEY:1642040", "IKEY:7989522", "IKEY:6619022", "IKEY:5509636", "IKEY:8206392", "IKEY:5152529", "IKEY:7298758", "IKEY:6777443", "IKEY:4270097", "IKEY:7989203", "IKEY:5354410", "IKEY:6906584", "IKEY:6912003", "IKEY:6618864", "IKEY:4538852", "IKEY:1389727", "IKEY:7368948", "IKEY:7989538", "IKEY:7759677", "IKEY:7219438", "IKEY:7946260", "IKEY:976019", "IKEY:6162880", "IKEY:1642040", "IKEY:7989522", "IKEY:6619022", "IKEY:5509636", "IKEY:8206392", "IKEY:5152529", "IKEY:7298758", "IKEY:6777443", "IKEY:4270097", "IKEY:7989203", "IKEY:5354410", "IKEY:6906584", "IKEY:6912003", "IKEY:6618864", "IKEY:4538852", "IKEY:1389727", "IKEY:7368948", "IKEY:7989538", "IKEY:7759677", "IKEY:7219438", "IKEY:7946260", "IKEY:976019", "IKEY:6162880", "IKEY:1642040", "IKEY:7989522", "IKEY:6619022", "IKEY:5509636", "IKEY:8206392", "IKEY:5152529", "IKEY:7298758", "IKEY:6777443", "IKEY:4270097", "10.1214/06-BA104", "10.1214/aos/1176342360", "10.5244/C.26.83", "10.1007/978-3-319-67361-5_40", "10.15607/RSS.2017.XIII.013", "10.1214/06-BA104", "10.1214/aos/1176342360", "10.5244/C.26.83", "10.1007/978-3-319-67361-5_40", "10.15607/RSS.2017.XIII.013", "10.1214/06-BA104", "10.1214/aos/1176342360", "10.5244/C.26.83", "10.1007/978-3-319-67361-5_40", "10.15607/RSS.2017.XIII.013"]}, "10.1109/TVCG.2019.2932237": {"doi": "10.1109/TVCG.2019.2932237", "author": ["O. Erat", "M. Hoell", "K. Haubenwallner", "C. Pirchheim", "D. Schmalstieg"], "title": "Real-Time View Planning for Unstructured Lumigraph Modeling", "year": "2019", "abstract": "We propose an algorithm for generating an unstructured lumigraph in real-time from an image stream. This problem has important applications in mixed reality, such as telepresence, interior design or as-built documentation. Unlike conventional texture optimization in structure from motion, our method must choose views from the input stream in a strictly incremental manner, since only a small number of views can be stored or transmitted. This requires formulating an online variant of the well-known view-planning problem, which must take into account what parts of the scene have already been seen and how the lumigraph sample distribution could improve in the future. We address this highly unconstrained problem by regularizing the scene structure using a regular grid structure. Upon the grid structure, we define a coverage metric describing how well the lumigraph samples cover the grid in terms of spatial and angular resolution, and we greedily keep incoming views if they improve the coverage. We evaluate the performance of our algorithm quantitatively and qualitatively on a variety of synthetic and real scenes, and demonstrate visually appealing results obtained at real-time frame rates (in the range of 3Hz-100Hz per incoming image, depending on configuration).", "keywords": ["graph theory", "image resolution", "image texture", "interactive systems", "rendering (computer graphics)", "virtual reality", "time view planning", "unstructured lumigraph modeling", "image stream", "mixed reality", "telepresence", "interior design", "conventional texture optimization", "strictly incremental manner", "online variant", "well-known view-planning problem", "lumigraph sample distribution", "highly unconstrained problem", "scene structure", "regular grid structure", "lumigraph samples", "spatial resolution", "angular resolution", "incoming views", "synthetic scenes", "real scenes", "real-time frame rates", "Planning", "Image reconstruction", "Real-time systems", "Geometry", "Rendering (computer graphics)", "Computational modeling", "Image color analysis", "Lumigraph", "virtual reality", "rendering", "real-time", "view planning", "keyframe selection", "multi-view"], "referenced_by": [], "referencing": ["IKEY:8260942", "IKEY:803344", "IKEY:8447549", "IKEY:7335516", "IKEY:8038808", "IKEY:6802048", "IKEY:4408933", "IKEY:7035859", "IKEY:4587688", "IKEY:5354179", "IKEY:6631215", "IKEY:8460673", "IKEY:4538852", "IKEY:7165673", "IKEY:6402548", "IKEY:7781763", "IKEY:1284395", "IKEY:6671774", "IKEY:8260942", "IKEY:803344", "IKEY:8447549", "IKEY:7335516", "IKEY:8038808", "IKEY:6802048", "IKEY:4408933", "IKEY:7035859", "IKEY:4587688", "IKEY:5354179", "IKEY:6631215", "IKEY:8460673", "IKEY:4538852", "IKEY:7165673", "IKEY:6402548", "IKEY:7781763", "IKEY:1284395", "IKEY:6671774", "IKEY:8260942", "IKEY:803344", "IKEY:8447549", "IKEY:7335516", "IKEY:8038808", "IKEY:6802048", "IKEY:4408933", "IKEY:7035859", "IKEY:4587688", "IKEY:5354179", "IKEY:6631215", "IKEY:8460673", "IKEY:4538852", "IKEY:7165673", "IKEY:6402548", "IKEY:7781763", "IKEY:1284395", "IKEY:6671774", "10.1145/2980179.2982420", "10.1145/3072959.3073610", "10.1145/237170.237200", "10.1145/285055.285059", "10.1145/383259.383309", "10.1145/237170.237199", "10.1145/2601097.2601134", "10.1145/344779.344932", "10.1145/237170.237191", "10.1145/566570.566601", "10.1145/2487228.2487238", "10.1145/2984511.2984517", "10.1145/2642918.2647372", "10.1145/641865.641868", "10.1145/3233794", "10.1145/2980179.2982420", "10.1145/3072959.3073610", "10.1145/237170.237200", "10.1145/285055.285059", "10.1145/383259.383309", "10.1145/237170.237199", "10.1145/2601097.2601134", "10.1145/344779.344932", "10.1145/237170.237191", "10.1145/566570.566601", "10.1145/2487228.2487238", "10.1145/2984511.2984517", "10.1145/2642918.2647372", "10.1145/641865.641868", "10.1145/3233794", "10.1145/2980179.2982420", "10.1145/3072959.3073610", "10.1145/237170.237200", "10.1145/285055.285059", "10.1145/383259.383309", "10.1145/237170.237199", "10.1145/2601097.2601134", "10.1145/344779.344932", "10.1145/237170.237191", "10.1145/566570.566601", "10.1145/2487228.2487238", "10.1145/2984511.2984517", "10.1145/2642918.2647372", "10.1145/641865.641868", "10.1145/3233794", "10.1007/978-3-319-10602-1_54", "10.1111/j.1467-8659.2003.00717.x", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1111/j.1467-8659.2012.03009.x", "10.1016/j.isprsjprs.2018.03.022", "10.1007/978-3-540-77072-5", "10.5244/C.26.70", "10.1007/3-540-45103-X_50", "10.1007/978-3-319-10602-1_54", "10.1111/j.1467-8659.2003.00717.x", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1111/j.1467-8659.2012.03009.x", "10.1016/j.isprsjprs.2018.03.022", "10.1007/978-3-540-77072-5", "10.5244/C.26.70", "10.1007/3-540-45103-X_50", "10.1007/978-3-319-10602-1_54", "10.1111/j.1467-8659.2003.00717.x", "10.1111/j.1467-8659.2009.01617.x", "10.1111/j.1467-8659.2008.01138.x", "10.1111/j.1467-8659.2012.03009.x", "10.1016/j.isprsjprs.2018.03.022", "10.1007/978-3-540-77072-5", "10.5244/C.26.70", "10.1007/3-540-45103-X_50"]}, "10.1109/TVCG.2019.2932172": {"doi": "10.1109/TVCG.2019.2932172", "author": ["D. Andersen", "P. Villano", "V. Popescu"], "title": "AR HMD Guidance for Controlled Hand-Held 3D Acquisition", "year": "2019", "abstract": "Photogrammetry is a popular method of 3D reconstruction that uses conventional photos as input. This method can achieve high quality reconstructions so long as the scene is densely acquired from multiple views with sufficient overlap between nearby images. However, it is challenging for a human operator to know during acquisition if sufficient coverage has been achieved. Insufficient coverage of the scene can result in holes, missing regions, or even a complete failure of reconstruction. These errors require manually repairing the model or returning to the scene to acquire additional views, which is time-consuming and often infeasible. We present a novel approach to photogrammetric acquisition that uses an AR HMD to predict a set of covering views and to interactively guide an operator to capture imagery from each view. The operator wears an AR HMD and uses a handheld camera rig that is tracked relative to the AR HMD with a fiducial marker. The AR HMD tracks its pose relative to the environment and automatically generates a coarse geometric model of the scene, which our approach analyzes at runtime to generate a set of human-reachable acquisition views covering the scene with consistent camera-to-scene distance and image overlap. The generated view locations are rendered to the operator on the AR HMD. Interactive visual feedback informs the operator how to align the camera to assume each suggested pose. When the camera is in range, an image is automatically captured. In this way, a set of images suitable for 3D reconstruction can be captured in a matter of minutes. In a user study, participants who were novices at photogrammetry were tasked with acquiring a challenging and complex scene either without guidance or with our AR HMD based guidance. Participants using our guidance achieved improved reconstructions without cases of reconstruction failure as in the control condition. Our AR HMD based approach is self-contained, portable, and provides specific acquisition guidance tailored to the geometry of the scene being captured.", "keywords": ["cameras", "computer vision", "data visualisation", "helmet mounted displays", "image capture", "image reconstruction", "image sensors", "photogrammetry", "rendering (computer graphics)", "stereo image processing", "high quality reconstructions", "nearby images", "human operator", "photogrammetric acquisition", "handheld camera rig", "human-reachable acquisition views", "consistent camera-to-scene distance", "AR HMD based guidance", "hand-held 3D acquisition control", "3D reconstruction", "coarse geometric model", "image overlap", "interactive visual feedback", "Image reconstruction", "Cameras", "Resists", "Three-dimensional displays", "Visualization", "Smart phones", "Geometry", "Augmented reality", "head-mounted display", "photogrammetry", "3D reconstruction"], "referenced_by": ["IKEY:9284793", "IKEY:9288396"], "referencing": ["IKEY:8446560", "IKEY:8447549", "IKEY:8460673", "IKEY:5336460", "IKEY:609300", "IKEY:8237831", "IKEY:8446560", "IKEY:8447549", "IKEY:8460673", "IKEY:5336460", "IKEY:609300", "IKEY:8237831", "IKEY:8446560", "IKEY:8447549", "IKEY:8460673", "IKEY:5336460", "IKEY:609300", "IKEY:8237831", "10.1145/1057432.1057457", "10.1145/3197517.3201295", "10.1145/37401.37422", "10.1145/3150165.3150166", "10.1145/3272127.3275010", "10.1145/1057432.1057457", "10.1145/3197517.3201295", "10.1145/37401.37422", "10.1145/3150165.3150166", "10.1145/3272127.3275010", "10.1145/1057432.1057457", "10.1145/3197517.3201295", "10.1145/37401.37422", "10.1145/3150165.3150166", "10.1145/3272127.3275010", "10.3390/rs11030306", "10.1007/s11042-015-2473-0", "10.1007/978-3-030-01790-3_5", "10.1016/j.autcon.2018.05.010", "10.1016/j.proeng.2013.09.086", "10.1016/j.patcog.2014.01.005", "10.1016/S0166-4115(08)62386-9", "10.5244/C.26.70", "10.1080/16864360.2017.1397886", "10.3390/rs11030306", "10.1007/s11042-015-2473-0", "10.1007/978-3-030-01790-3_5", "10.1016/j.autcon.2018.05.010", "10.1016/j.proeng.2013.09.086", "10.1016/j.patcog.2014.01.005", "10.1016/S0166-4115(08)62386-9", "10.5244/C.26.70", "10.1080/16864360.2017.1397886", "10.3390/rs11030306", "10.1007/s11042-015-2473-0", "10.1007/978-3-030-01790-3_5", "10.1016/j.autcon.2018.05.010", "10.1016/j.proeng.2013.09.086", "10.1016/j.patcog.2014.01.005", "10.1016/S0166-4115(08)62386-9", "10.5244/C.26.70", "10.1080/16864360.2017.1397886"]}, "10.1109/TVCG.2019.2932276": {"doi": "10.1109/TVCG.2019.2932276", "author": ["X. Min", "W. Zhang", "S. Sun", "N. Zhao", "S. Tang", "Y. Zhuang"], "title": "VPModel: High-Fidelity Product Simulation in a Virtual-Physical Environment", "year": "2019", "abstract": "In the development of a new product, the design team must describe the expected effects of the final products to potential users and stakeholders. However, existing prototyping tools can only present a product imperfectly, due to limitations at different levels. Specifically, the physical product model, which may be the product of 3D printing, could lack a visual interface; the presentation of the product through modeling software such as Rhinoceros 3D does not provide good realistic tactile perception; or the interface platforms, such as Axure RP, used to display the interactive effects differ from those to be used in the actual operation. Thus, we present the VPModel, a high-fidelity prototyping tool, able to integrate multiple prototyping methods simultaneously. It combines a touchable 3D-printed product model (3DPM) and a corresponding visualized virtual model, and the interactive interfaces are rendered synchronously in a mixed-reality device. Through the tangible, visual, and interactive demonstration, designers and normal users can each obtain a similar experience to the experience of the finished product. Furthermore, the VPModel also enhances design practices by enabling comparisons between modular models. However, the implementation of this system is a challenging task, which subsumes several fundamental problems as sub-tasks: object detection, real-time matching, hand-gesture detection and action recognition. To achieve the expected goals of the VPModel, this system uses physical hardware (a Microsoft MR HoloLens headset, a Leap Motion Controller, and a 3D printer) and existing machine learning algorithms. To evaluate our VPModel, we report the user experience of 16 participants, evaluated using a closed-ended questionnaire survey, a quantitative analysis of task performance, and a qualitative analysis of open-ended interviews. The results show a significant improvement in realism and enjoyment using the VPModel over the two traditional camera prototype approaches. In summary, the VPModel can be used to support design strategy and to convey design concepts fully and efficiently, which indicates a potential use for the VPModel in shortening product development cycles and reducing communication costs.", "keywords": ["data visualisation", "gesture recognition", "haptic interfaces", "image matching", "object detection", "product development", "production engineering computing", "stereo image processing", "three-dimensional printing", "user experience", "virtual reality", "VPModel", "high-fidelity product simulation", "virtual-physical environment", "physical product model", "visual interface", "Rhinoceros 3D", "high-fidelity prototyping tool", "3DPM", "modular models", "physical hardware", "user experience", "product development cycles", "tactile perception", "communication costs", "3D-printed product model", "object detection", "real-time matching", "hand-gesture detection", "action recognition", "machine learning algorithms", "visualized virtual model", "camera prototype approaches", "Solid modeling", "Three-dimensional displays", "Tools", "Prototypes", "Visualization", "Real-time systems", "Software", "Designers", "high-fidelity prototyping tool", "3D printed model", "mixed reality", "Adult", "Augmented Reality", "Computer Graphics", "Consumer Behavior", "Female", "Humans", "Male", "Printing, Three-Dimensional", "Surveys and Questionnaires", "User-Computer Interface", "Virtual Reality", "Young Adult"], "referenced_by": [], "referencing": ["IKEY:1047997", "IKEY:1115077", "IKEY:7361639", "IKEY:1115076", "IKEY:8331264", "IKEY:1544680", "IKEY:1047997", "IKEY:1115077", "IKEY:7361639", "IKEY:1115076", "IKEY:8331264", "IKEY:1544680", "IKEY:1047997", "IKEY:1115077", "IKEY:7361639", "IKEY:1115076", "IKEY:8331264", "IKEY:1544680", "10.1145/2185520.2185585", "10.1145/2331714.2331742", "10.1145/2814347.2814350", "10.1145/2858036.2858134", "10.1145/3173574.3173985", "10.1145/2148131.2148190", "10.1145/3173574.3173927", "10.1145/2858036.2858250", "10.1145/3229316.3229320", "10.1145/3025453.3025491", "10.1145/3173574.3174153", "10.1145/2556288.2557090", "10.1145/2185520.2185585", "10.1145/2331714.2331742", "10.1145/2814347.2814350", "10.1145/2858036.2858134", "10.1145/3173574.3173985", "10.1145/2148131.2148190", "10.1145/3173574.3173927", "10.1145/2858036.2858250", "10.1145/3229316.3229320", "10.1145/3025453.3025491", "10.1145/3173574.3174153", "10.1145/2556288.2557090", "10.1145/2185520.2185585", "10.1145/2331714.2331742", "10.1145/2814347.2814350", "10.1145/2858036.2858134", "10.1145/3173574.3173985", "10.1145/2148131.2148190", "10.1145/3173574.3173927", "10.1145/2858036.2858250", "10.1145/3229316.3229320", "10.1145/3025453.3025491", "10.1145/3173574.3174153", "10.1145/2556288.2557090", "10.1016/j.apergo.2016.02.015", "10.1007/s10845-016-1276-0", "10.1007/s10055-016-0293-9", "10.1080/02763869.2012.670604", "10.1201/b10624", "10.1061/(ASCE)EI.1943-5541.0000078", "10.1007/978-3-540-73011-8_17", "10.1007/978-3-319-32552-1_42", "10.1007/s00170-009-2012-0", "10.4028/www.scientific.net/KEM.693.1901", "10.1108/aa.2012.03332baa.010", "10.1016/j.compind.2008.09.001", "10.1260/147807709788921985", "10.3390/s130506380", "10.1016/S0926-5805(99)00012-6", "10.3103/S0146411616020073", "10.1007/s00138-010-0291-y", "10.1016/j.apergo.2016.02.015", "10.1007/s10845-016-1276-0", "10.1007/s10055-016-0293-9", "10.1080/02763869.2012.670604", "10.1201/b10624", "10.1061/(ASCE)EI.1943-5541.0000078", "10.1007/978-3-540-73011-8_17", "10.1007/978-3-319-32552-1_42", "10.1007/s00170-009-2012-0", "10.4028/www.scientific.net/KEM.693.1901", "10.1108/aa.2012.03332baa.010", "10.1016/j.compind.2008.09.001", "10.1260/147807709788921985", "10.3390/s130506380", "10.1016/S0926-5805(99)00012-6", "10.3103/S0146411616020073", "10.1007/s00138-010-0291-y", "10.1016/j.apergo.2016.02.015", "10.1007/s10845-016-1276-0", "10.1007/s10055-016-0293-9", "10.1080/02763869.2012.670604", "10.1201/b10624", "10.1061/(ASCE)EI.1943-5541.0000078", "10.1007/978-3-540-73011-8_17", "10.1007/978-3-319-32552-1_42", "10.1007/s00170-009-2012-0", "10.4028/www.scientific.net/KEM.693.1901", "10.1108/aa.2012.03332baa.010", "10.1016/j.compind.2008.09.001", "10.1260/147807709788921985", "10.3390/s130506380", "10.1016/S0926-5805(99)00012-6", "10.3103/S0146411616020073", "10.1007/s00138-010-0291-y"]}, "10.1109/TVCG.2019.2932248": {"doi": "10.1109/TVCG.2019.2932248", "author": ["S. Kagami", "K. Hashimoto"], "title": "Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane through a Direct Closed-Loop Alignment", "year": "2019", "abstract": "This paper presents a fast projection mapping method for moving image content projected onto a markerless planar surface using a low-latency Digital Micromirror Device (DMD) projector. By adopting a closed-loop alignment approach, in which not only the surface texture but also the projected image is tracked by a camera, the proposed method is free from a calibration or position adjustment between the camera and projector. We designed fiducial patterns to be inserted into a fast flapping sequence of binary frames of the DMD projector, which allows the simultaneous tracking of the surface texture and a fiducial geometry separate from a single image captured by the camera. The proposed method implemented on a CPU runs at 400 fps and enables arbitrary video contents to be \u201cstuck\u201d onto a variety of textured surfaces.", "keywords": ["calibration", "cameras", "closed loop systems", "image capture", "image sensors", "micromirrors", "optical design techniques", "optical images", "optical projectors", "photodetectors", "surface texture", "video signal processing", "single image capture", "fiducial geometry", "surface texture tracking", "direct closed-loop alignment approach", "low-latency digital micromirror device projector", "arbitrary video contents", "DMD projector", "fast flapping sequence", "fiducial patterns", "calibration", "markerless planar surface", "fast video projection mapping", "animated stickies", "Cameras", "Target tracking", "Surface texture", "Calibration", "Sensors", "Optimization", "Visualization", "Spatial augmented reality", "high-speed vision", "projector-camera system", "visual tracking"], "referenced_by": ["IKEY:9284678"], "referencing": ["IKEY:5540199", "IKEY:1383047", "IKEY:4538845", "IKEY:7844058", "IKEY:8088492", "IKEY:7383304", "IKEY:7328074", "IKEY:7516689", "IKEY:8797850", "IKEY:7138633", "IKEY:6948427", "IKEY:5540199", "IKEY:1383047", "IKEY:4538845", "IKEY:7844058", "IKEY:8088492", "IKEY:7383304", "IKEY:7328074", "IKEY:7516689", "IKEY:8797850", "IKEY:7138633", "IKEY:6948427", "IKEY:5540199", "IKEY:1383047", "IKEY:4538845", "IKEY:7844058", "IKEY:8088492", "IKEY:7383304", "IKEY:7328074", "IKEY:7516689", "IKEY:8797850", "IKEY:7138633", "IKEY:6948427", "10.1145/2988240.2988242", "10.1145/2818466.2818485", "10.1145/1186155.1186180", "10.1145/3272127.3275045", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/1409240.1409331", "10.1145/2047196.2047254", "10.1145/2858036.2858329", "10.1145/2988240.2988242", "10.1145/2818466.2818485", "10.1145/1186155.1186180", "10.1145/3272127.3275045", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/1409240.1409331", "10.1145/2047196.2047254", "10.1145/2858036.2858329", "10.1145/2988240.2988242", "10.1145/2818466.2818485", "10.1145/1186155.1186180", "10.1145/3272127.3275045", "10.1145/280814.280861", "10.1145/2816795.2818111", "10.1145/1409240.1409331", "10.1145/2047196.2047254", "10.1145/2858036.2858329", "10.1007/s10055-012-0210-9", "10.1023/B:VISI.0000011205.11775.fd", "10.1177/0278364907080252", "10.1111/cgf.13128", "10.1016/j.patcog.2014.01.005", "10.1007/s11263-010-0324-z", "10.1049/PBCE037E", "10.1587/transinf.2016PCP0015", "10.1007/s10055-012-0210-9", "10.1023/B:VISI.0000011205.11775.fd", "10.1177/0278364907080252", "10.1111/cgf.13128", "10.1016/j.patcog.2014.01.005", "10.1007/s11263-010-0324-z", "10.1049/PBCE037E", "10.1587/transinf.2016PCP0015", "10.1007/s10055-012-0210-9", "10.1023/B:VISI.0000011205.11775.fd", "10.1177/0278364907080252", "10.1111/cgf.13128", "10.1016/j.patcog.2014.01.005", "10.1007/s11263-010-0324-z", "10.1049/PBCE037E", "10.1587/transinf.2016PCP0015"]}, "10.1109/TVCG.2019.2932223": {"doi": "10.1109/TVCG.2019.2932223", "author": ["N. Gard", "A. Hilsmann", "P. Eisert"], "title": "Projection Distortion-based Object Tracking in Shader Lamp Scenarios", "year": "2019", "abstract": "Shader lamp systems augment the real environment by projecting new textures on known target geometries. In dynamic scenes, object tracking maintains the illusion if the physical and virtual objects are well aligned. However, traditional trackers based on texture or contour information are often distracted by the projected content and tend to fail. In this paper, we present a model-based tracking strategy, which directly takes advantage from the projected content for pose estimation in a projector-camera system. An iterative pose estimation algorithm captures and exploits visible distortions caused by object movements. In a closed-loop, the corrected pose allows the update of the projection for the subsequent frame. Synthetic frames simulating the projection on the model are rendered and an optical flow-based method minimizes the difference between edges of the rendered and the camera image. Since the thresholds automatically adapt to the synthetic image, a complicated radiometric calibration can be avoided. The pixel-wise linear optimization is designed to be easily implemented on the GPU. Our approach can be combined with a regular contour-based tracker and is transferable to other problems, like the estimation of the extrinsic pose between projector and camera. We evaluate our procedure with real and synthetic images and obtain very precise registration results.", "keywords": ["calibration", "cameras", "image registration", "image segmentation", "image sequences", "image texture", "iterative methods", "linear programming", "object tracking", "pose estimation", "rendering (computer graphics)", "object movements", "synthetic frames", "optical flow-based method", "camera image", "synthetic image", "regular contour-based tracker", "projection distortion-based object tracking", "shader lamp scenarios", "shader lamp systems", "dynamic scenes", "physical objects", "virtual objects", "texture", "model-based tracking strategy", "projected content", "projector-camera system", "iterative pose estimation", "Cameras", "Three-dimensional displays", "Pose estimation", "Mathematical model", "Calibration", "Object tracking", "Projector-camera systems", "projector-camera calibration", "shader lamp systems", "object tracking", "object registration", "spatial augmented reality", "projection mapping"], "referenced_by": [], "referencing": ["IKEY:7745145", "IKEY:7831400", "IKEY:5540199", "IKEY:8451038", "IKEY:7398391", "IKEY:6949562", "IKEY:6375029", "IKEY:7516689", "IKEY:7138633", "IKEY:7164353", "IKEY:5539939", "IKEY:1383042", "IKEY:7121014", "IKEY:6549358", "IKEY:7745145", "IKEY:7831400", "IKEY:5540199", "IKEY:8451038", "IKEY:7398391", "IKEY:6949562", "IKEY:6375029", "IKEY:7516689", "IKEY:7138633", "IKEY:7164353", "IKEY:5539939", "IKEY:1383042", "IKEY:7121014", "IKEY:6549358", "IKEY:7745145", "IKEY:7831400", "IKEY:5540199", "IKEY:8451038", "IKEY:7398391", "IKEY:6949562", "IKEY:6375029", "IKEY:7516689", "IKEY:7138633", "IKEY:7164353", "IKEY:5539939", "IKEY:1383042", "IKEY:7121014", "IKEY:6549358", "10.1145/212094.212141", "10.1145/2945078.2945083", "10.1145/2614217.2614226", "10.1145/1201775.882349", "10.1145/212094.212141", "10.1145/2945078.2945083", "10.1145/2614217.2614226", "10.1145/1201775.882349", "10.1145/212094.212141", "10.1145/2945078.2945083", "10.1145/2614217.2614226", "10.1145/1201775.882349", "10.1111/cgf.13128", "10.3171/2014.9.JNS141001", "10.1007/s00238-018-1395-2", "10.1889/JSID20.5.279", "10.1016/0004-3702(81)90024-2", "10.5194/isprs-annals-IV-2-W4-83-2017", "10.1016/j.jcde.2014.11.004", "10.15221/16.286", "10.1007/978-3-7091-6242-2_9", "10.1016/j.cviu.2017.03.005", "10.15607/RSS.2018.XIV.019", "10.1016/S0262-8856(96)01112-2", "10.1111/cgf.13128", "10.3171/2014.9.JNS141001", "10.1007/s00238-018-1395-2", "10.1889/JSID20.5.279", "10.1016/0004-3702(81)90024-2", "10.5194/isprs-annals-IV-2-W4-83-2017", "10.1016/j.jcde.2014.11.004", "10.15221/16.286", "10.1007/978-3-7091-6242-2_9", "10.1016/j.cviu.2017.03.005", "10.15607/RSS.2018.XIV.019", "10.1016/S0262-8856(96)01112-2", "10.1111/cgf.13128", "10.3171/2014.9.JNS141001", "10.1007/s00238-018-1395-2", "10.1889/JSID20.5.279", "10.1016/0004-3702(81)90024-2", "10.5194/isprs-annals-IV-2-W4-83-2017", "10.1016/j.jcde.2014.11.004", "10.15221/16.286", "10.1007/978-3-7091-6242-2_9", "10.1016/j.cviu.2017.03.005", "10.15607/RSS.2018.XIV.019", "10.1016/S0262-8856(96)01112-2"]}, "10.1109/TVCG.2019.2932238": {"doi": "10.1109/TVCG.2019.2932238", "author": ["X. Xia", "Y. Guan", "A. State", "P. Chakravarthula", "K. Rathinavel", "T. -J. Cham", "H. Fuchs"], "title": "Towards a Switchable AR/VR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support", "year": "2019", "abstract": "In this paper, we present our novel design for switchable AR/VR near-eye displays which can help solve the vergence-accommodation-conflict issue. The principal idea is to time-multiplex virtual imagery and real-world imagery and use a tunable lens to adjust focus for the virtual display and the see-through scene separately. With this novel design, prescription eyeglasses for near- and far-sighted users become unnecessary. This is achieved by integrating the wearer's corrective optical prescription into the tunable lens for both virtual display and see-through environment. We built a prototype based on the design, comprised of micro-display, optical systems, a tunable lens, and active shutters. The experimental results confirm that the proposed near-eye display design can switch between AR and VR and can provide correct accommodation for both.", "keywords": ["eye", "eye protection", "handicapped aids", "lenses", "patient care", "virtual reality", "AR-VR near-eye displays", "optical systems", "microdisplay", "prescription eyeglasses", "virtual display", "real-world imagery", "time-multiplex virtual imagery", "eyeglass prescription support", "near-eye display design", "Three-dimensional displays", "Lenses", "Optical switches", "Optical imaging", "Holography", "Holographic optical components", "Optical distortion", "Near-eye displays", "Augmented reality", "Virtual reality", "Focus accommodation", "Prescription correction", "Augmented Reality", "Computer Graphics", "Equipment Design", "Eyeglasses", "Holography", "Humans", "Image Processing, Computer-Assisted", "Virtual Reality"], "referenced_by": ["IKEY:9199563", "IKEY:9284652"], "referencing": ["IKEY:8458263", "IKEY:7829412", "IKEY:4637321", "IKEY:6671761", "IKEY:8456852", "IKEY:8630990", "IKEY:8458263", "IKEY:7829412", "IKEY:4637321", "IKEY:6671761", "IKEY:8456852", "IKEY:8630990", "IKEY:8458263", "IKEY:7829412", "IKEY:4637321", "IKEY:6671761", "IKEY:8456852", "IKEY:8630990", "10.1145/2766922", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2601097.2601141", "10.1145/1476589.1476686", "10.1145/2766922", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2601097.2601141", "10.1145/1476589.1476686", "10.1145/2766922", "10.1145/3130800.3130889", "10.1145/2508363.2508366", "10.1145/3072959.3073624", "10.1145/2601097.2601141", "10.1145/1476589.1476686", "10.1364/OE.25.028223", "10.1364/OE.23.018143", "10.1364/AO.48.002655", "10.1364/OE.25.008412", "10.1364/AOP.5.000456", "10.1364/ao.58.000a74", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1364/OL.43.000767", "10.1364/OE.24.019531", "10.1364/OE.27.000689", "10.1364/OE.25.009886", "10.1364/OE.26.004060", "10.1364/OE.17.015716", "10.1889/JSID17.3.185", "10.1364/AO.39.003209", "10.1364/OE.26.010140", "10.1117/12.2038330", "10.1364/OE.26.022985", "10.1364/OE.26.025076", "10.1364/OE.26.030703", "10.1364/ao.51.004703", "10.1364/OE.26.011553", "10.1002/jsid.122", "10.1364/OE.26.022866", "10.1364/OE.25.028223", "10.1364/OE.23.018143", "10.1364/AO.48.002655", "10.1364/OE.25.008412", "10.1364/AOP.5.000456", "10.1364/ao.58.000a74", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1364/OL.43.000767", "10.1364/OE.24.019531", "10.1364/OE.27.000689", "10.1364/OE.25.009886", "10.1364/OE.26.004060", "10.1364/OE.17.015716", "10.1889/JSID17.3.185", "10.1364/AO.39.003209", "10.1364/OE.26.010140", "10.1117/12.2038330", "10.1364/OE.26.022985", "10.1364/OE.26.025076", "10.1364/OE.26.030703", "10.1364/ao.51.004703", "10.1364/OE.26.011553", "10.1002/jsid.122", "10.1364/OE.26.022866", "10.1364/OE.25.028223", "10.1364/OE.23.018143", "10.1364/AO.48.002655", "10.1364/OE.25.008412", "10.1364/AOP.5.000456", "10.1364/ao.58.000a74", "10.1167/8.3.33", "10.1364/OE.22.013896", "10.1364/OE.22.013484", "10.1364/OL.43.000767", "10.1364/OE.24.019531", "10.1364/OE.27.000689", "10.1364/OE.25.009886", "10.1364/OE.26.004060", "10.1364/OE.17.015716", "10.1889/JSID17.3.185", "10.1364/AO.39.003209", "10.1364/OE.26.010140", "10.1117/12.2038330", "10.1364/OE.26.022985", "10.1364/OE.26.025076", "10.1364/OE.26.030703", "10.1364/ao.51.004703", "10.1364/OE.26.011553", "10.1002/jsid.122", "10.1364/OE.26.022866"]}, "10.1109/TVCG.2019.2933120": {"doi": "10.1109/TVCG.2019.2933120", "author": ["K. Rathinavel", "G. Wetzstein", "H. Fuchs"], "title": "Varifocal Occlusion-Capable Optical See-through Augmented Reality Display based on Focus-tunable Optics", "year": "2019", "abstract": "Optical see-through augmented reality (AR) systems are a next-generation computing platform that offer unprecedented user experiences by seamlessly combining physical and digital content. Many of the traditional challenges of these displays have been significantly improved over the last few years, but AR experiences offered by today's systems are far from seamless and perceptually realistic. Mutually consistent occlusions between physical and digital objects are typically not supported. When mutual occlusion is supported, it is only supported for a fixed depth. We propose a new optical see-through AR display system that renders mutual occlusion in a depth-dependent, perceptually realistic manner. To this end, we introduce varifocal occlusion displays based on focus-tunable optics, which comprise a varifocal lens system and spatial light modulators that enable depth-corrected hard-edge occlusions for AR experiences. We derive formal optimization methods and closed-form solutions for driving this tunable lens system and demonstrate a monocular varifocal occlusion-capable optical see-through AR display capable of perceptually realistic occlusion across a large depth range.", "keywords": ["augmented reality", "computer displays", "lenses", "optimisation", "spatial light modulators", "perceptually realistic occlusion", "monocular varifocal occlusion-capable", "tunable lens system", "depth-corrected hard-edge occlusions", "varifocal lens system", "varifocal occlusion displays", "perceptually realistic manner", "AR display system", "fixed depth", "mutual occlusion", "digital objects", "physical objects", "mutually consistent occlusions", "AR experiences", "digital content", "physical content", "focus-tunable optics", "augmented reality display", "Optical imaging", "Adaptive optics", "Optical design", "Optical distortion", "Lenses", "Optical diffraction", "Augmented Reality", "Computational Displays", "Varifocal Display", "Occlusion"], "referenced_by": ["IKEY:8998139", "IKEY:9199563", "IKEY:9199567", "IKEY:9284767"], "referencing": ["IKEY:1115088", "IKEY:1383039", "IKEY:8458263", "IKEY:6402574", "IKEY:8676155", "IKEY:8007218", "IKEY:8676153", "IKEY:1240696", "IKEY:880924", "IKEY:7523376", "IKEY:6671761", "IKEY:6549352", "IKEY:8446441", "IKEY:8456852", "IKEY:6788092", "IKEY:1115088", "IKEY:1383039", "IKEY:8458263", "IKEY:6402574", "IKEY:8676155", "IKEY:8007218", "IKEY:8676153", "IKEY:1240696", "IKEY:880924", "IKEY:7523376", "IKEY:6671761", "IKEY:6549352", "IKEY:8446441", "IKEY:8456852", "IKEY:6788092", "IKEY:1115088", "IKEY:1383039", "IKEY:8458263", "IKEY:6402574", "IKEY:8676155", "IKEY:8007218", "IKEY:8676153", "IKEY:1240696", "IKEY:880924", "IKEY:7523376", "IKEY:6671761", "IKEY:6549352", "IKEY:8446441", "IKEY:8456852", "IKEY:6788092", "10.1145/3139131.3139150", "10.1145/2858036.2858140", "10.1145/3214907.3214925", "10.1145/1882262.1866164", "10.1145/2601097.2601141", "10.1145/2185520.2185576", "10.1145/3139131.3139150", "10.1145/2858036.2858140", "10.1145/3214907.3214925", "10.1145/1882262.1866164", "10.1145/2601097.2601141", "10.1145/2185520.2185576", "10.1145/3139131.3139150", "10.1145/2858036.2858140", "10.1145/3214907.3214925", "10.1145/1882262.1866164", "10.1145/2601097.2601141", "10.1145/2185520.2185576", "10.1111/j.1467-8659.2008.01175.x", "10.1117/12.617963", "10.1080/713826091", "10.1364/OE.22.029465", "10.1016/B978-012240530-3/50005-5", "10.1117/12.2015937", "10.1002/jsid.545", "10.1364/OE.24.011808", "10.1016/S0097-8493(01)00119-4", "10.1016/j.displa.2004.07.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1073/pnas.1617251114", "10.1126/sciadv.aav6187", "10.1889/1.1987395", "10.1167/5.10.7", "10.1111/j.1467-8659.2010.01660.x", "10.1364/OE.25.030539", "10.1364/AO.55.00A144", "10.1111/j.1467-8659.2008.01175.x", "10.1117/12.617963", "10.1080/713826091", "10.1364/OE.22.029465", "10.1016/B978-012240530-3/50005-5", "10.1117/12.2015937", "10.1002/jsid.545", "10.1364/OE.24.011808", "10.1016/S0097-8493(01)00119-4", "10.1016/j.displa.2004.07.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1073/pnas.1617251114", "10.1126/sciadv.aav6187", "10.1889/1.1987395", "10.1167/5.10.7", "10.1111/j.1467-8659.2010.01660.x", "10.1364/OE.25.030539", "10.1364/AO.55.00A144", "10.1111/j.1467-8659.2008.01175.x", "10.1117/12.617963", "10.1080/713826091", "10.1364/OE.22.029465", "10.1016/B978-012240530-3/50005-5", "10.1117/12.2015937", "10.1002/jsid.545", "10.1364/OE.24.011808", "10.1016/S0097-8493(01)00119-4", "10.1016/j.displa.2004.07.004", "10.2352/J.ImagingSci.Technol.2009.53.3.030201", "10.1073/pnas.1617251114", "10.1126/sciadv.aav6187", "10.1889/1.1987395", "10.1167/5.10.7", "10.1111/j.1467-8659.2010.01660.x", "10.1364/OE.25.030539", "10.1364/AO.55.00A144"]}, "10.1109/TVCG.2019.2932235": {"doi": "10.1109/TVCG.2019.2932235", "author": ["T. Randhavane", "A. Bera", "K. Kapsaskis", "K. Gray", "D. Manocha"], "title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement Characteristics", "year": "2019", "abstract": "We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.", "keywords": ["augmented reality", "avatars", "human computer interaction", "multi-agent systems", "psychology", "social aspects of automation", "AR environment", "appropriate movement characteristics", "novel data-driven friendliness model", "user-study", "psychological characteristics", "friendly virtual agents", "FVA agents", "plausible movements", "interactive rates", "statistically significant improvement", "friendliness modeling", "friendliness measure", "social presence measure", "modeling perceived friendliness", "virtual agent", "Microsoft HoloLens", "AR setting", "Task analysis", "Psychology", "Computational modeling", "Three-dimensional displays", "Skeleton", "Computer science", "Avatars", "Social perception", "intelligent virtual agents", "friendliness", "gaits", "gestures", "gazing", "Adult", "Algorithms", "Augmented Reality", "Computer Graphics", "Female", "Fixation, Ocular", "Friends", "Gait", "Gestures", "Humans", "Male", "Models, Psychological", "Movement", "Social Perception", "Virtual Reality"], "referenced_by": ["IKEY:9284667"], "referencing": ["IKEY:6797229", "IKEY:8613756", "IKEY:7344749", "IKEY:7504683", "IKEY:8794465", "IKEY:8311384", "IKEY:6165146", "IKEY:8267290", "IKEY:6797229", "IKEY:8613756", "IKEY:7344749", "IKEY:7504683", "IKEY:8794465", "IKEY:8311384", "IKEY:6165146", "IKEY:8267290", "IKEY:6797229", "IKEY:8613756", "IKEY:7344749", "IKEY:7504683", "IKEY:8794465", "IKEY:8311384", "IKEY:6165146", "IKEY:8267290", "10.1145/3281505.3281524", "10.1145/3267851.3267898", "10.1145/1314161.1314180", "10.1145/3267851.3267878", "10.1145/3072959.3073663", "10.1145/2897824.2925975", "10.1145/3139131.3139151", "10.1145/3267851.3267870", "10.1145/3292037", "10.1145/3197517.3201401", "10.1145/3272127.3275075", "10.1145/2993369.2993378", "10.1145/2492494.2492500", "10.1145/3267851.3267901", "10.1145/3281505.3281524", "10.1145/3267851.3267898", "10.1145/1314161.1314180", "10.1145/3267851.3267878", "10.1145/3072959.3073663", "10.1145/2897824.2925975", "10.1145/3139131.3139151", "10.1145/3267851.3267870", "10.1145/3292037", "10.1145/3197517.3201401", "10.1145/3272127.3275075", "10.1145/2993369.2993378", "10.1145/2492494.2492500", "10.1145/3267851.3267901", "10.1145/3281505.3281524", "10.1145/3267851.3267898", "10.1145/1314161.1314180", "10.1145/3267851.3267878", "10.1145/3072959.3073663", "10.1145/2897824.2925975", "10.1145/3139131.3139151", "10.1145/3267851.3267870", "10.1145/3292037", "10.1145/3197517.3201401", "10.1145/3272127.3275075", "10.1145/2993369.2993378", "10.1145/2492494.2492500", "10.1145/3267851.3267901", "10.1037/0022-3514.85.4.768", "10.1002/ejsp.2420040202", "10.1007/3-540-44812-8_8", "10.1007/978-3-642-15892-6_28", "10.1111/j.1468-2958.2008.00322.x", "10.1007/978-3-642-33197-8_13", "10.1007/978-3-540-39396-2_52", "10.1007/978-3-642-33197-8_7", "10.1037/0022-3514.68.6.1030", "10.1016/j.riob.2011.10.004", "10.17815/CD.2016.1", "10.1016/j.tics.2006.11.005", "10.1037//0022-3514.82.6.878", "10.1007/3-540-44812-8_12", "10.1007/978-1-4612-5106-4_2", "10.15607/RSS.2014.X.025", "10.1007/978-3-319-67401-8_25", "10.1016/j.compedu.2016.04.002", "10.1037/0022-3514.55.4.547", "10.1016/j.chb.2018.04.036", "10.1007/978-3-319-21996-7_18", "10.1037/h0039792", "10.21236/ADA427495", "10.1037/0022-3514.85.4.768", "10.1002/ejsp.2420040202", "10.1007/3-540-44812-8_8", "10.1007/978-3-642-15892-6_28", "10.1111/j.1468-2958.2008.00322.x", "10.1007/978-3-642-33197-8_13", "10.1007/978-3-540-39396-2_52", "10.1007/978-3-642-33197-8_7", "10.1037/0022-3514.68.6.1030", "10.1016/j.riob.2011.10.004", "10.17815/CD.2016.1", "10.1016/j.tics.2006.11.005", "10.1037//0022-3514.82.6.878", "10.1007/3-540-44812-8_12", "10.1007/978-1-4612-5106-4_2", "10.15607/RSS.2014.X.025", "10.1007/978-3-319-67401-8_25", "10.1016/j.compedu.2016.04.002", "10.1037/0022-3514.55.4.547", "10.1016/j.chb.2018.04.036", "10.1007/978-3-319-21996-7_18", "10.1037/h0039792", "10.21236/ADA427495", "10.1037/0022-3514.85.4.768", "10.1002/ejsp.2420040202", "10.1007/3-540-44812-8_8", "10.1007/978-3-642-15892-6_28", "10.1111/j.1468-2958.2008.00322.x", "10.1007/978-3-642-33197-8_13", "10.1007/978-3-540-39396-2_52", "10.1007/978-3-642-33197-8_7", "10.1037/0022-3514.68.6.1030", "10.1016/j.riob.2011.10.004", "10.17815/CD.2016.1", "10.1016/j.tics.2006.11.005", "10.1037//0022-3514.82.6.878", "10.1007/3-540-44812-8_12", "10.1007/978-1-4612-5106-4_2", "10.15607/RSS.2014.X.025", "10.1007/978-3-319-67401-8_25", "10.1016/j.compedu.2016.04.002", "10.1037/0022-3514.55.4.547", "10.1016/j.chb.2018.04.036", "10.1007/978-3-319-21996-7_18", "10.1037/h0039792", "10.21236/ADA427495"]}, "10.1109/TVCG.2019.2932224": {"doi": "10.1109/TVCG.2019.2932224", "author": ["T. Feigl", "D. Roth", "S. Gradl", "M. Wirth", "M. E. Latoschik", "B. M. Eskofier", "M. Philippsen", "C. Mutschler"], "title": "Sick Moves! Motion Parameters as Indicators of Simulator Sickness", "year": "2019", "abstract": "We explore motion parameters, more specifically gait parameters, as an objective indicator to assess simulator sickness in Virtual Reality (VR). We discuss the potential relationships between simulator sickness, immersion, and presence. We used two different camera pose (position and orientation) estimation methods for the evaluation of motion tasks in a large-scale VR environment: a simple model and an optimized model that allows for a more accurate and natural mapping of human senses. Participants performed multiple motion tasks (walking, balancing, running) in three conditions: a physical reality baseline condition, a VR condition with the simple model, and a VR condition with the optimized model. We compared these conditions with regard to the resulting sickness and gait, as well as the perceived presence in the VR conditions. The subjective measures confirmed that the optimized pose estimation model reduces simulator sickness and increases the perceived presence. The results further show that both models affect the gait parameters and simulator sickness, which is why we further investigated a classification approach that deals with non-linear correlation dependencies between gait parameters and simulator sickness. We argue that our approach could be used to assess and predict simulator sickness based on human gait parameters and we provide implications for future research.", "keywords": ["gait analysis", "pose estimation", "virtual reality", "sick moves", "motion parameters", "simulator sickness", "large-scale VR environment", "optimized model", "multiple motion tasks", "physical reality baseline condition", "VR condition", "perceived presence", "optimized pose estimation model", "human gait parameters", "Legged locomotion", "Solid modeling", "Atmospheric measurements", "Particle measurements", "Computational modeling", "Reliability", "Pose estimation", "Human-centered computing", "virtual reality", "user studies", "computing methodologies", "perception", "machine learning", "Adult", "Algorithms", "Computer Graphics", "Female", "Gait", "Humans", "Image Processing, Computer-Assisted", "Machine Learning", "Male", "Models, Statistical", "Motion Sickness", "Movement", "Psychomotor Performance", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:9284654"], "referencing": ["IKEY:8446495", "IKEY:8446300", "IKEY:7875162", "IKEY:7892254", "IKEY:7383304", "IKEY:1191132", "IKEY:8010633", "IKEY:8447550", "IKEY:6787998", "IKEY:6789102", "IKEY:6790873", "IKEY:7743707", "IKEY:6790816", "IKEY:6788002", "IKEY:8446221", "IKEY:7223319", "IKEY:8446495", "IKEY:8446300", "IKEY:7875162", "IKEY:7892254", "IKEY:7383304", "IKEY:1191132", "IKEY:8010633", "IKEY:8447550", "IKEY:6787998", "IKEY:6789102", "IKEY:6790873", "IKEY:7743707", "IKEY:6790816", "IKEY:6788002", "IKEY:8446221", "IKEY:7223319", "IKEY:8446495", "IKEY:8446300", "IKEY:7875162", "IKEY:7892254", "IKEY:7383304", "IKEY:1191132", "IKEY:8010633", "IKEY:8447550", "IKEY:6787998", "IKEY:6789102", "IKEY:6790873", "IKEY:7743707", "IKEY:6790816", "IKEY:6788002", "IKEY:8446221", "IKEY:7223319", "10.1145/1961189.1961199", "10.1145/3022731", "10.1145/3139131.3139137", "10.1145/800031.808575", "10.1145/333329.333344", "10.1145/3145690.3145697", "10.1145/566654.566630", "10.1145/3139131.3139145", "10.1145/3134301", "10.1145/1778765.1778829", "10.1145/210079.210084", "10.1145/2043603.2043607", "10.1145/2993369.2993402", "10.1145/311535.311589", "10.1145/1961189.1961199", "10.1145/3022731", "10.1145/3139131.3139137", "10.1145/800031.808575", "10.1145/333329.333344", "10.1145/3145690.3145697", "10.1145/566654.566630", "10.1145/3139131.3139145", "10.1145/3134301", "10.1145/1778765.1778829", "10.1145/210079.210084", "10.1145/2043603.2043607", "10.1145/2993369.2993402", "10.1145/311535.311589", "10.1145/1961189.1961199", "10.1145/3022731", "10.1145/3139131.3139137", "10.1145/800031.808575", "10.1145/333329.333344", "10.1145/3145690.3145697", "10.1145/566654.566630", "10.1145/3139131.3139145", "10.1145/3134301", "10.1145/1778765.1778829", "10.1145/210079.210084", "10.1145/2043603.2043607", "10.1145/2993369.2993402", "10.1145/311535.311589", "10.1016/j.gaitpost.2006.10.014", "10.3390/s150306419", "10.1186/s12984-016-0174-1", "10.1016/j.displa.2016.07.002", "10.1016/j.neuroscience.2016.09.019", "10.1177/154193120204602602", "10.1007/978-3-642-23071-4_19", "10.1162/pres.1992.1.2.262", "10.1016/j.gaitpost.2006.09.075", "10.1016/j.gaitpost.2005.05.005", "10.1207/s15327108ijap0303_3", "10.1080/10447319609526139", "10.1177/0018720811403736", "10.3389/fpsyg.2015.00472", "10.21236/ADA295861", "10.1016/j.gaitpost.2017.07.040", "10.1111/j.1468-2885.2004.tb00302.x", "10.1002/9781119243649", "10.1016/j.gaitpost.2018.08.005", "10.1016/S0003-6870(00)00059-4", "10.3390/s16010134", "10.1007/s10484-005-6381-3", "10.1093/gerona/glq201", "10.1016/j.displa.2011.05.010", "10.1002/mds.25673", "10.1016/j.displa.2014.10.005", "10.1007/s10055-016-0285-9", "10.1007/BF02009710", "10.1068/p150173", "10.1037/neu0000405", "10.1371/journal.pone.0183989", "10.1098/rstb.2009.0138", "10.1162/pres.1994.3.2.130", "10.1016/j.gaitpost.2018.01.003", "10.1177/107118139704100292", "10.1007/978-1-4419-8432-6", "10.1016/j.humov.2011.06.002", "10.2466/pms.1986.63.2.555", "10.1016/j.humov.2017.03.004", "10.1111/j.1469-8986.1987.tb00324.x", "10.1016/j.gaitpost.2006.10.014", "10.3390/s150306419", "10.1186/s12984-016-0174-1", "10.1016/j.displa.2016.07.002", "10.1016/j.neuroscience.2016.09.019", "10.1177/154193120204602602", "10.1007/978-3-642-23071-4_19", "10.1162/pres.1992.1.2.262", "10.1016/j.gaitpost.2006.09.075", "10.1016/j.gaitpost.2005.05.005", "10.1207/s15327108ijap0303_3", "10.1080/10447319609526139", "10.1177/0018720811403736", "10.3389/fpsyg.2015.00472", "10.21236/ADA295861", "10.1016/j.gaitpost.2017.07.040", "10.1111/j.1468-2885.2004.tb00302.x", "10.1002/9781119243649", "10.1016/j.gaitpost.2018.08.005", "10.1016/S0003-6870(00)00059-4", "10.3390/s16010134", "10.1007/s10484-005-6381-3", "10.1093/gerona/glq201", "10.1016/j.displa.2011.05.010", "10.1002/mds.25673", "10.1016/j.displa.2014.10.005", "10.1007/s10055-016-0285-9", "10.1007/BF02009710", "10.1068/p150173", "10.1037/neu0000405", "10.1371/journal.pone.0183989", "10.1098/rstb.2009.0138", "10.1162/pres.1994.3.2.130", "10.1016/j.gaitpost.2018.01.003", "10.1177/107118139704100292", "10.1007/978-1-4419-8432-6", "10.1016/j.humov.2011.06.002", "10.2466/pms.1986.63.2.555", "10.1016/j.humov.2017.03.004", "10.1111/j.1469-8986.1987.tb00324.x", "10.1016/j.gaitpost.2006.10.014", "10.3390/s150306419", "10.1186/s12984-016-0174-1", "10.1016/j.displa.2016.07.002", "10.1016/j.neuroscience.2016.09.019", "10.1177/154193120204602602", "10.1007/978-3-642-23071-4_19", "10.1162/pres.1992.1.2.262", "10.1016/j.gaitpost.2006.09.075", "10.1016/j.gaitpost.2005.05.005", "10.1207/s15327108ijap0303_3", "10.1080/10447319609526139", "10.1177/0018720811403736", "10.3389/fpsyg.2015.00472", "10.21236/ADA295861", "10.1016/j.gaitpost.2017.07.040", "10.1111/j.1468-2885.2004.tb00302.x", "10.1002/9781119243649", "10.1016/j.gaitpost.2018.08.005", "10.1016/S0003-6870(00)00059-4", "10.3390/s16010134", "10.1007/s10484-005-6381-3", "10.1093/gerona/glq201", "10.1016/j.displa.2011.05.010", "10.1002/mds.25673", "10.1016/j.displa.2014.10.005", "10.1007/s10055-016-0285-9", "10.1007/BF02009710", "10.1068/p150173", "10.1037/neu0000405", "10.1371/journal.pone.0183989", "10.1098/rstb.2009.0138", "10.1162/pres.1994.3.2.130", "10.1016/j.gaitpost.2018.01.003", "10.1177/107118139704100292", "10.1007/978-1-4419-8432-6", "10.1016/j.humov.2011.06.002", "10.2466/pms.1986.63.2.555", "10.1016/j.humov.2017.03.004", "10.1111/j.1469-8986.1987.tb00324.x"]}, "10.1109/TVCG.2019.2932213": {"doi": "10.1109/TVCG.2019.2932213", "author": ["N. L. Williams", "T. C. Peck"], "title": "Estimation of Rotation Gain Thresholds Considering FOV, Gender, and Distractors", "year": "2019", "abstract": "Redirected walking techniques enable users to naturally locomote in virtual environments (VEs) that are larger than the tracked space. Redirected walking imperceptibly transforms the VE around the user with predefined estimated threshold gains. Previously estimated gains were evaluated with a 40\u00b0 field of view (FOV), and have not been evaluated in the presence of a distractor-a moving object in the VE that may capture the user's attention. We conducted a 2 (FOV: 40\u00b0, 110\u00b0) \u00d7 2 (Gender: female, male) \u00d7 2 (Distractor: without, with) user study to estimate and compare thresholds for rotation gains. Significant differences in detection thresholds were found between FOVs, and significant differences were found between female and male gains with a 110\u00b0 FOV. Males had significantly wider gains using a 110\u00b0 FOV compared to a 40\u00b0 FOV, and distractors affected females differently than males. Finally, strong correlations were found between simulator sickness scores and threshold gains.", "keywords": ["gait analysis", "virtual reality", "predefined estimated threshold gains", "rotation gains", "detection thresholds", "male gains", "distractors", "redirected walking techniques", "rotation gain thresholds", "moving object", "Visualization", "Legged locomotion", "Observers", "Usability", "Virtual environments", "Stimulated emission", "Virtual reality", "Locomotion", "Perception", "Detection thresholds", "Distractors", "Gender differences", "Simulator sickness", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Middle Aged", "Rotation", "Task Performance and Analysis", "Video Games", "Virtual Reality", "Visual Fields", "Walking", "Young Adult"], "referenced_by": [], "referencing": ["IKEY:6165134", "IKEY:7460053", "IKEY:4142862", "IKEY:799737", "IKEY:996517", "IKEY:7460055", "IKEY:996519", "IKEY:8255772", "IKEY:7504743", "IKEY:7504752", "IKEY:4663065", "IKEY:5444816", "IKEY:8613757", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6787863", "IKEY:6550194", "IKEY:6797503", "IKEY:1191170", "IKEY:6165134", "IKEY:7460053", "IKEY:4142862", "IKEY:799737", "IKEY:996517", "IKEY:7460055", "IKEY:996519", "IKEY:8255772", "IKEY:7504743", "IKEY:7504752", "IKEY:4663065", "IKEY:5444816", "IKEY:8613757", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6787863", "IKEY:6550194", "IKEY:6797503", "IKEY:1191170", "IKEY:6165134", "IKEY:7460053", "IKEY:4142862", "IKEY:799737", "IKEY:996517", "IKEY:7460055", "IKEY:996519", "IKEY:8255772", "IKEY:7504743", "IKEY:7504752", "IKEY:4663065", "IKEY:5444816", "IKEY:8613757", "IKEY:6549412", "IKEY:5072212", "IKEY:6180877", "IKEY:5759455", "IKEY:6787863", "IKEY:6550194", "IKEY:6797503", "IKEY:1191170", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2043603.2043604", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3225153.3225155", "10.1145/1502800.1502805", "10.1145/1394281.1394319", "10.1145/311535.311589", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2043603.2043604", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3225153.3225155", "10.1145/1502800.1502805", "10.1145/1394281.1394319", "10.1145/311535.311589", "10.1145/3095140.3095162", "10.1145/2931002.2931018", "10.1145/2043603.2043604", "10.1145/1179133.1179162", "10.1145/1394281.1394310", "10.1145/3225153.3225155", "10.1145/1502800.1502805", "10.1145/1394281.1394319", "10.1145/311535.311589", "10.1111/1468-5884.00151", "10.1007/BF00234474", "10.1037/h0025270", "10.3758/BF03200563", "10.4324/9781410605290", "10.1207/s15327876mp0203_4", "10.1007/978-3-319-91581-4_7", "10.1159/000309269", "10.21236/ADA155975", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/978-3-319-08234-9_253-1", "10.1016/S1364-6613(99)01364-9", "10.17705/1CAIS.03737", "10.1097/01.prs.0000304608.33432.67", "10.1016/S0966-6362(96)01109-5", "10.2307/271063", "10.3758/s13414-011-0129-3", "10.1007/978-3-7091-9433-1_2", "10.1518/hfes.45.3.504.27254", "10.3758/BF03194105", "10.3758/BF03211640", "10.1038/84054", "10.1016/S0304-3959(01)00473-0", "10.1111/1468-5884.00151", "10.1007/BF00234474", "10.1037/h0025270", "10.3758/BF03200563", "10.4324/9781410605290", "10.1207/s15327876mp0203_4", "10.1007/978-3-319-91581-4_7", "10.1159/000309269", "10.21236/ADA155975", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/978-3-319-08234-9_253-1", "10.1016/S1364-6613(99)01364-9", "10.17705/1CAIS.03737", "10.1097/01.prs.0000304608.33432.67", "10.1016/S0966-6362(96)01109-5", "10.2307/271063", "10.3758/s13414-011-0129-3", "10.1007/978-3-7091-9433-1_2", "10.1518/hfes.45.3.504.27254", "10.3758/BF03194105", "10.3758/BF03211640", "10.1038/84054", "10.1016/S0304-3959(01)00473-0", "10.1111/1468-5884.00151", "10.1007/BF00234474", "10.1037/h0025270", "10.3758/BF03200563", "10.4324/9781410605290", "10.1207/s15327876mp0203_4", "10.1007/978-3-319-91581-4_7", "10.1159/000309269", "10.21236/ADA155975", "10.1207/s15327108ijap0303_3", "10.21236/ADA295861", "10.1007/978-3-319-08234-9_253-1", "10.1016/S1364-6613(99)01364-9", "10.17705/1CAIS.03737", "10.1097/01.prs.0000304608.33432.67", "10.1016/S0966-6362(96)01109-5", "10.2307/271063", "10.3758/s13414-011-0129-3", "10.1007/978-3-7091-9433-1_2", "10.1518/hfes.45.3.504.27254", "10.3758/BF03194105", "10.3758/BF03211640", "10.1038/84054", "10.1016/S0304-3959(01)00473-0"]}, "10.1109/TVCG.2019.2932215": {"doi": "10.1109/TVCG.2019.2932215", "author": ["D. Wolf", "M. Rietzler", "L. Hnatek", "E. Rukzio"], "title": "Face/On: Multi-Modal Haptic Feedback for Head-Mounted Displays in Virtual Reality", "year": "2019", "abstract": "While the real world provides humans with a huge variety of sensory stimuli, virtual worlds most of all communicate their properties by visual and auditory feedback due to the design of current head mounted displays (HMDs). Since HMDs offer sufficient contact area to integrate additional actuators, prior works utilised a limited amount of haptic actuators to integrate respective information about the virtual world. With the Face/On prototype complex feedback patterns are introduced that combine a high number of vibration motors with additional thermal sources to transport multi-modal and spatial information. A pre-study determining the boundaries of the feedbacks' intensities as well as a user study showing a significant increase of presence and enjoyment validate Face/On's approach.", "keywords": ["haptic interfaces", "helmet mounted displays", "virtual reality", "head-mounted displays", "virtual reality", "sensory stimuli", "visual feedback", "auditory feedback", "HMDs", "haptic actuators", "spatial information", "multimodal haptic feedback", "thermal sources", "vibration motors", "Face-On prototype complex feedback patterns", "Haptic interfaces", "Actuators", "Resists", "Face", "Visualization", "Skin", "Virtual environments", "VR", "haptic feedback", "multi-modal", "thermal feedback", "Adult", "Computer Graphics", "Equipment Design", "Face", "Feedback, Sensory", "Female", "Head", "Humans", "Male", "Vibration", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:9296748"], "referencing": ["IKEY:6207766", "IKEY:756955", "IKEY:6189750", "IKEY:998951", "IKEY:996519", "IKEY:6207766", "IKEY:756955", "IKEY:6189750", "IKEY:998951", "IKEY:996519", "IKEY:6207766", "IKEY:756955", "IKEY:6189750", "IKEY:998951", "IKEY:996519", "10.1145/3242587.3242588", "10.1145/3024969.3025060", "10.1145/3131785.3131825", "10.1145/2556288.2557101", "10.1145/2160125.2160129", "10.1145/2910674.2910683", "10.1145/3173574.3174232", "10.1145/2858036.2858487", "10.1145/2984511.2984535", "10.1145/2858036.2858040", "10.1145/1275511.1275514", "10.1145/3267782.3267789", "10.1145/1978942.1979426", "10.1145/3025453.3025684", "10.1145/3281505.3281506", "10.1145/2807442.2807443", "10.1145/3025453.3025824", "10.1145/3025453.3025723", "10.1145/3173574.3174151", "10.1145/2338676.2338680", "10.1145/3025453.3026009", "10.1145/3134301", "10.1145/3266037.3266095", "10.1145/3242587.3242588", "10.1145/3024969.3025060", "10.1145/3131785.3131825", "10.1145/2556288.2557101", "10.1145/2160125.2160129", "10.1145/2910674.2910683", "10.1145/3173574.3174232", "10.1145/2858036.2858487", "10.1145/2984511.2984535", "10.1145/2858036.2858040", "10.1145/1275511.1275514", "10.1145/3267782.3267789", "10.1145/1978942.1979426", "10.1145/3025453.3025684", "10.1145/3281505.3281506", "10.1145/2807442.2807443", "10.1145/3025453.3025824", "10.1145/3025453.3025723", "10.1145/3173574.3174151", "10.1145/2338676.2338680", "10.1145/3025453.3026009", "10.1145/3134301", "10.1145/3266037.3266095", "10.1145/3242587.3242588", "10.1145/3024969.3025060", "10.1145/3131785.3131825", "10.1145/2556288.2557101", "10.1145/2160125.2160129", "10.1145/2910674.2910683", "10.1145/3173574.3174232", "10.1145/2858036.2858487", "10.1145/2984511.2984535", "10.1145/2858036.2858040", "10.1145/1275511.1275514", "10.1145/3267782.3267789", "10.1145/1978942.1979426", "10.1145/3025453.3025684", "10.1145/3281505.3281506", "10.1145/2807442.2807443", "10.1145/3025453.3025824", "10.1145/3025453.3025723", "10.1145/3173574.3174151", "10.1145/2338676.2338680", "10.1145/3025453.3026009", "10.1145/3134301", "10.1145/3266037.3266095", "10.1016/S0006-3495(84)84265-4", "10.1007/978-3-642-39405-8_19", "10.3109/07367228709144606", "10.1152/jappl.1963.18.6.1146", "10.1518/001872008X250638", "10.1207/s15327108ijap0303_3", "10.1007/978-3-662-44196-1_8", "10.1111/j.1469-8986.1992.tb01689.x", "10.1016/0010-0285(87)90008-9", "10.1111/j.1460-9568.2012.08092.x", "10.1007/978-3-319-93399-3_32", "10.1007/11758525_81", "10.1162/pres.1994.3.2.130", "10.1016/0011-7471(68)90057-0", "10.1016/j.ijhcs.2017.08.002", "10.1126/science.1254229", "10.1016/S0006-3495(84)84265-4", "10.1007/978-3-642-39405-8_19", "10.3109/07367228709144606", "10.1152/jappl.1963.18.6.1146", "10.1518/001872008X250638", "10.1207/s15327108ijap0303_3", "10.1007/978-3-662-44196-1_8", "10.1111/j.1469-8986.1992.tb01689.x", "10.1016/0010-0285(87)90008-9", "10.1111/j.1460-9568.2012.08092.x", "10.1007/978-3-319-93399-3_32", "10.1007/11758525_81", "10.1162/pres.1994.3.2.130", "10.1016/0011-7471(68)90057-0", "10.1016/j.ijhcs.2017.08.002", "10.1126/science.1254229", "10.1016/S0006-3495(84)84265-4", "10.1007/978-3-642-39405-8_19", "10.3109/07367228709144606", "10.1152/jappl.1963.18.6.1146", "10.1518/001872008X250638", "10.1207/s15327108ijap0303_3", "10.1007/978-3-662-44196-1_8", "10.1111/j.1469-8986.1992.tb01689.x", "10.1016/0010-0285(87)90008-9", "10.1111/j.1460-9568.2012.08092.x", "10.1007/978-3-319-93399-3_32", "10.1007/11758525_81", "10.1162/pres.1994.3.2.130", "10.1016/0011-7471(68)90057-0", "10.1016/j.ijhcs.2017.08.002", "10.1126/science.1254229"]}, "10.1109/TVCG.2019.2932173": {"doi": "10.1109/TVCG.2019.2932173", "author": ["A. Irlitti", "T. Piumsomboon", "D. Jackson", "B. H. Thomas"], "title": "Conveying spatial awareness cues in xR collaborations", "year": "2019", "abstract": "Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow, and radial, and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.", "keywords": ["augmented reality", "data visualisation", "helmet mounted displays", "spatial augmented reality systems", "extended reality technologies", "single exocentric visualization", "visual augmentation clutter", "world animated radial cue", "multiple collaborators", "single virtual collaborator", "virtual attachment cues", "single exocentric world", "independent dimensions", "egocentric cues", "determining dimensions", "spatial-communication taxonomy", "SAR environments", "spatial cues", "virtual environment", "augmented physical environment", "first-person camera view", "head-mounted display", "tablet interface", "viewing technology", "xR collaborations", "spatial awareness cues", "Collaboration", "Visualization", "Taxonomy", "Stakeholders", "Spatial augmented reality", "Virtual environments", "Animation", "Extended reality", "spatial augmented reality", "awareness", "augmented reality", "collaboration", "virtual reality", "Adult", "Computer Graphics", "Cues", "Female", "Head Movements", "Humans", "Male", "Middle Aged", "Video Games", "Virtual Reality", "Young Adult"], "referenced_by": ["IKEY:9284661", "IKEY:9284659"], "referencing": ["IKEY:8007333", "IKEY:8533895", "IKEY:1544668", "IKEY:816444", "IKEY:6671762", "IKEY:6193074", "IKEY:7893339", "IKEY:4637356", "IKEY:8007333", "IKEY:8533895", "IKEY:1544668", "IKEY:816444", "IKEY:6671762", "IKEY:6193074", "IKEY:7893339", "IKEY:4637356", "IKEY:8007333", "IKEY:8533895", "IKEY:1544668", "IKEY:816444", "IKEY:6671762", "IKEY:6193074", "IKEY:7893339", "IKEY:4637356", "10.1145/2491367.2491378", "10.1145/2807442.2807493", "10.1145/2642918.2647402", "10.1145/769953.769964", "10.1145/2671015.2671116", "10.1145/2642918.2647383", "10.1145/2070781.2024222", "10.1145/3025453.3025717", "10.1145/2807442.2807497", "10.1145/3173574.3173620", "10.1145/3290605.3300458", "10.1145/3132818.3132822", "10.1145/280814.280861", "10.1145/302979.303113", "10.1145/3126594.3126638", "10.1145/3279778.3279806", "10.1145/3173574.3173863", "10.1145/2207676.2207702", "10.1145/223904.223938", "10.1145/159544.159630", "10.1145/1866029.1866073", "10.1145/1978942.1978963", "10.1145/2491367.2491378", "10.1145/2807442.2807493", "10.1145/2642918.2647402", "10.1145/769953.769964", "10.1145/2671015.2671116", "10.1145/2642918.2647383", "10.1145/2070781.2024222", "10.1145/3025453.3025717", "10.1145/2807442.2807497", "10.1145/3173574.3173620", "10.1145/3290605.3300458", "10.1145/3132818.3132822", "10.1145/280814.280861", "10.1145/302979.303113", "10.1145/3126594.3126638", "10.1145/3279778.3279806", "10.1145/3173574.3173863", "10.1145/2207676.2207702", "10.1145/223904.223938", "10.1145/159544.159630", "10.1145/1866029.1866073", "10.1145/1978942.1978963", "10.1145/2491367.2491378", "10.1145/2807442.2807493", "10.1145/2642918.2647402", "10.1145/769953.769964", "10.1145/2671015.2671116", "10.1145/2642918.2647383", "10.1145/2070781.2024222", "10.1145/3025453.3025717", "10.1145/2807442.2807497", "10.1145/3173574.3173620", "10.1145/3290605.3300458", "10.1145/3132818.3132822", "10.1145/280814.280861", "10.1145/302979.303113", "10.1145/3126594.3126638", "10.1145/3279778.3279806", "10.1145/3173574.3173863", "10.1145/2207676.2207702", "10.1145/223904.223938", "10.1145/159544.159630", "10.1145/1866029.1866073", "10.1145/1978942.1978963", "10.1007/978-3-642-87512-0_15", "10.1016/S0097-8493(01)00117-0", "10.1037/10096-006", "10.1023/A:1021271517844", "10.1007/978-1-4471-3588-3_18", "10.1037//0022-0663.84.4.429", "10.1007/978-3-642-87512-0_15", "10.1016/S0097-8493(01)00117-0", "10.1037/10096-006", "10.1023/A:1021271517844", "10.1007/978-1-4471-3588-3_18", "10.1037//0022-0663.84.4.429", "10.1007/978-3-642-87512-0_15", "10.1016/S0097-8493(01)00117-0", "10.1037/10096-006", "10.1023/A:1021271517844", "10.1007/978-1-4471-3588-3_18", "10.1037//0022-0663.84.4.429"]}, "10.1109/TVCG.2019.2932239": {"doi": "10.1109/TVCG.2019.2932239", "author": ["D. Schneider", "A. Otte", "T. Gesslein", "P. Gagel", "B. Kuth", "M. S. Damlakhi", "O. Dietz", "E. Ofek", "M. Pahud", "P. O. Kristensson", "J. M\u00fcller", "J. Grubert"], "title": "ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality", "year": "2019", "abstract": "Physical keyboards are common peripherals for personal computers and are efficient standard text entry devices. Recent research has investigated how physical keyboards can be used in immersive head-mounted display-based Virtual Reality (VR). So far, the physical layout of keyboards has typically been transplanted into VR for replicating typing experiences in a standard desktop environment. In this paper, we explore how to fully leverage the immersiveness of VR to change the input and output characteristics of physical keyboard interaction within a VR environment. This allows individual physical keys to be reconfigured to the same or different actions and visual output to be distributed in various ways across the VR representation of the keyboard. We explore a set of input and output mappings for reconfiguring the virtual presentation of physical keyboards and probe the resulting design space by specifically designing, implementing and evaluating nine VR-relevant applications: emojis, languages and special characters, application shortcuts, virtual text processing macros, a window manager, a photo browser, a whack-a-mole game, secure password entry and a virtual touch bar. We investigate the feasibility of the applications in a user study with 20 participants and find that, among other things, they are usable in VR. We discuss the limitations and possibilities of remapping the input and output characteristics of physical keyboards in VR based on empirical findings and analysis and suggest future research directions in this area.", "keywords": ["helmet mounted displays", "keyboards", "virtual reality", "efficient standard text entry devices", "physical keyboard interaction", "VR environment", "standard text entry devices", "immersive head-mounted display-based virtual reality", "standard desktop environment", "VR representation", "virtual text processing macros", "window manager", "photo browser", "whack-a-mole game", "secure password entry", "virtual touch bar", "emojis", "languages", "ReconViguRation", "input-output mappings", "Keyboards", "Visualization", "Virtual reality", "Sensors", "Tracking", "Password", "Task analysis", "Virtual Reality", "Text Entry", "Physical Keyboards"], "referenced_by": ["IKEY:8951915", "IKEY:9089611", "IKEY:9187469", "IKEY:9212653", "IKEY:9284682"], "referencing": ["IKEY:4016678", "IKEY:8516449", "IKEY:6347035", "IKEY:6797427", "IKEY:6171142", "IKEY:8617763", "IKEY:8446250", "IKEY:8446059", "IKEY:1528429", "IKEY:7958585", "IKEY:756952", "IKEY:7917636", "IKEY:5444423", "IKEY:4016678", "IKEY:8516449", "IKEY:6347035", "IKEY:6797427", "IKEY:6171142", "IKEY:8617763", "IKEY:8446250", "IKEY:8446059", "IKEY:1528429", "IKEY:7958585", "IKEY:756952", "IKEY:7917636", "IKEY:5444423", "IKEY:4016678", "IKEY:8516449", "IKEY:6347035", "IKEY:6797427", "IKEY:6171142", "IKEY:8617763", "IKEY:8446250", "IKEY:8446059", "IKEY:1528429", "IKEY:7958585", "IKEY:756952", "IKEY:7917636", "IKEY:5444423", "10.1145/2858036.2858226", "10.1145/1753326.1753498", "10.1145/3173574.3173595", "10.1145/2556288.2556955", "10.1145/3173574.3174220", "10.1145/1622176.1622187", "10.1145/3025453.3025636", "10.1145/1182475.1182538", "10.1145/2858036.2858134", "10.1145/1866218.1866233", "10.1145/2556288.2557336", "10.1145/3173574.3173919", "10.1145/122672.122692", "10.1145/3102163.3102175", "10.1145/2702123.2702382", "10.1145/964696.964719", "10.1145/2580723.2580730", "10.1145/3131785.3131786", "10.1145/3173574.3174170", "10.1145/2556288.2557030", "10.1145/2556288.2557030", "10.1145/3025453.3025783", "10.1145/1166253.1166292", "10.1145/2556288.2557362", "10.1145/3242587.3242589", "10.1145/2858036.2858355", "10.1145/2858036.2858226", "10.1145/1753326.1753498", "10.1145/3173574.3173595", "10.1145/2556288.2556955", "10.1145/3173574.3174220", "10.1145/1622176.1622187", "10.1145/3025453.3025636", "10.1145/1182475.1182538", "10.1145/2858036.2858134", "10.1145/1866218.1866233", "10.1145/2556288.2557336", "10.1145/3173574.3173919", "10.1145/122672.122692", "10.1145/3102163.3102175", "10.1145/2702123.2702382", "10.1145/964696.964719", "10.1145/2580723.2580730", "10.1145/3131785.3131786", "10.1145/3173574.3174170", "10.1145/2556288.2557030", "10.1145/2556288.2557030", "10.1145/3025453.3025783", "10.1145/1166253.1166292", "10.1145/2556288.2557362", "10.1145/3242587.3242589", "10.1145/2858036.2858355", "10.1145/2858036.2858226", "10.1145/1753326.1753498", "10.1145/3173574.3173595", "10.1145/2556288.2556955", "10.1145/3173574.3174220", "10.1145/1622176.1622187", "10.1145/3025453.3025636", "10.1145/1182475.1182538", "10.1145/2858036.2858134", "10.1145/1866218.1866233", "10.1145/2556288.2557336", "10.1145/3173574.3173919", "10.1145/122672.122692", "10.1145/3102163.3102175", "10.1145/2702123.2702382", "10.1145/964696.964719", "10.1145/2580723.2580730", "10.1145/3131785.3131786", "10.1145/3173574.3174170", "10.1145/2556288.2557030", "10.1145/2556288.2557030", "10.1145/3025453.3025783", "10.1145/1166253.1166292", "10.1145/2556288.2557362", "10.1145/3242587.3242589", "10.1145/2858036.2858355", "10.1007/978-3-030-22643-5_33", "10.14722/usec.2017.23028", "10.1007/978-3-642-03658-3_30", "10.1007/978-3-319-92279-9_36", "10.1007/s10055-009-0126-1", "10.1007/978-3-642-39330-3_29", "10.1207/s15327590ijhc1802_1", "10.4218/etrij.13.0213.0117", "10.1007/978-3-642-23765-2_32", "10.1177/154193121005400612", "10.1007/978-3-319-91250-9_6", "10.1007/978-3-030-22643-5_33", "10.14722/usec.2017.23028", "10.1007/978-3-642-03658-3_30", "10.1007/978-3-319-92279-9_36", "10.1007/s10055-009-0126-1", "10.1007/978-3-642-39330-3_29", "10.1207/s15327590ijhc1802_1", "10.4218/etrij.13.0213.0117", "10.1007/978-3-642-23765-2_32", "10.1177/154193121005400612", "10.1007/978-3-319-91250-9_6", "10.1007/978-3-030-22643-5_33", "10.14722/usec.2017.23028", "10.1007/978-3-642-03658-3_30", "10.1007/978-3-319-92279-9_36", "10.1007/s10055-009-0126-1", "10.1007/978-3-642-39330-3_29", "10.1207/s15327590ijhc1802_1", "10.4218/etrij.13.0213.0117", "10.1007/978-3-642-23765-2_32", "10.1177/154193121005400612", "10.1007/978-3-319-91250-9_6"]}}