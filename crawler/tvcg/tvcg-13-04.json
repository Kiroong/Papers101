{"10.1109/TVCG.2013.43": {"doi": "10.1109/TVCG.2013.43", "author": ["B. Laha", "D. A. Bowman", "J. D. Schiffbauer"], "title": "Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data", "year": "2013", "abstract": "In our research agenda to study the effects of immersion (level of fidelity) on various tasks in virtual reality (VR) systems, we have found that the most generalizable findings come not from direct comparisons of different technologies, but from controlled simulations of those technologies. We call this the mixed reality (MR) simulation approach. However, the validity of MR simulation, especially when different simulator platforms are used, can be questioned. In this paper, we report the results of an experiment examining the effects of field of regard (FOR) and head tracking on the analysis of volume visualized micro-CT datasets, and compare them with those from a previous study. The original study used a CAVE-like display as the MR simulator platform, while the present study used a high-end head-mounted display (HMD). Out of the 24 combinations of system characteristics and tasks tested on the two platforms, we found that the results produced by the two different MR simulators were similar in 20 cases. However, only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study. Our observations provide evidence both for and against the validity of MR simulation, and give insight into the differences caused by different MR simulator platforms. The present experiment also examined new conditions not present in the original study, and produced new significant results, which confirm and extend previous existing knowledge on the effects of FOR and head tracking. We provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications.", "keywords": ["data analysis", "data visualisation", "digital simulation", "helmet mounted displays", "virtual reality", "MR simulation approach", "immersion effects", "visual volume data analysis", "virtual reality systems", "VR", "mixed reality simulation approach", "field of regard", "FOR", "head tracking", "volume visualized microCT datasets", "CAVE-like display", "MR simulator platform", "high-end head-mounted display", "HMD", "display systems", "volume visualization applications", "Virtual reality", "Visualization", "Mice", "Solid modeling", "Head", "Training", "Computational modeling", "MR simulator", "immersion", "micro-CT", "volume visualization", "virtual reality", "3D visualization", "HMD", "virtual environments.", "Adolescent", "Adult", "Computer Graphics", "Cues", "Equipment Design", "Equipment Failure Analysis", "Female", "Humans", "Imaging, Three-Dimensional", "Imaging, Three-Dimensional", "Male", "Task Performance and Analysis", "Tomography, X-Ray Computed", "User-Computer Interface", "Visual Perception", "Young Adult"], "referenced_by": ["IKEY:6550221", "IKEY:7460028", "IKEY:6777465", "IKEY:7042312", "IKEY:7504730", "IKEY:7892228", "IKEY:8642297", "IKEY:9089483", "IKEY:7094967", "10.1007/s00117-013-2540-3", "10.1007/s00371-015-1151-6", "10.1016/j.jbi.2017.07.009", "10.3154/jvs.37.146_2", "10.1016/j.cag.2019.09.006"], "referencing": ["IKEY:583052", "IKEY:4287241", "IKEY:1608019", "IKEY:5643560", "IKEY:1310069", "IKEY:6797763", "IKEY:6790922", "IKEY:6165141", "IKEY:5444820", "IKEY:6180890", "IKEY:6165144", "IKEY:4359501", "IKEY:1260740", "IKEY:964545", "IKEY:583052", "IKEY:4287241", "IKEY:1608019", "IKEY:5643560", "IKEY:1310069", "IKEY:6797763", "IKEY:6790922", "IKEY:6165141", "IKEY:5444820", "IKEY:6180890", "IKEY:6165144", "IKEY:4359501", "IKEY:1260740", "IKEY:964545", "IKEY:583052", "IKEY:4287241", "IKEY:1608019", "IKEY:5643560", "IKEY:1310069", "IKEY:6797763", "IKEY:6790922", "IKEY:6165141", "IKEY:5444820", "IKEY:6180890", "IKEY:6165144", "IKEY:4359501", "IKEY:1260740", "IKEY:964545", "10.1145/1180495.1180518", "10.1145/566654.566630", "10.1145/1315184.1315205", "10.1145/234972.234975", "10.1145/1180495.1180518", "10.1145/566654.566630", "10.1145/1315184.1315205", "10.1145/234972.234975", "10.1145/1180495.1180518", "10.1145/566654.566630", "10.1145/1315184.1315205", "10.1145/234972.234975", "10.1016/j.cageo.2007.11.009", "10.1007/978-3-540-89639-5_86", "10.1002/cav.159", "10.1130/G32546.1", "10.1016/j.cageo.2007.11.009", "10.1007/978-3-540-89639-5_86", "10.1002/cav.159", "10.1130/G32546.1", "10.1016/j.cageo.2007.11.009", "10.1007/978-3-540-89639-5_86", "10.1002/cav.159", "10.1130/G32546.1"]}, "10.1109/TVCG.2013.25": {"doi": "10.1109/TVCG.2013.25", "author": ["J. H. Chuah", "B. Lok", "E. Black"], "title": "Applying Mixed Reality to Simulate Vulnerable Populations for Practicing Clinical Communication Skills", "year": "2013", "abstract": "Health sciences students often practice and are evaluated on interview and exam skills by working with standardized patients (people that role play having a disease or condition). However, standardized patients do not exist for certain vulnerable populations such as children and the intellectually disabled. As a result, students receive little to no exposure to vulnerable populations before becoming working professionals. To address this problem and thereby increase exposure to vulnerable populations, we propose using virtual humans to simulate members of vulnerable populations. We created a mixed reality pediatric patient that allowed students to practice pediatric developmental exams. Practicing several exams is necessary for students to understand how to properly interact with and correctly assess a variety of children. Practice also increases a student's confidence in performing the exam. Effective practice requires students to treat the virtual child realistically. Treating the child realistically might be affected by how the student and virtual child physically interact, so we created two object interaction interfaces - a natural interface and a mouse-based interface. We tested the complete mixed reality exam and also compared the two object interaction interfaces in a within-subjects user study with 22 participants. Our results showed that the participants accepted the virtual child as a child and treated it realistically. Participants also preferred the natural interface, but the interface did not affect how realistically participants treated the virtual child.", "keywords": ["biomedical education", "computer based training", "medical computing", "mouse controllers (computers)", "user interfaces", "virtual reality", "vulnerable populations simulation", "clinical communication skills practicing", "health sciences students", "standardized patients", "intellectually disabled", "working professionals", "virtual humans", "mixed reality pediatric patient", "virtual child", "object interaction interfaces", "natural interface", "mouse-based interface", "complete mixed reality exam", "Pediatrics", "Shape", "Virtual reality", "Sociology", "Statistics", "Training", "Tutorials", "Virtual humans", "medical education", "social presence", "presence.", "Child, Preschool", "Communication", "Computer Graphics", "Computer-Assisted Instruction", "Developmental Disabilities", "Female", "Humans", "Male", "Neuropsychological Tests", "User-Computer Interface", "Vulnerable Populations"], "referenced_by": ["IKEY:8442093", "10.3389/fict.2016.00017", "10.1080/10401334.2019.1652180"], "referencing": ["IKEY:4811201", "IKEY:4811019", "IKEY:6180871", "IKEY:4811201", "IKEY:4811019", "IKEY:6180871", "IKEY:4811201", "IKEY:4811019", "IKEY:6180871", "10.1145/169059.169066", "10.1145/1240624.1240847", "10.1145/1240624.1240784", "10.1145/169059.169215", "10.1145/169059.169066", "10.1145/1240624.1240847", "10.1145/1240624.1240784", "10.1145/169059.169215", "10.1145/169059.169066", "10.1145/1240624.1240847", "10.1145/1240624.1240784", "10.1145/169059.169215", "10.1001/jama.1991.03470100082037", "10.1136/adc.2003.037325", "10.1542/peds.2006-3145", "10.1097/ACM.0b013e318248ed0a", "10.1007/978-3-642-15892-6_5", "10.1007/978-3-642-02806-9_12", "10.1186/1472-6920-6-29", "10.1177/0009922810363818", "10.1177/0146167203029007002", "10.1016/0950-7051(93)90017-N", "10.1001/jama.1991.03470100082037", "10.1136/adc.2003.037325", "10.1542/peds.2006-3145", "10.1097/ACM.0b013e318248ed0a", "10.1007/978-3-642-15892-6_5", "10.1007/978-3-642-02806-9_12", "10.1186/1472-6920-6-29", "10.1177/0009922810363818", "10.1177/0146167203029007002", "10.1016/0950-7051(93)90017-N", "10.1001/jama.1991.03470100082037", "10.1136/adc.2003.037325", "10.1542/peds.2006-3145", "10.1097/ACM.0b013e318248ed0a", "10.1007/978-3-642-15892-6_5", "10.1007/978-3-642-02806-9_12", "10.1186/1472-6920-6-29", "10.1177/0009922810363818", "10.1177/0146167203029007002", "10.1016/0950-7051(93)90017-N"]}, "10.1109/TVCG.2013.41": {"doi": "10.1109/TVCG.2013.41", "author": ["C. Lee", "G. A. Rincon", "G. Meyer", "T. H\u00f6llerer", "D. A. Bowman"], "title": "The Effects of Visual Realism on Search Tasks in Mixed Reality Simulation", "year": "2013", "abstract": "In this paper, we investigate the validity of Mixed Reality (MR) Simulation by conducting an experiment studying the effects of the visual realism of the simulated environment on various search tasks in Augmented Reality (AR). MR Simulation is a practical approach to conducting controlled and repeatable user experiments in MR, including AR. This approach uses a high-fidelity Virtual Reality (VR) display system to simulate a wide range of equal or lower fidelity displays from the MR continuum, for the express purpose of conducting user experiments. For the experiment, we created three virtual models of a real-world location, each with a different perceived level of visual realism. We designed and executed an AR experiment using the real-world location and repeated the experiment within VR using the three virtual models we created. The experiment looked into how fast users could search for both physical and virtual information that was present in the scene. Our experiment demonstrates the usefulness of MR Simulation and provides early evidence for the validity of MR Simulation with respect to AR search tasks performed in immersive VR.", "keywords": ["augmented reality", "visual realism", "search tasks", "mixed reality simulation", "MR simulation", "augmented reality", "high-fidelity virtual reality display system", "real-world location", "Visualization", "Solid modeling", "Lighting", "Cameras", "Geometry", "Virtual environments", "MR Simulation", "visual realism", "augmented reality.", "Adolescent", "Adult", "Computer Graphics", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Pattern Recognition, Visual", "Photic Stimulation", "Task Performance and Analysis", "User-Computer Interface", "Young Adult"], "referenced_by": ["IKEY:7460042", "IKEY:8186157", "IKEY:6948467", "IKEY:7042312", "IKEY:7547900", "IKEY:7833028", "IKEY:6802073", "IKEY:7223331", "IKEY:7504692", "IKEY:8943689", "IKEY:8943730", "IKEY:8998353", "IKEY:9284778", "10.1145/2663204.2663234", "10.1145/3267340", "10.1145/3361127", "10.1007/978-3-319-39298-1_5", "10.1007/978-3-319-41627-4_20", "10.1007/s00371-015-1151-6", "10.1016/B978-0-12-811272-4.00011-7", "10.1080/17458927.2017.1268822", "10.3389/fict.2016.00034", "10.3389/fict.2017.00013", "10.3389/fict.2017.00021", "10.1007/978-3-030-01790-3_12", "10.3390/mti2040071", "10.3389/fpsyg.2019.00141", "10.1007/s11596-019-1992-8", "10.1007/978-3-319-07458-0_22", "10.1007/978-3-030-20148-7_26", "10.1016/j.pmcj.2014.08.010", "10.3389/feduc.2020.00144"], "referencing": ["IKEY:4811058", "IKEY:6787998", "IKEY:4909120", "IKEY:1240690", "IKEY:5957485", "IKEY:6788002", "IKEY:5444820", "IKEY:6180890", "IKEY:6797341", "IKEY:5336464", "IKEY:4811058", "IKEY:6787998", "IKEY:4909120", "IKEY:1240690", "IKEY:5957485", "IKEY:6788002", "IKEY:5444820", "IKEY:6180890", "IKEY:6797341", "IKEY:5336464", "IKEY:4811058", "IKEY:6787998", "IKEY:4909120", "IKEY:1240690", "IKEY:5957485", "IKEY:6788002", "IKEY:5444820", "IKEY:6180890", "IKEY:6797341", "IKEY:5336464", "10.1145/1670671.1670673", "10.1145/1008653.1008669", "10.1145/1279640.1279643", "10.1145/1180495.1180569", "10.1145/1670671.1670673", "10.1145/1008653.1008669", "10.1145/1279640.1279643", "10.1145/1180495.1180569", "10.1145/1670671.1670673", "10.1145/1008653.1008669", "10.1145/1279640.1279643", "10.1145/1180495.1180569", "10.1007/978-3-7091-6242-2_22", "10.1007/978-3-7091-6242-2_22", "10.1007/978-3-7091-6242-2_22"]}, "10.1109/TVCG.2013.26": {"doi": "10.1109/TVCG.2013.26", "author": ["Z. Ren", "H. Yeh", "R. Klatzky", "M. C. Lin"], "title": "Auditory Perception of Geometry-Invariant Material Properties", "year": "2013", "abstract": "Accurately modeling the intrinsic material-dependent damping property for interactive sound rendering is a challenging problem. The Rayleigh damping model is commonly regarded as an adequate engineering model for interactive sound synthesis in virtual environment applications, but this assumption has never been rigorously analyzed. In this paper, we conduct a formal evaluation of this model. Our goal is to determine if auditory perception of material under Rayleigh damping assumption is 'geometryinvariant', i.e. if this approximation model is transferable across different shapes and sizes. First, audio recordings of same-material objects in various shapes and sizes are analyzed to determine if they can be approximated by the Rayleigh damping model with a single set of parameters. Next, we design and conduct a series of psychoacoustic experiments, in subjects evaluate if audio clips synthesized using the Rayleigh damping model are from the same material, when we alter the material, shape, and size parameters. Through both quantitative and qualitative evaluation, we show that the acoustic properties of the Rayleigh damping model for a single material is generally preserved across different geometries of objects consisting of homogeneous materials and is therefore a suitable, geometry-invariant sound model. Our study results also show that consistent with prior crossmodal expectations, visual perception of geometry can affect the auditory perception of materials. These findings facilitate the wide adoption of Rayleigh damping for interactive auditory systems and enable reuse of material parameters under this approximation model across different shapes and sizes, without laborious per-object parameter tuning.", "keywords": ["approximation theory", "audio signal processing", "rendering (computer graphics)", "signal synthesis", "virtual reality", "per-object parameter tuning", "qualitative evaluation", "quantitative evaluation", "size parameter", "shape parameter", "material parameter", "psychoacoustic experiment", "audio recording", "approximation model", "virtual environment application", "interactive sound synthesis", "Rayleigh damping model", "interactive sound rendering", "intrinsic material-dependent damping property", "geometry-invariant material property", "auditory perception", "Damping", "Shape", "Psychoacoustic models", "Geometry", "Glass", "Analytical models", "Sound synthesis", "human perception of material.", "Acoustic Stimulation", "Adult", "Auditory Perception", "Computer Graphics", "Cues", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Perceptual Masking", "Task Performance and Analysis", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:7361291", "IKEY:7384541", "IKEY:7223341", "IKEY:8642440", "10.1145/2421636.2421637", "10.1145/2897824.2925885", "10.1016/j.cag.2015.10.010", "10.1101/613778", "10.1007/978-1-4471-6533-0_4"], "referencing": ["IKEY:6797732", "IKEY:6787986", "IKEY:6788060", "IKEY:546002", "IKEY:5444796", "IKEY:5444799", "IKEY:6797732", "IKEY:6787986", "IKEY:6788060", "IKEY:546002", "IKEY:5444796", "IKEY:5444799", "IKEY:6797732", "IKEY:6787986", "IKEY:6788060", "IKEY:546002", "IKEY:5444796", "IKEY:5444799", "10.1145/1658349.1658350", "10.1145/1179352.1141983", "10.1145/545287.545290", "10.1145/1111411.1111429", "10.1145/2159616.2159618", "10.1145/142920.134063", "10.1145/383259.383322", "10.1145/1275808.1276473", "10.1145/1833349.1778806", "10.1145/1658349.1658350", "10.1145/1179352.1141983", "10.1145/545287.545290", "10.1145/1111411.1111429", "10.1145/2159616.2159618", "10.1145/142920.134063", "10.1145/383259.383322", "10.1145/1275808.1276473", "10.1145/1833349.1778806", "10.1145/1658349.1658350", "10.1145/1179352.1141983", "10.1145/545287.545290", "10.1145/1111411.1111429", "10.1145/2159616.2159618", "10.1145/142920.134063", "10.1145/383259.383322", "10.1145/1275808.1276473", "10.1145/1833349.1778806", "10.1006/jsvi.2000.3391", "10.1121/1.2149839", "10.1007/BFb0035211", "10.1121/1.1645855", "10.1006/jsvi.2000.3391", "10.1121/1.2149839", "10.1007/BFb0035211", "10.1121/1.1645855", "10.1006/jsvi.2000.3391", "10.1121/1.2149839", "10.1007/BFb0035211", "10.1121/1.1645855"]}, "10.1109/TVCG.2013.27": {"doi": "10.1109/TVCG.2013.27", "author": ["L. Antani", "D. Manocha"], "title": "Aural Proxies and Directionally-Varying Reverberation for Interactive Sound Propagation in Virtual Environments", "year": "2013", "abstract": "We present an efficient algorithm to compute spatially-varying, direction-dependent artificial reverberation and reflection filters in large dynamic scenes for interactive sound propagation in virtual environments and video games. Our approach performs Monte Carlo integration of local visibility and depth functions to compute directionally-varying reverberation effects. The algorithm also uses a dynamically-generated rectangular aural proxy to efficiently model 2-4 orders of early reflections. These two techniques are combined to generate reflection and reverberation filters which vary with the direction of incidence at the listener. This combination leads to better sound source localization and immersion. The overall algorithm is efficient, easy to implement, and can handle moving sound sources, listeners, and dynamic scenes, with minimal storage overhead. We have integrated our approach with the audio rendering pipeline in Valve's Source game engine, and use it to generate realistic directional sound propagation effects in indoor and outdoor scenes in real-time. We demonstrate, through quantitative comparisons as well as evaluations, that our approach leads to enhanced, immersive multi-modal interaction.", "keywords": ["acoustic wave propagation", "audio signal processing", "computer games", "filtering theory", "Monte Carlo methods", "rendering (computer graphics)", "reverberation", "virtual reality", "directional sound propagation effect", "audio rendering pipeline", "Valves Source game engine", "sound source immersion", "sound source localization", "listener incidence direction", "reverberation filter", "dynamically-generated rectangular aural proxy", "depth function", "visibility function", "Monte Carlo integration", "video games", "reflection filter", "direction-dependent artificial reverberation", "virtual environment", "interactive sound propagation", "directionally-varying reverberation effect", "Reverberation", "Computational modeling", "Geometry", "Face", "Games", "Mathematical model", "Absorption", "Sound propagation", "real-time", "directionally-varying reverberation", "local approximate models.", "Acoustic Stimulation", "Algorithms", "Auditory Perception", "Computer Graphics", "Humans", "Imaging, Three-Dimensional", "Sound Spectrography", "User-Computer Interface"], "referenced_by": ["10.1145/2943779", "10.1145/2980179.2982431", "10.1145/2601097.2601216", "10.1007/s11042-015-2943-4"], "referencing": ["IKEY:4658194", "IKEY:6143937", "IKEY:4376201", "IKEY:5165582", "IKEY:4658194", "IKEY:6143937", "IKEY:4376201", "IKEY:5165582", "IKEY:4658194", "IKEY:6143937", "IKEY:4376201", "IKEY:5165582", "10.1145/2070781.2024212", "10.1145/199404.199420", "10.1145/2077341.2077348", "10.1145/300523.300554", "10.1145/882262.882326", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/383259.383323", "10.1145/1778765.1778805", "10.1145/1230100.1230113", "10.1145/566570.566612", "10.1145/2070781.2024212", "10.1145/199404.199420", "10.1145/2077341.2077348", "10.1145/300523.300554", "10.1145/882262.882326", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/383259.383323", "10.1145/1778765.1778805", "10.1145/1230100.1230113", "10.1145/566570.566612", "10.1145/2070781.2024212", "10.1145/199404.199420", "10.1145/2077341.2077348", "10.1145/300523.300554", "10.1145/882262.882326", "10.1145/280814.280818", "10.1145/1631272.1631311", "10.1145/383259.383323", "10.1145/1778765.1778805", "10.1145/1230100.1230113", "10.1145/566570.566612", "10.1155/2007/70540", "10.1121/1.382599", "10.1121/1.1901884", "10.1121/1.3021297", "10.1121/1.2164987", "10.1121/1.398336", "10.1016/S0003-682X(99)00056-0", "10.1007/978-3-7091-6453-2_5", "10.1016/j.apacoust.2007.11.011", "10.1121/1.2766781", "10.1121/1.428071", "10.1155/2007/70540", "10.1121/1.382599", "10.1121/1.1901884", "10.1121/1.3021297", "10.1121/1.2164987", "10.1121/1.398336", "10.1016/S0003-682X(99)00056-0", "10.1007/978-3-7091-6453-2_5", "10.1016/j.apacoust.2007.11.011", "10.1121/1.2766781", "10.1121/1.428071", "10.1155/2007/70540", "10.1121/1.382599", "10.1121/1.1901884", "10.1121/1.3021297", "10.1121/1.2164987", "10.1121/1.398336", "10.1016/S0003-682X(99)00056-0", "10.1007/978-3-7091-6453-2_5", "10.1016/j.apacoust.2007.11.011", "10.1121/1.2766781", "10.1121/1.428071"]}, "10.1109/TVCG.2013.39": {"doi": "10.1109/TVCG.2013.39", "author": ["M. Knecht", "C. Traxler", "C. Winklhofer", "M. Wimmer"], "title": "Reflective and Refractive Objects for Mixed Reality", "year": "2013", "abstract": "In this paper, we present a novel rendering method which integrates reflective or refractive objects into a differential instant radiosity (DIR) framework usable for mixed-reality (MR) applications. This kind of objects are very special from the light interaction point of view, as they reflect and refract incident rays. Therefore they may cause high-frequency lighting effects known as caustics. Using instant-radiosity (IR) methods to approximate these high-frequency lighting effects would require a large amount of virtual point lights (VPLs) and is therefore not desirable due to real-time constraints. Instead, our approach combines differential instant radiosity with three other methods. One method handles more accurate reflections compared to simple cubemaps by using impostors. Another method is able to calculate two refractions in real-time, and the third method uses small quads to create caustic effects. Our proposed method replaces parts in light paths that belong to reflective or refractive objects using these three methods and thus tightly integrates into DIR. In contrast to previous methods which introduce reflective or refractive objects into MR scenarios, our method produces caustics that also emit additional indirect light. The method runs at real-time frame rates, and the results show that reflective and refractive objects with caustics improve the overall impression for MR scenarios.", "keywords": ["brightness", "lighting", "rendering (computer graphics)", "virtual reality", "refractive objects", "reflective objects", "rendering method", "differential instant radiosity framework", "DIR", "mixed-reality applications", "MR", "light interaction", "incident rays", "high-frequency lighting effects", "caustics", "instant-radiosity methods", "IR", "virtual point lights", "VPL", "real-time constraints", "caustic effects", "Virtual reality", "Image color analysis", "Rendering (computer graphics)", "Lighting", "Equations", "Cameras", "Streaming media", "Mixed reality", "reflections", "refractions", "caustics.", "Algorithms", "Computer Graphics", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Lighting", "Photometry", "Refractometry", "Reproducibility of Results", "Sensitivity and Specificity", "User-Computer Interface"], "referenced_by": ["IKEY:7295056", "10.1007/978-3-319-60928-7_30", "10.1007/s00371-014-1021-7", "10.1111/cgf.12591", "10.3390/app10020636"], "referencing": ["IKEY:4069236", "IKEY:5336457", "IKEY:5643556", "IKEY:5226625", "IKEY:5444836", "IKEY:4069236", "IKEY:5336457", "IKEY:5643556", "IKEY:5226625", "IKEY:5444836", "IKEY:4069236", "IKEY:5336457", "IKEY:5643556", "IKEY:5226625", "IKEY:5444836", "10.1145/237170.237282", "10.1145/1101389.1101431", "10.1145/1342250.1342276", "10.1145/1111411.1111439", "10.1145/1053427.1053460", "10.1145/1111411.1111428", "10.1145/280814.280864", "10.1145/2024156.2024191", "10.1145/258734.258769", "10.1145/237170.237282", "10.1145/1101389.1101431", "10.1145/1342250.1342276", "10.1145/1111411.1111439", "10.1145/1053427.1053460", "10.1145/1111411.1111428", "10.1145/280814.280864", "10.1145/2024156.2024191", "10.1145/258734.258769", "10.1145/237170.237282", "10.1145/1101389.1101431", "10.1145/1342250.1342276", "10.1145/1111411.1111439", "10.1145/1053427.1053460", "10.1145/1111411.1111428", "10.1145/280814.280864", "10.1145/2024156.2024191", "10.1145/258734.258769", "10.1111/j.1467-8659.2006.00950.x", "10.1111/j.1467-8659.2009.01370.x", "10.1201/b10644", "10.1111/j.1467-8659.2006.00816.x", "10.1016/j.cag.2012.04.013", "10.1111/j.1467-8659.2006.00950.x", "10.1111/j.1467-8659.2009.01370.x", "10.1201/b10644", "10.1111/j.1467-8659.2006.00816.x", "10.1016/j.cag.2012.04.013", "10.1111/j.1467-8659.2006.00950.x", "10.1111/j.1467-8659.2009.01370.x", "10.1201/b10644", "10.1111/j.1467-8659.2006.00816.x", "10.1016/j.cag.2012.04.013"]}, "10.1109/TVCG.2013.32": {"doi": "10.1109/TVCG.2013.32", "author": ["W. Steptoe", "A. Steed", "M. Slater"], "title": "Human Tails: Ownership and Control of Extended Humanoid Avatars", "year": "2013", "abstract": "This paper explores body ownership and control of an 'extended' humanoid avatar that features a distinct and flexible tail-like appendage protruding from its coccyx. Thirty-two participants took part in a between-groups study to puppeteer the avatar in an immersive CAVETM -like system. Participantsa' body movement was tracked, and the avatara's humanoid body synchronously reflected this motion. However, sixteen participants experienced the avatara's tail moving around randomly and asynchronous to their own movement, while the other participants experienced a tail that they could, potentially, control accurately and synchronously through hip movement. Participants in the synchronous condition experienced a higher degree of body ownership and agency, suggesting that visuomotor synchrony enhanced the probability of ownership over the avatar body despite of its extra-human form. Participants experiencing body ownership were also more likely to be more anxious and attempt to avoid virtual threats to the tail and body. The higher task performance of participants in the synchronous condition indicates that people are able to quickly learn how to remap normal degrees of bodily freedom in order to control virtual bodies that differ from the humanoid form. We discuss the implications and applications of extended humanoid avatars as a method for exploring the plasticity of the braina's representation of the body and for gestural human-computer interfaces.", "keywords": ["avatars", "human tails", "extended humanoid avatars", "immersive CAVETM -like system", "humanoid body", "synchronous condition", "visuomotor synchrony", "body ownership", "gestural human-computer interfaces", "Avatars", "Games", "Tracking", "Hip", "Visualization", "Educational institutions", "Joints", "Avatars", "virtual reality", "body ownership", "agency", "body schema", "plasticity", "gestural interfaces.", "Algorithms", "Animals", "Biomimetics", "Body Image", "Computer Graphics", "Computer Simulation", "Humans", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Models, Biological", "Reproducibility of Results", "Sensitivity and Specificity", "Tail", "User-Computer Interface"], "referenced_by": ["IKEY:8019388", "IKEY:7858996", "IKEY:7844777", "IKEY:7300728", "IKEY:7118232", "IKEY:7829415", "IKEY:7223377", "IKEY:7223378", "IKEY:7223379", "IKEY:7223405", "IKEY:7504682", "IKEY:7892241", "IKEY:7892339", "IKEY:7012029", "IKEY:7957712", "IKEY:8446448", "IKEY:8446229", "IKEY:8446569", "IKEY:8710499", "IKEY:8717127", "IKEY:8797787", "IKEY:8798193", "IKEY:8816112", "IKEY:8848005", "IKEY:8942332", "IKEY:8998305", "IKEY:9089510", "IKEY:9090409", "IKEY:9199571", "IKEY:9284776", "10.1002/9781118900772.etrds0165", "10.1002/9781119341031.ch5", "10.1007/978-3-319-08234-9_110-1", "10.1007/978-3-319-08234-9_245-1", "10.1007/s00221-013-3800-1", "10.1007/s10639-014-9320-1", "10.1007/s40708-017-0070-x", "10.1016/B978-0-12-809481-5.00009-2", "10.1016/j.neures.2015.11.001", "10.1016/j.neuropsychologia.2014.10.034", "10.1016/j.newideapsych.2018.02.003", "10.1016/j.pmcj.2014.05.011", "10.1111/desc.12557", "10.1111/jcc4.12107", "10.1111/jcc4.12188", "10.1177/0013916514551604", "10.3389/fict.2017.00021", "10.3389/fpsyg.2014.00943", "10.3389/fpsyg.2017.01125", "10.3389/frobt.2016.00027", "10.3389/frobt.2016.00074", "10.1007/s00426-018-1048-x", "10.1007/978-3-662-57876-6_6", "10.1117/12.2083441", "10.3389/frobt.2018.00112", "10.1007/978-3-030-06134-0_39", "10.1007/978-3-030-06134-0_57"], "referencing": ["IKEY:5444807", "IKEY:5995316", "IKEY:5444805", "IKEY:5444807", "IKEY:5995316", "IKEY:5444805", "IKEY:5444807", "IKEY:5995316", "IKEY:5444805", "10.1145/1753326.1753481", "10.1145/1753326.1753481", "10.1145/1753326.1753481", "10.1016/j.neuroimage.2007.03.046", "10.3389/neuro.09.006.2008", "10.1016/S0950-5601(54)80044-X", "10.1111/j.1468-2958.2007.00299.x", "10.1038/35784", "10.3171/jns.1999.91.1.0121", "10.1126/science.1097011", "10.1080/15213260802669474", "10.1016/j.intcom.2012.04.010", "10.1093/brain/34.2-3.102", "10.1016/S0010-9452(08)70460-X", "10.1093/brain/111.2.281", "10.1126/science.1143439", "10.1016/j.tics.2003.12.008", "10.1007/s11023-006-9044-0", "10.1371/journal.pone.0016128", "10.1097/WNR.0b013e32832a0a2a", "10.1038/377489a0", "10.1016/j.neuroimage.2007.03.046", "10.3389/neuro.09.006.2008", "10.1016/S0950-5601(54)80044-X", "10.1111/j.1468-2958.2007.00299.x", "10.1038/35784", "10.3171/jns.1999.91.1.0121", "10.1126/science.1097011", "10.1080/15213260802669474", "10.1016/j.intcom.2012.04.010", "10.1093/brain/34.2-3.102", "10.1016/S0010-9452(08)70460-X", "10.1093/brain/111.2.281", "10.1126/science.1143439", "10.1016/j.tics.2003.12.008", "10.1007/s11023-006-9044-0", "10.1371/journal.pone.0016128", "10.1097/WNR.0b013e32832a0a2a", "10.1038/377489a0", "10.1016/j.neuroimage.2007.03.046", "10.3389/neuro.09.006.2008", "10.1016/S0950-5601(54)80044-X", "10.1111/j.1468-2958.2007.00299.x", "10.1038/35784", "10.3171/jns.1999.91.1.0121", "10.1126/science.1097011", "10.1080/15213260802669474", "10.1016/j.intcom.2012.04.010", "10.1093/brain/34.2-3.102", "10.1016/S0010-9452(08)70460-X", "10.1093/brain/111.2.281", "10.1126/science.1143439", "10.1016/j.tics.2003.12.008", "10.1007/s11023-006-9044-0", "10.1371/journal.pone.0016128", "10.1097/WNR.0b013e32832a0a2a", "10.1038/377489a0"]}, "10.1109/TVCG.2013.24": {"doi": "10.1109/TVCG.2013.24", "author": ["D. Borland", "T. Peck", "M. Slater"], "title": "An Evaluation of Self-Avatar Eye Movement for Virtual Embodiment", "year": "2013", "abstract": "We present a novel technique for animating self-avatar eye movements in an immersive virtual environment without the use of eye-tracking hardware, and evaluate our technique via a two-alternative, forced-choice-with-confidence experiment that compares this simulated-eye-tracking condition to a no-eye-tracking condition and a real-eye-tracking condition in which the avatar's eyes were rotated with an eye tracker. Viewing the reflection of a tracked self-avatar is often used in virtual-embodiment scenarios to induce in the participant the illusion that the virtual body of the self-avatar belongs to them, however current tracking methods do not account for the movements of the participants eyes, potentially lessening this body-ownership illusion. The results of our experiment indicate that, although blind to the experimental conditions, participants noticed differences between eye behaviors, and found that the real and simulated conditions represented their behavior better than the no-eye-tracking condition. Additionally, no statistical difference was found when choosing between the real and simulated conditions. These results suggest that adding eye movements to selfavatars produces a subjective increase in self-identification with the avatar due to a more complete representation of the participant's behavior, which may be beneficial for inducing virtual embodiment, and that effective results can be obtained without the need for any specialized eye-tracking hardware.", "keywords": ["avatars", "statistical analysis", "self-avatar eye movement animation", "virtual embodiment", "immersive virtual environment", "forced-choice-with-confidence experiment", "simulated-eye-tracking condition", "no-eye-tracking condition", "real-eye-tracking condition", "statistical difference", "Avatars", "Tracking", "Mirrors", "Calibration", "Visualization", "Hardware", "Standards", "Virtual embodiment", "eye tracking", "virtual characters", "user studies.", "Algorithms", "Artificial Intelligence", "Computer Graphics", "Computer Simulation", "Eye Movements", "Humans", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Models, Biological", "Pattern Recognition, Automated", "Reproducibility of Results", "Sensitivity and Specificity", "User-Computer Interface"], "referenced_by": ["IKEY:6982331", "IKEY:8998352", "10.3389/frobt.2014.00009", "10.3389/frobt.2016.00074", "10.3389/frobt.2018.00074"], "referencing": ["IKEY:6797640", "IKEY:5444805", "IKEY:4811003", "IKEY:6797477", "IKEY:6797640", "IKEY:5444805", "IKEY:4811003", "IKEY:6797477", "IKEY:6797640", "IKEY:5444805", "IKEY:4811003", "IKEY:6797477", "10.1145/505008.505010", "10.1145/566570.566629", "10.1145/1314303.1314311", "10.1145/332040.332443", "10.1145/505008.505019", "10.1145/505008.505010", "10.1145/566570.566629", "10.1145/1314303.1314311", "10.1145/332040.332443", "10.1145/505008.505019", "10.1145/505008.505010", "10.1145/566570.566629", "10.1145/1314303.1314311", "10.1145/332040.332443", "10.1145/505008.505019", "10.1080/15534510802643750", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0016128", "10.1371/journal.pone.0003832", "10.3389/neuro.01.029.2009", "10.1371/journal.pone.0010564", "10.1111/j.1467-8659.2004.00001.x", "10.1111/j.1468-2958.2007.00299.x", "10.1080/15534510802643750", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0016128", "10.1371/journal.pone.0003832", "10.3389/neuro.01.029.2009", "10.1371/journal.pone.0010564", "10.1111/j.1467-8659.2004.00001.x", "10.1111/j.1468-2958.2007.00299.x", "10.1080/15534510802643750", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0016128", "10.1371/journal.pone.0003832", "10.3389/neuro.01.029.2009", "10.1371/journal.pone.0010564", "10.1111/j.1467-8659.2004.00001.x", "10.1111/j.1468-2958.2007.00299.x"]}, "10.1109/TVCG.2013.29": {"doi": "10.1109/TVCG.2013.29", "author": ["K. Kilteni", "I. Bergstrom", "M. Slater"], "title": "Drumming in Immersive Virtual Reality: The Body Shapes the Way We Play", "year": "2013", "abstract": "It has been shown that it is possible to generate perceptual illusions of ownership in immersive virtual reality (IVR) over a virtual body seen from first person perspective, in other words over a body that visually substitutes the person's real body. This can occur even when the virtual body is quite different in appearance from the person's real body. However, investigation of the psychological, behavioral and attitudinal consequences of such body transformations remains an interesting problem with much to be discovered. Thirty six Caucasian people participated in a between-groups experiment where they played a West-African Djembe hand drum while immersed in IVR and with a virtual body that substituted their own. The virtual hand drum was registered with a physical drum. They were alongside a virtual character that played a drum in a supporting, accompanying role. In a baseline condition participants were represented only by plainly shaded white hands, so that they were able merely to play. In the experimental condition they were represented either by a casually dressed dark-skinned virtual body (Casual Dark-Skinned - CD) or by a formal suited light-skinned body (Formal Light-Skinned - FL). Although participants of both groups experienced a strong body ownership illusion towards the virtual body, only those with the CD representation showed significant increases in their movement patterns for drumming compared to the baseline condition and compared with those embodied in the FL body. Moreover, the stronger the illusion of body ownership in the CD condition, the greater this behavioral change. A path analysis showed that the observed behavioral changes were a function of the strength of the illusion of body ownership towards the virtual body and its perceived appropriateness for the drumming task. These results demonstrate that full body ownership illusions can lead to substantial behavioral and possibly cognitive changes depending on the appearance of the virtual body. This could be important for many applications such as learning, education, training, psychotherapy and rehabilitation using IVR.", "keywords": ["virtual reality", "immersive virtual reality", "virtual body", "IVR", "attitudinal consequences", "psychological consequences", "behavioral consequences", "body transformations", "Caucasian people", "West-African Djembe hand drum", "virtual hand drum", "CD condition", "Avatars", "Rubber", "Mirrors", "Correlation", "Visualization", "Instruments", "Perception", "presence", "user studies", "experimental methods", "multimodal interaction", "training", "entertainment.", "Algorithms", "Body Image", "Computer Graphics", "Computer Simulation", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Models, Biological", "Music", "Psychomotor Performance", "User-Computer Interface", "Visual Perception", "Young Adult"], "referenced_by": ["IKEY:7460072", "IKEY:8007535", "IKEY:6861900", "IKEY:7148110", "IKEY:6777424", "IKEY:8260949", "IKEY:8263407", "IKEY:6802059", "IKEY:6802104", "IKEY:7223377", "IKEY:7504682", "IKEY:7358587", "IKEY:8346252", "IKEY:8446229", "IKEY:8516522", "IKEY:8516541", "IKEY:8798122", "IKEY:8798040", "IKEY:8747537", "IKEY:8848005", "IKEY:9089510", "IKEY:9089501", "IKEY:9284694", "IKEY:9284776", "10.1002/9781119341031.ch5", "10.1002/cav.1610", "10.1007/978-3-319-39907-2_2", "10.1007/978-3-319-40397-7_46", "10.1007/978-3-319-58521-5_46", "10.1007/978-3-319-58750-9_49", "10.1007/978-981-10-4157-0_33", "10.1007/978-981-10-5490-7_2", "10.1007/s00221-013-3800-1", "10.1007/s11042-017-4784-9", "10.1007/s11097-016-9488-5", "10.1007/s11211-017-0294-1", "10.1016/j.actpsy.2014.07.012", "10.1016/j.concog.2013.04.016", "10.1016/j.newideapsych.2018.02.003", "10.1016/j.tele.2017.09.020", "10.1016/j.tics.2014.11.001", "10.1038/srep13899", "10.1038/srep18345", "10.1053/j.jvca.2017.12.006", "10.1073/pnas.1306779110", "10.1080/02643294.2016.1185404", "10.1080/08832323.2017.1308308", "10.1098/rsif.2013.0300", "10.1371/journal.pone.0111933", "10.1371/journal.pone.0148060", "10.3156/jsoft.28.511", "10.3233/RNN-170756", "10.3389/fict.2016.00029"], "referencing": ["IKEY:5444805", "IKEY:6797599", "IKEY:6797605", "IKEY:6797477", "IKEY:6797640", "IKEY:4480794", "IKEY:6797281", "IKEY:5444807", "IKEY:799723", "IKEY:4359501", "IKEY:5444805", "IKEY:6797599", "IKEY:6797605", "IKEY:6797477", "IKEY:6797640", "IKEY:4480794", "IKEY:6797281", "IKEY:5444807", "IKEY:799723", "IKEY:4359501", "IKEY:5444805", "IKEY:6797599", "IKEY:6797605", "IKEY:6797477", "IKEY:6797640", "IKEY:4480794", "IKEY:6797281", "IKEY:5444807", "IKEY:799723", "IKEY:4359501", "10.1145/1833349.1778829", "10.1145/229459.229467", "10.1145/1833349.1778829", "10.1145/229459.229467", "10.1145/1833349.1778829", "10.1145/229459.229467", "10.1038/nrn3122", "10.1080/15534510802643750", "10.1080/15213260802669474", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0040682", "10.1016/j.concog.2012.04.011", "10.1016/j.actpsy.2009.02.003", "10.1093/ct/14.1.27", "10.1525/mp.2009.26.4.335", "10.1525/jlin.1999.9.1-2.184", "10.1080/14442210903527820", "10.1177/0305735608100372", "10.1177/0887302X9401200305", "10.1016/j.concog.2005.09.004", "10.1113/jphysiol.2011.204941", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010381", "10.1371/journal.pone.0010564", "10.1371/journal.pone.0016128", "10.1007/s11571-011-9178-5", "10.1371/journal.pone.0040867", "10.1126/science.1143439", "10.1126/science.1142175", "10.1371/journal.pone.0003832", "10.3389/fnhum.2012.00040", "10.1068/p5921", "10.1038/nrn1651", "10.1111/j.1468-2958.2007.00299.x", "10.1016/j.neuropsychologia.2009.09.034", "10.1038/35784", "10.1038/nrn3122", "10.1080/15534510802643750", "10.1080/15213260802669474", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0040682", "10.1016/j.concog.2012.04.011", "10.1016/j.actpsy.2009.02.003", "10.1093/ct/14.1.27", "10.1525/mp.2009.26.4.335", "10.1525/jlin.1999.9.1-2.184", "10.1080/14442210903527820", "10.1177/0305735608100372", "10.1177/0887302X9401200305", "10.1016/j.concog.2005.09.004", "10.1113/jphysiol.2011.204941", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010381", "10.1371/journal.pone.0010564", "10.1371/journal.pone.0016128", "10.1007/s11571-011-9178-5", "10.1371/journal.pone.0040867", "10.1126/science.1143439", "10.1126/science.1142175", "10.1371/journal.pone.0003832", "10.3389/fnhum.2012.00040", "10.1068/p5921", "10.1038/nrn1651", "10.1111/j.1468-2958.2007.00299.x", "10.1016/j.neuropsychologia.2009.09.034", "10.1038/35784", "10.1038/nrn3122", "10.1080/15534510802643750", "10.1080/15213260802669474", "10.1509/jmkr.48.SPL.S23", "10.1371/journal.pone.0040682", "10.1016/j.concog.2012.04.011", "10.1016/j.actpsy.2009.02.003", "10.1093/ct/14.1.27", "10.1525/mp.2009.26.4.335", "10.1525/jlin.1999.9.1-2.184", "10.1080/14442210903527820", "10.1177/0305735608100372", "10.1177/0887302X9401200305", "10.1016/j.concog.2005.09.004", "10.1113/jphysiol.2011.204941", "10.3389/neuro.09.006.2008", "10.1371/journal.pone.0010381", "10.1371/journal.pone.0010564", "10.1371/journal.pone.0016128", "10.1007/s11571-011-9178-5", "10.1371/journal.pone.0040867", "10.1126/science.1143439", "10.1126/science.1142175", "10.1371/journal.pone.0003832", "10.3389/fnhum.2012.00040", "10.1068/p5921", "10.1038/nrn1651", "10.1111/j.1468-2958.2007.00299.x", "10.1016/j.neuropsychologia.2009.09.034", "10.1038/35784"]}, "10.1109/TVCG.2013.40": {"doi": "10.1109/TVCG.2013.40", "author": ["H. Matsukura", "T. Yoneda", "H. Ishida"], "title": "Smelling Screen: Development and Evaluation of an Olfactory Display System for Presenting a Virtual Odor Source", "year": "2013", "abstract": "We propose a new olfactory display system that can generate an odor distribution on a two-dimensional display screen. The proposed system has four fans on the four corners of the screen. The airflows that are generated by these fans collide multiple times to create an airflow that is directed towards the user from a certain position on the screen. By introducing odor vapor into the airflows, the odor distribution is as if an odor source had been placed onto the screen. The generated odor distribution leads the user to perceive the odor as emanating from a specific region of the screen. The position of this virtual odor source can be shifted to an arbitrary position on the screen by adjusting the balance of the airflows from the four fans. Most users do not immediately notice the odor presentation mechanism of the proposed olfactory display system because the airflow and perceived odor come from the display screen rather than the fans. The airflow velocity can even be set below the threshold for airflow sensation, such that the odor alone is perceived by the user. We present experimental results that show the airflow field and odor distribution that are generated by the proposed system. We also report sensory test results to show how the generated odor distribution is perceived by the user and the issues that must be considered in odor presentation.", "keywords": ["virtual reality", "smelling screen", "olfactory display system", "odor distribution", "2D display screen", "airflow generation", "odor vapor", "virtual odor source position", "airflow balance", "fan", "odor presentation", "airflow velocity", "airflow sensation", "airflow field", "sensory test", "Olfactory", "Fans", "Face", "Position measurement", "Gas detectors", "Educational institutions", "Virtual reality", "multimedia", "olfactory display", "wind dislpay.", "Cathode Ray Tube", "Computer Graphics", "Equipment Design", "Equipment Failure Analysis", "Humans", "Odors", "Physical Stimulation", "Rheology", "Smell", "Stimulation, Chemical", "User-Computer Interface"], "referenced_by": ["IKEY:8258418", "IKEY:8172427", "IKEY:7089286", "IKEY:8260917", "IKEY:8357566", "IKEY:8444077", "IKEY:8630614", "IKEY:8752224", "IKEY:8823180", "IKEY:8823304", "IKEY:8971785", "IKEY:9277260", "10.1145/2660579.2660584", "10.1145/2816454", "10.1145/3233774", "10.1002/9781118426456.ch13", "10.1002/9781118768495.ch7", "10.1002/ange.201309508", "10.1002/anie.201309508", "10.1007/978-3-319-57738-8_5", "10.1007/978-3-319-70272-8_26", "10.1007/s10055-015-0260-x", "10.1007/s11042-015-2971-0", "10.1007/s12652-016-0409-9", "10.1007/s13361-017-1752-6", "10.1016/j.apergo.2015.06.024", "10.1016/j.ijhcs.2017.06.003", "10.1016/j.tibtech.2016.12.007", "10.1177/1094428114547952", "10.1541/ieejsmas.136.296", "10.2493/jjspe.82.15", "10.3389/fict.2014.00004", "10.3390/mti1020006", "10.3390/mti1020007", "10.4018/978-1-5225-2927-9.ch005", "10.3390/s18072329", "10.1541/ieejsmas.138.354", "10.7559/citarj.v10i1.501", "10.1016/j.ijhcs.2018.11.011", "10.1016/j.robot.2018.12.008", "10.3390/app9081697", "10.1007/978-3-319-21006-3_21"], "referencing": ["IKEY:6790640", "IKEY:1667646", "IKEY:4418754", "IKEY:1667645", "IKEY:4811015", "IKEY:6797285", "IKEY:5759448", "IKEY:4209193", "IKEY:6180915", "IKEY:6411525", "IKEY:6790640", "IKEY:1667646", "IKEY:4418754", "IKEY:1667645", "IKEY:4811015", "IKEY:6797285", "IKEY:5759448", "IKEY:4209193", "IKEY:6180915", "IKEY:6411525", "IKEY:6790640", "IKEY:1667646", "IKEY:4418754", "IKEY:1667645", "IKEY:4811015", "IKEY:6797285", "IKEY:5759448", "IKEY:4209193", "IKEY:6180915", "IKEY:6411525", "10.1145/1186223.1186267", "10.1145/1077534.1077558", "10.1145/1186223.1186267", "10.1145/1077534.1077558", "10.1145/1186223.1186267", "10.1145/1077534.1077558", "10.1016/j.concog.2007.06.005", "10.1093/chemse/25.1.111", "10.3758/BF03210754", "10.2307/1251931", "10.1093/ietfec/e89-a.11.3327", "10.1016/0092-8674(91)90418-X", "10.1007/978-3-642-23765-2_23", "10.1016/j.concog.2007.06.005", "10.1093/chemse/25.1.111", "10.3758/BF03210754", "10.2307/1251931", "10.1093/ietfec/e89-a.11.3327", "10.1016/0092-8674(91)90418-X", "10.1007/978-3-642-23765-2_23", "10.1016/j.concog.2007.06.005", "10.1093/chemse/25.1.111", "10.3758/BF03210754", "10.2307/1251931", "10.1093/ietfec/e89-a.11.3327", "10.1016/0092-8674(91)90418-X", "10.1007/978-3-642-23765-2_23"]}, "10.1109/TVCG.2013.33": {"doi": "10.1109/TVCG.2013.33", "author": ["S. Beck", "A. Kunert", "A. Kulik", "B. Froehlich"], "title": "Immersive Group-to-Group Telepresence", "year": "2013", "abstract": "We present a novel immersive telepresence system that allows distributed groups of users to meet in a shared virtual 3D world. Our approach is based on two coupled projection-based multi-user setups, each providing multiple users with perspectively correct stereoscopic images. At each site the users and their local interaction space are continuously captured using a cluster of registered depth and color cameras. The captured 3D information is transferred to the respective other location, where the remote participants are virtually reconstructed. We explore the use of these virtual user representations in various interaction scenarios in which local and remote users are face-to-face, side-by-side or decoupled. Initial experiments with distributed user groups indicate the mutual understanding of pointing and tracing gestures independent of whether they were performed by local or remote participants. Our users were excited about the new possibilities of jointly exploring a virtual city, where they relied on a world-in-miniature metaphor for mutual awareness of their respective locations.", "keywords": ["image colour analysis", "image sensors", "solid modelling", "stereo image processing", "virtual reality", "immersive group-to-group telepresence", "shared virtual 3D world", "coupled projection-based multiuser setups", "stereoscopic images", "local interaction space", "color cameras", "registered depth cameras", "captured 3D information", "virtual user representations", "virtual city", "world-in-miniature metaphor", "Calibration", "Cameras", "Servers", "Streaming media", "Image reconstruction", "Image color analysis", "Virtual reality", "Multi-user virtual reality", "telepresence", "3D capture.", "Computer Graphics", "Computer Simulation", "Group Processes", "Humans", "Imaging, Three-Dimensional", "Models, Biological", "Social Behavior", "Telecommunications", "User-Computer Interface"], "referenced_by": ["IKEY:7160929", "IKEY:7160932", "IKEY:7131731", "IKEY:7893339", "IKEY:7785085", "IKEY:7566020", "IKEY:7169465", "IKEY:7037006", "IKEY:6954321", "IKEY:7823466", "IKEY:6728903", "IKEY:7034277", "IKEY:7033022", "IKEY:7033039", "IKEY:7328101", "IKEY:8115408", "IKEY:7072561", "IKEY:6861875", "IKEY:7152799", "IKEY:6655764", "IKEY:7490357", "IKEY:7523447", "IKEY:8314105", "IKEY:7358826", "IKEY:7358858", "IKEY:6802048", "IKEY:6802104", "IKEY:7223323", "IKEY:7223340", "IKEY:7892243", "10.1145/2983530", "10.1145/2984511.2984517", "10.1145/3083187.3084019", "10.1145/3123266.3123395", "10.1002/cav.1645", "10.1007/978-3-319-24075-6_43", "10.1007/978-3-319-39862-4_13", "10.1007/978-3-319-45438-2_7", "10.1007/978-3-319-45853-3_4", "10.1007/978-3-319-50832-0_24", "10.1007/978-3-319-50835-1_43", "10.1007/s10055-013-0225-x", "10.1007/s11042-015-3116-1", "10.1016/j.cag.2017.08.003", "10.1061/9780784413616.097", "10.1111/cgf.12746", "10.3390/rs10020328", "10.3390/s17102294", "10.3390/s18082430", "10.1007/s11042-018-6564-6", "10.3389/frobt.2019.00005", "10.1007/s11042-019-7464-0", "10.1007/978-3-319-09912-5_42", "10.1007/978-3-319-14249-4_36", "10.1016/j.ijhcs.2019.05.011", "10.1007/978-3-319-07458-0_33", "10.1007/978-3-319-22698-9_43", "10.1007/978-3-319-22723-8_25", "10.1007/978-3-030-31908-3_15"], "referencing": ["IKEY:6162880", "IKEY:4811011", "IKEY:1667649", "IKEY:6797509", "IKEY:6165146", "IKEY:6190806", "IKEY:4480795", "IKEY:6162881", "IKEY:6180879", "IKEY:6162880", "IKEY:4811011", "IKEY:1667649", "IKEY:6797509", "IKEY:6165146", "IKEY:6190806", "IKEY:4480795", "IKEY:6162881", "IKEY:6180879", "IKEY:6162880", "IKEY:4811011", "IKEY:1667649", "IKEY:6797509", "IKEY:6165146", "IKEY:6190806", "IKEY:4480795", "IKEY:6162881", "IKEY:6180879", "10.1145/1054972.1055084", "10.1145/2207676.2208642", "10.1145/280814.280861", "10.1145/1460563.1460593", "10.1145/223904.223938", "10.1145/1152399.1152421", "10.1145/642700.642702", "10.1145/2207676.2207704", "10.1145/2207676.2208335", "10.1145/769953.769954", "10.1145/882262.882350", "10.1145/142750.142977", "10.1145/2047196.2047270", "10.1145/1576246.1531370", "10.1145/616707.616708", "10.1145/2407336.2407342", "10.1145/2207676.2208640", "10.1145/2070781.2024180", "10.1145/1015706.1015805", "10.1145/1054972.1055084", "10.1145/2207676.2208642", "10.1145/280814.280861", "10.1145/1460563.1460593", "10.1145/223904.223938", "10.1145/1152399.1152421", "10.1145/642700.642702", "10.1145/2207676.2207704", "10.1145/2207676.2208335", "10.1145/769953.769954", "10.1145/882262.882350", "10.1145/142750.142977", "10.1145/2047196.2047270", "10.1145/1576246.1531370", "10.1145/616707.616708", "10.1145/2407336.2407342", "10.1145/2207676.2208640", "10.1145/2070781.2024180", "10.1145/1015706.1015805", "10.1145/1054972.1055084", "10.1145/2207676.2208642", "10.1145/280814.280861", "10.1145/1460563.1460593", "10.1145/223904.223938", "10.1145/1152399.1152421", "10.1145/642700.642702", "10.1145/2207676.2207704", "10.1145/2207676.2208335", "10.1145/769953.769954", "10.1145/882262.882350", "10.1145/142750.142977", "10.1145/2047196.2047270", "10.1145/1576246.1531370", "10.1145/616707.616708", "10.1145/2407336.2407342", "10.1145/2207676.2208640", "10.1145/2070781.2024180", "10.1145/1015706.1015805", "10.1053/j.jvca.2007.06.003", "10.1117/12.854571", "10.1007/s005300050123", "10.1053/j.jvca.2007.06.003", "10.1117/12.854571", "10.1007/s005300050123", "10.1053/j.jvca.2007.06.003", "10.1117/12.854571", "10.1007/s005300050123"]}, "10.1109/TVCG.2013.23": {"doi": "10.1109/TVCG.2013.23", "author": ["J. Spillmann", "S. Tuchschmid", "M. Harders"], "title": "Adaptive Space Warping to Enhance Passive Haptics in an Arthroscopy Surgical Simulator", "year": "2013", "abstract": "Passive haptics, also known as tactile augmentation, denotes the use of a physical counterpart to a virtual environment to provide tactile feedback. Employing passive haptics can result in more realistic touch sensations than those from active force feedback, especially for rigid contacts. However, changes in the virtual environment would necessitate modifications of the physical counterparts. In recent work space warping has been proposed as one solution to overcome this limitation. In this technique virtual space is distorted such that a variety of virtual models can be mapped onto one single physical object. In this paper, we propose as an extension adaptive space warping; we show how this technique can be employed in a mixed-reality surgical training simulator in order to map different virtual patients onto one physical anatomical model. We developed methods to warp different organ geometries onto one physical mock-up, to handle different mechanical behaviors of the virtual patients, and to allow interactive modifications of the virtual structures, while the physical counterparts remain unchanged. Various practical examples underline the wide applicability of our approach. To the best of our knowledge this is the first practical usage of such a technique in the specific context of interactive medical training.", "keywords": ["geometry", "medical computing", "surgery", "virtual reality", "adaptive space warping", "passive haptics", "arthroscopy surgical simulator", "tactile augmentation", "physical anatomical model", "organ geometries", "virtual patients", "interactive medical training", "interactive modifications", "Haptic interfaces", "Avatars", "Bones", "Geometry", "Joints", "Surgery", "Shape", "Virtual reality", "passive haptics", "surgical training simulation.", "Arthroscopy", "Arthroscopy", "Artificial Intelligence", "Biofeedback, Psychology", "Computer Graphics", "Computer Simulation", "Computer-Assisted Instruction", "Humans", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Models, Biological", "Surgery, Computer-Assisted", "Touch", "User-Computer Interface"], "referenced_by": ["IKEY:7392296", "IKEY:7410101", "IKEY:7542122", "IKEY:7989892", "IKEY:8488133", "IKEY:8798143", "IKEY:8959300", "10.1007/s00167-014-2888-6", "10.1007/s00167-016-4022-4", "10.1016/j.jsurg.2017.01.005", "10.1541/ieejjia.6.83", "10.1007/978-3-030-47483-6_7"], "referencing": ["IKEY:6183793", "IKEY:398446", "IKEY:1492747", "IKEY:5453367", "IKEY:658423", "IKEY:4142840", "IKEY:5444703", "IKEY:6184193", "IKEY:5336463", "IKEY:1383073", "IKEY:6790943", "IKEY:1667624", "IKEY:6183793", "IKEY:398446", "IKEY:1492747", "IKEY:5453367", "IKEY:658423", "IKEY:4142840", "IKEY:5444703", "IKEY:6184193", "IKEY:5336463", "IKEY:1383073", "IKEY:6790943", "IKEY:1667624", "IKEY:6183793", "IKEY:398446", "IKEY:1492747", "IKEY:5453367", "IKEY:658423", "IKEY:4142840", "IKEY:5444703", "IKEY:6184193", "IKEY:5336463", "IKEY:1383073", "IKEY:6790943", "IKEY:1667624", "10.1145/332040.332494", "10.1145/1141897.1141901", "10.1145/1152399.1152451", "10.1145/985692.985723", "10.1145/566654.566630", "10.1145/332040.332494", "10.1145/1141897.1141901", "10.1145/1152399.1152451", "10.1145/985692.985723", "10.1145/566654.566630", "10.1145/332040.332494", "10.1145/1141897.1141901", "10.1145/1152399.1152451", "10.1145/985692.985723", "10.1145/566654.566630", "10.1007/s12213-009-0016-3", "10.1038/415429a", "10.1037/h0059826", "10.1007/s00221-006-0814-y", "10.1016/S0031-9384(02)00914-9", "10.1097/01.blo.0000194678.10130.ff", "10.1007/s10055-008-0106-x", "10.1007/s10055-007-0080-8", "10.1038/35086588", "10.1126/science.143.3606.594", "10.2466/pms.1994.78.2.395", "10.1007/s00371-011-0561-3", "10.1007/s12213-009-0016-3", "10.1038/415429a", "10.1037/h0059826", "10.1007/s00221-006-0814-y", "10.1016/S0031-9384(02)00914-9", "10.1097/01.blo.0000194678.10130.ff", "10.1007/s10055-008-0106-x", "10.1007/s10055-007-0080-8", "10.1038/35086588", "10.1126/science.143.3606.594", "10.2466/pms.1994.78.2.395", "10.1007/s00371-011-0561-3", "10.1007/s12213-009-0016-3", "10.1038/415429a", "10.1037/h0059826", "10.1007/s00221-006-0814-y", "10.1016/S0031-9384(02)00914-9", "10.1097/01.blo.0000194678.10130.ff", "10.1007/s10055-008-0106-x", "10.1007/s10055-007-0080-8", "10.1038/35086588", "10.1126/science.143.3606.594", "10.2466/pms.1994.78.2.395", "10.1007/s00371-011-0561-3"]}, "10.1109/TVCG.2013.28": {"doi": "10.1109/TVCG.2013.28", "author": ["E. Hodgson", "E. Bachmann"], "title": "Comparing Four Approaches to Generalized Redirected Walking: Simulation and Live User Data", "year": "2013", "abstract": "Redirected walking algorithms imperceptibly rotate a virtual scene and scale movements to guide users of immersive virtual environment systems away from tracking area boundaries. These distortions ideally permit users to explore large and potentially unbounded virtual worlds while walking naturally through a physically limited space. Estimates of the physical space required to perform effective redirected walking have been based largely on the ability of humans to perceive the distortions introduced by redirected walking and have not examined the impact the overall steering strategy used. This work compares four generalized redirected walking algorithms, including Steer-to-Center, Steer-to-Orbit, Steer-to-Multiple-Targets and Steer-to-Multiple+Center. Two experiments are presented based on simulated navigation as well as live-user navigation carried out in a large immersive virtual environment facility. Simulations were conducted with both synthetic paths and previously-logged user data. Primary comparison metrics include mean and maximum distances from the tracking area center for each algorithm, number of wall contacts, and mean rates of redirection. Results indicated that Steer-to-Center out-performed all other algorithms relative to these metrics. Steer-to-Orbit also performed well in some circumstances.", "keywords": ["human computer interaction", "navigation", "virtual reality", "live user data", "virtual scene", "scale movement", "immersive virtual environment system", "tracking area boundary", "virtual world", "physical space", "steering strategy", "generalized redirected walking algorithm", "Steer-to-Center", "Steer-to-Orbit", "Steer-to-Multiple-Targets", "Steer-to-Multiple+Center", "live-user navigation", "large immersive virtual environment facility", "synthetic path", "tracking area center", "wall contact", "Legged locomotion", "Orbits", "Navigation", "Algorithm design and analysis", "Space vehicles", "Visualization", "Tracking", "Redirected walking", "virtual environments", "navigation", "human computer interaction", "live users", "simulation.", "Algorithms", "Biofeedback, Psychology", "Computer Graphics", "Cues", "Humans", "Imaging, Three-Dimensional", "User-Computer Interface", "Visual Perception", "Walking"], "referenced_by": ["IKEY:6798851", "IKEY:7460038", "IKEY:7463195", "IKEY:6777456", "IKEY:7833190", "IKEY:8260943", "IKEY:6802053", "IKEY:7892227", "IKEY:7892235", "IKEY:7859537", "IKEY:8336063", "IKEY:8448288", "IKEY:8645699", "IKEY:8645818", "IKEY:8798121", "IKEY:8797983", "IKEY:8798286", "IKEY:8797818", "IKEY:8797989", "IKEY:9003250", "IKEY:8998570", "IKEY:8580399", "IKEY:9090595", "IKEY:9089532", "IKEY:9212396", "10.1145/3130800.3130893", "10.1145/3197517.3201294", "10.1145/3345554", "10.1007/978-3-319-08234-9_253-1", "10.1016/j.procir.2016.02.086", "10.1007/978-3-030-22514-8_12", "10.1016/j.cag.2019.09.005", "10.1007/978-3-030-31908-3_14", "10.1016/j.cortex.2020.01.018"], "referencing": ["IKEY:4142862", "IKEY:5072212", "IKEY:5759214", "IKEY:6200791", "IKEY:4142862", "IKEY:5072212", "IKEY:5759214", "IKEY:6200791", "IKEY:4142862", "IKEY:5072212", "IKEY:5759214", "IKEY:6200791", "10.1145/1450579.1450612", "10.1145/263407.263550", "10.1145/1450579.1450611", "10.1145/2043603.2043604", "10.1145/1450579.1450612", "10.1145/263407.263550", "10.1145/1450579.1450611", "10.1145/2043603.2043604", "10.1145/1450579.1450612", "10.1145/263407.263550", "10.1145/1450579.1450611", "10.1145/2043603.2043604", "10.3758/BF03192976", "10.1111/1467-9280.00058", "10.1016/j.cub.2009.07.053", "10.3758/BF03192976", "10.1111/1467-9280.00058", "10.1016/j.cub.2009.07.053", "10.3758/BF03192976", "10.1111/1467-9280.00058", "10.1016/j.cub.2009.07.053"]}, "10.1109/TVCG.2013.31": {"doi": "10.1109/TVCG.2013.31", "author": ["S. Gebhardt", "S. Pick", "F. Leithold", "B. Hentschel", "T. Kuhlen"], "title": "Extended Pie Menus for Immersive Virtual Environments", "year": "2013", "abstract": "Pie menus are a well-known technique for interacting with 2D environments and so far a large body of research documents their usage and optimizations. Yet, comparatively little research has been done on the usability of pie menus in immersive virtual environments (IVEs). In this paper we reduce this gap by presenting an implementation and evaluation of an extended hierarchical pie menu system for IVEs that can be operated with a six-degrees-of-freedom input device. Following an iterative development process, we first developed and evaluated a basic hierarchical pie menu system. To better understand how pie menus should be operated in IVEs, we tested this system in a pilot user study with 24 participants and focus on item selection. Regarding the results of the study, the system was tweaked and elements like check boxes, sliders, and color map editors were added to provide extended functionality. An expert review with five experts was performed with the extended pie menus being integrated into an existing VR application to identify potential design issues. Overall results indicated high performance and efficient design.", "keywords": ["graphical user interfaces", "human computer interaction", "virtual reality", "extended pie menu", "immersive virtual environment", "2D environment interaction", "pie menu usability", "IVE", "extended hierarchical pie menu system", "six-degrees-of-freedom input device", "iterative development process", "item selection", "check box", "slider", "color map editor", "VR application", "potential design issue", "Layout", "Usability", "Error analysis", "Context", "Performance evaluation", "Atmospheric measurements", "Particle measurements", "Pie menus", "interaction", "user interfaces", "user study.", "Algorithms", "Computer Graphics", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Information Storage and Retrieval", "Reproducibility of Results", "Sensitivity and Specificity", "User-Computer Interface"], "referenced_by": ["IKEY:6798837", "IKEY:7893318", "IKEY:7893319", "IKEY:6664348", "IKEY:7041352", "IKEY:6898722", "IKEY:7383336", "IKEY:7892336", "IKEY:8809589", "IKEY:8786823", "IKEY:8921254", "IKEY:9284682", "10.1007/978-3-319-47452-6_4", "10.1007/978-3-319-47665-0_39", "10.3389/fninf.2014.00042", "10.3389/fninf.2015.00029", "10.1007/978-3-030-21607-8_8", "10.1007/978-3-030-23528-4_55", "10.1080/10447318.2020.1809245"], "referencing": ["IKEY:913781", "IKEY:5444721", "IKEY:1167041", "IKEY:1287200", "IKEY:6184185", "IKEY:913781", "IKEY:5444721", "IKEY:1167041", "IKEY:1287200", "IKEY:6184185", "IKEY:913781", "IKEY:5444721", "IKEY:1167041", "IKEY:1287200", "IKEY:6184185", "10.1145/57167.57182", "10.1145/964696.964716", "10.1145/1556262.1556294", "10.1145/215585.215973", "10.1145/1029632.1029639", "10.1145/57167.57182", "10.1145/964696.964716", "10.1145/1556262.1556294", "10.1145/215585.215973", "10.1145/1029632.1029639", "10.1145/57167.57182", "10.1145/964696.964716", "10.1145/1556262.1556294", "10.1145/215585.215973", "10.1145/1029632.1029639", "10.1037/h0045689", "10.1007/978-3-322-80058-9_19", "10.1207/s15327051hci0801_1", "10.1007/978-3-540-89350-9_6", "10.1080/10447319509526110", "10.1016/0020-7373(92)90039-N", "10.1037/h0045689", "10.1007/978-3-322-80058-9_19", "10.1207/s15327051hci0801_1", "10.1007/978-3-540-89350-9_6", "10.1080/10447319509526110", "10.1016/0020-7373(92)90039-N", "10.1037/h0045689", "10.1007/978-3-322-80058-9_19", "10.1207/s15327051hci0801_1", "10.1007/978-3-540-89350-9_6", "10.1080/10447319509526110", "10.1016/0020-7373(92)90039-N"]}, "10.1109/TVCG.2013.38": {"doi": "10.1109/TVCG.2013.38", "author": ["L. Terziman", "M. Marchal", "F. Multon", "B. Arnaldi", "A. L\u00e9cuyer"], "title": "Personified and Multistate Camera Motions for First-Person Navigation in Desktop Virtual Reality", "year": "2013", "abstract": "In this paper we introduce novel 'Camera Motions' (CMs) to improve the sensations related to locomotion in virtual environments (VE). Traditional Camera Motions are artificial oscillating motions applied to the subjective viewpoint when walking in the VE, and they are meant to evoke and reproduce the visual flow generated during a human walk. Our novel camera motions are: (1) multistate, (2) personified, and (3) they can take into account the topography of the virtual terrain. Being multistate, our CMs can account for different states of locomotion in VE namely: walking, but also running and sprinting. Being personified, our CMs can be adapted to avatars physiology such as to its size, weight or training status. They can then take into account avatars fatigue and recuperation for updating visual CMs accordingly. Last, our approach is adapted to the topography of the VE. Running over a strong positive slope would rapidly decrease the advance speed of the avatar, increase its energy loss, and eventually change the locomotion mode, influencing the visual feedback of the camera motions. Our new approach relies on a locomotion simulator partially inspired by human physiology and implemented for a real-time use in Desktop VR. We have conducted a series of experiments to evaluate the perception of our new CMs by naive participants. Results notably show that participants could discriminate and perceive transitions between the different locomotion modes, by relying exclusively on our CMs. They could also perceive some properties of the avatar being used and, overall, very well appreciated the new CMs techniques. Taken together, our results suggest that our new CMs could be introduced in Desktop VR applications involving first-person navigation, in order to enhance sensations of walking, running, and sprinting, with potentially different avatars and over uneven terrains, such as for: training, virtual visits or video games.", "keywords": ["virtual reality", "personified camera motion", "multistate camera motion", "first-person navigation", "desktop virtual reality", "artificial oscillating motion", "visual flow", "virtual terrain", "avatars physiology", "locomotion mode", "visual feedback", "Desktop VR", "Cameras", "Navigation", "Visualization", "Vibrations", "Mathematical model", "Legged locomotion", "Oscillators", "Navigation", "camera motions", "locomotion simulation.", "Algorithms", "Biofeedback, Psychology", "Computer Graphics", "Cues", "Humans", "Imaging, Three-Dimensional", "Locomotion", "Orientation", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:8798158", "10.1002/9781119341031.ch5", "10.3390/computers7040061", "10.1080/14794713.2019.1671695"], "referencing": ["IKEY:756956", "IKEY:4480749", "IKEY:913779", "IKEY:6788040", "IKEY:1667621", "IKEY:1667620", "IKEY:5446238", "IKEY:677050", "IKEY:4811012", "IKEY:6184179", "IKEY:756956", "IKEY:4480749", "IKEY:913779", "IKEY:6788040", "IKEY:1667621", "IKEY:1667620", "IKEY:5446238", "IKEY:677050", "IKEY:4811012", "IKEY:6184179", "IKEY:756956", "IKEY:4480749", "IKEY:913779", "IKEY:6788040", "IKEY:1667621", "IKEY:1667620", "IKEY:5446238", "IKEY:677050", "IKEY:4811012", "IKEY:6184179", "10.1145/1152399.1152415", "10.1145/1152399.1152415", "10.1145/1152399.1152415", "10.1515/9781400849512", "10.1055/s-2001-11357", "10.1016/j.pmr.2005.02.007", "10.1007/BF00839153", "10.1007/s100550200008", "10.1007/s002210050781", "10.1249/00005768-198706000-00012", "10.1115/1.1372322", "10.1016/S0966-6362(97)00038-6", "10.1007/978-3-642-15841-4_13", "10.1007/BF00429037", "10.1007/BF00230842", "10.1515/9781400849512", "10.1055/s-2001-11357", "10.1016/j.pmr.2005.02.007", "10.1007/BF00839153", "10.1007/s100550200008", "10.1007/s002210050781", "10.1249/00005768-198706000-00012", "10.1115/1.1372322", "10.1016/S0966-6362(97)00038-6", "10.1007/978-3-642-15841-4_13", "10.1007/BF00429037", "10.1007/BF00230842", "10.1515/9781400849512", "10.1055/s-2001-11357", "10.1016/j.pmr.2005.02.007", "10.1007/BF00839153", "10.1007/s100550200008", "10.1007/s002210050781", "10.1249/00005768-198706000-00012", "10.1115/1.1372322", "10.1016/S0966-6362(97)00038-6", "10.1007/978-3-642-15841-4_13", "10.1007/BF00429037", "10.1007/BF00230842"]}, "10.1109/TVCG.2013.35": {"doi": "10.1109/TVCG.2013.35", "author": ["A. Robb", "R. Kopper", "R. Ambani", "F. Qayyum", "D. Lind", "L. Su", "B. Lok"], "title": "Leveraging Virtual Humans to Effectively Prepare Learners for Stressful Interpersonal Experiences", "year": "2013", "abstract": "Stressful interpersonal experiences can be difficult to prepare for. Virtual humans may be leveraged to allow learners to safely gain exposure to stressful interpersonal experiences. In this paper we present a between-subjects study exploring how the presence of a virtual human affected learners while practicing a stressful interpersonal experience. Twenty-six fourth-year medical students practiced performing a prostate exam on a prostate exam simulator. Participants in the experimental condition examined a simulator augmented with a virtual human. Other participants examined a standard unaugmented simulator. Participants reactions were assessed using self-reported, behavioral, and physiological metrics. Participants who examined the virtual human experienced significantly more stress, measured via skin conductance. Participants stress was correlated with previous experience performing real prostate exams; participants who had performed more real prostate exams were more likely to experience stress while examining the virtual human. Participants who examined the virtual human showed signs of greater engagement; non-stressed participants performed better prostate exams while stressed participants treated the virtual human more realistically. Results indicated that stress evoked by virtual humans is linked to similar previous real-world stressful experiences, implying that learners real-world experience must be taken into account when using virtual humans to prepare them for stressful interpersonal experiences.", "keywords": ["augmented reality", "physiology", "psychology", "stressful interpersonal experience", "virtual human", "learner", "prostate exam simulator", "augmented simulator", "self-reported metric", "behavioral metric", "physiological metric", "skin conductance", "Standards", "Stress", "Measurement", "Interviews", "Training", "Educational institutions", "Prostate cancer", "Virtual/digital characters", "mixed reality", "training", "user studies.", "Adaptation, Psychological", "Adult", "Computer Graphics", "Computer Simulation", "Computer-Assisted Instruction", "Digital Rectal Examination", "Digital Rectal Examination", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "Models, Biological", "Stress, Psychological", "Stress, Psychological", "Stress, Psychological", "Students, Medical", "User-Computer Interface", "Virtual Reality Exposure Therapy"], "referenced_by": ["IKEY:6777428", "IKEY:7014272", "IKEY:7383334", "IKEY:7223346", "10.1002/chp.21302", "10.1016/j.chb.2015.05.043", "10.1016/j.compbiomed.2018.03.010", "10.1016/j.gmod.2019.01.001", "10.3390/app9102078", "10.1007/978-3-319-09767-1_51", "10.1080/10401334.2019.1652180", "10.1016/j.physleta.2019.126080", "10.1136/bmjstel-2020-000587"], "referencing": ["IKEY:4689554", "IKEY:4689554", "IKEY:4689554", "10.1145/1240624.1240784", "10.1145/1753846.1754056", "10.1145/566570.566630", "10.1145/1240624.1240784", "10.1145/1753846.1754056", "10.1145/566570.566630", "10.1145/1240624.1240784", "10.1145/1753846.1754056", "10.1145/566570.566630", "10.1177/0146167203029007002", "10.1016/j.amjsurg.2008.11.025", "10.1097/01.nmd.0000082212.83842.fe", "10.1016/0002-8703(94)90748-X", "10.1038/pcan.2011.38", "10.1016/j.amjsurg.2010.09.004", "10.1016/j.jbtep.2007.07.007", "10.1162/105474602317343668", "10.1080/10401334.2012.664534", "10.1023/A:1024772308758", "10.1037/0022-006X.70.2.428", "10.1371/journal.pone.0000039", "10.1007/978-3-540-85483-8_12", "10.1037/h0027806", "10.1177/0146167203029007002", "10.1016/j.amjsurg.2008.11.025", "10.1097/01.nmd.0000082212.83842.fe", "10.1016/0002-8703(94)90748-X", "10.1038/pcan.2011.38", "10.1016/j.amjsurg.2010.09.004", "10.1016/j.jbtep.2007.07.007", "10.1162/105474602317343668", "10.1080/10401334.2012.664534", "10.1023/A:1024772308758", "10.1037/0022-006X.70.2.428", "10.1371/journal.pone.0000039", "10.1007/978-3-540-85483-8_12", "10.1037/h0027806", "10.1177/0146167203029007002", "10.1016/j.amjsurg.2008.11.025", "10.1097/01.nmd.0000082212.83842.fe", "10.1016/0002-8703(94)90748-X", "10.1038/pcan.2011.38", "10.1016/j.amjsurg.2010.09.004", "10.1016/j.jbtep.2007.07.007", "10.1162/105474602317343668", "10.1080/10401334.2012.664534", "10.1023/A:1024772308758", "10.1037/0022-006X.70.2.428", "10.1371/journal.pone.0000039", "10.1007/978-3-540-85483-8_12", "10.1037/h0027806"]}, "10.1109/TVCG.2013.34": {"doi": "10.1109/TVCG.2013.34", "author": ["G. Cirio", "A. Olivier", "M. Marchal", "J. Pettr\u00e9"], "title": "Kinematic Evaluation of Virtual Walking Trajectories", "year": "2013", "abstract": "Virtual walking, a fundamental task in Virtual Reality (VR), is greatly influenced by the locomotion interface being used, by the specificities of input and output devices, and by the way the virtual environment is represented. No matter how virtual walking is controlled, the generation of realistic virtual trajectories is absolutely required for some applications, especially those dedicated to the study of walking behaviors in VR, navigation through virtual places for architecture, rehabilitation and training. Previous studies focused on evaluating the realism of locomotion trajectories have mostly considered the result of the locomotion task (efficiency, accuracy) and its subjective perception (presence, cybersickness). Few focused on the locomotion trajectory itself, but in situation of geometrically constrained task. In this paper, we study the realism of unconstrained trajectories produced during virtual walking by addressing the following question: did the user reach his destination by virtually walking along a trajectory he would have followed in similar real conditions? To this end, we propose a comprehensive evaluation framework consisting on a set of trajectographical criteria and a locomotion model to generate reference trajectories. We consider a simple locomotion task where users walk between two oriented points in space. The travel path is analyzed both geometrically and temporally in comparison to simulated reference trajectories. In addition, we demonstrate the framework over a user study which considered an initial set of common and frequent virtual walking conditions, namely different input devices, output display devices, control laws, and visualization modalities. The study provides insight into the relative contributions of each condition to the overall realism of the resulting virtual trajectories.", "keywords": ["gait analysis", "kinematics", "navigation", "virtual reality", "visualization modalities", "locomotion trajectories", "navigation", "locomotion interface", "VR", "virtual reality", "virtual walking trajectories", "kinematic evaluation", "Trajectory", "Legged locomotion", "Logic gates", "Visualization", "Cameras", "Virtual environments", "Angular velocity", "Locomotion", "evaluation", "motor control", "vision", "perception-action.", "Computer Graphics", "Computer Simulation", "Gait", "Humans", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Models, Biological", "User-Computer Interface", "Walking"], "referenced_by": ["IKEY:7027325", "IKEY:7893328", "IKEY:7398420", "IKEY:7014249", "IKEY:7835276", "IKEY:7504714", "IKEY:7955099", "IKEY:7946183", "IKEY:8446180", "IKEY:8446235", "IKEY:8798043", "IKEY:8797721", "IKEY:8798204", "IKEY:9089452", "IKEY:9089531", "IKEY:9262615", "IKEY:9284660", "10.1145/3230648", "10.1007/s00371-016-1229-9", "10.1016/j.trpro.2014.09.015", "10.1016/j.ijhcs.2018.08.002", "10.1111/cgf.13591", "10.1007/978-3-030-05129-7_2", "10.3390/informatics6020018", "10.3390/app9091873", "10.1002/cav.1928", "10.1016/j.jbmt.2020.06.031", "10.3390/bs10090130", "10.1007/s00221-020-05980-y", "10.1016/j.ijhcs.2020.102578", "10.1007/978-3-030-64556-4_48"], "referencing": ["IKEY:1199309", "IKEY:6165135", "IKEY:4115591", "IKEY:6790789", "IKEY:6788040", "IKEY:5759212", "IKEY:6797599", "IKEY:4756020", "IKEY:5204082", "IKEY:1492762", "IKEY:1512020", "IKEY:1199309", "IKEY:6165135", "IKEY:4115591", "IKEY:6790789", "IKEY:6788040", "IKEY:5759212", "IKEY:6797599", "IKEY:4756020", "IKEY:5204082", "IKEY:1492762", "IKEY:1512020", "IKEY:1199309", "IKEY:6165135", "IKEY:4115591", "IKEY:6790789", "IKEY:6788040", "IKEY:5759212", "IKEY:6797599", "IKEY:4756020", "IKEY:5204082", "IKEY:1492762", "IKEY:1512020", "10.1145/1227134.1227136", "10.1145/210079.210084", "10.1145/1012551.1012558", "10.1145/1272582.1272590", "10.1145/1227134.1227136", "10.1145/210079.210084", "10.1145/1012551.1012558", "10.1145/1272582.1272590", "10.1145/1227134.1227136", "10.1145/210079.210084", "10.1145/1012551.1012558", "10.1145/1272582.1272590", "10.1016/j.cub.2007.10.059", "10.1037/0096-1523.29.2.343", "10.1097/00001756-199604260-00015", "10.1016/S0042-6989(02)00066-4", "10.1016/j.visres.2007.03.012", "10.1111/j.1460-9568.2007.05836.x", "10.1007/s00221-001-0983-7", "10.1016/j.gaitpost.2005.05.005", "10.1007/3-540-69342-4_1", "10.1177/154193129503902006", "10.1016/j.chb.2011.06.014", "10.1016/S1364-6613(99)01364-9", "10.1016/S0966-6362(96)01109-5", "10.1007/s002210050932", "10.1111/j.1460-9568.2007.05835.x", "10.1016/S0304-3940(02)01390-3", "10.1016/S0960-9822(07)00492-7", "10.1016/j.visres.2005.06.017", "10.1038/84054", "10.1167/3.11.3", "10.1016/j.cub.2007.10.059", "10.1037/0096-1523.29.2.343", "10.1097/00001756-199604260-00015", "10.1016/S0042-6989(02)00066-4", "10.1016/j.visres.2007.03.012", "10.1111/j.1460-9568.2007.05836.x", "10.1007/s00221-001-0983-7", "10.1016/j.gaitpost.2005.05.005", "10.1007/3-540-69342-4_1", "10.1177/154193129503902006", "10.1016/j.chb.2011.06.014", "10.1016/S1364-6613(99)01364-9", "10.1016/S0966-6362(96)01109-5", "10.1007/s002210050932", "10.1111/j.1460-9568.2007.05835.x", "10.1016/S0304-3940(02)01390-3", "10.1016/S0960-9822(07)00492-7", "10.1016/j.visres.2005.06.017", "10.1038/84054", "10.1167/3.11.3", "10.1016/j.cub.2007.10.059", "10.1037/0096-1523.29.2.343", "10.1097/00001756-199604260-00015", "10.1016/S0042-6989(02)00066-4", "10.1016/j.visres.2007.03.012", "10.1111/j.1460-9568.2007.05836.x", "10.1007/s00221-001-0983-7", "10.1016/j.gaitpost.2005.05.005", "10.1007/3-540-69342-4_1", "10.1177/154193129503902006", "10.1016/j.chb.2011.06.014", "10.1016/S1364-6613(99)01364-9", "10.1016/S0966-6362(96)01109-5", "10.1007/s002210050932", "10.1111/j.1460-9568.2007.05835.x", "10.1016/S0304-3940(02)01390-3", "10.1016/S0960-9822(07)00492-7", "10.1016/j.visres.2005.06.017", "10.1038/84054", "10.1167/3.11.3"]}, "10.1109/TVCG.2013.30": {"doi": "10.1109/TVCG.2013.30", "author": ["D. J. Roberts", "J. Rae", "T. W. Duckworth", "C. M. Moore", "R. Aspin"], "title": "Estimating the Gaze of a Virtuality Human", "year": "2013", "abstract": "The aim of our experiment is to determine if eye-gaze can be estimated from a virtuality human: to within the accuracies that underpin social interaction; and reliably across gaze poses and camera arrangements likely in every day settings. The scene is set by explaining why Immersive Virtuality Telepresence has the potential to meet the grand challenge of faithfully communicating both the appearance and the focus of attention of a remote human participant within a shared 3D computer-supported context. Within the experiment n=22 participants rotated static 3D virtuality humans, reconstructed from surround images, until they felt most looked at. The dependent variable was absolute angular error, which was compared to that underpinning social gaze behaviour in the natural world. Independent variables were 1) relative orientations of eye, head and body of captured subject; and 2) subset of cameras used to texture the form. Analysis looked for statistical and practical significance and qualitative corroborating evidence. The analysed results tell us much about the importance and detail of the relationship between gaze pose, method of video based reconstruction, and camera arrangement. They tell us that virtuality can reproduce gaze to an accuracy useful in social interaction, but with the adopted method of Video Based Reconstruction, this is highly dependent on combination of gaze pose and camera arrangement. This suggests changes in the VBR approach in order to allow more flexible camera arrangements. The work is of interest to those wanting to support expressive meetings that are both socially and spatially situated, and particular those using or building Immersive Virtuality Telepresence to accomplish this. It is also of relevance to the use of virtuality humans in applications ranging from the study of human interactions to gaming and the crossing of the stage line in films and TV.", "keywords": ["image reconstruction", "iris recognition", "virtuality humans", "flexible camera arrangement", "video based reconstruction", "gaze pose", "qualitative corroborating evidence", "social gaze behaviour", "angular error", "image reconstruction", "3D virtuality human", "3D computer supported context", "immersive virtuality telepresence", "social interaction", "eye gaze", "gaze estimation", "Cameras", "Estimation", "Accuracy", "Image reconstruction", "Face", "Visualization", "Cinematography", "virtual worlds", "virtual environments", "camera placement", "hierarchical finite state machines.", "Algorithms", "Attention", "Computer Graphics", "Computer Simulation", "Fixation, Ocular", "Humans", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Models, Biological", "Pattern Recognition, Automated", "Reproducibility of Results", "Sensitivity and Specificity", "User-Computer Interface"], "referenced_by": ["IKEY:7118994", "IKEY:7039218", "IKEY:7490357", "IKEY:7790904", "IKEY:8797852", "IKEY:9089612", "10.1007/978-3-319-40216-1_54", "10.1007/978-3-319-64027-3_22", "10.1007/978-3-319-72323-5_9", "10.1016/j.ijhcs.2015.10.004", "10.1016/j.ijhcs.2016.05.002", "10.3389/frvir.2020.00011", "10.1016/j.ijhcs.2020.102563"], "referencing": ["IKEY:4811013", "IKEY:1578734", "IKEY:4497206", "IKEY:4811013", "IKEY:1578734", "IKEY:4497206", "IKEY:4811013", "IKEY:1578734", "IKEY:4497206", "10.1145/2070719.2070724", "10.1145/2070719.2070724", "10.1145/2070719.2070724", "10.2307/1420441", "10.1080/027249800410562", "10.1007/s10055-010-0175-5", "10.2307/1420441", "10.1080/027249800410562", "10.1007/s10055-010-0175-5", "10.2307/1420441", "10.1080/027249800410562", "10.1007/s10055-010-0175-5"]}, "10.1109/TVCG.2013.36": {"doi": "10.1109/TVCG.2013.36", "author": ["K. Ponto", "M. Gleicher", "R. G. Radwin", "H. J. Shin"], "title": "Perceptual Calibration for Immersive Display Environments", "year": "2013", "abstract": "The perception of objects, depth, and distance has been repeatedly shown to be divergent between virtual and physical environments. We hypothesize that many of these discrepancies stem from incorrect geometric viewing parameters, specifically that physical measurements of eye position are insufficiently precise to provide proper viewing parameters. In this paper, we introduce a perceptual calibration procedure derived from geometric models. While most research has used geometric models to predict perceptual errors, we instead use these models inversely to determine perceptually correct viewing parameters. We study the advantages of these new psychophysically determined viewing parameters compared to the commonly used measured viewing parameters in an experiment with 20 subjects. The perceptually calibrated viewing parameters for the subjects generally produced new virtual eye positions that were wider and deeper than standard practices would estimate. Our study shows that perceptually calibrated viewing parameters can significantly improve depth acuity, distance estimation, and the perception of shape.", "keywords": ["distance measurement", "virtual reality", "immersive display environments", "object perception", "depth perception", "distance perception", "virtual environment", "physical environment", "perceptual calibration procedure", "geometric model", "perceptually calibrated viewing parameter", "depth acuity", "distance estimation", "shape perception", "virtual reality", "Calibration", "Solid modeling", "Estimation", "Shape", "Virtual environments", "Cameras", "Virtual reality", "calibration", "perception", "distance estimation", "shape perception", "depth compression", "stereo vision displays.", "Adult", "Aged", "Algorithms", "Calibration", "Computer Graphics", "Female", "Humans", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Male", "Middle Aged", "Reproducibility of Results", "Sensitivity and Specificity", "Task Performance and Analysis", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:7131734", "IKEY:7032592", "IKEY:6977006", "IKEY:7892353", "IKEY:7504693", "IKEY:8642347", "IKEY:8797911", "10.1145/2724716", "10.1002/jsid.378", "10.1007/978-3-319-39907-2_17", "10.1007/978-3-319-60922-5_23", "10.1007/s10055-014-0257-x", "10.1007/s12008-017-0377-0", "10.1177/0018720814523067", "10.1177/0018720814562231", "10.1007/978-3-030-01692-0_36", "10.1371/journal.pone.0232290", "10.3758/s13428-019-01336-9", "10.3390/app10114049"], "referencing": ["IKEY:517685", "IKEY:4480794", "IKEY:6165140", "IKEY:6797703", "IKEY:4811007", "IKEY:5643530", "IKEY:4811026", "IKEY:378266", "IKEY:6165139", "IKEY:5759488", "IKEY:1240699", "IKEY:6787840", "IKEY:517685", "IKEY:4480794", "IKEY:6165140", "IKEY:6797703", "IKEY:4811007", "IKEY:5643530", "IKEY:4811026", "IKEY:378266", "IKEY:6165139", "IKEY:5759488", "IKEY:1240699", "IKEY:6787840", "IKEY:517685", "IKEY:4480794", "IKEY:6165140", "IKEY:6797703", "IKEY:4811007", "IKEY:5643530", "IKEY:4811026", "IKEY:378266", "IKEY:6165139", "IKEY:5759488", "IKEY:1240699", "IKEY:6787840", "10.1145/2010324.1964991", "10.1145/323663.323692", "10.1145/1394281.1394285", "10.1145/968363.968387", "10.1145/1620993.1620998", "10.1145/1140491.1140530", "10.1145/2010324.1964991", "10.1145/323663.323692", "10.1145/1394281.1394285", "10.1145/968363.968387", "10.1145/1620993.1620998", "10.1145/1140491.1140530", "10.1145/2010324.1964991", "10.1145/323663.323692", "10.1145/1394281.1394285", "10.1145/968363.968387", "10.1145/1620993.1620998", "10.1145/1140491.1140530", "10.1518/001872098779591278", "10.1167/4.12.1", "10.1016/S0042-6989(03)00458-9", "10.1177/154193129503902006", "10.1068/p2737", "10.1038/35102562", "10.3758/BF03210980", "10.1111/1467-9280.00377", "10.1177/154193129403800413", "10.1111/j.1475-1313.1993.tb00490.x", "10.1006/ijhc.1996.0035", "10.1117/12.157041", "10.1518/001872098779591278", "10.1167/4.12.1", "10.1016/S0042-6989(03)00458-9", "10.1177/154193129503902006", "10.1068/p2737", "10.1038/35102562", "10.3758/BF03210980", "10.1111/1467-9280.00377", "10.1177/154193129403800413", "10.1111/j.1475-1313.1993.tb00490.x", "10.1006/ijhc.1996.0035", "10.1117/12.157041", "10.1518/001872098779591278", "10.1167/4.12.1", "10.1016/S0042-6989(03)00458-9", "10.1177/154193129503902006", "10.1068/p2737", "10.1038/35102562", "10.3758/BF03210980", "10.1111/1467-9280.00377", "10.1177/154193129403800413", "10.1111/j.1475-1313.1993.tb00490.x", "10.1006/ijhc.1996.0035", "10.1117/12.157041"]}, "10.1109/TVCG.2013.37": {"doi": "10.1109/TVCG.2013.37", "author": ["J. A. Jones", "J. E. Swan II", "M. Bolas"], "title": "Peripheral Stimulation and its Effect on Perceived Spatial Scale in Virtual Environments", "year": "2013", "abstract": "The following series of experiments explore the effect of static peripheral stimulation on the perception of distance and spatial scale in a typical head-mounted virtual environment. It was found that applying constant white light in an observers far periphery enabled the observer to more accurately judge distances using blind walking. An effect of similar magnitude was also found when observers estimated the size of a virtual space using a visual scale task. The presence of the effect across multiple psychophysical tasks provided confidence that a perceptual change was, in fact, being invoked by the addition of the peripheral stimulation. These results were also compared to observer performance in a very large field of view virtual environment and in the real world. The subsequent findings raise the possibility that distance judgments in virtual environments might be considerably more similar to those in the real world than previous work has suggested.", "keywords": ["helmet mounted displays", "peripheral interfaces", "virtual reality", "field of view virtual environment", "observer performance", "perceptual change", "psychophysical task", "visual scale task", "virtual space", "magnitude", "blind walking", "constant white light", "head-mounted virtual environment", "distance perception", "static peripheral stimulation", "perceived spatial scale", "Virtual environments", "Legged locomotion", "Visualization", "Observers", "Adaptive optics", "Stimulated emission", "Optical imaging", "Virtual environments", "spatial perception", "distance judgments", "field of view", "periphery.", "Adult", "Algorithms", "Artificial Intelligence", "Cues", "Female", "Humans", "Image Enhancement", "Image Interpretation, Computer-Assisted", "Imaging, Three-Dimensional", "Photic Stimulation", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:8280417", "IKEY:7460041", "IKEY:7460042", "IKEY:7460055", "IKEY:7893321", "IKEY:7210403", "IKEY:7836482", "IKEY:7517276", "IKEY:6777453", "IKEY:6777444", "IKEY:6777445", "IKEY:7127036", "IKEY:8302409", "IKEY:7223328", "IKEY:7504764", "IKEY:7859535", "IKEY:8642384", "IKEY:8409318", "IKEY:8798065", "IKEY:8797975", "IKEY:8797756", "IKEY:8797752", "IKEY:8798258", "IKEY:8798059", "IKEY:8903262", "IKEY:9020416", "10.1145/2983631", "10.1145/3165286", "10.1145/3196885", "10.1145/2807442.2807493", "10.1145/2858036.2858212", "10.1002/jsid.378", "10.1007/978-3-319-72323-5_8", "10.1016/bs.plm.2014.09.006", "10.1080/15397734.2015.1035784", "10.1016/B978-0-12-800965-9.16001-5", "10.1007/978-3-319-09858-6_1", "10.3389/frobt.2019.00044", "10.1098/rsif.2019.0197", "10.1002/jsid.832", "10.3390/app9214652", "10.1177/0301006620951997", "10.1007/978-3-030-62655-6_2"], "referencing": ["IKEY:1667620", "IKEY:6165140", "IKEY:4811007", "IKEY:6180933", "IKEY:5620907", "IKEY:4135650", "IKEY:1667622", "IKEY:880938", "IKEY:1667620", "IKEY:6165140", "IKEY:4811007", "IKEY:6180933", "IKEY:5620907", "IKEY:4135650", "IKEY:1667622", "IKEY:880938", "IKEY:1667620", "IKEY:6165140", "IKEY:4811007", "IKEY:6180933", "IKEY:5620907", "IKEY:4135650", "IKEY:1667622", "IKEY:880938", "10.1145/2338676.2338699", "10.1145/2077451.2077457", "10.1145/1394281.1394283", "10.1145/2338676.2338679", "10.1145/1577755.1577762", "10.1145/1836248.1836277", "10.1145/2019627.2019631", "10.1145/1012551.1012558", "10.1145/1498700.1498702", "10.1145/2338676.2338699", "10.1145/2077451.2077457", "10.1145/1394281.1394283", "10.1145/2338676.2338679", "10.1145/1577755.1577762", "10.1145/1836248.1836277", "10.1145/2019627.2019631", "10.1145/1012551.1012558", "10.1145/1498700.1498702", "10.1145/2338676.2338699", "10.1145/2077451.2077457", "10.1145/1394281.1394283", "10.1145/2338676.2338679", "10.1145/1577755.1577762", "10.1145/1836248.1836277", "10.1145/2019627.2019631", "10.1145/1012551.1012558", "10.1145/1498700.1498702", "10.2466/PMS.70.1.35-45", "10.1037/11332-000", "10.1068/p5144", "10.3758/BF03200563", "10.1518/001872098779591278", "10.1016/S0166-4115(08)62771-5", "10.3758/BF03214141", "10.1016/0042-6989(79)90227-X", "10.1162/1054746042545238", "10.1037/0096-1523.18.4.906", "10.1016/0042-6989(84)90140-8", "10.3758/PP.70.8.1459", "10.1518/001872007X200139", "10.1167/11.5.13", "10.1162/1054746042545292", "10.1037/0096-1523.9.3.427", "10.3758/BF03207058", "10.1518/001872098779591340", "10.1038/nature02350", "10.2466/PMS.70.1.35-45", "10.1037/11332-000", "10.1068/p5144", "10.3758/BF03200563", "10.1518/001872098779591278", "10.1016/S0166-4115(08)62771-5", "10.3758/BF03214141", "10.1016/0042-6989(79)90227-X", "10.1162/1054746042545238", "10.1037/0096-1523.18.4.906", "10.1016/0042-6989(84)90140-8", "10.3758/PP.70.8.1459", "10.1518/001872007X200139", "10.1167/11.5.13", "10.1162/1054746042545292", "10.1037/0096-1523.9.3.427", "10.3758/BF03207058", "10.1518/001872098779591340", "10.1038/nature02350", "10.2466/PMS.70.1.35-45", "10.1037/11332-000", "10.1068/p5144", "10.3758/BF03200563", "10.1518/001872098779591278", "10.1016/S0166-4115(08)62771-5", "10.3758/BF03214141", "10.1016/0042-6989(79)90227-X", "10.1162/1054746042545238", "10.1037/0096-1523.18.4.906", "10.1016/0042-6989(84)90140-8", "10.3758/PP.70.8.1459", "10.1518/001872007X200139", "10.1167/11.5.13", "10.1162/1054746042545292", "10.1037/0096-1523.9.3.427", "10.3758/BF03207058", "10.1518/001872098779591340", "10.1038/nature02350"]}, "10.1109/TVCG.2013.42": {"doi": "10.1109/TVCG.2013.42", "author": ["E. Bekele", "Z. Zheng", "A. Swanson", "J. Crittendon", "Z. Warren", "N. Sarkar"], "title": "Understanding How Adolescents with Autism Respond to Facial Expressions in Virtual Reality Environments", "year": "2013", "abstract": "Autism Spectrum Disorders (ASD) are characterized by atypical patterns of behaviors and impairments in social communication. Among the fundamental social impairments in the ASD population are challenges in appropriately recognizing and responding to facial expressions. Traditional intervention approaches often require intensive support and well-trained therapists to address core deficits, with many with ASD having tremendous difficulty accessing such care due to lack of available trained therapists as well as intervention costs. As a result, emerging technology such as virtual reality (VR) has the potential to offer useful technology-enabled intervention systems. In this paper, an innovative VR-based facial emotional expression presentation system was developed that allows monitoring of eye gaze and physiological signals related to emotion identification to explore new efficient therapeutic paradigms. A usability study of this new system involving ten adolescents with ASD and ten typically developing adolescents as a control group was performed. The eye tracking and physiological data were analyzed to determine intragroup and intergroup variations of gaze and physiological patterns. Performance data, eye tracking indices and physiological features indicated that there were differences in the way adolescents with ASD process and recognize emotional faces compared to their typically developing peers. These results will be used in the future for an online adaptive VR-based multimodal social interaction system to improve emotion recognition abilities of individuals with ASD.", "keywords": ["emotion recognition", "handicapped aids", "interactive systems", "patient treatment", "virtual reality", "adolescents", "virtual reality environments", "autism spectrum disorders", "ASD", "social communication", "core deficits", "trained therapists", "technology-enabled intervention systems", "innovative VR-based facial emotional expression presentation system", "eye gaze", "physiological signals", "therapeutic paradigms", "physiological data", "eye tracking indices", "intragroup gaze variations", "intergroup gaze variations", "online adaptive VR-based multimodal social interaction system", "emotion recognition abilities", "Variable speed drives", "Biomedical monitoring", "Monitoring", "Animation", "Emotion recognition", "Physiology", "Autism", "3D Interaction", "multimodal interaction", "psychology", "usability", "vr-based response systems.", "Adolescent", "Autistic Disorder", "Autistic Disorder", "Computer Graphics", "Diagnosis, Computer-Assisted", "Facial Expression", "Female", "Humans", "Imaging, Three-Dimensional", "Male", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:6617623", "IKEY:8284387", "IKEY:8296983", "IKEY:8079705", "IKEY:7574455", "IKEY:7379261", "IKEY:7898495", "IKEY:7513425", "IKEY:7223382", "IKEY:7504695", "IKEY:7764551", "IKEY:8010470", "IKEY:9294011", "10.1145/2669557.2669560", "10.1145/2892636", "10.1007/978-981-10-7635-0_26", "10.1007/s10803-014-2035-8", "10.1007/s12559-014-9276-x", "10.1016/j.neubiorev.2017.06.016", "10.1080/10400419.2014.929413", "10.1089/cyber.2014.0682", "10.1155/2016/2696723", "10.1177/1473871615609787", "10.1201/9781315371375-16", "10.3389/fnhum.2014.00400", "10.3389/fnhum.2014.00807", "10.3389/fpsyg.2015.00320", "10.3390/app7101051", "10.4018/978-1-5225-0816-8.ch011", "10.4074/S0013754515004139", "10.5772/intechopen.71000", "10.3390/s18082486", "10.1016/j.psychres.2018.10.014", "10.1016/j.chb.2018.12.001", "10.21565/ozelegitimdergisi.349440", "10.1111/pcn.12799", "10.1016/j.buildenv.2019.01.009", "10.1007/s10209-019-00646-1", "10.1007/978-3-319-07464-1_21", "10.1007/978-3-319-07458-0_2", "10.1007/978-3-319-20684-4_64", "10.1007/s10803-019-04246-z", "10.3390/sym11101189"], "referencing": ["IKEY:6787759", "IKEY:5773090", "IKEY:4412808", "IKEY:6787759", "IKEY:5773090", "IKEY:4412808", "IKEY:6787759", "IKEY:5773090", "IKEY:4412808", "10.1145/1463689.1463725", "10.1145/355017.355028", "10.1145/1463689.1463725", "10.1145/355017.355028", "10.1145/1463689.1463725", "10.1145/355017.355028", "10.1007/s10826-006-9094-1", "10.1001/archpedi.161.4.343", "10.1093/phr/118.5.393", "10.1177/1362361305056082", "10.1207/s15326942dn2703_6", "10.1111/j.1469-7610.1992.tb00936.x", "10.1111/j.1469-7610.1987.tb00658.x", "10.1111/j.1469-7610.1986.tb01836.x", "10.1023/A:1016378718278", "10.1177/1088357608316678", "10.1089/cpb.2005.8.272", "10.1007/978-3-540-74997-4_19", "10.1007/s10803-006-0189-8", "10.1016/j.ijhsc.2008.04.003", "10.1080/08856257.2011.593831", "10.1159/000320847", "10.1007/978-3-540-73110-8_29", "10.1007/s00146-009-0199-0", "10.1007/s10803-006-0222-y", "10.1007/s12369-010-0063-x", "10.1080/13803390500376790", "10.1177/10883576050200040601", "10.1017/CBO9780511546396", "10.1037/0003-066X.48.4.384", "10.1007/s10803-006-0280-1", "10.1001/archpsyc.59.9.809", "10.1007/BF02172283", "10.1111/j.1469-7610.2004.00304.x", "10.1177/000841740507200107", "10.1007/s10826-006-9094-1", "10.1001/archpedi.161.4.343", "10.1093/phr/118.5.393", "10.1177/1362361305056082", "10.1207/s15326942dn2703_6", "10.1111/j.1469-7610.1992.tb00936.x", "10.1111/j.1469-7610.1987.tb00658.x", "10.1111/j.1469-7610.1986.tb01836.x", "10.1023/A:1016378718278", "10.1177/1088357608316678", "10.1089/cpb.2005.8.272", "10.1007/978-3-540-74997-4_19", "10.1007/s10803-006-0189-8", "10.1016/j.ijhsc.2008.04.003", "10.1080/08856257.2011.593831", "10.1159/000320847", "10.1007/978-3-540-73110-8_29", "10.1007/s00146-009-0199-0", "10.1007/s10803-006-0222-y", "10.1007/s12369-010-0063-x", "10.1080/13803390500376790", "10.1177/10883576050200040601", "10.1017/CBO9780511546396", "10.1037/0003-066X.48.4.384", "10.1007/s10803-006-0280-1", "10.1001/archpsyc.59.9.809", "10.1007/BF02172283", "10.1111/j.1469-7610.2004.00304.x", "10.1177/000841740507200107", "10.1007/s10826-006-9094-1", "10.1001/archpedi.161.4.343", "10.1093/phr/118.5.393", "10.1177/1362361305056082", "10.1207/s15326942dn2703_6", "10.1111/j.1469-7610.1992.tb00936.x", "10.1111/j.1469-7610.1987.tb00658.x", "10.1111/j.1469-7610.1986.tb01836.x", "10.1023/A:1016378718278", "10.1177/1088357608316678", "10.1089/cpb.2005.8.272", "10.1007/978-3-540-74997-4_19", "10.1007/s10803-006-0189-8", "10.1016/j.ijhsc.2008.04.003", "10.1080/08856257.2011.593831", "10.1159/000320847", "10.1007/978-3-540-73110-8_29", "10.1007/s00146-009-0199-0", "10.1007/s10803-006-0222-y", "10.1007/s12369-010-0063-x", "10.1080/13803390500376790", "10.1177/10883576050200040601", "10.1017/CBO9780511546396", "10.1037/0003-066X.48.4.384", "10.1007/s10803-006-0280-1", "10.1001/archpsyc.59.9.809", "10.1007/BF02172283", "10.1111/j.1469-7610.2004.00304.x", "10.1177/000841740507200107"]}}