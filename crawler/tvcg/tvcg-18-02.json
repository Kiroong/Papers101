{"10.1109/TVCG.2017.2780598": {"doi": "10.1109/TVCG.2017.2780598", "author": ["L. D. Floriani"], "title": "State of the Journal", "year": "2018", "abstract": "Presents information on the current status of the journal.", "keywords": [""], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2658570": {"doi": "10.1109/TVCG.2017.2658570", "author": ["M. Krichenbauer", "G. Yamamoto", "T. Taketom", "C. Sandor", "H. Kato"], "title": "Augmented Reality versus Virtual Reality for 3D Object Manipulation", "year": "2018", "abstract": "Virtual Reality (VR) Head-Mounted Displays (HMDs) are on the verge of becoming commodity hardware available to the average user and feasible to use as a tool for 3D work. Some HMDs include front-facing cameras, enabling Augmented Reality (AR) functionality. Apart from avoiding collisions with the environment, interaction with virtual objects may also be affected by seeing the real environment. However, whether these effects are positive or negative has not yet been studied extensively. For most tasks it is unknown whether AR has any advantage over VR. In this work we present the results of a user study in which we compared user performance measured in task completion time on a 9 degrees of freedom object selection and transformation task performed either in AR or VR, both with a 3D input device and a mouse. Our results show faster task completion time in AR over VR. When using a 3D input device, a purely VR environment increased task completion time by 22.5 percent on average compared to AR ( ${p}<0.024$ ). Surprisingly, a similar effect occurred when using a mouse: users were about 17.3 percent slower in VR than in AR ( ${p}<0.04$ ). Mouse and 3D input device produced similar task completion times in each condition (AR or VR) respectively. We further found no differences in reported comfort.", "keywords": ["Three-dimensional displays", "Performance evaluation", "Mice", "Resists", "Visualization", "Training", "Augmented reality", "Artificial", "augmented", "and virtual realities-multimedia information systems-information interfaces and representation", "interaction techniques-methodology and techniques-computer graphics"], "referenced_by": ["IKEY:8442093", "IKEY:8617763", "IKEY:8710954", "IKEY:8713064", "IKEY:8779366", "IKEY:8943730", "IKEY:9089453", "IKEY:9155164", "IKEY:9212043", "IKEY:9284793", "10.1145/3132272.3134124"], "referencing": ["IKEY:4145190", "IKEY:6479181", "IKEY:4145190", "IKEY:6479181", "IKEY:4145190", "IKEY:6479181", "10.1145/1394281.1394283", "10.1145/1501750.1501755", "10.1145/1889863.1889883", "10.1145/2556288.2557003", "10.1145/1080402.1080423", "10.1145/1394281.1394283", "10.1145/1501750.1501755", "10.1145/1889863.1889883", "10.1145/2556288.2557003", "10.1145/1080402.1080423", "10.1145/1394281.1394283", "10.1145/1501750.1501755", "10.1145/1889863.1889883", "10.1145/2556288.2557003", "10.1145/1080402.1080423", "10.1016/j.cag.2010.08.001", "10.1080/0144929X.2013.815277", "10.1007/s00268-006-0724-y", "10.1518/001872098779591322", "10.1126/science.290.5497.1782", "10.1002/sim.4780140810", "10.1016/j.cag.2010.08.001", "10.1080/0144929X.2013.815277", "10.1007/s00268-006-0724-y", "10.1518/001872098779591322", "10.1126/science.290.5497.1782", "10.1002/sim.4780140810", "10.1016/j.cag.2010.08.001", "10.1080/0144929X.2013.815277", "10.1007/s00268-006-0724-y", "10.1518/001872098779591322", "10.1126/science.290.5497.1782", "10.1002/sim.4780140810"]}, "10.1109/TVCG.2017.2657511": {"doi": "10.1109/TVCG.2017.2657511", "author": ["W. Yang"], "title": "Context-Aware Computer Aided Inbetweening", "year": "2018", "abstract": "This paper presents a context-aware computer aided inbetweening (CACAI) technique that interpolates planar strokes to generate inbetween frames from a given set of key frames. The inbetweening is context-aware in the sense that not only the stroke's shape but also the context (i.e., the neighborhood of a stroke) in which a stroke appears are taken into account for the stroke correspondence and interpolation. Given a pair of successive key frames, the CACAI automatically constructs the stroke correspondence between them by exploiting the context coherence between the corresponding strokes. Meanwhile, the construction algorithm is able to incorporate the user's interaction with ease and allows the user more effective control over the correspondence process than existing stroke matching techniques. With a one-to-one stroke correspondence, the CACAI interpolates the shape and context between the corresponding strokes for the generation of intermediate frames. In the interpolation sequence, both the shape of individual strokes and the spatial layout between them are well retained such that the feature characteristics and visual appearance of the objects in the key frames can be fully preserved even when complex motions are involved in these objects. We have developed a prototype system to demonstrate the ease of use and effectiveness of the CACAI.", "keywords": ["computational geometry", "computer animation", "interpolation", "mesh generation", "context coherence", "corresponding strokes", "correspondence process", "stroke matching techniques", "stroke correspondence", "CACAI", "intermediate frames", "individual strokes", "context-aware computer", "inbetweening technique", "inbetween frames", "stroke appears", "successive key frames", "context-aware computer aided inbetweening", "Interpolation", "Shape", "Context", "Layout", "Two dimensional displays", "Visualization", "Distortion", "Inbetween", "correspondence construction", "stroke interpolation", "context mesh", "CACAI"], "referenced_by": [], "referencing": ["IKEY:4815232", "IKEY:1159951", "IKEY:4418752", "IKEY:4815232", "IKEY:1159951", "IKEY:4418752", "IKEY:4815232", "IKEY:1159951", "IKEY:4418752", "10.1145/965161.806814", "10.1145/508530.508552", "10.1145/218380.218417", "10.1145/965139.807414", "10.1145/1073204.1073218", "10.1145/563732.563743", "10.1145/1572614.1572619", "10.1145/1576246.1531348", "10.1145/2024676.2024691", "10.1145/360349.360357", "10.1145/344779.344859", "10.1145/1178477.1178489", "10.1145/142920.134001", "10.1145/1141911.1141920", "10.1145/882262.882323", "10.1145/1073204.1073323", "10.1145/1833349.1778796", "10.1145/2019627.2019635", "10.1145/965161.806814", "10.1145/508530.508552", "10.1145/218380.218417", "10.1145/965139.807414", "10.1145/1073204.1073218", "10.1145/563732.563743", "10.1145/1572614.1572619", "10.1145/1576246.1531348", "10.1145/2024676.2024691", "10.1145/360349.360357", "10.1145/344779.344859", "10.1145/1178477.1178489", "10.1145/142920.134001", "10.1145/1141911.1141920", "10.1145/882262.882323", "10.1145/1073204.1073323", "10.1145/1833349.1778796", "10.1145/2019627.2019635", "10.1145/965161.806814", "10.1145/508530.508552", "10.1145/218380.218417", "10.1145/965139.807414", "10.1145/1073204.1073218", "10.1145/563732.563743", "10.1145/1572614.1572619", "10.1145/1576246.1531348", "10.1145/2024676.2024691", "10.1145/360349.360357", "10.1145/344779.344859", "10.1145/1178477.1178489", "10.1145/142920.134001", "10.1145/1141911.1141920", "10.1145/882262.882323", "10.1145/1073204.1073323", "10.1145/1833349.1778796", "10.1145/2019627.2019635", "10.1111/j.1467-8659.2009.01630.x", "10.1111/j.1467-8659.2011.01969.x", "10.1016/j.neucom.2011.10.003", "10.1002/cav.122", "10.1142/S0218654303000115", "10.1002/cav.86", "10.1016/j.cag.2009.03.007", "10.1111/j.1467-8659.2012.03218.x", "10.1111/j.1467-8659.2009.01630.x", "10.1111/j.1467-8659.2011.01969.x", "10.1016/j.neucom.2011.10.003", "10.1002/cav.122", "10.1142/S0218654303000115", "10.1002/cav.86", "10.1016/j.cag.2009.03.007", "10.1111/j.1467-8659.2012.03218.x", "10.1111/j.1467-8659.2009.01630.x", "10.1111/j.1467-8659.2011.01969.x", "10.1016/j.neucom.2011.10.003", "10.1002/cav.122", "10.1142/S0218654303000115", "10.1002/cav.86", "10.1016/j.cag.2009.03.007", "10.1111/j.1467-8659.2012.03218.x"]}, "10.1109/TVCG.2017.2653117": {"doi": "10.1109/TVCG.2017.2653117", "author": ["F. Buttussi", "L. Chittaro"], "title": "Effects of Different Types of Virtual Reality Display on Presence and Learning in a Safety Training Scenario", "year": "2018", "abstract": "The increasing availability of head-mounted displays (HMDs) for home use motivates the study of the possible effects that adopting this new hardware might have on users. Moreover, while the impact of display type has been studied for different kinds of tasks, it has been scarcely explored in procedural training. Our study considered three different types of displays used by participants for training in aviation safety procedures with a serious game. The three displays were respectively representative of: (i) desktop VR (a standard desktop monitor), (ii) many setups for immersive VR used in the literature (an HMD with narrow field of view and a 3-DOF tracker), and (iii) new setups for immersive home VR (an HMD with wide field of view and 6-DOF tracker). We assessed effects on knowledge gain, and different self-reported measures (self-efficacy, engagement, presence). Unlike previous studies of display type that measured effects only immediately after the VR experience, we considered also a longer time span (2 weeks). Results indicated that the display type played a significant role in engagement and presence. The training benefits (increased knowledge and self-efficacy) were instead obtained, and maintained at two weeks, regardless of the display used. The paper discusses the implications of these results.", "keywords": ["computer based training", "computer displays", "helmet mounted displays", "virtual reality", "head-mounted displays", "HMDs", "3-DOF tracker", "6-DOF tracker", "safety training scenario", "virtual reality display", "training benefits", "immersive home VR", "immersive VR", "standard desktop monitor", "desktop VR", "aviation safety procedures", "procedural training", "Training", "Safety", "Games", "Virtual reality", "Virtual reality", "displays", "fidelity", "training", "user study", "aviation", "safety"], "referenced_by": ["IKEY:8102472", "IKEY:8615164", "IKEY:8519621", "IKEY:8709270", "IKEY:8797858", "IKEY:8787776", "IKEY:8797771", "IKEY:8795446", "IKEY:8940386", "IKEY:8960616", "IKEY:8985216", "IKEY:8994487", "IKEY:9040016", "IKEY:8998353", "IKEY:9089451", "IKEY:9133071", "IKEY:9155107", "IKEY:9240533", "IKEY:9284723", "IKEY:9288454"], "referencing": ["10.1145/375735.376390", "10.1145/258734.258744", "10.1145/2087756.2087849", "10.1145/2533810.2533811", "10.1145/1357054.1357330", "10.1145/2512142.2512146", "10.1145/1394669.1394672", "10.1145/375735.376390", "10.1145/258734.258744", "10.1145/2087756.2087849", "10.1145/2533810.2533811", "10.1145/1357054.1357330", "10.1145/2512142.2512146", "10.1145/1394669.1394672", "10.1145/375735.376390", "10.1145/258734.258744", "10.1145/2087756.2087849", "10.1145/2533810.2533811", "10.1145/1357054.1357330", "10.1145/2512142.2512146", "10.1145/1394669.1394672", "10.1067/msy.2002.125723", "10.1016/j.firesaf.2012.01.004", "10.1016/j.compedu.2013.07.033", "10.1111/j.1365-2729.2012.00489.x", "10.1016/S0097-8493(01)00117-0", "10.1016/j.ijhcs.2003.10.001", "10.1162/1054746053967012", "10.1016/j.actaastro.2007.11.001", "10.1007/s11042-008-0223-2", "10.1162/pres.1997.6.6.603", "10.1146/annurev.psych.52.1.1", "10.1089/cpb.2006.9993", "10.1177/1754073908100432", "10.1177/0956797611407932", "10.1016/j.cmpb.2013.12.024", "10.1162/105474698565631", "10.1016/j.ijpsycho.2011.12.003", "10.1016/j.compedu.2007.06.014", "10.1089/109493101300117938", "10.1016/j.compedu.2011.09.002", "10.1162/pres_a_00016", "10.1177/1359105305055307", "10.1162/105474601300343603", "10.1016/j.ridd.2006.07.001", "10.1016/S1364-6613(03)00103-7", "10.1067/msy.2002.125723", "10.1016/j.firesaf.2012.01.004", "10.1016/j.compedu.2013.07.033", "10.1111/j.1365-2729.2012.00489.x", "10.1016/S0097-8493(01)00117-0", "10.1016/j.ijhcs.2003.10.001", "10.1162/1054746053967012", "10.1016/j.actaastro.2007.11.001", "10.1007/s11042-008-0223-2", "10.1162/pres.1997.6.6.603", "10.1146/annurev.psych.52.1.1", "10.1089/cpb.2006.9993", "10.1177/1754073908100432", "10.1177/0956797611407932", "10.1016/j.cmpb.2013.12.024", "10.1162/105474698565631", "10.1016/j.ijpsycho.2011.12.003", "10.1016/j.compedu.2007.06.014", "10.1089/109493101300117938", "10.1016/j.compedu.2011.09.002", "10.1162/pres_a_00016", "10.1177/1359105305055307", "10.1162/105474601300343603", "10.1016/j.ridd.2006.07.001", "10.1016/S1364-6613(03)00103-7", "10.1067/msy.2002.125723", "10.1016/j.firesaf.2012.01.004", "10.1016/j.compedu.2013.07.033", "10.1111/j.1365-2729.2012.00489.x", "10.1016/S0097-8493(01)00117-0", "10.1016/j.ijhcs.2003.10.001", "10.1162/1054746053967012", "10.1016/j.actaastro.2007.11.001", "10.1007/s11042-008-0223-2", "10.1162/pres.1997.6.6.603", "10.1146/annurev.psych.52.1.1", "10.1089/cpb.2006.9993", "10.1177/1754073908100432", "10.1177/0956797611407932", "10.1016/j.cmpb.2013.12.024", "10.1162/105474698565631", "10.1016/j.ijpsycho.2011.12.003", "10.1016/j.compedu.2007.06.014", "10.1089/109493101300117938", "10.1016/j.compedu.2011.09.002", "10.1162/pres_a_00016", "10.1177/1359105305055307", "10.1162/105474601300343603", "10.1016/j.ridd.2006.07.001", "10.1016/S1364-6613(03)00103-7"]}, "10.1109/TVCG.2016.2642109": {"doi": "10.1109/TVCG.2016.2642109", "author": ["S. Nusrat", "M. J. Alam", "S. Kobourov"], "title": "Evaluating Cartogram Effectiveness", "year": "2018", "abstract": "Cartograms are maps in which areas of geographic regions, such as countries and states, appear in proportion to some variable of interest, such as population or income. Cartograms are popular visualizations for geo-referenced data that have been used for over a century to illustrate patterns and trends in the world around us. Despite the popularity of cartograms, and the large number of cartogram types, there are few studies evaluating the effectiveness of cartograms in conveying information. Based on a recent task taxonomy for cartograms, we evaluate four major types of cartograms: contiguous, non-contiguous, rectangular, and Dorling cartograms. We first evaluate the effectiveness of these cartogram types by quantitative performance analysis (time and error). Second, we collect qualitative data with an attitude study and by analyzing subjective preferences. Third, we compare the quantitative and qualitative results with the results of a metrics-based cartogram evaluation. Fourth, we analyze the results of our study in the context of cartography, geography, visual perception, and demography. Finally, we consider implications for design and possible improvements.", "keywords": ["cartography", "data visualisation", "demography", "geography", "cartogram effectiveness", "cartogram types", "cartogram evaluation", "geographic regions", "geo-referenced data vizualizations", "task taxonomy", "Dorling cartogram", "rectangular cartogram", "noncontiguous cartogram", "contiguous cartogram", "metrics-based cartogram evaluation", "geography", "visual perception", "demography", "Shape", "Sociology", "Statistics", "Voting", "Topology", "Data visualization", "Geography", "Cartograms", "geo-visualization", "subjective evaluation"], "referenced_by": ["IKEY:8078198", "IKEY:8456575", "IKEY:8809223"], "referencing": ["IKEY:1382888", "IKEY:1260761", "IKEY:1438259", "IKEY:6634181", "IKEY:1382888", "IKEY:1260761", "IKEY:1438259", "IKEY:6634181", "IKEY:1382888", "IKEY:1260761", "IKEY:1438259", "IKEY:6634181", "10.1145/1753326.1753357", "10.1145/1753326.1753357", "10.1145/1753326.1753357", "10.1111/cgf.12647", "10.1111/bjop.12132", "10.1007/978-3-642-33024-7_3", "10.1111/cgf.12648", "10.1080/01621459.1982.10477844", "10.2307/2288400", "10.1142/S0218195910003268", "10.1559/152304075784313278", "10.1111/j.0033-0124.1985.00075.x", "10.1073/pnas.0400280101", "10.1559/152304083783948258", "10.1057/palgrave.ivs.9500039", "10.1007/978-3-642-15300-6_12", "10.1111/cgf.12932", "10.1111/j.0033-0124.1976.00371.x", "10.2307/208794", "10.2190/EM.27.1.b", "10.1037/h0046162", "10.1179/000870409X12525737905169", "10.2307/1420573", "10.1111/j.1749-6632.1973.tb41401.x", "10.1111/j.1467-8306.2004.09401004.x", "10.1016/j.comgeo.2006.06.002", "10.1111/cgf.12647", "10.1111/bjop.12132", "10.1007/978-3-642-33024-7_3", "10.1111/cgf.12648", "10.1080/01621459.1982.10477844", "10.2307/2288400", "10.1142/S0218195910003268", "10.1559/152304075784313278", "10.1111/j.0033-0124.1985.00075.x", "10.1073/pnas.0400280101", "10.1559/152304083783948258", "10.1057/palgrave.ivs.9500039", "10.1007/978-3-642-15300-6_12", "10.1111/cgf.12932", "10.1111/j.0033-0124.1976.00371.x", "10.2307/208794", "10.2190/EM.27.1.b", "10.1037/h0046162", "10.1179/000870409X12525737905169", "10.2307/1420573", "10.1111/j.1749-6632.1973.tb41401.x", "10.1111/j.1467-8306.2004.09401004.x", "10.1016/j.comgeo.2006.06.002", "10.1111/cgf.12647", "10.1111/bjop.12132", "10.1007/978-3-642-33024-7_3", "10.1111/cgf.12648", "10.1080/01621459.1982.10477844", "10.2307/2288400", "10.1142/S0218195910003268", "10.1559/152304075784313278", "10.1111/j.0033-0124.1985.00075.x", "10.1073/pnas.0400280101", "10.1559/152304083783948258", "10.1057/palgrave.ivs.9500039", "10.1007/978-3-642-15300-6_12", "10.1111/cgf.12932", "10.1111/j.0033-0124.1976.00371.x", "10.2307/208794", "10.2190/EM.27.1.b", "10.1037/h0046162", "10.1179/000870409X12525737905169", "10.2307/1420573", "10.1111/j.1749-6632.1973.tb41401.x", "10.1111/j.1467-8306.2004.09401004.x", "10.1016/j.comgeo.2006.06.002"]}, "10.1109/TVCG.2017.2657634": {"doi": "10.1109/TVCG.2017.2657634", "author": ["H. Asayama", "D. Iwai", "K. Sato"], "title": "Fabricating Diminishable Visual Markers for Geometric Registration in Projection Mapping", "year": "2018", "abstract": "We propose a visual marker embedding method for the pose estimation of a projection surface to correctly map projected images onto the surface. Assuming that the surface is fabricated by a full-color or multi-material three-dimensional (3D) printer, we propose to automatically embed visual markers on the surface with mechanical accuracy. The appearance of the marker is designed such that the marker is detected by infrared cameras even when printed on a non-planar surface while its appearance can be diminished by the projection to be as imperceptible as possible to human observers. The marker placement is optimized using a genetic algorithm to maximize the number of valid viewpoints from which the pose of the object can be estimated correctly using a stereo camera system. We also propose a radiometric compensation technique to quickly diminish the marker appearance. Experimental results confirm that the pose of projection objects are correctly estimated while the appearance of the markers was diminished to an imperceptible level. At the same time, we confirmed the limitations of the current method; only one object can be handled, and pose estimation is not performed at interactive frame rates. Finally, we demonstrate the proposed technique to show that it works successfully for various surface shapes and target textures.", "keywords": ["cameras", "genetic algorithms", "image colour analysis", "image registration", "image texture", "optical projectors", "pose estimation", "stereo image processing", "geometric registration", "projection mapping", "radiometric compensation technique", "target textures", "valid viewpoints", "genetic algorithm", "multi-material three-dimensional printer", "full-color 3D printer", "diminishable visual markers", "surface shapes", "projection objects", "marker appearance", "stereo camera system", "marker placement", "infrared cameras", "projection surface", "pose estimation", "visual marker embedding method", "Three-dimensional displays", "Cameras", "Printers", "Visualization", "Shape", "Surface treatment", "Fabrication", "Digital fabrication", "spatial augmented reality", "projection mapping", "diminished reality", "marker-based tracking"], "referenced_by": ["IKEY:8115404", "IKEY:8007248", "IKEY:8172039", "IKEY:8466021", "IKEY:8794641", "IKEY:9010866", "IKEY:9036886", "IKEY:8998378", "IKEY:9284678", "10.2493/jjspe.83.494"], "referencing": ["IKEY:1667678", "IKEY:970539", "IKEY:6549358", "IKEY:7265057", "IKEY:6193074", "IKEY:7165652", "IKEY:5643572", "IKEY:888718", "IKEY:88573", "IKEY:291441", "IKEY:7164353", "IKEY:7014259", "IKEY:1544657", "IKEY:1667678", "IKEY:970539", "IKEY:6549358", "IKEY:7265057", "IKEY:6193074", "IKEY:7165652", "IKEY:5643572", "IKEY:888718", "IKEY:88573", "IKEY:291441", "IKEY:7164353", "IKEY:7014259", "IKEY:1544657", "IKEY:1667678", "IKEY:970539", "IKEY:6549358", "IKEY:7265057", "IKEY:6193074", "IKEY:7165652", "IKEY:5643572", "IKEY:888718", "IKEY:88573", "IKEY:291441", "IKEY:7164353", "IKEY:7014259", "IKEY:1544657", "10.1145/882262.882349", "10.1145/2499474.2499476", "10.1145/1709886.1709897", "10.1145/2945078.2945083", "10.1145/2816795.2818111", "10.1145/2407336.2407368", "10.1145/882262.882349", "10.1145/2499474.2499476", "10.1145/1709886.1709897", "10.1145/2945078.2945083", "10.1145/2816795.2818111", "10.1145/2407336.2407368", "10.1145/882262.882349", "10.1145/2499474.2499476", "10.1145/1709886.1709897", "10.1145/2945078.2945083", "10.1145/2816795.2818111", "10.1145/2407336.2407368", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.1111/j.1467-8659.2008.01175.x", "10.3169/mta.1.343", "10.1007/s10055-010-0159-5", "10.1007/s10055-014-0256-y", "10.1038/35104092", "10.1016/j.jbi.2014.09.001", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.1111/j.1467-8659.2008.01175.x", "10.3169/mta.1.343", "10.1007/s10055-010-0159-5", "10.1007/s10055-014-0256-y", "10.1038/35104092", "10.1016/j.jbi.2014.09.001", "10.1201/b10624", "10.1007/978-3-7091-6242-2_9", "10.1111/j.1467-8659.2008.01175.x", "10.3169/mta.1.343", "10.1007/s10055-010-0159-5", "10.1007/s10055-014-0256-y", "10.1038/35104092", "10.1016/j.jbi.2014.09.001"]}, "10.1109/TVCG.2017.2660488": {"doi": "10.1109/TVCG.2017.2660488", "author": ["S. Lin", "C. C. Morace", "C. Lin", "L. Hsu", "T. Lee"], "title": "Generation of Escher Arts with Dual Perception", "year": "2018", "abstract": "Escher transmutation is a graphic art that smoothly transforms one tile pattern into another tile pattern with dual perception. A classic example is the artwork called Sky and Water, in which a compelling figure-ground arrangement is applied to portray the transmutation of a bird in sky and a fish in water. The shape of a bird is progressively deformed and dissolves into the background while the background gradually reveals the shape of a fish. This paper introduces a system to create a variety of Escher-like transmutations, which includes the algorithms for initializing a tile pattern with dual figure-ground arrangement, for searching for the best matched shape of a user-specified motif from a database, and for transforming the content and shapes of tile patterns using a content-aware warping technique. The proposed system, integrating the graphic techniques of tile initialization, shape matching, and shape warping, allows users to create various Escher-like transmutations with minimal user interaction. Experimental results and conducted user studies demonstrate the feasibility and flexibility of the proposed system in Escher art generation.", "keywords": ["art", "computer graphics", "dual perception", "Escher transmutation", "graphic art", "compelling figure-ground arrangement", "Escher-like transmutations", "dual figure-ground arrangement", "tile initialization", "shape matching", "shape warping", "Escher art generation", "Shape", "Art", "Databases", "Interpolation", "Electronic mail", "Transforms", "Escher art", "shape matching", "content-aware warping"], "referenced_by": ["IKEY:8440073", "IKEY:9104977"], "referencing": ["10.1145/566654.566633", "10.1145/1778765.1778789", "10.1145/1618452.1618502", "10.1145/1360612.1360661", "10.1145/2010324.1964995", "10.1145/1778765.1778788", "10.1145/364338.364371", "10.1145/344779.345022", "10.1145/566654.566633", "10.1145/1778765.1778789", "10.1145/1618452.1618502", "10.1145/1360612.1360661", "10.1145/2010324.1964995", "10.1145/1778765.1778788", "10.1145/364338.364371", "10.1145/344779.345022", "10.1145/566654.566633", "10.1145/1778765.1778789", "10.1145/1618452.1618502", "10.1145/1360612.1360661", "10.1145/2010324.1964995", "10.1145/1778765.1778788", "10.1145/364338.364371", "10.1145/344779.345022", "10.1007/s00373-011-1022-5", "10.1080/17513470903185626", "10.1167/8.16.4", "10.1037/a0012729", "10.1068/p5728", "10.1016/j.patrec.2008.08.013", "10.1007/BFb0014497", "10.1080/10556789908805754", "10.1007/s00373-011-1022-5", "10.1080/17513470903185626", "10.1167/8.16.4", "10.1037/a0012729", "10.1068/p5728", "10.1016/j.patrec.2008.08.013", "10.1007/BFb0014497", "10.1080/10556789908805754", "10.1007/s00373-011-1022-5", "10.1080/17513470903185626", "10.1167/8.16.4", "10.1037/a0012729", "10.1068/p5728", "10.1016/j.patrec.2008.08.013", "10.1007/BFb0014497", "10.1080/10556789908805754"]}, "10.1109/TVCG.2017.2657751": {"doi": "10.1109/TVCG.2017.2657751", "author": ["Y. Huang", "W. Lin", "I. Yeh", "T. Lee"], "title": "Geometric and Textural Blending for 3D Model Stylization", "year": "2018", "abstract": "Stylizing a 3D model with characteristic shapes or appearances is common in product design, particularly in the design of 3D model merchandise, such as souvenirs, toys, furniture, and stylized items. A model stylization approach is proposed in this study. The approach combines base and style models while preserving user-specified shape features of the base model and the attractive features of the style model with limited assistance from a user. The two models are first combined at the topological level. A tree-growing technique is utilized to search for all possible combinations of the two models. Second, the models are combined at textural and geometric levels by employing a morphing technique. Results show that the proposed approach generates various appealing models and allows users to control the diversity of the output models and adjust the blending degree between the base and style models. The results of this work are also experimentally compared with those of a recent work through a user study. The comparison indicates that our results are more appealing, feature-preserving, and reasonable than those of the compared previous study. The proposed system allows product designers to easily explore design possibilities and assists novice users in creating their own stylized models.", "keywords": ["product design", "solid modelling", "trees (mathematics)", "textural blending", "3D model stylization", "product design", "3D model merchandise", "model stylization approach", "user-specified shape features", "stylized models", "morphing technique", "tree-growing technique", "geometric blending", "Solid modeling", "Computational modeling", "Three-dimensional displays", "Shape", "Feature extraction", "Topology", "Geometry", "Computer graphics", "modeling"], "referenced_by": [], "referencing": ["IKEY:4767851", "IKEY:4767851", "IKEY:4767851", "10.1145/2766902", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964930", "10.1145/2185520.2185551", "10.1145/2185520.2185553", "10.1145/2601097.2601102", "10.1145/2461912.2461935", "10.1145/1057432.1057456", "10.1145/2766902", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964930", "10.1145/2185520.2185551", "10.1145/2185520.2185553", "10.1145/2601097.2601102", "10.1145/2461912.2461935", "10.1145/1057432.1057456", "10.1145/2766902", "10.1145/1015706.1015775", "10.1145/1882261.1866205", "10.1145/2010324.1964930", "10.1145/2185520.2185551", "10.1145/2185520.2185553", "10.1145/2601097.2601102", "10.1145/2461912.2461935", "10.1145/1057432.1057456", "10.1007/978-3-319-20886-2_52", "10.1016/j.ijresmar.2012.04.001", "10.1111/cgf.12015", "10.1007/s00371-014-0999-1", "10.1111/cgf.12307", "10.1111/1467-8659.00575", "10.1111/j.1467-8659.2008.01283.x", "10.1016/j.cagd.2011.09.004", "10.1016/S0167-8396(96)00031-3", "10.1007/PL00007211", "10.1016/j.imavis.2006.01.011", "10.1007/s003710050192", "10.1111/j.1467-8659.2012.03042.x", "10.1111/cgf.12039", "10.1142/S0218195905001816", "10.1007/s00371-007-0197-5", "10.1111/1467-8659.00669", "10.1016/j.cag.2010.11.012", "10.1007/978-3-319-20886-2_52", "10.1016/j.ijresmar.2012.04.001", "10.1111/cgf.12015", "10.1007/s00371-014-0999-1", "10.1111/cgf.12307", "10.1111/1467-8659.00575", "10.1111/j.1467-8659.2008.01283.x", "10.1016/j.cagd.2011.09.004", "10.1016/S0167-8396(96)00031-3", "10.1007/PL00007211", "10.1016/j.imavis.2006.01.011", "10.1007/s003710050192", "10.1111/j.1467-8659.2012.03042.x", "10.1111/cgf.12039", "10.1142/S0218195905001816", "10.1007/s00371-007-0197-5", "10.1111/1467-8659.00669", "10.1016/j.cag.2010.11.012", "10.1007/978-3-319-20886-2_52", "10.1016/j.ijresmar.2012.04.001", "10.1111/cgf.12015", "10.1007/s00371-014-0999-1", "10.1111/cgf.12307", "10.1111/1467-8659.00575", "10.1111/j.1467-8659.2008.01283.x", "10.1016/j.cagd.2011.09.004", "10.1016/S0167-8396(96)00031-3", "10.1007/PL00007211", "10.1016/j.imavis.2006.01.011", "10.1007/s003710050192", "10.1111/j.1467-8659.2012.03042.x", "10.1111/cgf.12039", "10.1142/S0218195905001816", "10.1007/s00371-007-0197-5", "10.1111/1467-8659.00669", "10.1016/j.cag.2010.11.012"]}, "10.1109/TVCG.2017.2655523": {"doi": "10.1109/TVCG.2017.2655523", "author": ["J. Wu", "N. Aage", "R. Westermann", "O. Sigmund"], "title": "Infill Optimization for Additive Manufacturing\u2014Approaching Bone-Like Porous Structures", "year": "2018", "abstract": "Porous structures such as trabecular bone are widely seen in nature. These structures are lightweight and exhibit strong mechanical properties. In this paper, we present a method to generate bone-like porous structures as lightweight infill for additive manufacturing. Our method builds upon and extends voxel-wise topology optimization. In particular, for the purpose of generating sparse yet stable structures distributed in the interior of a given shape, we propose upper bounds on the localized material volume in the proximity of each voxel in the design domain. We then aggregate the local per-voxel constraints by their p-norm into an equivalent global constraint, in order to facilitate an efficient optimization process. Implemented on a high-resolution topology optimization framework, our results demonstrate mechanically optimized, detailed porous structures which mimic those found in nature. We further show variants of the optimized structures subject to different design specifications, and we analyze the optimality and robustness of the obtained structures.", "keywords": ["optimisation", "porous materials", "topology", "trabecular bone", "voxel-wise topology optimization", "sparse yet stable structures", "per-voxel constraints", "efficient optimization process", "high-resolution topology optimization framework", "optimized structures", "mechanical properties", "porous structures", "additive manufacturing-approaching bone-like porous structures", "Optimization", "Bones", "Topology", "Solids", "Three-dimensional printing", "Shape", "Mechanical factors", "Infill", "additive manufacturing", "trabecular bone", "porous structures", "topology optimization"], "referenced_by": ["IKEY:8590761", "IKEY:8930530", "IKEY:8703138", "IKEY:8823060", "10.1007/978-3-319-68195-5_51", "10.1111/cgf.13268", "10.1016/j.cag.2017.07.008", "10.1007/s00158-017-1786-1", "10.1007/s00158-017-1780-7"], "referencing": ["IKEY:5290754", "IKEY:7332965", "IKEY:5290754", "IKEY:7332965", "IKEY:5290754", "IKEY:7332965", "10.1145/2659467.2675050", "10.1145/566654.566580", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/2897824.2925958", "10.1145/2766926", "10.1145/2766937", "10.1145/2897824.2925922", "10.1145/37401.37422", "10.1145/218380.218473", "10.1145/2659467.2675050", "10.1145/566654.566580", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/2897824.2925958", "10.1145/2766926", "10.1145/2766937", "10.1145/2897824.2925922", "10.1145/37401.37422", "10.1145/218380.218473", "10.1145/2659467.2675050", "10.1145/566654.566580", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/2897824.2925958", "10.1145/2766926", "10.1145/2766937", "10.1145/2897824.2925922", "10.1145/37401.37422", "10.1145/218380.218473", "10.1055/s-0028-1144106", "10.1016/j.pmatsci.2007.05.002", "10.1038/nmat4089", "10.1016/0045-7825(88)90086-2", "10.1016/j.cad.2015.04.001", "10.1016/j.cad.2016.07.006", "10.1016/j.mechmat.2013.09.018", "10.1016/j.cma.2015.02.028", "10.1007/s00158-013-0978-6", "10.1007/s00158-013-0956-z", "10.1007/s001580050176", "10.1002/nme.694", "10.1007/s00419-015-1106-4", "10.1002/nme.1064", "10.1007/s00158-008-0250-7", "10.1016/j.commatsci.2014.12.017", "10.1038/35015116", "10.1016/j.jbiomech.2009.01.020", "10.1016/j.biomaterials.2016.01.012", "10.1137/110850335", "10.1007/s00158-010-0602-y", "10.1007/BF01743693", "10.1007/s00158-010-0594-7", "10.1002/nme.1620240207", "10.1007/s00158-012-0869-2", "10.1007/s00158-016-1420-7", "10.1007/s10439-012-0714-1", "10.1007/s00158-013-1001-y", "10.1007/s00158-006-0087-x", "10.1007/BF01742459", "10.1016/j.cma.2011.08.006", "10.1007/s00158-016-1551-x", "10.1002/nme.5461", "10.1055/s-0028-1144106", "10.1016/j.pmatsci.2007.05.002", "10.1038/nmat4089", "10.1016/0045-7825(88)90086-2", "10.1016/j.cad.2015.04.001", "10.1016/j.cad.2016.07.006", "10.1016/j.mechmat.2013.09.018", "10.1016/j.cma.2015.02.028", "10.1007/s00158-013-0978-6", "10.1007/s00158-013-0956-z", "10.1007/s001580050176", "10.1002/nme.694", "10.1007/s00419-015-1106-4", "10.1002/nme.1064", "10.1007/s00158-008-0250-7", "10.1016/j.commatsci.2014.12.017", "10.1038/35015116", "10.1016/j.jbiomech.2009.01.020", "10.1016/j.biomaterials.2016.01.012", "10.1137/110850335", "10.1007/s00158-010-0602-y", "10.1007/BF01743693", "10.1007/s00158-010-0594-7", "10.1002/nme.1620240207", "10.1007/s00158-012-0869-2", "10.1007/s00158-016-1420-7", "10.1007/s10439-012-0714-1", "10.1007/s00158-013-1001-y", "10.1007/s00158-006-0087-x", "10.1007/BF01742459", "10.1016/j.cma.2011.08.006", "10.1007/s00158-016-1551-x", "10.1002/nme.5461", "10.1055/s-0028-1144106", "10.1016/j.pmatsci.2007.05.002", "10.1038/nmat4089", "10.1016/0045-7825(88)90086-2", "10.1016/j.cad.2015.04.001", "10.1016/j.cad.2016.07.006", "10.1016/j.mechmat.2013.09.018", "10.1016/j.cma.2015.02.028", "10.1007/s00158-013-0978-6", "10.1007/s00158-013-0956-z", "10.1007/s001580050176", "10.1002/nme.694", "10.1007/s00419-015-1106-4", "10.1002/nme.1064", "10.1007/s00158-008-0250-7", "10.1016/j.commatsci.2014.12.017", "10.1038/35015116", "10.1016/j.jbiomech.2009.01.020", "10.1016/j.biomaterials.2016.01.012", "10.1137/110850335", "10.1007/s00158-010-0602-y", "10.1007/BF01743693", "10.1007/s00158-010-0594-7", "10.1002/nme.1620240207", "10.1007/s00158-012-0869-2", "10.1007/s00158-016-1420-7", "10.1007/s10439-012-0714-1", "10.1007/s00158-013-1001-y", "10.1007/s00158-006-0087-x", "10.1007/BF01742459", "10.1016/j.cma.2011.08.006", "10.1007/s00158-016-1551-x", "10.1002/nme.5461"]}, "10.1109/TVCG.2017.2653106": {"doi": "10.1109/TVCG.2017.2653106", "author": ["Y. Wang", "F. Han", "L. Zhu", "O. Deussen", "B. Chen"], "title": "Line Graph or Scatter Plot? Automatic Selection of Methods for Visualizing Trends in Time Series", "year": "2018", "abstract": "Line graphs are usually considered to be the best choice for visualizing time series data, whereas sometimes also scatter plots are used for showing main trends. So far there are no guidelines that indicate which of these visualization methods better display trends in time series for a given canvas. Assuming that the main information in a time series is its overall trend, we propose an algorithm that automatically picks the visualization method that reveals this trend best. This is achieved by measuring the visual consistency between the trend curve represented by a LOESS fit and the trend described by a scatter plot or a line graph. To measure the consistency between our algorithm and user choices, we performed an empirical study with a series of controlled experiments that show a large correspondence. In a factor analysis we furthermore demonstrate that various visual and data factors have effects on the preference for a certain type of visualization.", "keywords": ["data visualisation", "time series", "line graph", "scatter plot", "time series data", "visualization method", "visual consistency", "trend curve", "visual data factors", "LOESS fit", "Market research", "Time series analysis", "Data visualization", "Visualization", "Bandwidth", "Kernel", "Estimation", "Line graph", "scatter plot", "time series", "trend"], "referenced_by": ["IKEY:7969602", "IKEY:8440843", "IKEY:8805429"], "referencing": ["IKEY:6634156", "IKEY:4658136", "IKEY:5613429", "IKEY:801851", "IKEY:963273", "IKEY:5613426", "IKEY:5429613", "IKEY:6065009", "IKEY:7192735", "IKEY:6876021", "IKEY:4015420", "IKEY:6064993", "IKEY:6327267", "IKEY:4376151", "IKEY:5613435", "IKEY:5742384", "IKEY:6484064", "IKEY:4135678", "IKEY:6875950", "IKEY:6634178", "IKEY:6634156", "IKEY:4658136", "IKEY:5613429", "IKEY:801851", "IKEY:963273", "IKEY:5613426", "IKEY:5429613", "IKEY:6065009", "IKEY:7192735", "IKEY:6876021", "IKEY:4015420", "IKEY:6064993", "IKEY:6327267", "IKEY:4376151", "IKEY:5613435", "IKEY:5742384", "IKEY:6484064", "IKEY:4135678", "IKEY:6875950", "IKEY:6634178", "IKEY:6634156", "IKEY:4658136", "IKEY:5613429", "IKEY:801851", "IKEY:963273", "IKEY:5613426", "IKEY:5429613", "IKEY:6065009", "IKEY:7192735", "IKEY:6876021", "IKEY:4015420", "IKEY:6064993", "IKEY:6327267", "IKEY:4376151", "IKEY:5613435", "IKEY:5742384", "IKEY:6484064", "IKEY:4135678", "IKEY:6875950", "IKEY:6634178", "10.1145/1014052.1014104", "10.1145/1133265.1133348", "10.1145/1753326.1753357", "10.1145/1518701.1518897", "10.1145/2470654.2466443", "10.1145/2858036.2858300", "10.1145/1014052.1014104", "10.1145/1133265.1133348", "10.1145/1753326.1753357", "10.1145/1518701.1518897", "10.1145/2470654.2466443", "10.1145/2858036.2858300", "10.1145/1014052.1014104", "10.1145/1133265.1133348", "10.1145/1753326.1753357", "10.1145/1518701.1518897", "10.1145/2470654.2466443", "10.1145/2858036.2858300", "10.1080/01621459.1979.10481038", "10.1007/978-1-4899-3324-9", "10.1023/A:1026543900054", "10.1080/01621459.1926.10502165", "10.1080/01621459.1927.10502976", "10.2307/2288400", "10.1111/j.1467-8659.2009.01440.x", "10.1111/j.1467-8659.2011.01912.x", "10.1080/01621459.1976.10481532", "10.1080/01621459.1979.10481038", "10.1007/978-1-4899-3324-9", "10.1023/A:1026543900054", "10.1080/01621459.1926.10502165", "10.1080/01621459.1927.10502976", "10.2307/2288400", "10.1111/j.1467-8659.2009.01440.x", "10.1111/j.1467-8659.2011.01912.x", "10.1080/01621459.1976.10481532", "10.1080/01621459.1979.10481038", "10.1007/978-1-4899-3324-9", "10.1023/A:1026543900054", "10.1080/01621459.1926.10502165", "10.1080/01621459.1927.10502976", "10.2307/2288400", "10.1111/j.1467-8659.2009.01440.x", "10.1111/j.1467-8659.2011.01912.x", "10.1080/01621459.1976.10481532"]}, "10.1109/TVCG.2017.2656897": {"doi": "10.1109/TVCG.2017.2656897", "author": ["T. Zirr", "C. Dachsbacher"], "title": "Memory-Efficient On-the-Fly Voxelization and Rendering of Particle Data", "year": "2018", "abstract": "In this paper we present a novel GPU-friendly real-time voxelization technique for rendering homogeneous media that is defined by particles, e.g., fluids obtained from particle-based simulations such as Smoothed Particle Hydrodynamics (SPH). Our method computes view-adaptive binary voxelizations with on-the-fly compression of a tiled perspective voxel grid, achieving higher resolutions than previous approaches. It allows for interactive generation of realistic images, enabling advanced rendering techniques such as ray casting-based refraction and reflection, light scattering and absorption, and ambient occlusion. In contrast to previous methods, it does not rely on preprocessing such as expensive, and often coarse, scalar field conversion or mesh generation steps. Our method directly takes unsorted particle data as input. It can be further accelerated by identifying fully populated simulation cells during simulation. The extracted surface can be filtered to achieve smooth surface appearance. Finally, we provide a new scheme for accelerated ray casting inside the voxelization.", "keywords": ["image processing", "mesh generation", "ray tracing", "realistic images", "rendering (computer graphics)", "tiled perspective voxel grid", "interactive generation", "realistic images", "advanced rendering techniques", "ray casting", "refraction", "reflection", "light scattering", "absorption", "ambient occlusion", "scalar field conversion", "unsorted particle data", "fully populated simulation cells", "smooth surface appearance", "accelerated ray", "homogeneous media", "particle-based simulations", "Smoothed Particle Hydrodynamics", "SPH", "method computes", "view-adaptive binary voxelizations", "on-the-fly compression", "memory-efficient on-the-fly voxelization", "particle data rendering", "GPU-friendly real-time voxelization technique", "mesh generation", "Rendering (computer graphics)", "Surface morphology", "Casting", "Computational modeling", "Acceleration", "Graphics processing units", "Data visualization", "Surface extraction", "interactive particle visualization", "ray tracing"], "referenced_by": ["10.1016/j.gmod.2017.06.004"], "referencing": ["IKEY:5613495", "IKEY:4293019", "IKEY:6691717", "IKEY:5613495", "IKEY:4293019", "IKEY:6691717", "IKEY:5613495", "IKEY:4293019", "IKEY:6691717", "10.1145/37402.37422", "10.1145/1073204.1073298", "10.1145/1276377.1276437", "10.1145/2421636.2421641", "10.1145/1507149.1507164", "10.1145/357306.357310", "10.1145/1507149.1507166", "10.1145/2790060.2790069", "10.1145/1597990.1598035", "10.1145/1281500.1281671", "10.1145/37402.37422", "10.1145/1073204.1073298", "10.1145/1276377.1276437", "10.1145/2421636.2421641", "10.1145/1507149.1507164", "10.1145/357306.357310", "10.1145/1507149.1507166", "10.1145/2790060.2790069", "10.1145/1597990.1598035", "10.1145/1281500.1281671", "10.1145/37402.37422", "10.1145/1073204.1073298", "10.1145/1276377.1276437", "10.1145/2421636.2421641", "10.1145/1507149.1507164", "10.1145/357306.357310", "10.1145/1507149.1507166", "10.1145/2790060.2790069", "10.1145/1597990.1598035", "10.1145/1281500.1281671", "10.1111/j.1467-8659.2010.01832.x", "10.1002/cav.162", "10.1111/j.1467-8659.2012.02096.x", "10.1111/j.1467-8659.2008.01132.x", "10.1111/1467-8659.00178", "10.1111/1467-8659.00687", "10.1111/j.1467-8659.2012.03062.x", "10.1111/j.1467-8659.2010.01734.x", "10.1111/j.1467-8659.2009.01698.x", "10.1111/j.1467-8659.2010.01737.x", "10.1111/j.1467-8659.2010.01724.x", "10.1111/j.1467-8659.2010.01832.x", "10.1002/cav.162", "10.1111/j.1467-8659.2012.02096.x", "10.1111/j.1467-8659.2008.01132.x", "10.1111/1467-8659.00178", "10.1111/1467-8659.00687", "10.1111/j.1467-8659.2012.03062.x", "10.1111/j.1467-8659.2010.01734.x", "10.1111/j.1467-8659.2009.01698.x", "10.1111/j.1467-8659.2010.01737.x", "10.1111/j.1467-8659.2010.01724.x", "10.1111/j.1467-8659.2010.01832.x", "10.1002/cav.162", "10.1111/j.1467-8659.2012.02096.x", "10.1111/j.1467-8659.2008.01132.x", "10.1111/1467-8659.00178", "10.1111/1467-8659.00687", "10.1111/j.1467-8659.2012.03062.x", "10.1111/j.1467-8659.2010.01734.x", "10.1111/j.1467-8659.2009.01698.x", "10.1111/j.1467-8659.2010.01737.x", "10.1111/j.1467-8659.2010.01724.x"]}, "10.1109/TVCG.2017.2648790": {"doi": "10.1109/TVCG.2017.2648790", "author": ["Q. Chao", "Z. Deng", "J. Ren", "Q. Ye", "X. Jin"], "title": "Realistic Data-Driven Traffic Flow Animation Using Texture Synthesis", "year": "2018", "abstract": "We present a novel data-driven approach to populate virtual road networks with realistic traffic flows. Specifically, given a limited set of vehicle trajectories as the input samples, our approach first synthesizes a large set of vehicle trajectories. By taking the spatio-temporal information of traffic flows as a 2D texture, the generation of new traffic flows can be formulated as a texture synthesis process, which is solved by minimizing a newly developed traffic texture energy. The synthesized output captures the spatio-temporal dynamics of the input traffic flows, and the vehicle interactions in it strictly follow traffic rules. After that, we position the synthesized vehicle trajectory data to virtual road networks using a cage-based registration scheme, where a few traffic-specific constraints are enforced to maintain each vehicle's original spatial location and synchronize its motion in concert with its neighboring vehicles. Our approach is intuitive to control and scalable to the complexity of virtual road networks. We validated our approach through many experiments and paired comparison user studies.", "keywords": ["image registration", "image texture", "road traffic", "traffic engineering computing", "virtual reality", "traffic texture energy", "texture synthesis process", "spatio-temporal information", "vehicle trajectories", "virtual road networks", "traffic flow animation", "Vehicles", "Roads", "Trajectory", "Animation", "Two dimensional displays", "Solid modeling", "Virtual environments", "Traffic flow animation", "crowd simulation", "data-driven method", "texture synthesis"], "referenced_by": ["IKEY:8445690", "IKEY:8481568", "IKEY:8600335", "IKEY:9216472"], "referencing": ["IKEY:4135665", "IKEY:6893032", "IKEY:4135665", "IKEY:6893032", "IKEY:4135665", "IKEY:6893032", "10.1145/2601097.2601170", "10.1145/2461912.2462021", "10.1145/1399504.1360679", "10.1145/1576246.1531385", "10.1145/1833349.1778770", "10.1145/566654.566646", "10.1145/1073204.1073263", "10.1145/1179849.1179929", "10.1145/344779.344987", "10.1145/882262.882267", "10.1145/2421636.2421639", "10.1145/344779.345009", "10.1145/1141911.1141921", "10.1145/1073204.1073323", "10.1145/2070781.2024169", "10.1145/2601097.2601170", "10.1145/2461912.2462021", "10.1145/1399504.1360679", "10.1145/1576246.1531385", "10.1145/1833349.1778770", "10.1145/566654.566646", "10.1145/1073204.1073263", "10.1145/1179849.1179929", "10.1145/344779.344987", "10.1145/882262.882267", "10.1145/2421636.2421639", "10.1145/344779.345009", "10.1145/1141911.1141921", "10.1145/1073204.1073323", "10.1145/2070781.2024169", "10.1145/2601097.2601170", "10.1145/2461912.2462021", "10.1145/1399504.1360679", "10.1145/1576246.1531385", "10.1145/1833349.1778770", "10.1145/566654.566646", "10.1145/1073204.1073263", "10.1145/1179849.1179929", "10.1145/344779.344987", "10.1145/882262.882267", "10.1145/2421636.2421639", "10.1145/344779.345009", "10.1145/1141911.1141921", "10.1145/1073204.1073323", "10.1145/2070781.2024169", "10.1098/rspa.1955.0089", "10.1137/S0036139997332099", "10.1016/S0191-2615(00)00050-3", "10.1111/j.1467-8659.2009.01613.x", "10.1524/auto.2001.49.11.478", "10.1016/j.gmod.2012.04.002", "10.1111/cgf.12316", "10.1002/cav.1540", "10.3141/1999-10", "10.1016/j.gmod.2013.07.003", "10.1111/j.1467-8659.2004.00753.x", "10.1016/S0167-8396(03)00002-5", "10.3141/2088-10", "10.1093/biomet/31.3-4.324", "10.1098/rspa.1955.0089", "10.1137/S0036139997332099", "10.1016/S0191-2615(00)00050-3", "10.1111/j.1467-8659.2009.01613.x", "10.1524/auto.2001.49.11.478", "10.1016/j.gmod.2012.04.002", "10.1111/cgf.12316", "10.1002/cav.1540", "10.3141/1999-10", "10.1016/j.gmod.2013.07.003", "10.1111/j.1467-8659.2004.00753.x", "10.1016/S0167-8396(03)00002-5", "10.3141/2088-10", "10.1093/biomet/31.3-4.324", "10.1098/rspa.1955.0089", "10.1137/S0036139997332099", "10.1016/S0191-2615(00)00050-3", "10.1111/j.1467-8659.2009.01613.x", "10.1524/auto.2001.49.11.478", "10.1016/j.gmod.2012.04.002", "10.1111/cgf.12316", "10.1002/cav.1540", "10.3141/1999-10", "10.1016/j.gmod.2013.07.003", "10.1111/j.1467-8659.2004.00753.x", "10.1016/S0167-8396(03)00002-5", "10.3141/2088-10", "10.1093/biomet/31.3-4.324"]}, "10.1109/TVCG.2016.2642958": {"doi": "10.1109/TVCG.2016.2642958", "author": ["Q. Yin", "S. Liu"], "title": "Sounding Solid Combustibles: Non-Premixed Flame Sound Synthesis for Different Solid Combustibles", "year": "2018", "abstract": "With the rapidly growing VR industry, in recent years, more and more attention has been paid for fire sound synthesis. However, previous methods usually ignore the influences of the different solid combustibles, leading to unrealistic sounding results. This paper proposes SSC (sounding solid combustibles), which is a new recording-driven non-premixed flame sound synthesis framework accounting for different solid combustibles. SSC consists of three components: combustion noise, vortex noise and popping sounds. The popping sounds are the keys to distinguish the differences of solid combustibles. To improve the quality of fire sound, we extract the features of popping sounds from the real fire sound examples based on modified Empirical Mode Decomposition (EMD) method. Unlike previous methods, we take both direct combustion noise and vortex noise into account because the fire model is non-premixed flame. In our method, we also greatly resolve the synchronization problem during blending the three components of SSC. Due to the introduction of the popping sounds, it is easy to distinguish the fire sounds of different solid combustibles by our method, with great potential in practical applications such as games, VR system, etc. Various experiments and comparisons are presented to validate our method.", "keywords": ["chemically reactive flow", "combustion", "flames", "noise", "vortices", "sounding solid combustibles", "vortex noise", "popping sounds", "direct combustion noise", "fire sound synthesis", "nonpremixed flame sound synthesis", "modified empirical mode decomposition method", "fire model", "Combustion", "Solids", "Synchronization", "Mathematical model", "Animation", "Heating", "Bandwidth", "Non-premixed fire sound", "solid combustibles", "direct combustion noise", "vortex noise", "popping sounds"], "referenced_by": [], "referencing": ["IKEY:1016697", "IKEY:1016697", "IKEY:1016697", "10.1145/2010324.1964979", "10.1145/1531326.1531343", "10.1145/882262.882339", "10.1145/383259.383296", "10.1145/1399504.1360647", "10.1145/54852.378514", "10.1145/1805964.1805965", "10.1145/566570.566643", "10.1145/2421636.2421637", "10.1145/2601097.2601178", "10.1145/344779.345009", "10.1145/2010324.1964979", "10.1145/1531326.1531343", "10.1145/882262.882339", "10.1145/383259.383296", "10.1145/1399504.1360647", "10.1145/54852.378514", "10.1145/1805964.1805965", "10.1145/566570.566643", "10.1145/2421636.2421637", "10.1145/2601097.2601178", "10.1145/344779.345009", "10.1145/2010324.1964979", "10.1145/1531326.1531343", "10.1145/882262.882339", "10.1145/383259.383296", "10.1145/1399504.1360647", "10.1145/54852.378514", "10.1145/1805964.1805965", "10.1145/566570.566643", "10.1145/2421636.2421637", "10.1145/2601097.2601178", "10.1145/344779.345009", "10.1007/978-1-4471-0399-8", "10.1111/j.1467-8659.2004.00785.x", "10.1098/rspa.1998.0193", "10.1007/s10055-015-0271-7", "10.1121/1.1918931", "10.1017/S0022112009990681", "10.1007/978-3-642-02038-4", "10.2307/3680788", "10.1007/978-1-4471-0399-8", "10.1111/j.1467-8659.2004.00785.x", "10.1098/rspa.1998.0193", "10.1007/s10055-015-0271-7", "10.1121/1.1918931", "10.1017/S0022112009990681", "10.1007/978-3-642-02038-4", "10.2307/3680788", "10.1007/978-1-4471-0399-8", "10.1111/j.1467-8659.2004.00785.x", "10.1098/rspa.1998.0193", "10.1007/s10055-015-0271-7", "10.1121/1.1918931", "10.1017/S0022112009990681", "10.1007/978-3-642-02038-4", "10.2307/3680788"]}, "10.1109/TVCG.2017.2657766": {"doi": "10.1109/TVCG.2017.2657766", "author": ["L. Yang", "Q. Yan", "Y. Fu", "C. Xiao"], "title": "Surface Reconstruction via Fusing Sparse-Sequence of Depth Images", "year": "2018", "abstract": "Handheld scanning using commodity depth cameras provides a flexible and low-cost manner to get 3D models. The existing methods scan a target by densely fusing all the captured depth images, yet most frames are redundant. The jittering frames inevitably embedded in handheld scanning process will cause feature blurring on the reconstructed model and even trigger the scan failure (i.e., camera tracking losing). To address these problems, in this paper, we propose a novel sparse-sequence fusion (SSF) algorithm for handheld scanning using commodity depth cameras. It first extracts related measurements for analyzing camera motion. Then based on these measurements, we progressively construct a supporting subset for the captured depth image sequence to decrease the data redundancy and the interference from jittering frames. Since SSF will reveal the intrinsic heavy noise of the original depth images, our method introduces a refinement process to eliminate the raw noise and recover geometric features for the depth images selected into the supporting subset. We finally obtain the fused result by integrating the refined depth images into the truncated signed distance field (TSDF) of the target. Multiple comparison experiments are conducted and the results verify the feasibility and validity of SSF for handheld scanning with a commodity depth camera.", "keywords": ["cameras", "feature extraction", "geometry", "image denoising", "image fusion", "image motion analysis", "image reconstruction", "image representation", "image resolution", "image sequences", "object tracking", "surface reconstruction", "commodity depth camera", "jittering frames", "handheld scanning process", "camera tracking", "SSF", "camera motion", "TSDF", "truncated signed distance field", "geometric features recovery", "raw noise elimination", "refinement process", "depth images capture", "sparse-sequence fusion algorithm", "depth image sequence", "Cameras", "Image reconstruction", "Surface reconstruction", "Three-dimensional displays", "Image sequences", "Image segmentation", "Solid modeling", "Depth image refinement", "handheld scanning", "sparse-sequence fusion", "surface reconstruction", "supporting subset"], "referenced_by": ["IKEY:8099507", "IKEY:8891701", "IKEY:9156569"], "referencing": ["IKEY:6165146", "IKEY:6162880", "IKEY:121791", "IKEY:223166", "IKEY:6827958", "IKEY:4767965", "IKEY:6165146", "IKEY:6162880", "IKEY:121791", "IKEY:223166", "IKEY:6827958", "IKEY:4767965", "IKEY:6165146", "IKEY:6162880", "IKEY:121791", "IKEY:223166", "IKEY:6827958", "IKEY:4767965", "10.1145/1667239.1667247", "10.1145/345370.345399", "10.1145/2461912.2461919", "10.1145/2047196.2047270", "10.1145/2461912.2461940", "10.1145/237170.237269", "10.1145/192161.192241", "10.1145/2661229.2661232", "10.1145/2816795.2818075", "10.1145/2768821", "10.1145/2816795.2818068", "10.1145/1667239.1667247", "10.1145/345370.345399", "10.1145/2461912.2461919", "10.1145/2047196.2047270", "10.1145/2461912.2461940", "10.1145/237170.237269", "10.1145/192161.192241", "10.1145/2661229.2661232", "10.1145/2816795.2818075", "10.1145/2768821", "10.1145/2816795.2818068", "10.1145/1667239.1667247", "10.1145/345370.345399", "10.1145/2461912.2461919", "10.1145/2047196.2047270", "10.1145/2461912.2461940", "10.1145/237170.237269", "10.1145/192161.192241", "10.1145/2661229.2661232", "10.1145/2816795.2818075", "10.1145/2768821", "10.1145/2816795.2818068", "10.5244/C.26.112", "10.1016/0262-8856(92)90066-C", "10.1117/12.2005094", "10.1016/j.gmod.2014.03.010", "10.1111/j.1467-8659.2009.01388.x", "10.5244/C.26.112", "10.1016/0262-8856(92)90066-C", "10.1117/12.2005094", "10.1016/j.gmod.2014.03.010", "10.1111/j.1467-8659.2009.01388.x", "10.5244/C.26.112", "10.1016/0262-8856(92)90066-C", "10.1117/12.2005094", "10.1016/j.gmod.2014.03.010", "10.1111/j.1467-8659.2009.01388.x"]}, "10.1109/TVCG.2017.2672987": {"doi": "10.1109/TVCG.2017.2672987", "author": ["B. Wang", "K. Mueller"], "title": "The Subspace Voyager: Exploring High-Dimensional Data along a Continuum of Salient 3D Subspaces", "year": "2018", "abstract": "Analyzing high-dimensional data and finding hidden patterns is a difficult problem and has attracted numerous research efforts. Automated methods can be useful to some extent but bringing the data analyst into the loop via interactive visual tools can help the discovery process tremendously. An inherent problem in this effort is that humans lack the mental capacity to truly understand spaces exceeding three spatial dimensions. To keep within this limitation, we describe a framework that decomposes a high-dimensional data space into a continuum of generalized 3D subspaces. Analysts can then explore these 3D subspaces individually via the familiar trackball interface while using additional facilities to smoothly transition to adjacent subspaces for expanded space comprehension. Since the number of such subspaces suffers from combinatorial explosion, we provide a set of data-driven subspace selection and navigation tools which can guide users to interesting subspaces and views. A subspace trail map allows users to manage the explored subspaces, keep their bearings, and return to interesting subspaces and views. Both trackball and trail map are each embedded into a word cloud of attribute labels which aid in navigation. We demonstrate our system via several use cases in a diverse set of application areas-cluster analysis and refinement, information discovery, and supervised training of classifiers. We also report on a user study that evaluates the usability of the various interactions our system provides.", "keywords": ["data analysis", "data mining", "data visualisation", "learning (artificial intelligence)", "pattern classification", "pattern clustering", "visualization", "supervised classifier training", "information discovery", "cluster analysis", "salient 3D subspace continuum", "high-dimensional data analysis", "interactive visual tools", "data analyst", "subspace voyager", "subspace trail map", "data-driven subspace selection", "expanded space comprehension", "Three-dimensional displays", "Visualization", "Space exploration", "Two dimensional displays", "Navigation", "Cognition", "Layout", "High-dimensional data", "subspace navigation", "trackball", "PCA", "ant colony optimization1"], "referenced_by": ["IKEY:8440838", "IKEY:8802486", "IKEY:9146191"], "referencing": ["IKEY:4658123", "IKEY:6832613", "IKEY:1672644", "IKEY:6634146", "IKEY:4376146", "IKEY:7192671", "IKEY:6560006", "IKEY:7192684", "IKEY:6155716", "IKEY:7061477", "IKEY:7536217", "IKEY:5620902", "IKEY:6297588", "IKEY:6634155", "IKEY:6183569", "IKEY:4658123", "IKEY:6832613", "IKEY:1672644", "IKEY:6634146", "IKEY:4376146", "IKEY:7192671", "IKEY:6560006", "IKEY:7192684", "IKEY:6155716", "IKEY:7061477", "IKEY:7536217", "IKEY:5620902", "IKEY:6297588", "IKEY:6634155", "IKEY:6183569", "IKEY:4658123", "IKEY:6832613", "IKEY:1672644", "IKEY:6634146", "IKEY:4376146", "IKEY:7192671", "IKEY:6560006", "IKEY:7192684", "IKEY:6155716", "IKEY:7061477", "IKEY:7536217", "IKEY:5620902", "IKEY:6297588", "IKEY:6634155", "IKEY:6183569", "10.1145/1345448.1345451", "10.1145/1168149.1168154", "10.1145/502512.502530", "10.1145/1497577.1497578", "10.1145/1345448.1345451", "10.1145/1168149.1168154", "10.1145/502512.502530", "10.1145/1497577.1497578", "10.1145/1345448.1345451", "10.1145/1168149.1168154", "10.1145/502512.502530", "10.1145/1497577.1497578", "10.1137/0906011", "10.1093/biomet/58.3.453", "10.1080/00949657508810123", "10.1111/cgf.12639", "10.1080/10867651.2002.10487551", "10.1117/12.2000701", "10.1111/j.1467-8659.2009.01467.x", "10.1016/j.ejor.2006.06.046", "10.1016/S0167-9473(02)00286-4", "10.1137/0906011", "10.1093/biomet/58.3.453", "10.1080/00949657508810123", "10.1111/cgf.12639", "10.1080/10867651.2002.10487551", "10.1117/12.2000701", "10.1111/j.1467-8659.2009.01467.x", "10.1016/j.ejor.2006.06.046", "10.1016/S0167-9473(02)00286-4", "10.1137/0906011", "10.1093/biomet/58.3.453", "10.1080/00949657508810123", "10.1111/cgf.12639", "10.1080/10867651.2002.10487551", "10.1117/12.2000701", "10.1111/j.1467-8659.2009.01467.x", "10.1016/j.ejor.2006.06.046", "10.1016/S0167-9473(02)00286-4"]}, "10.1109/TVCG.2017.2779098": {"doi": "10.1109/TVCG.2017.2779098", "author": [""], "title": "2017 Index IEEE Transactions on Visualization and Computer Graphics Vol. 23", "year": "2018", "abstract": "Presents the 2017 subject/author index for this publication.", "keywords": [""], "referenced_by": [], "referencing": []}}