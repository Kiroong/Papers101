{"10.1109/TVCG.2017.2744878": {"doi": "10.1109/TVCG.2017.2744878", "author": ["K. Wongsuphasawat", "D. Smilkov", "J. Wexler", "J. Wilson", "D. Man\u00e9", "D. Fritz", "D. Krishnan", "F. B. Vi\u00e9gas", "M. Wattenberg"], "title": "Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow", "year": "2018", "abstract": "We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.", "keywords": ["data flow graphs", "data visualisation", "graph theory", "learning (artificial intelligence)", "deep learning models", "TensorFlow Graph Visualizer", "TensorFlow machine intelligence platform", "complex machine learning architectures", "graph transformations", "standard layout techniques", "legible interactive diagram", "decouple noncritical nodes", "clustered graph", "hierarchical structure", "nested structure", "stable cluster expansion", "responsive cluster expansion", "dataflow graphs", "user feedback", "Visualization", "Layout", "Machine learning", "Computational modeling", "Tools", "Neural networks", "Standards", "Neural Network", "Graph Visualization", "Dataflow Graph", "Clustered Graph"], "referenced_by": ["10.1109/MCG.2018.042731661", "10.1109/ICACCE.2018.8441674", "10.1109/CBI.2018.10059", "10.1109/TVCG.2018.2864499", "10.1109/TVCG.2018.2865230", "10.1109/TVCG.2018.2864812", "10.1109/TVCG.2018.2864838", "10.1109/TVCG.2018.2865027", "10.1109/TVCG.2018.2864500", "10.1109/EI2.2018.8582200", "10.1109/MCG.2018.2878902", "10.1109/BigData.2018.8622443", "10.1109/I-SPAN.2018.00026", "10.1109/TVCG.2019.2903943", "10.1109/VIZSEC.2018.8709231", "10.1109/APSEC.2018.00079", "10.1109/MCG.2018.2881501", "10.1109/TVCG.2018.2843369", "10.1109/INISTA.2019.8778406", "10.1109/VAST.2018.8802509", "10.1109/ICPC.2019.00028", "10.1109/TII.2019.2910416", "10.1109/ACCESS.2019.2938537", "10.1109/VLHCC.2019.8818751", "10.1109/QRS.2019.00059", "10.1109/TIE.2019.2903770", "10.1109/TVCG.2019.2934591", "10.1109/TVCG.2019.2934595", "10.1109/TVCG.2019.2934261", "10.1109/TVCG.2019.2934629"], "referencing": []}, "10.1109/TVCG.2017.2745181": {"doi": "10.1109/TVCG.2017.2745181", "author": ["M. Stein", "H. Janetzko", "A. Lamprecht", "T. Breitkreutz", "P. Zimmermann", "B. Goldl\u00fccke", "T. Schreck", "G. Andrienko", "M. Grossniklaus", "D. A. Keim"], "title": "Bring It to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis", "year": "2018", "abstract": "Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team. Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.", "keywords": ["behavioural sciences computing", "computer vision", "data analysis", "data visualisation", "image motion analysis", "sport", "video recording", "video signal processing", "team sport analysts", "movement data", "professional team sport", "team behavior", "coached team", "team videos", "Information Visualization", "video analysis", "abstract data model", "abstract visual mappings", "visual analytics system", "team sport video recordings", "abstract visualization", "trajectory data", "video input", "movement analysis techniques", "relevant team sport", "player analysis", "soccer analysis", "visualization modalities", "computer vision techniques", "Data visualization", "Trajectory", "Visualization", "Feature extraction", "Video recording", "Image color analysis", "Computer vision", "visual analytics", "sport analytics", "immersive analytics"], "referenced_by": ["IKEY:8440804", "IKEY:8564146", "IKEY:8622592", "IKEY:8304678", "IKEY:8716107", "IKEY:8812067", "IKEY:8807235", "IKEY:8805439", "IKEY:8911315", "IKEY:8585103", "IKEY:9258032"], "referencing": []}, "10.1109/TVCG.2017.2744419": {"doi": "10.1109/TVCG.2017.2744419", "author": ["N. Cao", "C. Lin", "Q. Zhu", "Y. Lin", "X. Teng", "X. Wen"], "title": "Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data", "year": "2018", "abstract": "The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \u201csmart city\u201d as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.", "keywords": ["data analysis", "data mining", "data visualisation", "interactive systems", "pattern clustering", "social aspects of automation", "spatiotemporal phenomena", "tensors", "visual anomaly monitoring", "smart city", "anomalous patterns ranking", "online interactivity", "tensor-based anomaly analysis algorithm", "spatiotemporal data", "interactive anomaly detection", "data visualization", "streaming spatiotemporal data", "visual anomaly detection", "interpretable data summaries", "interaction design", "online monitoring", "streaming data source", "Voila", "unified visual interactive system", "spatiotemporal context", "multifaceted context", "heterogeneous data", "dynamic data", "abnormal patterns", "temporal context", "spatial context", "Spatiotemporal phenomena", "Data visualization", "Anomaly detection", "Visualization", "Tensile stress", "Algorithm design and analysis", "Data models", "Anomaly Detection", "Visual Analysis"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744322": {"doi": "10.1109/TVCG.2017.2744322", "author": ["G. Andrienko", "N. Andrienko", "G. Fuchs", "J. M. C. Garcia"], "title": "Clustering Trajectories by Relevant Parts for Air Traffic Analysis", "year": "2018", "abstract": "Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.", "keywords": ["air traffic", "data mining", "data visualisation", "pattern clustering", "air traffic analysis", "distance function", "trajectory points", "analysis process", "movement data analysis", "relevance-aware trajectory clustering", "interactive filtering tool", "Trajectory", "Data visualization", "Three-dimensional displays", "Guidelines", "Visualization", "Clustering algorithms", "Algorithm design and analysis", "Visual analytics", "movement data analysis", "trajectory clustering", "air traffic"], "referenced_by": ["IKEY:8440121", "IKEY:8440039", "IKEY:8644631", "IKEY:8633409", "IKEY:8767400", "IKEY:8823586", "IKEY:8867884", "IKEY:8876625", "IKEY:8809845", "IKEY:8906505", "IKEY:8585048", "IKEY:9155962", "IKEY:9256644", "IKEY:9291971"], "referencing": []}, "10.1109/TVCG.2017.2745083": {"doi": "10.1109/TVCG.2017.2745083", "author": ["Y. Chen", "P. Xu", "L. Ren"], "title": "Sequence Synopsis: Optimize Visual Summary of Temporal Event Data", "year": "2018", "abstract": "Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.", "keywords": ["data analysis", "data visualisation", "fault diagnosis", "maintenance engineering", "real-world event sequence data", "high event cardinality", "minimum description length principle", "information loss", "visualization design", "information content", "simultaneous sequence", "pattern extraction", "missing events", "additional events", "visual analytics framework", "interactive data exploration", "real-world datasets", "application domain", "event sequence visualization", "fault development path analysis", "sequence synopsis", "optimize visual summary", "temporal event data", "event sequences analysis", "electronic health record analysis", "vehicle fault diagnosis", "visualization technique", "visual clutter", "Data visualization", "Data mining", "Algorithm design and analysis", "Data models", "Visual analytics", "Noise measurement", "Time Series Data", "Data Transformation and Representation", "Visual Knowledge Representation", "Visual Analytics"], "referenced_by": ["IKEY:8440851", "IKEY:8440811", "IKEY:8622571", "IKEY:8666650", "IKEY:8419283", "IKEY:8805466", "IKEY:8805450", "IKEY:8827944", "IKEY:8807354", "IKEY:8805439", "IKEY:8807233", "IKEY:8805463", "IKEY:8933770", "IKEY:8933584", "IKEY:8930535", "IKEY:8700242", "IKEY:9308628"], "referencing": []}, "10.1109/TVCG.2017.2745320": {"doi": "10.1109/TVCG.2017.2745320", "author": ["S. Guo", "K. Xu", "R. Zhao", "D. Gotz", "H. Zha", "N. Cao"], "title": "EventThread: Visual Summarization and Stage Analysis of Event Sequence Data", "year": "2018", "abstract": "Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.", "keywords": ["data visualisation", "diseases", "pattern clustering", "statistical analysis", "tensors", "statistical analysis", "EventThread", "tensor analysis", "latent stage categories", "evolution patterns", "visual summarization", "event progression", "event types", "event sequence analysis", "car service records", "event sequence data", "Data visualization", "Visualization", "Automobiles", "Hidden Markov models", "Algorithm design and analysis", "Semantics", "Clustering algorithms", "Visual Knowledge Representation", "Visual Knowledge Discovery", "Data Clustering", "Time Series Data", "Illustrative Visualization"], "referenced_by": ["IKEY:8440811", "IKEY:8454905", "IKEY:8622571", "IKEY:8387525", "IKEY:8827944", "IKEY:8807220", "IKEY:8807274", "IKEY:8933770", "IKEY:8930535", "IKEY:9308628"], "referencing": []}, "10.1109/TVCG.2017.2744686": {"doi": "10.1109/TVCG.2017.2744686", "author": ["A. Unger", "N. Dr\u00e4ger", "M. Sips", "D. J. Lehmann"], "title": "Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores", "year": "2018", "abstract": "This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.", "keywords": ["data visualisation", "feature extraction", "geophysics computing", "graphical user interfaces", "lakes", "sediments", "climate development", "geoscientific research field", "visual exploration", "similar sequence search", "sequence ranking", "unique sequence extraction", "visual interface", "similar sequences identification", "categorical sequences", "climate conditions", "landscape evolution", "time sequence", "lake sediment cores", "categorical states", "similarity measure", "tailored visual analysis concept", "temporal similarity", "similar time periods", "semantic similarity", "Meteorology", "Semantics", "Sediments", "Visualization", "Lakes", "Microscopy", "Prototypes", "Visualization in Earth Science", "Time Series Data", "Categorical Data", "Design Study"], "referenced_by": ["IKEY:8400404", "IKEY:8807220", "IKEY:8986923"], "referencing": []}, "10.1109/TVCG.2017.2744938": {"doi": "10.1109/TVCG.2017.2744938", "author": ["M. Liu", "J. Shi", "K. Cao", "J. Zhu", "S. Liu"], "title": "Analyzing the Training Processes of Deep Generative Models", "year": "2018", "abstract": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.", "keywords": ["data analysis", "data visualisation", "learning (artificial intelligence)", "time series", "machine learning", "credit assignment algorithm", "visual clutter reduction", "blue-noise polyline sampling scheme", "visual analytics approach", "training DGM", "training dynamics", "deep generative models", "training failure", "failed training process", "time series samples", "Training", "Neurons", "Time series analysis", "Tools", "Visual analytics", "Analytical models", "deep learning", "deep generative models", "blue noise sampling", "credit assignment"], "referenced_by": ["IKEY:8320546", "IKEY:8402187", "IKEY:8466814", "IKEY:8440091", "IKEY:8454904", "IKEY:8440124", "IKEY:8440116", "IKEY:8440813", "IKEY:8454905", "IKEY:8440049", "IKEY:8617747", "IKEY:8358974", "IKEY:8739137", "IKEY:8371286", "IKEY:8600732", "IKEY:8802509", "IKEY:8884176", "IKEY:8827593", "IKEY:8807299", "IKEY:8812988", "IKEY:8930535", "IKEY:8986943", "IKEY:9006128", "IKEY:9086289", "IKEY:9086290", "IKEY:8732351", "IKEY:9308627", "IKEY:9308631"], "referencing": []}, "10.1109/TVCG.2017.2744718": {"doi": "10.1109/TVCG.2017.2744718", "author": ["M. Kahng", "P. Y. Andrews", "A. Kalro", "D. H. Chau"], "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models", "year": "2018", "abstract": "While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.", "keywords": ["data visualisation", "learning (artificial intelligence)", "neural nets", "visual tools", "large-scale datasets", "participatory design sessions", "interactive visualization system", "large-scale deep learning models", "model architecture", "complex deep neural network models", "visual exploration", "industry-scale deep neural network models", "ActiVis system", "machine learning platform", "Computational modeling", "Tools", "Machine learning", "Data models", "Neurons", "Facebook", "Data visualization", "Visual analytics", "deep learning", "machine learning", "information visualization"], "referenced_by": ["IKEY:8365991", "IKEY:8460239", "IKEY:8440091", "IKEY:8454904", "IKEY:8440085", "IKEY:8440124", "IKEY:8494828", "IKEY:8440842", "IKEY:8454905", "IKEY:8440049", "IKEY:8617747", "IKEY:8622443", "IKEY:8646474", "IKEY:8667661", "IKEY:8667702", "IKEY:8371286", "IKEY:8789856", "IKEY:8802509", "IKEY:8852223", "IKEY:8855933", "IKEY:8805421", "IKEY:8827593", "IKEY:8805457", "IKEY:8811606", "IKEY:8827944", "IKEY:8807283", "IKEY:8812988", "IKEY:8807299", "IKEY:8807294", "IKEY:8933744"], "referencing": ["IKEY:7347637", "IKEY:6634124", "IKEY:6876047", "IKEY:7192665", "IKEY:7536654", "IKEY:7539329", "IKEY:7539404", "IKEY:6102453", "IKEY:7347637", "IKEY:6634124", "IKEY:6876047", "IKEY:7192665", "IKEY:7536654", "IKEY:7539329", "IKEY:7539404", "IKEY:6102453", "IKEY:7347637", "IKEY:6634124", "IKEY:6876047", "IKEY:7192665", "IKEY:7536654", "IKEY:7539329", "IKEY:7539404", "IKEY:6102453", "10.1145/2702123.2702509", "10.1145/2835776.2835848", "10.1145/2959100.2959190", "10.1145/2648584.2648589", "10.1145/2939502.2939503", "10.1145/2858036.2858529", "10.1145/2678025.2701399", "10.1145/2030365.2030367", "10.1145/2487575.2488200", "10.1145/1866029.1866038", "10.1145/1357054.1357160", "10.1145/2939672.2939778", "10.1145/2702123.2702509", "10.1145/2835776.2835848", "10.1145/2959100.2959190", "10.1145/2648584.2648589", "10.1145/2939502.2939503", "10.1145/2858036.2858529", "10.1145/2678025.2701399", "10.1145/2030365.2030367", "10.1145/2487575.2488200", "10.1145/1866029.1866038", "10.1145/1357054.1357160", "10.1145/2939672.2939778", "10.1145/2702123.2702509", "10.1145/2835776.2835848", "10.1145/2959100.2959190", "10.1145/2648584.2648589", "10.1145/2939502.2939503", "10.1145/2858036.2858529", "10.1145/2678025.2701399", "10.1145/2030365.2030367", "10.1145/2487575.2488200", "10.1145/1866029.1866038", "10.1145/1357054.1357160", "10.1145/2939672.2939778", "10.1007/978-3-319-27857-5_77", "10.3115/v1/D14-1181", "10.3115/1072228.1072378", "10.1007/978-3-319-27857-5_77", "10.3115/v1/D14-1181", "10.3115/1072228.1072378", "10.1007/978-3-319-27857-5_77", "10.3115/v1/D14-1181", "10.3115/1072228.1072378"]}, "10.1109/TVCG.2017.2744358": {"doi": "10.1109/TVCG.2017.2744358", "author": ["N. Pezzotti", "T. H\u00f6llt", "J. Van Gemert", "B. P. F. Lelieveldt", "E. Eisemann", "A. Vilanova"], "title": "DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks", "year": "2018", "abstract": "Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.", "keywords": ["data visualisation", "image recognition", "learning (artificial intelligence)", "neural net architecture", "DeepEyes", "pattern recognition problems", "complex features", "network architecture parameters", "superfluous filters", "trained network", "deep neural networks", "progressive visual analytics system", "Neurons", "Training", "Visual analytics", "Neural networks", "Three-dimensional displays", "Layout", "Kernel", "Progressive visual analytics", "deep neural networks", "machine learning"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2745178": {"doi": "10.1109/TVCG.2017.2745178", "author": ["A. Kumpf", "B. Tost", "M. Baumgart", "M. Riemer", "R. Westermann", "M. Rautenhaus"], "title": "Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses", "year": "2018", "abstract": "In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of \u201cTropical Cyclone Karl\u201d, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.", "keywords": ["data analysis", "geophysics computing", "pattern clustering", "statistical analysis", "storms", "weather forecasting", "ensemble weather forecast analyses", "ensemble weather predictions", "interactive visual interface", "simultaneous visualization", "cluster membership", "cluster robustness information", "subsequent ensemble analysis", "visual analytics", "ensemble members", "Tropical Cyclone Karl", "Weather forecasting", "Data visualization", "Robustness", "Market research", "Visualization", "Uncertainty", "Uncertainty visualization", "ensemble visualization", "clustering", "meteorology"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744805": {"doi": "10.1109/TVCG.2017.2744805", "author": ["D. Sacha", "M. Kraus", "J. Bernard", "M. Behrisch", "T. Schreck", "Y. Asano", "D. A. Keim"], "title": "SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance", "year": "2018", "abstract": "Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.", "keywords": ["data analysis", "data mining", "data visualisation", "interactive systems", "pattern clustering", "self-organising feature maps", "time series", "exploratory cluster analysis", "analytic provenance", "core building block", "hidden structures", "raw datasets", "particular groups", "visual-interactive cluster analysis techniques", "useful clusterings", "user interactions", "data preprocessing", "algorithms", "iterative cluster refinement", "visual platform", "previous analytical activities", "earlier cluster refinements", "useful patterns", "data partitions", "pair analytics experiments", "interactive data analysis", "clustering results", "interactive process", "self-organizing maps", "SOMFlow", "data analysis", "multistage visual analytics approach", "VA", "SOM", "time series data analysis", "flow graph", "interestingness measures", "pattern discovery", "subject matter expert", "speech intonation research", "Data visualization", "Visualization", "Time series analysis", "Clustering algorithms", "Self-organizing feature maps", "Algorithm design and analysis", "Speech", "Visual Analytics", "Interaction", "Visual Cluster Analysis", "Quality Metrics", "Guidance", "Self-Organizing Maps", "Time Series"], "referenced_by": ["IKEY:8440035", "IKEY:8440124", "IKEY:8440840", "IKEY:8827593", "IKEY:8827951", "IKEY:8933618", "IKEY:8930535", "IKEY:8986940"], "referencing": []}, "10.1109/TVCG.2017.2745258": {"doi": "10.1109/TVCG.2017.2745258", "author": ["J. Wenskovitch", "I. Crandell", "N. Ramakrishnan", "L. House", "S. Leman", "C. North"], "title": "Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics", "year": "2018", "abstract": "Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.", "keywords": ["data analysis", "data visualisation", "feature extraction", "learning (artificial intelligence)", "pattern clustering", "systematic combination", "visual analytics", "dimension reduction algorithms", "clustering algorithms", "visualization system", "Clustering algorithms", "Algorithm design and analysis", "Data visualization", "Partitioning algorithms", "Visualization", "Manifolds", "Dimension reduction", "clustering", "algorithms", "visual analytics"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2745085": {"doi": "10.1109/TVCG.2017.2745085", "author": ["B. C. Kwon", "B. Eysenbach", "J. Verma", "K. Ng", "C. De Filippi", "W. F. Stewart", "A. Perer"], "title": "Clustervision: Visual Supervision of Unsupervised Clustering", "year": "2018", "abstract": "Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.", "keywords": ["data analysis", "data visualisation", "pattern clustering", "unsupervised learning", "user interfaces", "Clustervision", "unsupervised machine learning", "complex multidimensional data", "data scientists", "dataset", "analytical tasks", "visual analytics tool", "system clusters data", "task-relevant constraints", "visual user interface", "high quality clustering results", "coordinated visualization techniques", "complex data", "visual supervision", "unsupervised clustering", "Clustering algorithms", "Measurement", "Visual analytics", "Partitioning algorithms", "Data visualization", "Indexes", "Unsupervised Clustering", "Visual Analytics", "Quality Metrics", "Interactive Visual Clustering"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744683": {"doi": "10.1109/TVCG.2017.2744683", "author": ["A. Bilal", "A. Jourabloo", "M. Ye", "X. Liu", "L. Ren"], "title": "Do Convolutional Neural Networks Learn Class Hierarchy?", "year": "2018", "abstract": "Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.", "keywords": ["data visualisation", "feedforward neural nets", "image classification", "convolutional neural networks", "state-of-the-art accuracy", "image classification", "growing number", "class confusion patterns", "hierarchical structure", "visual-analytics methods", "similar classes", "CNN-internal data", "learning behavior", "early layers", "specialized feature detectors", "individual classes", "class hierarchy learning", "hierarchy-aware CNN design", "model convergence", "training data", "Data visualization", "Training", "Neurons", "Feature extraction", "Training data", "Image recognition", "Convolutional Neural Networks", "deep learning", "image classification", "large-scale classification", "confusion matrix"], "referenced_by": ["IKEY:8454904", "IKEY:8440085", "IKEY:8454905", "IKEY:8440049", "IKEY:8371286", "IKEY:8802509", "IKEY:8851954", "IKEY:8805421", "IKEY:8807299", "IKEY:8807294", "IKEY:9006128", "IKEY:9086289", "IKEY:9104365", "IKEY:9156918", "IKEY:9172872", "IKEY:9198366", "IKEY:8732351", "IKEY:9308631", "10.1145/3240765.3240845", "10.1049/iet-rsn.2018.5438"], "referencing": []}, "10.1109/TVCG.2017.2744378": {"doi": "10.1109/TVCG.2017.2744378", "author": ["S. Liu", "J. Xiao", "J. Liu", "X. Wang", "J. Wu", "J. Zhu"], "title": "Visual Diagnosis of Tree Boosting Methods", "year": "2018", "abstract": "Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.", "keywords": ["data visualisation", "decision trees", "image classification", "learning (artificial intelligence)", "time-consuming process", "visual diagnosis tool", "temporal confusion matrix visualization", "tree visualization", "visualization components", "tree boosting model", "unsatisfactory training process", "tree boosting algorithms", "tree boosting methods", "typically decision trees", "machine learning method", "Boosting", "Training", "Decision trees", "Tools", "Vegetation", "Visualization", "Analytical models", "tree boosting", "model analysis", "temporal confusion matrix", "tree visualization"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2745158": {"doi": "10.1109/TVCG.2017.2745158", "author": ["T. M\u00fchlbacher", "L. Linhardt", "T. M\u00f6ller", "H. Piringer"], "title": "TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees", "year": "2018", "abstract": "Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.", "keywords": ["decision trees", "TreePOD", "sensitivity-aware selection", "Pareto-optimal decision trees", "sensitivity-aware model selection", "tree construction algorithms", "quantitative tree aspects", "qualitative tree aspects", "Pareto-optimal tree candidates", "tree generation process", "decision tree selection", "Decision trees", "Vegetation", "Data models", "Buildings", "Visualization", "Measurement", "Focusing", "Model selection", "classification trees", "visual parameter search", "sensitivity analysis", "Pareto optimality"], "referenced_by": ["IKEY:8454906", "IKEY:8440124", "IKEY:8827593", "IKEY:8969926", "IKEY:9212433"], "referencing": []}, "10.1109/TVCG.2017.2745280": {"doi": "10.1109/TVCG.2017.2745280", "author": ["A. G. Forbes", "A. Burks", "K. Lee", "X. Li", "P. Boutillier", "J. Krivine", "W. Fontana"], "title": "Dynamic Influence Networks for Rule-Based Models", "year": "2018", "abstract": "We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule-based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types, we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.", "keywords": ["biochemistry", "biology computing", "data analysis", "data visualisation", "molecular biophysics", "physiological models", "proteins", "stochastic processes", "node-link diagram", "individual biological components", "interactive DIN-Viz software tool", "dynamic network", "biological processes", "circadian clock model", "KaiC protein phosphorylation cycle", "Dynamic Influence networks", "Dynamic Influence Network", "novel visual analytics technique", "protein-protein interaction networks", "biological models", "open source stochastic simulator", "Biological system modeling", "Data visualization", "Analytical models", "Proteins", "Computational modeling", "Data models", "Dynamic networks", "biological data visualization", "rule-based modeling", "protein-protein interaction networks"], "referenced_by": ["IKEY:8365972", "IKEY:8933596", "IKEY:9130852"], "referencing": []}, "10.1109/TVCG.2017.2744458": {"doi": "10.1109/TVCG.2017.2744458", "author": ["J. Zhao", "M. Sun", "F. Chen", "P. Chiu"], "title": "BiDots: Visual Exploration of Weighted Biclusters", "year": "2018", "abstract": "Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity. This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.", "keywords": ["bioinformatics", "data analysis", "data mining", "data visualisation", "genetics", "pattern clustering", "gene expression", "bio-informatics", "biclustering techniques", "initial low-level insights", "algorithmic output complexity", "visualization technique", "bicluster visualizations", "flexible analysis", "weighted biclusters", "computed biclusters", "investigative document analysis task", "visual exploration", "critical task", "real-world applications", "intelligence analysis", "BiDots", "document analysis", "text corpus", "Visualization", "Algorithm design and analysis", "Organizations", "Data mining", "Sparse matrices", "Data visualization", "Gene expression", "Biclustering", "coordinated relationship analysis", "visual analytics"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744080": {"doi": "10.1109/TVCG.2017.2744080", "author": ["S. Fu", "H. Dong", "W. Cui", "J. Zhao", "H. Qu"], "title": "How Do Ancestral Traits Shape Family Trees Over Generations?", "year": "2018", "abstract": "Whether and how does the structure of family trees differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, previous work mostly focuses on parent-child scenarios due to the lack of proper tools to handle the complexity of extending the research to multi-generational processes. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).", "keywords": ["human computer interaction", "statistical analysis", "tree data structures", "structural heterogeneity", "multigenerational transmission research", "parent-child scenarios", "TreeEvo", "structural features", "complex structures", "tree structure", "ancestral traits", "MLM", "multinomial logit model", "semantic zooming", "space-efficient visualization", "pixel-based technique", "Sankey diagram", "family trees", "Tools", "Sociology", "Statistics", "Visual analytics", "Scalability", "Animation", "Quantitative social science", "Design study", "Multiple tree visualization", "Sankey diagram"], "referenced_by": ["IKEY:8307265", "IKEY:8805439", "IKEY:8845772"], "referencing": []}, "10.1109/TVCG.2017.2744898": {"doi": "10.1109/TVCG.2017.2744898", "author": ["R. Pienta", "F. Hohman", "A. Endert", "A. Tamersoy", "K. Roundy", "C. Gates", "S. Navathe", "D. H. Chau"], "title": "VIGOR: Interactive Visual Exploration of Graph Query Results", "year": "2018", "abstract": "Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.", "keywords": ["data mining", "data structures", "database management systems", "graph theory", "query processing", "interaction technique", "node value constraints", "leading graph database management system", "interactive visual exploration", "graph query results", "biological systems", "graph databases", "potentially shared node-values", "interactive visual analytics system", "analyst sensemaking process", "feature-aware subgraph result summarization", "VIGOR", "Data visualization", "Logic gates", "Computer security", "Database systems", "Data mining", "Visualization", "graph querying", "subgraph results", "query result visualization"], "referenced_by": ["IKEY:8454344", "IKEY:8440813", "IKEY:8805439", "IKEY:8963712"], "referencing": []}, "10.1109/TVCG.2017.2744843": {"doi": "10.1109/TVCG.2017.2744843", "author": ["A. Srinivasan", "H. Park", "A. Endert", "R. C. Basole"], "title": "Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization", "year": "2018", "abstract": "Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.", "keywords": ["data visualisation", "formal specification", "graphical user interfaces", "human computer interaction", "interactive systems", "SQL", "predefined networks", "cluster identification", "custom scripts programming", "attribute-based edges", "interactive specification", "tabular dataset", "Graphiti", "demonstration-based interaction technique", "complex SQL commands", "resulting network", "model networks", "node-link diagrams", "network visualizations", "network modeling", "Tools", "Data visualization", "Joining processes", "Data models", "Prototypes", "Image color analysis", "Computational modeling", "Network modeling", "visual analytics", "user interaction"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744098": {"doi": "10.1109/TVCG.2017.2744098", "author": ["J. Xia", "F. Ye", "W. Chen", "Y. Wang", "W. Chen", "Y. Ma", "A. K. H. Tung"], "title": "LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets", "year": "2018", "abstract": "Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the $x$ axis and $y$ axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS ($x$ axis) and the variation of LTS in structures (the combination of $x$ axis and $y$ axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.", "keywords": ["Manifolds", "Data visualization", "Data models", "Visualization", "Tools", "Analytical models", "Principal component analysis", "High-dimensional data", "low-dimensional structure", "subspace", "manifold", "visual exploration"], "referenced_by": ["IKEY:8440829", "IKEY:8358974", "IKEY:8736844", "IKEY:8764395", "IKEY:8802486", "IKEY:8801911", "IKEY:8868541", "IKEY:8794768", "IKEY:8807303", "IKEY:8807351", "IKEY:8490694", "IKEY:9146191", "IKEY:9308623"], "referencing": ["IKEY:1410261", "IKEY:6064985", "IKEY:663916", "IKEY:6634192", "IKEY:5652392", "IKEY:146402", "IKEY:726791", "IKEY:1249013", "IKEY:7192699", "IKEY:7536217", "IKEY:7534787", "IKEY:7192695", "IKEY:5332628", "IKEY:5714408", "IKEY:7192728", "IKEY:6634155", "IKEY:1410261", "IKEY:6064985", "IKEY:663916", "IKEY:6634192", "IKEY:5652392", "IKEY:146402", "IKEY:726791", "IKEY:1249013", "IKEY:7192699", "IKEY:7536217", "IKEY:7534787", "IKEY:7192695", "IKEY:5332628", "IKEY:5714408", "IKEY:7192728", "IKEY:6634155", "IKEY:1410261", "IKEY:6064985", "IKEY:663916", "IKEY:6634192", "IKEY:5652392", "IKEY:146402", "IKEY:726791", "IKEY:1249013", "IKEY:7192699", "IKEY:7536217", "IKEY:7534787", "IKEY:7192695", "IKEY:5332628", "IKEY:5714408", "IKEY:7192728", "IKEY:6634155", "10.1145/1345448.1345451", "10.1145/1345448.1345451", "10.1145/1345448.1345451", "10.1007/3-540-44503-X_27", "10.1137/1.9781611972733.5", "10.1007/978-3-540-69497-7_27", "10.1111/cgf.12639", "10.1111/cgf.12878", "10.1162/089976698300017467", "10.1214/13-AOS1199", "10.1126/science.290.5500.2319", "10.1023/A:1004678431677", "10.1137/S1064827502419154", "10.1007/3-540-44503-X_27", "10.1137/1.9781611972733.5", "10.1007/978-3-540-69497-7_27", "10.1111/cgf.12639", "10.1111/cgf.12878", "10.1162/089976698300017467", "10.1214/13-AOS1199", "10.1126/science.290.5500.2319", "10.1023/A:1004678431677", "10.1137/S1064827502419154", "10.1007/3-540-44503-X_27", "10.1137/1.9781611972733.5", "10.1007/978-3-540-69497-7_27", "10.1111/cgf.12639", "10.1111/cgf.12878", "10.1162/089976698300017467", "10.1214/13-AOS1199", "10.1126/science.290.5500.2319", "10.1023/A:1004678431677", "10.1137/S1064827502419154"]}, "10.1109/TVCG.2017.2744738": {"doi": "10.1109/TVCG.2017.2744738", "author": ["X. Zhao", "Y. Wu", "W. Cui", "X. Du", "Y. Chen", "Y. Wang", "D. L. Lee", "H. Qu"], "title": "SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data", "year": "2018", "abstract": "Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.", "keywords": ["data analysis", "data visualisation", "decision making", "query processing", "SkyLens", "visual analysis", "multidimensional data", "skyline queries", "multicriteria decision making", "decision-making overhead", "skyline points", "multidimensional space", "skyline understanding", "data items", "visual analytic system", "Urban areas", "Decision making", "Visual analytics", "Measurement", "Industries", "Skyline query", "skyline visualization", "multi-dimensional data", "visual analytics", "multi-criteria decision making"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744685": {"doi": "10.1109/TVCG.2017.2744685", "author": ["L. Wilkinson"], "title": "Visualizing Big Data Outliers Through Distributed Aggregation", "year": "2018", "abstract": "Visualizing outliers in massive datasets requires statistical pre-processing in order to reduce the scale of the problem to a size amenable to rendering systems like D3, Plotly or analytic systems like R or SAS. This paper presents a new algorithm, called hdoutliers, for detecting multidimensional outliers. It is unique for a) dealing with a mixture of categorical and continuous variables, b) dealing with big-p (many columns of data), c) dealing with big-$n$ (many rows of data), d) dealing with outliers that mask other outliers, and e) dealing consistently with unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers, hdoutliers is based on a distributional model that allows outliers to be tagged with a probability. This critical feature reduces the likelihood of false discoveries.", "keywords": ["Anomaly detection", "Gaussian distribution", "Standards", "Sociology", "Robustness", "Covariance matrices", "Outliers", "Anomalies"], "referenced_by": ["IKEY:8440116", "IKEY:8638183", "IKEY:8805439", "IKEY:8930535", "IKEY:8955560", "IKEY:8986943", "IKEY:8706590"], "referencing": ["10.1145/342009.335388", "10.1145/1541880.1541882", "10.1145/502512.502530", "10.1145/342009.335388", "10.1145/1541880.1541882", "10.1145/502512.502530", "10.1145/342009.335388", "10.1145/1541880.1541882", "10.1145/502512.502530", "10.1007/978-1-4614-6396-2", "10.1080/00401706.1960.10489888", "10.1002/0471725153", "10.15837/ijccc.2013.1.165", "10.1111/j.1467-9892.2006.00483.x", "10.1006/jmva.1995.1015", "10.1080/10691898.2011.11889610", "10.1214/aoms/1177729693", "10.1198/016214502760047131", "10.1214/aoms/1177729885", "10.1007/978-94-015-3994-4", "10.1016/j.csda.2007.11.008", "10.1016/S0167-8655(00)00131-8", "10.1080/01621459.2014.983231", "10.1090/conm/026/737400", "10.1007/978-3-642-01307-2_86", "10.1137/1.9781611972818.2", "10.1111/cgf.12845", "10.1007/978-3-540-79721-0_69", "10.1111/cgf.12884", "10.2307/2529711", "10.1057/palgrave.ivs.9500072", "10.2307/2288718", "10.1080/01621459.1990.10474920", "10.1002/0471725382", "10.1007/978-3-7908-2604-3_8", "10.1016/0306-4573(88)90021-0", "10.1007/978-3-642-17103-1_6", "10.1201/9780203910894", "10.1007/BF02293863", "10.1086/jar.33.4.3629752", "10.1016/B978-0-12-396963-7.00007-6", "10.1007/978-1-4614-6396-2", "10.1080/00401706.1960.10489888", "10.1002/0471725153", "10.15837/ijccc.2013.1.165", "10.1111/j.1467-9892.2006.00483.x", "10.1006/jmva.1995.1015", "10.1080/10691898.2011.11889610", "10.1214/aoms/1177729693", "10.1198/016214502760047131", "10.1214/aoms/1177729885", "10.1007/978-94-015-3994-4", "10.1016/j.csda.2007.11.008", "10.1016/S0167-8655(00)00131-8", "10.1080/01621459.2014.983231", "10.1090/conm/026/737400", "10.1007/978-3-642-01307-2_86", "10.1137/1.9781611972818.2", "10.1111/cgf.12845", "10.1007/978-3-540-79721-0_69", "10.1111/cgf.12884", "10.2307/2529711", "10.1057/palgrave.ivs.9500072", "10.2307/2288718", "10.1080/01621459.1990.10474920", "10.1002/0471725382", "10.1007/978-3-7908-2604-3_8", "10.1016/0306-4573(88)90021-0", "10.1007/978-3-642-17103-1_6", "10.1201/9780203910894", "10.1007/BF02293863", "10.1086/jar.33.4.3629752", "10.1016/B978-0-12-396963-7.00007-6", "10.1007/978-1-4614-6396-2", "10.1080/00401706.1960.10489888", "10.1002/0471725153", "10.15837/ijccc.2013.1.165", "10.1111/j.1467-9892.2006.00483.x", "10.1006/jmva.1995.1015", "10.1080/10691898.2011.11889610", "10.1214/aoms/1177729693", "10.1198/016214502760047131", "10.1214/aoms/1177729885", "10.1007/978-94-015-3994-4", "10.1016/j.csda.2007.11.008", "10.1016/S0167-8655(00)00131-8", "10.1080/01621459.2014.983231", "10.1090/conm/026/737400", "10.1007/978-3-642-01307-2_86", "10.1137/1.9781611972818.2", "10.1111/cgf.12845", "10.1007/978-3-540-79721-0_69", "10.1111/cgf.12884", "10.2307/2529711", "10.1057/palgrave.ivs.9500072", "10.2307/2288718", "10.1080/01621459.1990.10474920", "10.1002/0471725382", "10.1007/978-3-7908-2604-3_8", "10.1016/0306-4573(88)90021-0", "10.1007/978-3-642-17103-1_6", "10.1201/9780203910894", "10.1007/BF02293863", "10.1086/jar.33.4.3629752", "10.1016/B978-0-12-396963-7.00007-6"]}, "10.1109/TVCG.2017.2745180": {"doi": "10.1109/TVCG.2017.2745180", "author": ["D. Edge", "N. H. Riche", "J. Larson", "C. White"], "title": "Beyond Tasks: An Activity Typology for Visual Analytics", "year": "2018", "abstract": "As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.", "keywords": ["data analysis", "data visualisation", "human computer interaction", "interactive systems", "user centred design", "Human-Computer Interaction", "Activity Theory", "interactive tools", "activity typology", "abstract task taxonomies", "Information Visualization", "Visual Analytics", "activity concepts", "activity-centered design process", "VA systems", "VA research", "human activities modelling", "Visual analytics", "Human computer interaction", "Task analysis", "Activity theory", "visual analytics", "activity-centered design", "literature review", "human-computer interaction"], "referenced_by": ["IKEY:8807351"], "referencing": []}, "10.1109/TVCG.2017.2743990": {"doi": "10.1109/TVCG.2017.2743990", "author": ["A. Batch", "N. Elmqvist"], "title": "The Interactive Visualization Gap in Initial Exploratory Data Analysis", "year": "2018", "abstract": "Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a \u201cvisualization gap\u201d during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.", "keywords": ["data analysis", "data visualisation", "interactive systems", "interactive visualization gap", "initial exploratory data analysis", "dissemination phase", "visualization scientists", "initial exploratory analysis", "visualization researchers", "professional data analysts", "interactive initial exploratory visualization guidelines", "Data visualization", "Data science", "Tools", "Visualization", "Big Data", "Interviews", "Data science", "visualization", "visual analytics", "contextual inquiry", "semi-structured interviews"], "referenced_by": ["IKEY:8440803", "IKEY:8440828", "IKEY:8634026", "IKEY:8820171", "IKEY:8807280", "IKEY:9065491"], "referencing": []}, "10.1109/TVCG.2017.2745078": {"doi": "10.1109/TVCG.2017.2745078", "author": ["E. Wall", "S. Das", "R. Chawla", "B. Kalidindi", "E. T. Brown", "A. Endert"], "title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "year": "2018", "abstract": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "keywords": ["data analysis", "data visualisation", "decision making", "learning (artificial intelligence)", "support vector machines", "Podium", "mixed-initiative visual analytics", "multiattribute ranking systems", "data-driven decisions", "visual analytic application", "multivariate data points", "prototype system", "Ranking SVM", "attribute weights", "data points", "data ranking", "ranking SVM", "user subjective preferences", "Data visualization", "Support vector machines", "Visual analytics", "Data models", "Prototypes", "Computational modeling", "Mixed-initiative visual analytics", "multi-attribute ranking", "user interaction"], "referenced_by": ["IKEY:8440116", "IKEY:8735919", "IKEY:8802486", "IKEY:8805443", "IKEY:8807283", "IKEY:8809678"], "referencing": []}, "10.1109/TVCG.2017.2744818": {"doi": "10.1109/TVCG.2017.2744818", "author": ["J. Bernard", "M. Hutter", "M. Zeppelzauer", "D. Fellner", "M. Sedlmair"], "title": "Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study", "year": "2018", "abstract": "Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.", "keywords": ["data reduction", "data visualisation", "encoding", "interactive systems", "learning (artificial intelligence)", "active learning", "machine learning", "visual analytics", "visual-interactive labeling process", "visual interactive labeling", "visual encodings", "visual labeling strategies", "dimension reduction", "Labeling", "Visual analytics", "Data models", "Uncertainty", "Data visualization", "Labeling", "Visual-Interactive Labeling", "Information Visualization", "Visual Analytics", "Active Learning", "Machine Learning", "Classification", "Evaluation", "Experiment", "Dimensionality Reduction"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744684": {"doi": "10.1109/TVCG.2017.2744684", "author": ["E. Hoque", "V. Setlur", "M. Tory", "I. Dykeman"], "title": "Applying Pragmatics Principles for Interaction with Visual Analytics", "year": "2018", "abstract": "Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.", "keywords": ["data analysis", "data visualisation", "interactive systems", "natural language interfaces", "interactive conversations", "natural language interface", "interaction paradigms", "language pragmatics", "visual analytical conversations", "conversation interfaces", "visual analytics tools", "pragmatics principles", "interactive visual data analysis", "engaging users", "Evizeon", "Pragmatics", "Visual analytics", "Natural languages", "Data visualization", "Coherence", "Tools", "natural language", "interaction", "language pragmatics", "visual analytics", "ambiguity", "feedback"], "referenced_by": ["IKEY:8440115", "IKEY:8807265", "IKEY:8933766", "IKEY:8933569", "IKEY:8986918", "IKEY:8977320", "IKEY:9118800", "IKEY:9303381"], "referencing": []}, "10.1109/TVCG.2017.2744418": {"doi": "10.1109/TVCG.2017.2744418", "author": ["J. Liu", "T. Dwyer", "K. Marriott", "J. Millar", "A. Haworth"], "title": "Understanding the Relationship Between Interactive Optimisation and Visual Analytics in the Context of Prostate Brachytherapy", "year": "2018", "abstract": "The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose \u201cblack-box\u201d solver. In practice, however, many problems cannot be solved completely automatically, but require a \u201chuman-in-the-loop\u201d to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient's tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.", "keywords": ["biological organs", "brachytherapy", "cancer", "cellular biophysics", "interactive systems", "medical computing", "tumours", "prostate brachytherapy", "operations research", "computer science", "automatic solver techniques", "human-in-the-loop", "interactive optimisation workflow", "visual analytics sense-making loop", "problem-solving loop analogous", "prototype interactive optimisation tool", "prostate cancer treatment plans", "brachytherapy treatment planning", "black-box solver", "sense-making loop", "in-depth analysis", "tumour cells", "Optimization", "Tools", "Visual analytics", "Mathematical model", "Brachytherapy", "Planning", "Visual analytics", "interactive optimisation", "interactive systems and tools", "prostate brachytherapy", "Brachytherapy", "Humans", "Image Processing, Computer-Assisted", "Male", "Prostate", "Prostatic Neoplasms", "Radiotherapy Planning, Computer-Assisted"], "referenced_by": [], "referencing": ["IKEY:5290759", "IKEY:5613488", "IKEY:6634138", "IKEY:7536109", "IKEY:7042481", "IKEY:6876043", "IKEY:823289", "IKEY:949485", "IKEY:6064952", "IKEY:5290759", "IKEY:5613488", "IKEY:6634138", "IKEY:7536109", "IKEY:7042481", "IKEY:6876043", "IKEY:823289", "IKEY:949485", "IKEY:6064952", "IKEY:5290759", "IKEY:5613488", "IKEY:6634138", "IKEY:7536109", "IKEY:7042481", "IKEY:6876043", "IKEY:823289", "IKEY:949485", "IKEY:6064952", "10.1145/1186562.1015718", "10.1145/2501988.2502024", "10.1145/1240624.1240739", "10.1145/642712.642713", "10.1145/1378773.1378804", "10.1145/2645710.2645761", "10.1145/1753326.1753529", "10.1145/2808234", "10.1145/1240624.1240746", "10.1145/2939672.2939778", "10.1145/1015706.1015720", "10.1145/503376.503405", "10.1145/2501988.2502007", "10.1145/1186562.1015718", "10.1145/2501988.2502024", "10.1145/1240624.1240739", "10.1145/642712.642713", "10.1145/1378773.1378804", "10.1145/2645710.2645761", "10.1145/1753326.1753529", "10.1145/2808234", "10.1145/1240624.1240746", "10.1145/2939672.2939778", "10.1145/1015706.1015720", "10.1145/503376.503405", "10.1145/2501988.2502007", "10.1145/1186562.1015718", "10.1145/2501988.2502024", "10.1145/1240624.1240739", "10.1145/642712.642713", "10.1145/1378773.1378804", "10.1145/2645710.2645761", "10.1145/1753326.1753529", "10.1145/2808234", "10.1145/1240624.1240746", "10.1145/2939672.2939778", "10.1145/1015706.1015720", "10.1145/503376.503405", "10.1145/2501988.2502007", "10.1080/09613210802243159", "10.1007/s12008-010-0100-x", "10.1111/j.1467-8659.2011.01940.x", "10.1007/978-3-540-40899-4_28", "10.1016/j.compind.2006.06.004", "10.1016/j.ijhcs.2011.10.001", "10.1111/cgf.12650", "10.1088/0031-9155/49/16/012", "10.1088/0031-9155/61/1/430", "10.1016/j.brachy.2013.04.008", "10.1287/ijoc.6.3.221", "10.1016/j.envsoft.2007.02.001", "10.1111/j.1464-410X.2011.10825.x", "10.1007/s00291-012-0297-0", "10.1007/b98874", "10.1007/s10676-010-9253-3", "10.1111/cgf.12899", "10.1016/j.radonc.2007.06.020", "10.1016/j.advengsoft.2008.05.004", "10.1007/s10111-007-0106-8", "10.1080/09613210802243159", "10.1007/s12008-010-0100-x", "10.1111/j.1467-8659.2011.01940.x", "10.1007/978-3-540-40899-4_28", "10.1016/j.compind.2006.06.004", "10.1016/j.ijhcs.2011.10.001", "10.1111/cgf.12650", "10.1088/0031-9155/49/16/012", "10.1088/0031-9155/61/1/430", "10.1016/j.brachy.2013.04.008", "10.1287/ijoc.6.3.221", "10.1016/j.envsoft.2007.02.001", "10.1111/j.1464-410X.2011.10825.x", "10.1007/s00291-012-0297-0", "10.1007/b98874", "10.1007/s10676-010-9253-3", "10.1111/cgf.12899", "10.1016/j.radonc.2007.06.020", "10.1016/j.advengsoft.2008.05.004", "10.1007/s10111-007-0106-8", "10.1080/09613210802243159", "10.1007/s12008-010-0100-x", "10.1111/j.1467-8659.2011.01940.x", "10.1007/978-3-540-40899-4_28", "10.1016/j.compind.2006.06.004", "10.1016/j.ijhcs.2011.10.001", "10.1111/cgf.12650", "10.1088/0031-9155/49/16/012", "10.1088/0031-9155/61/1/430", "10.1016/j.brachy.2013.04.008", "10.1287/ijoc.6.3.221", "10.1016/j.envsoft.2007.02.001", "10.1111/j.1464-410X.2011.10825.x", "10.1007/s00291-012-0297-0", "10.1007/b98874", "10.1007/s10676-010-9253-3", "10.1111/cgf.12899", "10.1016/j.radonc.2007.06.020", "10.1016/j.advengsoft.2008.05.004", "10.1007/s10111-007-0106-8"]}, "10.1109/TVCG.2017.2744758": {"doi": "10.1109/TVCG.2017.2744758", "author": ["R. A. Leite", "T. Gschwandtner", "S. Miksch", "S. Kriglstein", "M. Pohl", "E. Gstrein", "J. Kuntner"], "title": "EVA: Visual Analytics to Identify Fraudulent Events", "year": "2018", "abstract": "Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.", "keywords": ["banking", "data analysis", "data mining", "data visualisation", "fraud", "security of data", "EVA", "financial institutions", "harmful transactions", "fraudulent operations", "data mining techniques", "customer profile analysis", "financial fraud detection systems", "false positive alarm reduction", "fine-tuned fraud detection algorithms", "visual analytics techniques", "Data visualization", "Complexity theory", "Visual analytics", "Data mining", "Event detection", "Visual Knowledge Discovery", "Time Series Data", "Business and Finance Visualization", "Financial Fraud Detection"], "referenced_by": ["IKEY:8805439", "IKEY:8933598", "IKEY:8963712", "IKEY:9076536", "IKEY:8587186", "IKEY:9308622"], "referencing": []}, "10.1109/TVCG.2017.2745279": {"doi": "10.1109/TVCG.2017.2745279", "author": ["J. Zhao", "M. Glueck", "P. Isenberg", "F. Chevalier", "A. Khan"], "title": "Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs", "year": "2018", "abstract": "During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.", "keywords": ["data visualisation", "encoding", "groupware", "interactive systems", "knowledge management", "mobile computing", "mobility management (mobile radio)", "support explicit communication", "uncertainty", "annotation", "implicit communication", "interactive visual analysis system", "asynchronous investigative document analysis task", "two-phase user study", "investigative performance", "effective handoff strategies", "asynchronous collaborative sensemaking", "knowledge-transfer graphs", "asynchronous collaborative analysis", "partial findings", "tacit aspects", "explicit authoring", "handoff annotations", "analysis coverage", "Collaboration", "Visual analytics", "Data analysis", "Data visualization", "History", "Tools", "Collaboration", "sensemaking", "handoff", "handover", "structured externalizations", "interactive visual analysis"], "referenced_by": ["IKEY:8500765", "IKEY:8564153", "IKEY:8805439", "IKEY:8807301", "IKEY:8477163", "IKEY:8973385", "IKEY:8585048"], "referencing": []}, "10.1109/TVCG.2017.2745139": {"doi": "10.1109/TVCG.2017.2745139", "author": ["X. Wang", "J. Chou", "W. Chen", "H. Guan", "W. Chen", "T. Lao", "K. Ma"], "title": "A Utility-Aware Visual Approach for Anonymizing Multi-Attribute Tabular Data", "year": "2018", "abstract": "Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.", "keywords": ["authorisation", "data privacy", "data visualisation", "sensitive information lekage prevention", "multiattribute tabular data anonymization", "differential privacy", "syntactic anonymity", "utility loss", "data manipulation pipeline", "visual interface", "privacy preserving visualizations", "sensitive information", "utility-aware visual approach", "Privacy", "Data privacy", "Syntactics", "Visualization", "Data models", "Data visualization", "Pipelines", "Privacy preserving visualization", "utility aware anonymization", "syntactic anonymity", "differential privacy"], "referenced_by": ["IKEY:8440807", "IKEY:8632641", "IKEY:8513838", "IKEY:9161608", "IKEY:9308627", "IKEY:9308625"], "referencing": []}, "10.1109/TVCG.2017.2744478": {"doi": "10.1109/TVCG.2017.2744478", "author": ["D. Park", "S. Kim", "J. Lee", "J. Choo", "N. Diakopoulos", "N. Elmqvist"], "title": "ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding", "year": "2018", "abstract": "Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.", "keywords": ["natural language processing", "text analysis", "ConceptVector", "text visual analytics", "word embedding", "text analysis methods", "semantically related keywords", "false positive errors", "natural language", "visual analytics system", "real-world datasets", "fine-grained analysis", "bipolar concept model", "interactive lexicon building interface", "bipolar lexicon", "interactive lexicon building", "Text analysis", "Visual analytics", "Semantics", "Buildings", "Computational modeling", "Data visualization", "Sentiment analysis", "Text analytics", "visual analytics", "word embedding", "text summarization", "text classification", "concepts", "Algorithms", "Cluster Analysis", "Computer Graphics", "Data Mining", "Databases, Factual", "Image Processing, Computer-Assisted", "Semantics"], "referenced_by": ["IKEY:8440832", "IKEY:8494828", "IKEY:8564153", "IKEY:8371286", "IKEY:8807224", "IKEY:8805439", "IKEY:9161563", "IKEY:9308101"], "referencing": []}, "10.1109/TVCG.2017.2745118": {"doi": "10.1109/TVCG.2017.2745118", "author": ["M. Glueck", "M. P. Naeini", "F. Doshi-Velez", "F. Chevalier", "A. Khan", "D. Wigdor", "M. Brudno"], "title": "PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models", "year": "2018", "abstract": "PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes-each with its own temporally evolving prevalence and co-occurrence of phenotypes-without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.", "keywords": ["data analysis", "data mining", "data visualisation", "diseases", "genetics", "health care", "learning (artificial intelligence)", "medical computing", "ontologies (artificial intelligence)", "disease subtyping", "topic models", "disease subtypes", "longitudinal phenotype data", "disease subtype topics", "phenolines", "phenotype comparison visualizations", "cross-sectional patient comorbidity data", "human phenotype ontology", "data transformation workflow", "machine learning", "clinical experts", "Diseases", "Data models", "Visualization", "Biological system modeling", "Analytical models", "Data visualization", "Tools", "Developmental disorder", "Human Phenotype Ontology (HPO)", "Phenotypes", "Topic models", "Topology simplification"], "referenced_by": [], "referencing": ["IKEY:6064996", "IKEY:6634168", "IKEY:6875938", "IKEY:7534774", "IKEY:7192670", "IKEY:7593335", "IKEY:7192665", "IKEY:5290695", "IKEY:6327248", "IKEY:6415893", "IKEY:7536174", "IKEY:6064996", "IKEY:6634168", "IKEY:6875938", "IKEY:7534774", "IKEY:7192670", "IKEY:7593335", "IKEY:7192665", "IKEY:5290695", "IKEY:6327248", "IKEY:6415893", "IKEY:7536174", "IKEY:6064996", "IKEY:6634168", "IKEY:6875938", "IKEY:7534774", "IKEY:7192670", "IKEY:7593335", "IKEY:7192665", "IKEY:5290695", "IKEY:6327248", "IKEY:6415893", "IKEY:7536174", "10.1145/1143844.1143859", "10.1145/2783258.2783365", "10.1145/2254556.2254572", "10.1145/2089094.2089099", "10.1145/1401890.1401937", "10.1145/2089094.2089101", "10.1145/2678025.2701407", "10.1145/1357054.1357129", "10.1145/1150402.1150450", "10.1145/1978942.1979196", "10.1145/1143844.1143859", "10.1145/2783258.2783365", "10.1145/2254556.2254572", "10.1145/2089094.2089099", "10.1145/1401890.1401937", "10.1145/2089094.2089101", "10.1145/2678025.2701407", "10.1145/1357054.1357129", "10.1145/1150402.1150450", "10.1145/1978942.1979196", "10.1145/1143844.1143859", "10.1145/2783258.2783365", "10.1145/2254556.2254572", "10.1145/2089094.2089099", "10.1145/1401890.1401937", "10.1145/2089094.2089101", "10.1145/2678025.2701407", "10.1145/1357054.1357129", "10.1145/1150402.1150450", "10.1145/1978942.1979196", "10.1093/nar/gkh061", "10.1016/j.jvlc.2003.06.003", "10.1186/s13326-016-0047-3", "10.1542/peds.2013-0819", "10.1179/000870403235002042", "10.1001/jama.296.2.212", "10.1016/j.ajhg.2009.09.003", "10.1111/j.1467-8659.2012.03108.x", "10.1371/journal.pone.0159621", "10.1137/0105003", "10.1016/j.websem.2010.03.005", "10.1016/j.jbi.2015.10.001", "10.1007/978-3-642-25364-5_22", "10.1561/1100000039", "10.1101/mcs.a000372", "10.3115/v1/W14-3110", "10.1177/1473871614526077", "10.1093/nar/gkh061", "10.1016/j.jvlc.2003.06.003", "10.1186/s13326-016-0047-3", "10.1542/peds.2013-0819", "10.1179/000870403235002042", "10.1001/jama.296.2.212", "10.1016/j.ajhg.2009.09.003", "10.1111/j.1467-8659.2012.03108.x", "10.1371/journal.pone.0159621", "10.1137/0105003", "10.1016/j.websem.2010.03.005", "10.1016/j.jbi.2015.10.001", "10.1007/978-3-642-25364-5_22", "10.1561/1100000039", "10.1101/mcs.a000372", "10.3115/v1/W14-3110", "10.1177/1473871614526077", "10.1093/nar/gkh061", "10.1016/j.jvlc.2003.06.003", "10.1186/s13326-016-0047-3", "10.1542/peds.2013-0819", "10.1179/000870403235002042", "10.1001/jama.296.2.212", "10.1016/j.ajhg.2009.09.003", "10.1111/j.1467-8659.2012.03108.x", "10.1371/journal.pone.0159621", "10.1137/0105003", "10.1016/j.websem.2010.03.005", "10.1016/j.jbi.2015.10.001", "10.1007/978-3-642-25364-5_22", "10.1561/1100000039", "10.1101/mcs.a000372", "10.3115/v1/W14-3110", "10.1177/1473871614526077"]}, "10.1109/TVCG.2017.2745080": {"doi": "10.1109/TVCG.2017.2745080", "author": ["M. El-Assady", "R. Sevastjanova", "F. Sperrle", "D. Keim", "C. Collins"], "title": "Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework", "year": "2018", "abstract": "Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.", "keywords": ["decision making", "interactive systems", "learning (artificial intelligence)", "relevance feedback", "text analysis", "progressive learning", "topic modeling parameters", "thematic composition", "text corpora", "modular visual analytics framework", "reinforcement learning process", "document corpus", "algorithm configurations", "parameter space analysis", "document separability", "model complexity", "interactive visual workspace", "topic summaries", "parameter distributions", "iterative decision-making technique", "user-endorsed topic distribution", "topic model quality improvements", "topic model adaptability", "topic model understandability", "Analytical models", "Adaptation models", "Computational modeling", "Visual analytics", "Data models", "Learning (artificial intelligence)", "Topic Model Configuration", "Reinforcement Learning", "Feature Detection and Tracking", "Iterative Optimization"], "referenced_by": ["IKEY:8534018", "IKEY:8440842", "IKEY:8467535", "IKEY:8356097", "IKEY:8740868", "IKEY:8781583", "IKEY:8807224", "IKEY:8807299", "IKEY:8805463", "IKEY:8933646", "IKEY:8973381", "IKEY:8986917", "IKEY:8663312", "IKEY:9238590"], "referencing": []}, "10.1109/TVCG.2017.2744359": {"doi": "10.1109/TVCG.2017.2744359", "author": ["D. A. Szafir"], "title": "Modeling Color Difference for Visualization Design", "year": "2018", "abstract": "Color is frequently used to encode values in visualizations. For color encodings to be effective, the mapping between colors and values must preserve important differences in the data. However, most guidelines for effective color choice in visualization are based on either color perceptions measured using large, uniform fields in optimal viewing environments or on qualitative intuitions. These limitations may cause data misinterpretation in visualizations, which frequently use small, elongated marks. Our goal is to develop quantitative metrics to help people use color more effectively in visualizations. We present a series of crowdsourced studies measuring color difference perceptions for three common mark types: points, bars, and lines. Our results indicate that peoples' abilities to perceive color differences varies significantly across mark types. Probabilistic models constructed from the resulting data can provide objective guidance for designers, allowing them to anticipate viewer perceptions in order to inform effective encoding design.", "keywords": ["data visualisation", "image coding", "image colour analysis", "modeling color difference", "visualization design", "color encodings", "color perceptions", "uniform fields", "optimal viewing environments", "qualitative intuitions", "data misinterpretation", "color difference perceptions", "color differences", "probabilistic models", "effective encoding design", "Image color analysis", "Color", "Encoding", "Visualization", "Measurement", "Data visualization", "Computational modeling", "Color Perception", "Graphical Perception", "Color Models", "Crowdsourcing"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744184": {"doi": "10.1109/TVCG.2017.2744184", "author": ["A. Sarikaya", "M. Gleicher"], "title": "Scatterplots: Tasks, Data, and Designs", "year": "2018", "abstract": "Traditional scatterplots fail to scale as the complexity and amount of data increases. In response, there exist many design options that modify or expand the traditional scatterplot design to meet these larger scales. This breadth of design options creates challenges for designers and practitioners who must select appropriate designs for particular analysis goals. In this paper, we help designers in making design choices for scatterplot visualizations. We survey the literature to catalog scatterplot-specific analysis tasks. We look at how data characteristics influence design decisions. We then survey scatterplot-like designs to understand the range of design options. Building upon these three organizations, we connect data characteristics, analysis tasks, and design choices in order to generate challenges, open questions, and example best practices for the effective design of scatterplots.", "keywords": ["data visualisation", "design options", "traditional scatterplot design", "scatterplot visualizations", "catalog scatterplot-specific analysis tasks", "data characteristics influence design decisions", "traditional scatterplots", "Data visualization", "Taxonomy", "Correlation", "Complexity theory", "Organizations", "Visualization", "Scatterplots", "task taxonomies", "study of designs"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744199": {"doi": "10.1109/TVCG.2017.2744199", "author": ["M. Gleicher"], "title": "Considerations for Visualizing Comparison", "year": "2018", "abstract": "Supporting comparison is a common and diverse challenge in visualization. Such support is difficult to design because solutions must address both the specifics of their scenario as well as the general issues of comparison. This paper aids designers by providing a strategy for considering those general issues. It presents four considerations that abstract comparison. These considerations identify issues and categorize solutions in a domain independent manner. The first considers how the common elements of comparison-a target set of items that are related and an action the user wants to perform on that relationship-are present in an analysis problem. The second considers why these elements lead to challenges because of their scale, in number of items, complexity of items, or complexity of relationship. The third considers what strategies address the identified scaling challenges, grouping solutions into three broad categories. The fourth considers which visual designs map to these strategies to provide solutions for a comparison analysis problem. In sequence, these considerations provide a process for developers to consider support for comparison in the design of visualization tools. Case studies show how these considerations can help in the design and evaluation of visualization solutions for comparison problems.", "keywords": ["data visualisation", "visualizing comparison", "visual design map", "visualization tools", "comparison analysis problem", "abstract comparison", "Visualization", "Tools", "Data visualization", "Electronic mail", "Complexity theory", "Encoding", "Information Visualization", "Comparison", "Taxonomies", "Visualization Models", "Task Analysis", "Computer Graphics", "Genomics", "Humans", "Image Processing, Computer-Assisted", "Models, Theoretical"], "referenced_by": ["IKEY:8370204", "IKEY:8440817", "IKEY:8440851", "IKEY:8440115", "IKEY:8440856", "IKEY:8467535", "IKEY:8802509", "IKEY:8809920", "IKEY:8827593", "IKEY:8807320", "IKEY:8805450", "IKEY:8807224", "IKEY:8816695", "IKEY:8845772", "IKEY:8933646", "IKEY:8933671", "IKEY:8933655", "IKEY:9086215", "IKEY:8636969", "IKEY:8676380", "IKEY:9308624"], "referencing": []}, "10.1109/TVCG.2017.2743998": {"doi": "10.1109/TVCG.2017.2743998", "author": ["R. Vuillemot", "J. Boy"], "title": "Structuring Visualization Mock-Ups at the Graphical Level by Dividing the Display Space", "year": "2018", "abstract": "Mock-ups are rapid, low fidelity prototypes, that are used in many design-related fields to generate and share ideas. While their creation is supported by many mature methods and tools, surprisingly few are suited for the needs of information visualization. In this article, we introduce a novel approach to creating visualizations mock-ups, based on a dialogue between graphic design and parametric toolkit explorations. Our approach consists in iteratively subdividing the display space, while progressively informing each division with realistic data. We show that a wealth of mock-ups can easily be created using only temporary data attributes, as we wait for more realistic data to become available. We describe the implementation of this approach in a D3-based toolkit, which we use to highlight its generative power, and we discuss the potential for transitioning towards higher fidelity prototypes.", "keywords": ["data visualisation", "software prototyping", "user centred design", "information visualization", "graphic design", "parametric toolkit explorations", "display space", "temporary data attributes", "visualization mock-ups", "graphical level", "low fidelity prototypes", "design-related fields", "visualizations mock-ups", "Data visualization", "Visualization", "Tools", "Prototypes", "Rendering (computer graphics)", "Pipelines", "Design Methodologies", "Rapid Prototyping", "Graphic Design", "Mock-Ups", "Toolkit Design"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744319": {"doi": "10.1109/TVCG.2017.2744319", "author": ["H. Lam", "M. Tory", "T. Munzner"], "title": "Bridging from Goals to Tasks with Design Study Analysis Reports", "year": "2018", "abstract": "Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.", "keywords": ["data analysis", "data visualisation", "pattern classification", "specification languages", "design study analysis reports", "abstract task classifications", "low-level actions", "analysis goal", "analysis process", "open-coding 20 design study papers", "analysis steps", "abstracting domain problems", "visualization designers", "analysis stream", "design evaluation", "design generation", "domain-specific language", "compare entities", "explain differences", "evaluate hypothesis", "discover observation", "describe observation", "identify main cause", "collect evidence", "Sociology", "Statistics", "Data visualization", "Data analysis", "Bridges", "Market research", "Visualization", "Framework", "Data Analysis", "Analysis Goals", "Design Studies", "Open Coding", "Task Classifications"], "referenced_by": ["IKEY:8440834", "IKEY:8440080", "IKEY:8864010", "IKEY:8809920", "IKEY:8805434", "IKEY:8919687", "IKEY:8933542"], "referencing": []}, "10.1109/TVCG.2017.2743898": {"doi": "10.1109/TVCG.2017.2743898", "author": ["J. Hullman", "M. Kay", "Y. Kim", "S. Shrestha"], "title": "Imagining Replications: Graphical Prediction Discrete Visualizations Improve Recall Estimation of Effect Uncertainty", "year": "2018", "abstract": "People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an interactive, graphical uncertainty prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one's ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples' estimates of probability distributions and to communicate uncertainty effectively.", "keywords": ["cognition", "computer aided instruction", "data visualisation", "inference mechanisms", "mathematics computing", "statistical analysis", "statistical distributions", "uncertainty handling", "effect uncertainty", "uncertain processes", "scientific experiments", "uncertainty visualizations", "active learning", "statistical reasoning", "scientific reports", "interactive uncertainty prediction technique", "graphical uncertainty prediction technique", "sampling distribution", "experiment replications", "discrete outcomes", "statistical knowledge", "discrete visualizations", "probability distributions", "Uncertainty", "Sociology", "Statistics", "Data visualization", "Cognition", "Probability distribution", "Visualization", "Graphical prediction", "interactive uncertainty visualization", "replication crisis", "probability distribution"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2745941": {"doi": "10.1109/TVCG.2017.2745941", "author": ["B. Bach", "R. Sicat", "J. Beyer", "M. Cordeil", "H. Pfister"], "title": "The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?", "year": "2018", "abstract": "We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user's real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.", "keywords": ["augmented reality", "data visualisation", "helmet mounted displays", "spatial positioning", "tangible markers", "visualization environments", "human perceptual", "interaction modalities", "spatial proximity", "cognitive proximity", "spatial perception", "desktop environment", "interactive exploration", "immersive tangible augmented reality", "natural human perception", "Microsoft HoloLens", "handheld tablet", "HoloLens display", "virtual content", "spatial position", "3D exploration", "augmented-reality head-mounted display", "3D hologram", "3D content", "3D visualization", "2D surface", "Three-dimensional displays", "Data visualization", "Two dimensional displays", "Augmented reality", "Stereo image processing", "Visualization", "Mice", "Augmented Reality", "3D Interaction", "User Study", "Immersive Displays", "Computer Graphics", "Female", "Holography", "Humans", "Imaging, Three-Dimensional", "Male", "Perception", "Task Performance and Analysis", "User-Computer Interface", "Virtual Reality"], "referenced_by": ["IKEY:8410303", "IKEY:8406848", "IKEY:8489864", "IKEY:8549615", "IKEY:8534024", "IKEY:8440858", "IKEY:8640072", "IKEY:8797978", "IKEY:8797733", "IKEY:8798056", "IKEY:8797812", "IKEY:8820844", "IKEY:8864548", "IKEY:8836087", "IKEY:8805467", "IKEY:8962793", "IKEY:9054812", "IKEY:9089446", "IKEY:8611113", "IKEY:9141416", "IKEY:9211184", "IKEY:9178307", "IKEY:9212069", "IKEY:9199566", "IKEY:9207831", "IKEY:9287824"], "referencing": []}, "10.1109/TVCG.2017.2744198": {"doi": "10.1109/TVCG.2017.2744198", "author": ["Z. Qu", "J. Hullman"], "title": "Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring", "year": "2018", "abstract": "Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.", "keywords": ["constraint satisfaction problems", "constraint theory", "data visualisation", "dashboard", "visualization design guidelines", "visualization authoring software", "exploration-to-presentation visualization design scenario", "consistent encodings", "constraint validations", "automatic consistency checking", "consistency mechanisms", "encoding-specific constraints", "constraint-satisfaction", "Tableau users", "Encoding", "Data visualization", "Image color analysis", "Visualization", "Adaptation models", "Color", "Guidelines", "Visualization Design", "Qualitative Study", "Evaluation"], "referenced_by": ["IKEY:8416240", "IKEY:8440829", "IKEY:8440847", "IKEY:8440847", "IKEY:8443395", "IKEY:8440856", "IKEY:8440846", "IKEY:8634420", "IKEY:8634026", "IKEY:8802415", "IKEY:8805442", "IKEY:8933582", "IKEY:8933655", "IKEY:8945039", "IKEY:8973383", "IKEY:9086193", "IKEY:8636969"], "referencing": []}, "10.1109/TVCG.2017.2743858": {"doi": "10.1109/TVCG.2017.2743858", "author": ["O. Kwon", "T. Crnovrsanin", "K. Ma"], "title": "What Would a Graph Look Like in this Layout? A Machine Learning Approach to Large Graph Visualization", "year": "2018", "abstract": "Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a \u201cgood\u201d layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.", "keywords": ["data visualisation", "graph theory", "learning (artificial intelligence)", "graph kernel", "graph look", "graph visualization", "viewer perceives different information", "visual inspection", "visual appearances", "aesthetic metrics", "topological similarity", "estimation calculation", "perceptual similarity", "machine learning", "Kernel", "Layout", "Measurement", "Visualization", "Inspection", "Data visualization", "Support vector machines", "Graph visualization", "graph layout", "aesthetics", "machine learning", "graph kernel", "graphlet", "Adolescent", "Adult", "Algorithms", "Computer Graphics", "Esthetics", "Female", "Humans", "Machine Learning", "Male", "Task Performance and Analysis", "Young Adult"], "referenced_by": ["IKEY:8440813", "IKEY:8739137", "IKEY:8807275", "IKEY:8805452", "IKEY:9006128", "IKEY:9086287", "IKEY:9262659", "IKEY:9259656", "IKEY:9308624"], "referencing": []}, "10.1109/TVCG.2017.2745919": {"doi": "10.1109/TVCG.2017.2745919", "author": ["Y. Wang", "Y. Wang", "Y. Sun", "L. Zhu", "K. Lu", "C. Fu", "M. Sedlmair", "O. Deussen", "B. Chen"], "title": "Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization", "year": "2018", "abstract": "We present an improved stress majorization method that incorporates various constraints, including directional constraints without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to parallelize computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. As a result, we allow the constraint-based exploration of large graphs with 10K nodes - an approach which previous methods cannot support.", "keywords": ["computational geometry", "data visualisation", "gradient methods", "graph theory", "graphics processing units", "interactive systems", "mathematics computing", "optimisation", "parallel processing", "improved stress majorization method", "directional constraints", "layout constraints", "GPU conjugant gradient solver", "interactive constrained graph visualization", "cluster separation constraints", "unconstrained graph visualizations", "constrained graph visualizations", "node distances", "edge lengths", "edge vectors", "constraint optimization problem", "Stress", "Layout", "Visualization", "Springs", "Optimization", "Computational modeling", "Shape", "Graph visualization", "stress majorization", "constraints"], "referenced_by": ["IKEY:8440835", "IKEY:8807275", "IKEY:8807379", "IKEY:8807234", "IKEY:8993793", "IKEY:9047320", "IKEY:8625484"], "referencing": []}, "10.1109/TVCG.2017.2744338": {"doi": "10.1109/TVCG.2017.2744338", "author": ["C. Hurter", "S. Puechmorel", "F. Nicol", "A. Telea"], "title": "Functional Decomposition for Bundled Simplification of Trail Sets", "year": "2018", "abstract": "Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.", "keywords": ["computational geometry", "data visualisation", "graph theory", "pattern clustering", "polynomials", "principal component analysis", "tensors", "vectors", "trail sets", "principal component functions", "trajectory analysis", "vector field visualization", "tensor field visualization", "graph bundling", "3D curve-sets", "bundling method", "cluster centroids", "complementary term", "centroid curve", "associated expansion coefficients", "piecewise-polynomial basis functions", "linear combinations", "graph drawings", "aggregates curves", "bundled simplification", "functional decomposition", "Shape", "Data models", "Two dimensional displays", "Three-dimensional displays", "Data visualization", "Splines (mathematics)", "Clutter", "path visualization", "trajectory visualization", "edge bundles", "functional decomposition", "path generation", "streamlines"], "referenced_by": ["IKEY:8440808", "IKEY:8809845", "IKEY:8807234"], "referencing": []}, "10.1109/TVCG.2017.2745219": {"doi": "10.1109/TVCG.2017.2745219", "author": ["A. Srinivasan", "J. Stasko"], "title": "Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks", "year": "2018", "abstract": "Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.", "keywords": ["data visualisation", "interactive systems", "natural language interfaces", "Orko", "multimodal interaction", "visual exploration", "data visualization systems", "direct manipulation interfaces", "natural language interfaces", "complementary interaction techniques", "direct manipulation input", "prototype visualization system", "multimodal network visualization interfaces", "touch-based direct manipulation input", "WIMP-based interfaces", "Data visualization", "Natural languages", "Visualization", "Taxonomy", "Prototypes", "Mice", "Speech", "Multimodal interaction", "network visualization", "natural language input", "direct manipulation", "multitouch input"], "referenced_by": ["IKEY:8440115", "IKEY:8440860", "IKEY:8440846", "IKEY:8440813", "IKEY:8587063", "IKEY:8807265", "IKEY:8933569", "IKEY:8943760", "IKEY:8986918", "IKEY:9023497", "IKEY:8977320", "IKEY:9118800", "IKEY:9303381"], "referencing": []}, "10.1109/TVCG.2017.2745978": {"doi": "10.1109/TVCG.2017.2745978", "author": ["F. Lekschas", "B. Bach", "P. Kerpedjiev", "N. Gehlenborg", "H. Pfister"], "title": "HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples", "year": "2018", "abstract": "This paper presents an interactive visualization interface-HiPiler-for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like \u201csnippets\u201d. Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.", "keywords": ["genomics", "interactive systems", "matrix algebra", "medical computing", "HiPiler", "visual exploration", "interactive visualization interface", "scatterplots", "thumbnail-like snippets", "first-class objects", "navigable genome interaction matrix", "region-of-interest", "visual patterns", "ROI", "Bioinformatics", "Genomics", "Algorithm design and analysis", "Interviews", "Visualization", "Data visualization", "Interactive Small Multiples", "Matrix Comparison", "Biomedical Visualization", "Genomics", "Algorithms", "Computer Graphics", "Genomics", "Humans", "Image Processing, Computer-Assisted", "Software"], "referenced_by": ["IKEY:8534028", "IKEY:8809750", "IKEY:8807245"], "referencing": []}, "10.1109/TVCG.2017.2745278": {"doi": "10.1109/TVCG.2017.2745278", "author": ["B. C. M. Cappers", "J. J. van Wijk"], "title": "Exploring Multivariate Event Sequences Using Rules, Aggregations, and Selections", "year": "2018", "abstract": "Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are some examples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Current exploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariate data only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously by interactively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types of events or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-up glyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defined domain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication traffic and hospital treatments.", "keywords": ["data visualisation", "pattern clustering", "rewriting systems", "telecommunication traffic", "ubiquitous computing", "telecommunication conversations", "travel history", "hospital treatment", "telecommunication traffic", "bottom-up glyph-oriented approach", "Eventpad", "multivariate level", "structural analysis", "temporal analysis", "multivariate event sequence analysis", "multivariate regular expressions", "sequential level", "Data visualization", "Visualization", "Sequences", "Data mining", "Encoding", "Communications technology", "Hospitals", "Event Visualization", "Multivariate Events", "Regular Expressions", "Sequence Alignment", "Interaction", "Communication", "Computer Graphics", "Data Mining", "Humans", "Informatics", "User-Computer Interface"], "referenced_by": ["IKEY:8440803", "IKEY:8440851", "IKEY:8440811", "IKEY:8585619", "IKEY:8706292", "IKEY:8709212", "IKEY:8709230", "IKEY:8387525", "IKEY:8400404", "IKEY:8807220", "IKEY:8805450", "IKEY:8933770", "IKEY:8986923", "IKEY:9308628"], "referencing": []}, "10.1109/TVCG.2017.2744339": {"doi": "10.1109/TVCG.2017.2744339", "author": ["J. Matute", "A. C. Telea", "L. Linsen"], "title": "Skeleton-Based Scagnostics", "year": "2018", "abstract": "Scatterplot matrices (SPLOMs) are widely used for exploring multidimensional data. Scatterplot diagnostics (scagnostics) approaches measure characteristics of scatterplots to automatically find potentially interesting plots, thereby making SPLOMs more scalable with the dimension count. While statistical measures such as regression lines can capture orientation, and graph-theoretic scagnostics measures can capture shape, there is no scatterplot characterization measure that uses both descriptors. Based on well-known results in shape analysis, we propose a scagnostics approach that captures both scatterplot shape and orientation using skeletons (or medial axes). Our representation can handle complex spatial distributions, helps discovery of principal trends in a multiscale way, scales visually well with the number of samples, is robust to noise, and is automatic and fast to compute. We define skeleton-based similarity metrics for the visual exploration and analysis of SPLOMs. We perform a user study to measure the human perception of scatterplot similarity and compare the outcome to our results as well as to graph-based scagnostics and other visual quality metrics. Our skeleton-based metrics outperform previously defined measures both in terms of closeness to perceptually-based similarity and computation time efficiency.", "keywords": ["data visualisation", "graph theory", "matrix algebra", "statistical analysis", "human perception", "complex spatial distributions", "medial axes", "scatterplot shape", "shape analysis", "scatterplot characterization measure", "graph-theoretic scagnostics measures", "regression lines", "statistical measures", "scatterplot diagnostics", "multidimensional data", "scatterplot matrices", "visual quality metrics", "scatterplot similarity", "SPLOMs", "visual exploration", "skeleton-based similarity metrics", "Shape", "Two dimensional displays", "Visualization", "Shape measurement", "Skeleton", "Correlation", "Multidimensional Data (primary keyword)", "High-Dimensional Data"], "referenced_by": ["IKEY:8369062", "IKEY:8667672", "IKEY:8781594", "IKEY:8807247", "IKEY:8933670", "IKEY:9086221"], "referencing": []}, "10.1109/TVCG.2017.2745141": {"doi": "10.1109/TVCG.2017.2745141", "author": ["S. Liu", "P. Bremer", "J. J. Thiagarajan", "V. Srikumar", "B. Wang", "Y. Livnat", "V. Pascucci"], "title": "Visual Exploration of Semantic Relationships in Neural Word Embeddings", "year": "2018", "abstract": "Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.", "keywords": ["data visualisation", "natural language processing", "principal component analysis", "stochastic processes", "neural word embeddings", "distributed representations", "neural language models", "crucial component", "natural language processing", "NLP community", "high-dimensional visualization techniques", "stochastic neighbor embeddings", "two-dimensional embeddings", "word analogies", "semantic relationships", "embedding techniques", "semantic analogies", "syntactic analogies", "salient structures", "analogy relationships", "domain-specific tasks", "visual exploration", "vector spaces", "linear relationships", "domain-specific visualization", "t-SNE embeddings", "Semantics", "Principal component analysis", "Visualization", "Natural language processing", "Tools", "Data visualization", "Natural Language Processing", "Word Embedding", "High-Dimensional Data"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2743939": {"doi": "10.1109/TVCG.2017.2743939", "author": ["L. E. Matzen", "M. J. Haass", "K. M. Divis", "Z. Wang", "A. T. Wilson"], "title": "Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations", "year": "2018", "abstract": "Evaluating the effectiveness of data visualizations is a challenging undertaking and often relies on one-off studies that test a visualization in the context of one specific task. Researchers across the fields of data science, visualization, and human-computer interaction are calling for foundational tools and principles that could be applied to assessing the effectiveness of data visualizations in a more rapid and generalizable manner. One possibility for such a tool is a model of visual saliency for data visualizations. Visual saliency models are typically based on the properties of the human visual cortex and predict which areas of a scene have visual features (e.g. color, luminance, edges) that are likely to draw a viewer's attention. While these models can accurately predict where viewers will look in a natural scene, they typically do not perform well for abstract data visualizations. In this paper, we discuss the reasons for the poor performance of existing saliency models when applied to data visualizations. We introduce the Data Visualization Saliency (DVS) model, a saliency model tailored to address some of these weaknesses, and we test the performance of the DVS model and existing saliency models by comparing the saliency maps produced by the models to eye tracking data obtained from human viewers. Finally, we describe how modified saliency models could be used as general tools for assessing the effectiveness of visualizations, including the strengths and weaknesses of this approach.", "keywords": ["computer vision", "data visualisation", "feature extraction", "image colour analysis", "natural scenes", "neurophysiology", "object detection", "saliency maps", "DVS model", "data visualization saliency model", "human visual cortex", "visual saliency models", "modified saliency models", "eye tracking data", "abstract data visualizations", "visual features", "Data visualization", "Visualization", "Measurement", "Data models", "Brain modeling", "Predictive models", "Tools", "Visual saliency", "evaluation", "eye tracking"], "referenced_by": ["IKEY:8454276", "IKEY:8933779"], "referencing": []}, "10.1109/TVCG.2017.2745086": {"doi": "10.1109/TVCG.2017.2745086", "author": ["D. Burlinson", "K. Subramanian", "P. Goolkasian"], "title": "Open vs. Closed Shapes: New Perceptual Categories?", "year": "2018", "abstract": "Effective communication using visualization relies in part on the use of viable encoding strategies. For example, a viewer's ability to rapidly and accurately discern between two or more categorical variables in a chart or figure is contingent upon the distinctiveness of the encodings applied to each variable. Research in perception suggests that color is a more salient visual feature when compared to shape and although that finding is supported by visualization studies, characteristics of shape also yield meaningful differences in distinctiveness. We propose that open or closed shapes (that is, whether shapes are composed of line segments that are bounded across a region of space or not) represent a salient characteristic that influences perceptual processing. Three experiments were performed to test the reliability of the open/closed category; the first two from the perspective of attentional allocation, and the third experiment in the context of multi-class scatterplot displays. In the first, a flanker paradigm was used to test whether perceptual load and open/closed feature category would modulate the effect of the flanker on target processing. Results showed an influence of both variables. The second experiment used a Same/Different reaction time task to replicate and extend those findings. Results from both show that responses are faster and more accurate when closed rather than open shapes are processed as targets, and there is more processing interference when two competing shapes come from the same rather than different open or closed feature categories. The third experiment employed three commonly used visual analytic tasks - perception of average value, numerosity, and linear relationships with both single and dual displays of open and closed symbols. Our findings show that for numerosity and trend judgments, in particular, that different symbols from the same open or closed feature category cause more perceptual interference when they are presented together in a plot than symbols from different categories. Moreover, the extent of the interference appears to depend upon whether the participant is focused on processing open or closed symbols.", "keywords": ["image coding", "image sampling", "visual perception", "closed shapes", "categorical variables", "visual analytic tasks", "open feature categories", "encoding strategies", "different reaction time task", "same reaction time task", "perceptual interference", "open symbols", "closed feature categories", "processing interference", "target processing", "perceptual load", "flanker paradigm", "multiclass scatterplot displays", "perceptual processing", "line segments", "open shapes", "salient visual feature", "Shape", "Encoding", "Visual analytics", "Data visualization", "Visual perception", "Interference", "scatterplot", "visualization design", "perceptual category", "open shape", "closed shape", "Adult", "Computer Graphics", "Female", "Humans", "Male", "Reaction Time", "Task Performance and Analysis", "Visual Perception", "Young Adult"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744138": {"doi": "10.1109/TVCG.2017.2744138", "author": ["A. C. Valdez", "M. Ziefle", "M. Sedlmair"], "title": "Priming and Anchoring Effects in Visualization", "year": "2018", "abstract": "We investigate priming and anchoring effects on perceptual tasks in visualization. Priming or anchoring effects depict the phenomena that a stimulus might influence subsequent human judgments on a perceptual level, or on a cognitive level by providing a frame of reference. Using visual class separability in scatterplots as an example task, we performed a set of five studies to investigate the potential existence of priming and anchoring effects. Our findings show that - under certain circumstances - such effects indeed exist. In other words, humans judge class separability of the same scatterplot differently depending on the scatterplot(s) they have seen before. These findings inform future work on better understanding and more accurately modeling human perception of visual patterns.", "keywords": ["behavioural sciences computing", "cognition", "data visualisation", "human factors", "visual perception", "visual class separability", "scatterplot", "visualization", "cognitive level", "human perception", "visual patterns", "perceptual level", "perceptual tasks", "priming anchoring effects", "Data visualization", "Visualization", "Visual perception", "Correlation", "Cognition", "Data models", "Uncertainty", "Perception", "Anchoring", "Bias", "Scatterplots", "Visualization", "MTurk Study", "Bias", "Crowdsourcing", "Databases, Factual", "Humans", "Models, Psychological", "Psychological Tests", "Repetition Priming", "Research Design", "Visual Perception"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744118": {"doi": "10.1109/TVCG.2017.2744118", "author": ["N. W. Kim", "B. Bach", "H. Im", "S. Schriber", "M. Gross", "H. Pfister"], "title": "Visualizing Nonlinear Narratives with Story Curves", "year": "2018", "abstract": "In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.", "keywords": ["cinematography", "data visualisation", "interactive systems", "user interfaces", "nonlinear narrative", "story curves", "acclaimed movies", "communicating nonlinear narratives", "actual temporal order", "actual chronological order", "visual patterns", "Story Explorer", "narrative patterns", "nonlinear movies", "Visualization", "Motion pictures", "Metadata", "Tools", "Writing", "Production", "Games", "Nonlinear narrative", "storytelling", "visualization"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2743859": {"doi": "10.1109/TVCG.2017.2743859", "author": ["P. Koytek", "C. Perin", "J. Vermeulen", "E. Andr\u00e9", "S. Carpendale"], "title": "MyBrush: Brushing and Linking with Personal Agency", "year": "2018", "abstract": "We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.", "keywords": ["data visualisation", "interactive systems", "organisational aspects", "user interfaces", "design space", "MyBrush", "multiple brushes", "complex tasks", "interaction technique", "personal agency", "brushing linking technique", "information visualization", "unified interface", "Joining processes", "Visualization", "Data visualization", "Brushes", "Image color analysis", "Shape", "Complexity theory", "Brushing", "linking", "personal agency", "coordinated multiple views", "interaction", "design space", "information visualization", "Computer Graphics", "Humans", "Personal Autonomy", "Software"], "referenced_by": ["IKEY:8440836", "IKEY:8440846", "IKEY:8739141", "IKEY:8797978", "IKEY:9239006"], "referencing": []}, "10.1109/TVCG.2017.2744018": {"doi": "10.1109/TVCG.2017.2744018", "author": ["N. Rodrigues", "D. Weiskopf"], "title": "Nonlinear Dot Plots", "year": "2018", "abstract": "Conventional dot plots use a constant dot size and are typically applied to show the frequency distribution of small data sets. Unfortunately, they are not designed for a high dynamic range of frequencies. We address this problem by introducing nonlinear dot plots. Adopting the idea of nonlinear scaling from logarithmic bar charts, our plots allow for dots of varying size so that columns with a large number of samples are reduced in height. For the construction of these diagrams, we introduce an efficient two-way sweep algorithm that leads to a dense and symmetrical layout. We compensate aliasing artifacts at high dot densities by a specifically designed low-pass filtering method. Examples of nonlinear dot plots are compared to conventional dot plots as well as linear and logarithmic histograms. Finally, we include feedback from an expert review.", "keywords": ["data handling", "data visualisation", "low-pass filters", "constant dot size", "nonlinear dot plots", "logarithmic histograms", "linear histograms", "low-pass filtering method", "aliasing artifacts", "two-way sweep algorithm", "logarithmic bar charts", "nonlinear scaling", "frequency distribution", "Histograms", "Data visualization", "Layout", "Dynamic range", "Bars", "Rendering (computer graphics)", "Algorithm design and analysis", "Nonlinear dot plot", "statistical graphics", "sweep algorithm", "layout"], "referenced_by": ["IKEY:8802509"], "referencing": []}, "10.1109/TVCG.2017.2744019": {"doi": "10.1109/TVCG.2017.2744019", "author": ["R. Langner", "T. Horak", "R. Dachselt"], "title": "VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration", "year": "2018", "abstract": "We present VisTiles, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with different individual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated & multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and flexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the final prototype.", "keywords": ["data visualisation", "groupware", "mobile computing", "VisTiles", "mobile devices", "visual data exploration", "information visualization", "dynamic user", "collaborative data exploration", "smart adaptations", "multivariate data collection", "Data visualization", "Visualization", "Mobile communication", "Prototypes", "Smart phones", "Collaboration", "Mobile devices", "coordinated & multiple views", "multi-display environment", "cross-device interaction"], "referenced_by": ["IKEY:8440803", "IKEY:8440846", "IKEY:8805467", "IKEY:8914209", "IKEY:8933673", "IKEY:8945039", "IKEY:9006637"], "referencing": []}, "10.1109/TVCG.2017.2744320": {"doi": "10.1109/TVCG.2017.2744320", "author": ["J. Poco", "A. Mayhua", "J. Heer"], "title": "Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations", "year": "2018", "abstract": "Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations \u201cin the wild\u201d often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.", "keywords": ["data visualisation", "feature extraction", "image colour analysis", "image segmentation", "optical character recognition", "text detection", "user interfaces", "legend text", "OCR methods", "discrete color encoding", "continuous color encoding", "legend location", "bitmap visualization image", "color encodings", "perceptual color design principles", "categorical data", "quantitative data", "bitmap images", "static visualizations", "color mappings", "accurate automatic inference", "interactive overlays", "perceptual effectiveness", "automatic recoloring", "annotation interface", "correct interpretation errors", "Image color analysis", "Data visualization", "Data mining", "Encoding", "Image coding", "Optical character recognition software", "Feature extraction", "Visualization", "color", "chart understanding", "information extraction", "redesign", "computer vision"], "referenced_by": ["IKEY:8440847", "IKEY:8440831", "IKEY:8614322", "IKEY:8809832", "IKEY:8805429", "IKEY:8807266"], "referencing": []}, "10.1109/TVCG.2017.2745859": {"doi": "10.1109/TVCG.2017.2745859", "author": ["Y. Wang", "X. Chu", "C. Bao", "L. Zhu", "O. Deussen", "B. Chen", "M. Sedlmair"], "title": "EdWordle: Consistency-Preserving Word Cloud Editing", "year": "2018", "abstract": "We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.", "keywords": ["data visualisation", "solid modelling", "text editing", "EdWordle", "user performance", "user satisfaction", "consistency-preserving word cloud editing", "constrained rigid body simulation", "neighborhood-aware local Wordle algorithm", "Tag clouds", "Layout", "Tools", "Visualization", "Semantics", "Heuristic algorithms", "Data visualization", "Wordle", "consistency", "text visualization"], "referenced_by": ["IKEY:8564149", "IKEY:8621984", "IKEY:8807355", "IKEY:8665933"], "referencing": []}, "10.1109/TVCG.2017.2746018": {"doi": "10.1109/TVCG.2017.2746018", "author": ["C. Felix", "S. Franconeri", "E. Bertini"], "title": "Taking Word Clouds Apart: An Empirical Investigation of the Design Space for Keyword Summaries", "year": "2018", "abstract": "In this paper we present a set of four user studies aimed at exploring the visual design space of what we call keyword summaries: lists of words with associated quantitative values used to help people derive an intuition of what information a given document collection (or part of it) may contain. We seek to systematically study how different visual representations may affect people's performance in extracting information out of keyword summaries. To this purpose, we first create a design space of possible visual representations and compare the possible solutions in this design space through a variety of representative tasks and performance metrics. Other researchers have, in the past, studied some aspects of effectiveness with word clouds, however, the existing literature is somewhat scattered and do not seem to address the problem in a sufficiently systematic and holistic manner. The results of our studies showed a strong dependency on the tasks users are performing. In this paper we present details of our methodology, the results, as well as, guidelines on how to design effective keyword summaries based in our discoveries.", "keywords": ["data visualisation", "information retrieval", "text analysis", "keyword summaries", "word clouds", "visual representations", "document collection", "quantitative values", "performance metrics", "visual design space", "empirical investigation", "Tag clouds", "Visualization", "Layout", "Extraterrestrial measurements", "Encoding", "Data mining", "Systematics", "Word Clouds", "Tag Clouds", "Text Visualization", "Keyword Summaries"], "referenced_by": ["IKEY:8320320", "IKEY:8365992", "IKEY:8802449", "IKEY:8807355", "IKEY:8807224", "IKEY:8665933", "IKEY:9200056", "IKEY:9237998", "10.1016/j.jvlc.2017.11.009"], "referencing": []}, "10.1109/TVCG.2017.2744158": {"doi": "10.1109/TVCG.2017.2744158", "author": ["H. Strobelt", "S. Gehrmann", "H. Pfister", "A. M. Rush"], "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks", "year": "2018", "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community.", "keywords": ["data visualisation", "learning (artificial intelligence)", "recurrent neural nets", "statistical analysis", "LSTMVis", "hidden state dynamics", "recurrent neural networks", "short-term memory networks", "remarkably effective tool", "black-box hidden representation", "hidden state representations", "machine learning community", "statistical analysis", "chord progressions", "phrase structure", "nesting", "long-term usage data", "specific hidden state properties", "local state changes", "visual analysis tool", "Tools", "Recurrent neural networks", "Visualization", "Pattern matching", "Computational modeling", "Data models", "Visualization", "Machine Learning", "Recurrent Neural Networks", "LSTM", "Computer Graphics", "Machine Learning", "Neural Networks (Computer)", "Pattern Recognition, Automated", "User-Computer Interface"], "referenced_by": ["IKEY:8320546", "IKEY:8402187", "IKEY:8490816", "IKEY:8440091", "IKEY:8454904", "IKEY:8440085", "IKEY:8440124", "IKEY:8494828", "IKEY:8440842", "IKEY:8454905", "IKEY:8440049", "IKEY:8617747", "IKEY:8622502", "IKEY:8667702", "IKEY:8371286", "IKEY:8757142", "IKEY:8802509", "IKEY:8852223", "IKEY:8851954", "IKEY:8805457", "IKEY:8805421", "IKEY:8827593", "IKEY:8827944", "IKEY:8807299", "IKEY:8812988", "IKEY:8827951", "IKEY:8933677", "IKEY:8933744", "IKEY:8633392", "IKEY:8937837"], "referencing": []}, "10.1109/TVCG.2017.2745298": {"doi": "10.1109/TVCG.2017.2745298", "author": ["C. Niederer", "H. Stitz", "R. Hourieh", "F. Grassinger", "W. Aigner", "M. Streit"], "title": "TACO: Visualizing Changes in Tables Over Time", "year": "2018", "abstract": "Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions/deletions, reorder, merge/split, and content changes. For textual data, a variety of commonplace \u201cdiff\u201d tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selected tables. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.", "keywords": ["data structures", "data visualisation", "public domain software", "text analysis", "TACO", "tabular data", "derived tables", "content changes", "textual data", "multiple table instances", "interactive comparison tool", "aggregated changes", "selected table versions", "data structures", "Tools", "Data visualization", "Heating systems", "Games", "Encoding", "Bars", "Biology", "Table comparison", "matrix", "difference visualization"], "referenced_by": ["IKEY:8530134", "IKEY:8843909", "IKEY:9308627", "IKEY:9308630"], "referencing": []}, "10.1109/TVCG.2017.2745105": {"doi": "10.1109/TVCG.2017.2745105", "author": ["P. Ivson", "D. Nascimento", "W. Celes", "S. D. Barbosa"], "title": "CasCADe: A Novel 4D Visualization System for Virtual Construction Planning", "year": "2018", "abstract": "Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil & Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.", "keywords": ["CAD/CAM", "civil engineering computing", "computer aided production planning", "data visualisation", "engineering information systems", "production engineering computing", "project management", "scheduling", "information visualization", "AEC industry", "CasCADe", "novel 4D visualization system", "virtual construction planning", "Building Information Modeling", "task sequencing", "architecture engineering construction industry", "scheduling", "Three-dimensional displays", "Visualization", "Data visualization", "Solid modeling", "Schedules", "Animation", "Visualization in physical sciences and engineering", "design studies", "integrating spatial and non-spatial data visualization", "task and requirements analysis"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2743918": {"doi": "10.1109/TVCG.2017.2743918", "author": ["C. Perin", "T. Wun", "R. Pusch", "S. Carpendale"], "title": "Assessing the Graphical Perception of Time and Speed on 2D+Time Trajectories", "year": "2018", "abstract": "We empirically evaluate the extent to which people perceive non-constant time and speed encoded on 2D paths. In our graphical perception study, we evaluate nine encodings from the literature for both straight and curved paths. Visualizing time and speed information is a challenge when the x and y axes already encode other data dimensions, for example when plotting a trip on a map. This is particularly true in disciplines such as time-geography and movement analytics that often require visualizing spatio-temporal trajectories. A common approach is to use 2D+time trajectories, which are 2D paths for which time is an additional dimension. However, there are currently no guidelines regarding how to represent time and speed on such paths. Our study results provide InfoVis designers with clear guidance regarding which encodings to use and which ones to avoid; in particular, we suggest using color value to encode speed and segment length to encode time whenever possible.", "keywords": ["data visualisation", "spatio-temporal trajectories", "2D paths", "InfoVis designers", "movement analytics", "time-geography", "curved paths", "straight paths", "graphical perception study", "nonconstant time", "2D+time trajectories", "Encoding", "Trajectory", "Image color analysis", "Two dimensional displays", "Visualization", "Data visualization", "Guidelines", "Trajectory visualization", "visual encoding", "movement data", "graphical perception", "quantitative evaluation"], "referenced_by": ["IKEY:8609613"], "referencing": []}, "10.1109/TVCG.2017.2744218": {"doi": "10.1109/TVCG.2017.2744218", "author": ["Y. Wu", "J. Lan", "X. Shu", "C. Ji", "K. Zhao", "J. Wang", "H. Zhang"], "title": "iTTVis: Interactive Visualization of Table Tennis Data", "year": "2018", "abstract": "The rapid development of information technology paved the way for the recording of fine-grained data, such as stroke techniques and stroke placements, during a table tennis match. This data recording creates opportunities to analyze and evaluate matches from new perspectives. Nevertheless, the increasingly complex data poses a significant challenge to make sense of and gain insights into. Analysts usually employ tedious and cumbersome methods which are limited to watching videos and reading statistical tables. However, existing sports visualization methods cannot be applied to visualizing table tennis competitions due to different competition rules and particular data attributes. In this work, we collaborate with data analysts to understand and characterize the sophisticated domain problem of analysis of table tennis data. We propose iTTVis, a novel interactive table tennis visualization system, which to our knowledge, is the first visual analysis system for analyzing and exploring table tennis data. iTTVis provides a holistic visualization of an entire match from three main perspectives, namely, time-oriented, statistical, and tactical analyses. The proposed system with several well-coordinated views not only supports correlation identification through statistics and pattern detection of tactics with a score timeline but also allows cross analysis to gain insights. Data analysts have obtained several new insights by using iTTVis. The effectiveness and usability of the proposed system are demonstrated with four case studies.", "keywords": ["data visualisation", "interactive systems", "sport", "iTTVis", "holistic visualization", "data analysts", "interactive visualization", "fine-grained data", "table tennis match", "data recording", "particular data attributes", "novel interactive table tennis visualization system", "visual analysis system", "statistical tables", "sports visualization methods", "table tennis competition visualization", "table tennis data", "Data visualization", "Games", "Hafnium", "Visualization", "Videos", "Mathematical model", "Correlation", "Sports visualization", "visual knowledge discovery", "sports analytics", "visual knowledge representation"], "referenced_by": ["IKEY:8534022", "IKEY:8440804", "IKEY:8564153", "IKEY:8754444", "IKEY:8795584", "IKEY:8807264", "IKEY:9308628"], "referencing": []}, "10.1109/TVCG.2017.2743959": {"doi": "10.1109/TVCG.2017.2743959", "author": ["J. G\u00f6rtler", "C. Schulz", "D. Weiskopf", "O. Deussen"], "title": "Bubble Treemaps for Uncertainty Visualization", "year": "2018", "abstract": "We present a novel type of circular treemap, where we intentionally allocate extra space for additional visual variables. With this extended visual design space, we encode hierarchically structured data along with their uncertainties in a combined diagram. We introduce a hierarchical and force-based circle-packing algorithm to compute Bubble Treemaps, where each node is visualized using nested contour arcs. Bubble Treemaps do not require any color or shading, which offers additional design choices. We explore uncertainty visualization as an application of our treemaps using standard error and Monte Carlo-based statistical models. To this end, we discuss how uncertainty propagates within hierarchies. Furthermore, we show the effectiveness of our visualization using three different examples: the package structure of Flare, the S&P 500 index, and the US consumer expenditure survey.", "keywords": ["data visualisation", "Monte Carlo methods", "statistical analysis", "tree data structures", "extended visual design space", "hierarchically structured data", "circle-packing algorithm", "uncertainty visualization", "circular treemap", "visual variables", "bubble treemaps", "nested contour arcs", "standard error", "Monte Carlo-based statistical model", "S&P 500 index", "US consumer expenditure survey", "Flare", "Uncertainty", "Data visualization", "Layout", "Visualization", "Computational modeling", "Standards", "Indexes", "Uncertainty visualization", "hierarchy visualization", "treemaps", "tree layout", "circle packing", "contours"], "referenced_by": ["IKEY:8530134", "IKEY:8554961", "IKEY:8443124", "IKEY:8467535", "IKEY:8634026", "IKEY:8811914", "IKEY:8923062", "IKEY:8993793", "IKEY:9086235", "IKEY:9086292"], "referencing": []}, "10.1109/TVCG.2017.2745140": {"doi": "10.1109/TVCG.2017.2745140", "author": ["M. Sondag", "B. Speckmann", "K. Verbeek"], "title": "Stable Treemaps via Local Moves", "year": "2018", "abstract": "Treemaps are a popular tool to visualize hierarchical data: items are represented by nested rectangles and the area of each rectangle corresponds to the data being visualized for this item. The visual quality of a treemap is commonly measured via the aspect ratio of the rectangles. If the data changes, then a second important quality criterion is the stability of the treemap: how much does the treemap change as the data changes. We present a novel stable treemapping algorithm that has very high visual quality. Whereas existing treemapping algorithms generally recompute the treemap every time the input changes, our algorithm changes the layout of the treemap using only local modifications. This approach not only gives us direct control over stability, but it also allows us to use a larger set of possible layouts, thus provably resulting in treemaps of higher visual quality compared to existing algorithms. We further prove that we can reach all possible treemap layouts using only our local modifications. Furthermore, we introduce a new measure for stability that better captures the relative positions of rectangles. We finally show via experiments on real-world data that our algorithm outperforms existing treemapping algorithms also in practice on either visual quality and/or stability. Our algorithm scores high on stability regardless of whether we use an existing stability measure or our new measure.", "keywords": ["data visualisation", "tree data structures", "data changes", "high visual quality", "local modifications", "rectangles", "treemap change", "visual quality", "stable treemapping algorithm", "treemap layouts", "hierarchical data visualization", "Layout", "Visualization", "Stability criteria", "Space exploration", "Position measurement", "Binary trees", "Treemap", "Stability", "Local Moves"], "referenced_by": ["IKEY:8443124", "IKEY:8440856", "IKEY:8807213", "IKEY:8923062", "IKEY:8933545", "IKEY:9086235", "IKEY:9240497", "IKEY:9240484"], "referencing": []}, "10.1109/TVCG.2017.2744318": {"doi": "10.1109/TVCG.2017.2744318", "author": ["T. H\u00f6llt", "N. Pezzotti", "V. van Unen", "F. Koning", "B. P. F. Lelieveldt", "A. Vilanova"], "title": "CyteGuide: Visual Guidance for Hierarchical Single-Cell Analysis", "year": "2018", "abstract": "Single-cell analysis through mass cytometry has become an increasingly important tool for immunologists to study the immune system in health and disease. Mass cytometry creates a high-dimensional description vector for single cells by time-of-flight measurement. Recently, t-Distributed Stochastic Neighborhood Embedding (t-SNE) has emerged as one of the state-of-the-art techniques for the visualization and exploration of single-cell data. Ever increasing amounts of data lead to the adoption of Hierarchical Stochastic Neighborhood Embedding (HSNE), enabling the hierarchical representation of the data. Here, the hierarchy is explored selectively by the analyst, who can request more and more detail in areas of interest. Such hierarchies are usually explored by visualizing disconnected plots of selections in different levels of the hierarchy. This poses problems for navigation, by imposing a high cognitive load on the analyst. In this work, we present an interactive summary-visualization to tackle this problem. CyteGuide guides the analyst through the exploration of hierarchically represented single-cell data, and provides a complete overview of the current state of the analysis. We conducted a two-phase user study with domain experts that use HSNE for data exploration. We first studied their problems with their current workflow using HSNE and the requirements to ease this workflow in a field study. These requirements have been the basis for our visual design. In the second phase, we verified our proposed solution in a user evaluation.", "keywords": ["biology computing", "cellular biophysics", "data analysis", "data structures", "data visualisation", "interactive systems", "stochastic processes", "CyteGuide", "visual guidance", "Hierarchical single-cell analysis", "mass cytometry", "high-dimensional description vector", "time-of-flight measurement", "t-Distributed Stochastic Neighborhood Embedding", "single-cell data", "Hierarchical Stochastic Neighborhood Embedding", "HSNE", "hierarchical representation", "high cognitive load", "interactive summary-visualization", "data exploration", "visual design", "Visualization", "Data visualization", "Tools", "Navigation", "Biomedical imaging", "Manuals", "Hierarchical Data", "HSNE", "Single-Cell Analysis", "Visual Guidance", "Cluster Analysis", "Computer Graphics", "Humans", "Image Processing, Computer-Assisted", "Single-Cell Analysis", "Software"], "referenced_by": ["IKEY:8564153", "IKEY:8356097", "IKEY:8809834", "IKEY:8805461", "IKEY:8933618"], "referencing": []}, "10.1109/TVCG.2017.2745138": {"doi": "10.1109/TVCG.2017.2745138", "author": ["E. Dimara", "A. Bezerianos", "P. Dragicevic"], "title": "Conceptual and Methodological Issues in Evaluating Multidimensional Visualizations for Decision Support", "year": "2018", "abstract": "We explore how to rigorously evaluate multidimensional visualizations for their ability to support decision making. We first define multi-attribute choice tasks, a type of decision task commonly performed with such visualizations. We then identify which of the existing multidimensional visualizations are compatible with such tasks, and set out to evaluate three elementary visualizations: parallel coordinates, scatterplot matrices and tabular visualizations. Our method consists in first giving participants low-level analytic tasks, in order to ensure that they properly understood the visualizations and their interactions. Participants are then given multi-attribute choice tasks consisting of choosing holiday packages. We assess decision support through multiple objective and subjective metrics, including a decision accuracy metric based on the consistency between the choice made and self-reported preferences for attributes. We found the three visualizations to be comparable on most metrics, with a slight advantage for tabular visualizations. In particular, tabular visualizations allow participants to reach decisions faster. Thus, although decision time is typically not central in assessing decision support, it can be used as a tie-breaker when visualizations achieve similar decision accuracy. Our results also suggest that indirect methods for assessing choice confidence may allow to better distinguish between visualizations than direct ones. We finally discuss the limitations of our methods and directions for future work, such as the need for more sensitive metrics of decision support.", "keywords": ["data analysis", "data visualisation", "decision making", "decision support systems", "travel industry", "decision accuracy metric", "tabular visualizations", "particular visualizations", "decision time", "decision support", "decision making", "elementary visualizations", "self-reported attribute preference", "choice confidence", "multidimensional visualizations", "multiattribute choice tasks", "parallel coordinates", "scatterplot matrices", "low-level analytic tasks", "Data visualization", "Decision making", "Measurement", "Tools", "Visualization", "Employment", "decision making", "multidimensional visualization", "parallel coordinates", "scatterplot matrix", "tabular visualization", "evaluation"], "referenced_by": ["IKEY:8440829", "IKEY:8440809", "IKEY:8354901", "IKEY:8476234"], "referencing": []}, "10.1109/TVCG.2017.2745240": {"doi": "10.1109/TVCG.2017.2745240", "author": ["Y. Kim", "K. Reinecke", "J. Hullman"], "title": "Data Through Others' Eyes: The Impact of Visualizing Others' Expectations on Visualization Interpretation", "year": "2018", "abstract": "In addition to visualizing input data, interactive visualizations have the potential to be social artifacts that reveal other people's perspectives on the data. However, how such social information embedded in a visualization impacts a viewer's interpretation of the data remains unknown. Inspired by recent interactive visualizations that display people's expectations of data against the data, we conducted a controlled experiment to evaluate the effect of showing social information in the form of other people's expectations on people's ability to recall the data, the degree to which they adjust their expectations to align with the data, and their trust in the accuracy of the data. We found that social information that exhibits a high degree of consensus lead participants to recall the data more accurately relative to participants who were exposed to the data alone. Additionally, participants trusted the accuracy of the data less and were more likely to maintain their initial expectations when other people's expectations aligned with their own initial expectations but not with the data. We conclude by characterizing the design space for visualizing others' expectations alongside data.", "keywords": ["data visualisation", "human computer interaction", "social networking (online)", "social artifacts", "social information", "input data visualization", "interactive visualizations", "consensus degree", "Data visualization", "Uncertainty", "Collaboration", "Social network services", "Market research", "Focusing", "Data analysis", "Social influence", "Social visualization", "Data interpretation", "Computer Graphics", "Crowdsourcing", "Data Visualization", "Databases, Factual", "Humans", "Mental Recall", "Motivation", "Social Perception", "Trust"], "referenced_by": ["IKEY:8449328", "IKEY:8789402", "IKEY:8802449", "IKEY:8805448"], "referencing": ["IKEY:1532126", "IKEY:6064988", "IKEY:6876023", "IKEY:4376131", "IKEY:1532122", "IKEY:1634320", "IKEY:4376132", "IKEY:1532126", "IKEY:6064988", "IKEY:6876023", "IKEY:4376131", "IKEY:1532122", "IKEY:1634320", "IKEY:4376132", "IKEY:1532126", "IKEY:6064988", "IKEY:6876023", "IKEY:4376131", "IKEY:1532122", "IKEY:1634320", "IKEY:4376132", "10.1145/1240624.1240781", "10.1145/1978942.1979157", "10.1145/3025453.3025592", "10.1145/2675133.2675246", "10.1145/1958824.1958865", "10.1145/1978942.1979407", "10.1145/1240624.1240781", "10.1145/1978942.1979157", "10.1145/3025453.3025592", "10.1145/2675133.2675246", "10.1145/1958824.1958865", "10.1145/1978942.1979407", "10.1145/1240624.1240781", "10.1145/1978942.1979157", "10.1145/3025453.3025592", "10.1145/2675133.2675246", "10.1145/1958824.1958865", "10.1145/1978942.1979407", "10.1002/acp.2350070407", "10.1037/h0046408", "10.1080/13546780542000005", "10.1037/h0056932", "10.1177/001872675400700202", "10.1111/j.1751-5823.2002.tb00336.x", "10.1111/ajps.12004", "10.1057/palgrave.ivs.9500167", "10.1007/978-3-540-25931-2_1", "10.1207/s1532690xci2104_1", "10.1016/S1041-6080(97)90018-2", "10.1002/bdm.3960030403", "10.1086/226792", "10.1111/1467-8624.00232", "10.1002/ejsp.2420130103", "10.1037/1089-2680.2.2.175", "10.1037/0022-0663.91.3.497", "10.1037//0022-0663.91.4.690", "10.1007/s10699-005-3007-4", "10.3758/BF03201236", "10.1002/acp.2350070407", "10.1037/h0046408", "10.1080/13546780542000005", "10.1037/h0056932", "10.1177/001872675400700202", "10.1111/j.1751-5823.2002.tb00336.x", "10.1111/ajps.12004", "10.1057/palgrave.ivs.9500167", "10.1007/978-3-540-25931-2_1", "10.1207/s1532690xci2104_1", "10.1016/S1041-6080(97)90018-2", "10.1002/bdm.3960030403", "10.1086/226792", "10.1111/1467-8624.00232", "10.1002/ejsp.2420130103", "10.1037/1089-2680.2.2.175", "10.1037/0022-0663.91.3.497", "10.1037//0022-0663.91.4.690", "10.1007/s10699-005-3007-4", "10.3758/BF03201236", "10.1002/acp.2350070407", "10.1037/h0046408", "10.1080/13546780542000005", "10.1037/h0056932", "10.1177/001872675400700202", "10.1111/j.1751-5823.2002.tb00336.x", "10.1111/ajps.12004", "10.1057/palgrave.ivs.9500167", "10.1007/978-3-540-25931-2_1", "10.1207/s1532690xci2104_1", "10.1016/S1041-6080(97)90018-2", "10.1002/bdm.3960030403", "10.1086/226792", "10.1111/1467-8624.00232", "10.1002/ejsp.2420130103", "10.1037/1089-2680.2.2.175", "10.1037/0022-0663.91.3.497", "10.1037//0022-0663.91.4.690", "10.1007/s10699-005-3007-4", "10.3758/BF03201236"]}, "10.1109/TVCG.2017.2745958": {"doi": "10.1109/TVCG.2017.2745958", "author": ["J. Walny", "S. Huron", "C. Perin", "T. Wun", "R. Pusch", "S. Carpendale"], "title": "Active Reading of Visualizations", "year": "2018", "abstract": "We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.", "keywords": ["data visualisation", "text analysis", "interactive node-link visualization", "low-level visualization tasks", "active reading space", "flexible visualization reading experience", "effective visualization reading experience", "observable active reading techniques", "node-link visualizations", "physical action", "Visualization", "Data visualization", "Decoding", "Education", "Systematics", "Collaboration", "Indexes", "active reading of visualizations", "active reading", "information visualization", "spectrum of physical engagement"], "referenced_by": ["IKEY:8440833", "IKEY:8454489", "IKEY:8634261", "IKEY:8642122", "IKEY:8809711", "IKEY:8805424", "IKEY:9117084", "IKEY:9307765", "10.1080/15230406.2018.1507758", "10.1016/j.visinf.2018.12.006", "10.1111/cgf.13720"], "referencing": []}, "10.1109/TVCG.2017.2744298": {"doi": "10.1109/TVCG.2017.2744298", "author": ["P. Dragicevic", "Y. Jansen"], "title": "Blinded with Science or Informed by Charts? A Replication Study", "year": "2018", "abstract": "We provide a reappraisal of Tal and Wansink's study \u201cBlinded with Science\u201d, where seemingly trivial charts were shown to increase belief in drug efficacy, presumably because charts are associated with science. Through a series of four replications conducted on two crowdsourcing platforms, we investigate an alternative explanation, namely, that the charts allowed participants to better assess the drug's efficacy. Considered together, our experiments suggest that the chart seems to have indeed promoted understanding, although the effect is likely very small. Meanwhile, we were unable to replicate the original study's findings, as text with chart appeared to be no more persuasive - and sometimes less persuasive - than text alone. This suggests that the effect may not be as robust as claimed and may need specific conditions to be reproduced. Regardless, within our experimental settings and considering our study as a whole ($\\mathrm{N}=623$), the chart's contribution to understanding was clearly larger than its contribution to persuasion.", "keywords": ["Drugs", "Bars", "Data visualization", "Sociology", "Statistics", "Data mining", "Replication study", "persuasion", "charts", "data comprehension", "methodology", "Adult", "Comprehension", "Computer Graphics", "Crowdsourcing", "Data Visualization", "Female", "Humans", "Male", "Persuasive Communication", "Research Design"], "referenced_by": ["IKEY:8634261", "IKEY:8634392"], "referencing": ["IKEY:6875906", "IKEY:7539348", "IKEY:6875917", "IKEY:6327259", "IKEY:6876023", "IKEY:7192695", "IKEY:6875906", "IKEY:7539348", "IKEY:6875917", "IKEY:6327259", "IKEY:6876023", "IKEY:7192695", "IKEY:6875906", "IKEY:7539348", "IKEY:6875917", "IKEY:6327259", "IKEY:6876023", "IKEY:7192695", "10.1145/62959.62971", "10.1145/2702123.2702608", "10.1145/1753326.1753679", "10.1145/62959.62971", "10.1145/2702123.2702608", "10.1145/1753326.1753679", "10.1145/62959.62971", "10.1145/2702123.2702608", "10.1145/1753326.1753679", "10.1136/bmj.d2090", "10.1197/jamia.M2115", "10.1023/A:1013176309260", "10.3758/s13428-013-0365-7", "10.1086/208501", "10.1111/j.1540-5915.1984.tb01236.x", "10.1007/978-3-319-26633-6_13", "10.1007/978-3-540-70956-5_1", "10.1111/j.1539-6053.2008.00033.x", "10.1518/001872098779480640", "10.1037/h0024049", "10.1080/0270271880090204", "10.1207/s15327027hc1604_2", "10.1002/sim.4780140810", "10.3758/s13428-013-0330-5", "10.1111/j.1551-6708.1987.tb00863.x", "10.2501/JAR-40-3-67-72", "10.1016/j.vaccine.2014.11.017", "10.1037/a0036844", "10.1016/S0959-4752(02)00017-8", "10.1515/2151-7509.1054", "10.1037/0096-1523.16.4.683", "10.1177/070674370204700307", "10.1177/0963662514549688", "10.2307/249469", "10.1097/01445442-198507000-00012", "10.1111/j.1540-5915.1991.tb00344.x", "10.1016/0378-7206(94)90010-8", "10.1037/h0070054", "10.1162/jocn.2008.20040", "10.1177/107769906404100105", "10.2307/2276774", "10.1136/bmj.d2090", "10.1197/jamia.M2115", "10.1023/A:1013176309260", "10.3758/s13428-013-0365-7", "10.1086/208501", "10.1111/j.1540-5915.1984.tb01236.x", "10.1007/978-3-319-26633-6_13", "10.1007/978-3-540-70956-5_1", "10.1111/j.1539-6053.2008.00033.x", "10.1518/001872098779480640", "10.1037/h0024049", "10.1080/0270271880090204", "10.1207/s15327027hc1604_2", "10.1002/sim.4780140810", "10.3758/s13428-013-0330-5", "10.1111/j.1551-6708.1987.tb00863.x", "10.2501/JAR-40-3-67-72", "10.1016/j.vaccine.2014.11.017", "10.1037/a0036844", "10.1016/S0959-4752(02)00017-8", "10.1515/2151-7509.1054", "10.1037/0096-1523.16.4.683", "10.1177/070674370204700307", "10.1177/0963662514549688", "10.2307/249469", "10.1097/01445442-198507000-00012", "10.1111/j.1540-5915.1991.tb00344.x", "10.1016/0378-7206(94)90010-8", "10.1037/h0070054", "10.1162/jocn.2008.20040", "10.1177/107769906404100105", "10.2307/2276774", "10.1136/bmj.d2090", "10.1197/jamia.M2115", "10.1023/A:1013176309260", "10.3758/s13428-013-0365-7", "10.1086/208501", "10.1111/j.1540-5915.1984.tb01236.x", "10.1007/978-3-319-26633-6_13", "10.1007/978-3-540-70956-5_1", "10.1111/j.1539-6053.2008.00033.x", "10.1518/001872098779480640", "10.1037/h0024049", "10.1080/0270271880090204", "10.1207/s15327027hc1604_2", "10.1002/sim.4780140810", "10.3758/s13428-013-0330-5", "10.1111/j.1551-6708.1987.tb00863.x", "10.2501/JAR-40-3-67-72", "10.1016/j.vaccine.2014.11.017", "10.1037/a0036844", "10.1016/S0959-4752(02)00017-8", "10.1515/2151-7509.1054", "10.1037/0096-1523.16.4.683", "10.1177/070674370204700307", "10.1177/0963662514549688", "10.2307/249469", "10.1097/01445442-198507000-00012", "10.1111/j.1540-5915.1991.tb00344.x", "10.1016/0378-7206(94)90010-8", "10.1037/h0070054", "10.1162/jocn.2008.20040", "10.1177/107769906404100105", "10.2307/2276774"]}, "10.1109/TVCG.2017.2745878": {"doi": "10.1109/TVCG.2017.2745878", "author": ["J. C. Roberts", "P. D. Ritsos", "J. R. Jackson", "C. Headleand"], "title": "The Explanatory Visualization Framework: An Active Learning Framework for Teaching Creative Computing Using Explanatory Visualizations", "year": "2018", "abstract": "Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.", "keywords": ["computer aided instruction", "computer science education", "data visualisation", "teaching", "effective learning strategies", "three-stage framework", "creative visualizations", "innovative visualizations", "purposeful visualizations", "explanatory visualization framework", "active learning framework", "creative skills", "Data visualization", "Education", "Visualization", "Algorithm design and analysis", "Creativity", "Computational modeling", "Explanatory visualization", "Information Visualization", "Teaching visualization", "Learning Support"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2743958": {"doi": "10.1109/TVCG.2017.2743958", "author": ["K. Bladin", "E. Axelsson", "E. Broberg", "C. Emmart", "P. Ljung", "A. Bock", "A. Ynnerman"], "title": "Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization", "year": "2018", "abstract": "Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software [8], which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.", "keywords": ["geophysical image processing", "image resolution", "rendering (computer graphics)", "spatiotemporal phenomena", "terrain mapping", "virtual reality", "mission planning", "multiple data processing", "visualization methods", "geospatial surface data", "science communication", "dynamic data sources", "online repositories", "image acquisition pipeline", "out-of-core rendering approach", "interactive exploration", "global maps", "high-resolution digital terrain models", "high-resolution map data", "dynamic processes", "temporal datasets", "interactive presentations", "interactive touch tables", "globe browsing", "contextualized spatio-temporal planetary surface visualization", "planetary mapping", "celestial bodies", "Data visualization", "Mars", "Pluto", "Rendering (computer graphics)", "Surface treatment", "Earth", "Astronomical visualization", "globe rendering", "public dissemination", "science communication", "space mission visualization"], "referenced_by": ["IKEY:8474501", "IKEY:8797777", "IKEY:8805465", "IKEY:8805462", "IKEY:8961146"], "referencing": ["IKEY:7429503", "IKEY:663860", "IKEY:4015402", "IKEY:1432682", "IKEY:4840339", "IKEY:5740588", "IKEY:5210067", "IKEY:20292", "IKEY:7429503", "IKEY:663860", "IKEY:4015402", "IKEY:1432682", "IKEY:4840339", "IKEY:5740588", "IKEY:5210067", "IKEY:20292", "IKEY:7429503", "IKEY:663860", "IKEY:4015402", "IKEY:1432682", "IKEY:4840339", "IKEY:5740588", "IKEY:5210067", "IKEY:20292", "10.1145/364338.364376", "10.1145/1186562.1015799", "10.1145/2950040", "10.1145/364338.364376", "10.1145/1186562.1015799", "10.1145/2950040", "10.1145/364338.364376", "10.1145/1186562.1015799", "10.1145/2950040", "10.1016/0032-0633(95)00107-7", "10.1111/cgf.13202", "10.1088/1538-3873/aa5456", "10.21105/joss.00281", "10.1007/978-3-7091-6783-0_12", "10.1016/S0034-4257(02)00084-6", "10.1086/671412", "10.1029/2005JE002605", "10.1016/0032-0633(95)00101-8", "10.1016/j.ascom.2016.02.002", "10.1080/00221340701678032", "10.1029/2000JE001364", "10.1126/science.aad1815", "10.1007/978-3-540-74831-1_5", "10.1029/2006JE002701", "10.1016/0032-0633(95)00107-7", "10.1111/cgf.13202", "10.1088/1538-3873/aa5456", "10.21105/joss.00281", "10.1007/978-3-7091-6783-0_12", "10.1016/S0034-4257(02)00084-6", "10.1086/671412", "10.1029/2005JE002605", "10.1016/0032-0633(95)00101-8", "10.1016/j.ascom.2016.02.002", "10.1080/00221340701678032", "10.1029/2000JE001364", "10.1126/science.aad1815", "10.1007/978-3-540-74831-1_5", "10.1029/2006JE002701", "10.1016/0032-0633(95)00107-7", "10.1111/cgf.13202", "10.1088/1538-3873/aa5456", "10.21105/joss.00281", "10.1007/978-3-7091-6783-0_12", "10.1016/S0034-4257(02)00084-6", "10.1086/671412", "10.1029/2005JE002605", "10.1016/0032-0633(95)00101-8", "10.1016/j.ascom.2016.02.002", "10.1080/00221340701678032", "10.1029/2000JE001364", "10.1126/science.aad1815", "10.1007/978-3-540-74831-1_5", "10.1029/2006JE002701"]}, "10.1109/TVCG.2017.2743980": {"doi": "10.1109/TVCG.2017.2743980", "author": ["A. Bock", "H. Doraiswamy", "A. Summers", "C. Silva"], "title": "TopoAngler: Interactive Topology-Based Extraction of Fishes", "year": "2018", "abstract": "We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-Ioop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single dataset contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.", "keywords": ["biological techniques", "biology computing", "computerised tomography", "data visualisation", "image segmentation", "interactive systems", "zoology", "visualization framework", "microCT scan", "automatic segmentation", "computational topology", "interactive visual interface", "hierarchical segmentation", "interactive user-guided segmentation", "topoangler", "interactive topology-based extraction", "human-in-the-Ioop", "ScanAllFish project", "fish segmentation", "Image segmentation", "Computed tomography", "Feature extraction", "Topology", "Visualization", "Biomedical imaging", "Mathematical model", "Computational topology", "join trees", "branch decomposition", "hierarchical segmentation", "interaction", "visualization system", "Algorithms", "Animals", "Computer Graphics", "Fishes", "Image Processing, Computer-Assisted", "Marine Biology", "Skeleton", "Software", "X-Ray Microtomography"], "referenced_by": ["IKEY:8739196", "IKEY:8944365", "IKEY:8730513", "IKEY:9308048", "10.2110/palo.2018.51"], "referencing": ["IKEY:5128904", "IKEY:4376157", "IKEY:6876004", "IKEY:5710907", "IKEY:6634109", "IKEY:5290767", "IKEY:6682956", "IKEY:4015464", "IKEY:6876035", "IKEY:5290695", "IKEY:6172570", "IKEY:5613476", "IKEY:4130436", "IKEY:7150427", "IKEY:1260759", "IKEY:6867925", "IKEY:4069241", "IKEY:5721820", "IKEY:5128904", "IKEY:4376157", "IKEY:6876004", "IKEY:5710907", "IKEY:6634109", "IKEY:5290767", "IKEY:6682956", "IKEY:4015464", "IKEY:6876035", "IKEY:5290695", "IKEY:6172570", "IKEY:5613476", "IKEY:4130436", "IKEY:7150427", "IKEY:1260759", "IKEY:6867925", "IKEY:4069241", "IKEY:5721820", "IKEY:5128904", "IKEY:4376157", "IKEY:6876004", "IKEY:5710907", "IKEY:6634109", "IKEY:5290767", "IKEY:6682956", "IKEY:4015464", "IKEY:6876035", "IKEY:5290695", "IKEY:6172570", "IKEY:5613476", "IKEY:4130436", "IKEY:7150427", "IKEY:1260759", "IKEY:6867925", "IKEY:4069241", "IKEY:5721820", "10.1145/235815.235821", "10.1145/777792.777846", "10.1145/235815.235821", "10.1145/777792.777846", "10.1145/235815.235821", "10.1145/777792.777846", "10.1007/s10851-007-0035-4", "10.1016/S0925-7721(02)00093-7", "10.1111/1467-8659.00697", "10.1017/CBO9780511530067", "10.1007/s00454-003-2926-5", "10.1016/S0169-2607(97)01803-8", "10.1016/j.ecoinf.2014.08.004", "10.1097/00004728-199203000-00019", "10.1029/2009WR008087", "10.1007/3-540-63046-5_36", "10.1007/s00453-003-1052-3", "10.1007/b106657_2", "10.1007/s11390-013-1383-8", "10.1016/j.gmod.2003.08.002", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-15948-0_9", "10.1007/s10851-007-0035-4", "10.1016/S0925-7721(02)00093-7", "10.1111/1467-8659.00697", "10.1017/CBO9780511530067", "10.1007/s00454-003-2926-5", "10.1016/S0169-2607(97)01803-8", "10.1016/j.ecoinf.2014.08.004", "10.1097/00004728-199203000-00019", "10.1029/2009WR008087", "10.1007/3-540-63046-5_36", "10.1007/s00453-003-1052-3", "10.1007/b106657_2", "10.1007/s11390-013-1383-8", "10.1016/j.gmod.2003.08.002", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-15948-0_9", "10.1007/s10851-007-0035-4", "10.1016/S0925-7721(02)00093-7", "10.1111/1467-8659.00697", "10.1017/CBO9780511530067", "10.1007/s00454-003-2926-5", "10.1016/S0169-2607(97)01803-8", "10.1016/j.ecoinf.2014.08.004", "10.1097/00004728-199203000-00019", "10.1029/2009WR008087", "10.1007/3-540-63046-5_36", "10.1007/s00453-003-1052-3", "10.1007/b106657_2", "10.1007/s11390-013-1383-8", "10.1016/j.gmod.2003.08.002", "10.1016/j.neuroimage.2006.01.015", "10.1007/978-3-642-15948-0_9"]}, "10.1109/TVCG.2017.2744321": {"doi": "10.1109/TVCG.2017.2744321", "author": ["B. Rieck", "U. Fugacci", "J. Lukasczyk", "H. Leitte"], "title": "Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks", "year": "2018", "abstract": "Complex networks require effective tools and visualizations for their analysis and comparison. Clique communities have been recognized as a powerful concept for describing cohesive structures in networks. We propose an approach that extends the computation of clique communities by considering persistent homology, a topological paradigm originally introduced to characterize and compare the global structure of shapes. Our persistence-based algorithm is able to detect clique communities and to keep track of their evolution according to different edge weight thresholds. We use this information to define comparison metrics and a new centrality measure, both reflecting the relevance of the clique communities inherent to the network. Moreover, we propose an interactive visualization tool based on nested graphs that is capable of compactly representing the evolving relationships between communities for different thresholds and clique degrees. We demonstrate the effectiveness of our approach on various network types.", "keywords": ["complex networks", "data analysis", "data visualisation", "graph theory", "network theory (graphs)", "clique community persistence", "topological visual analysis approach", "complex networks", "interactive visualization tool", "clique degrees", "network types", "cohesive structures", "nested graphs", "Tools", "Complex networks", "Visualization", "Layout", "Algorithm design and analysis", "Shape", "Transmission line matrix methods", "Persistent homology", "topological persistence", "cliques", "complex networks", "visual analysis"], "referenced_by": ["IKEY:8974553", "IKEY:9112200", "IKEY:9150930"], "referencing": []}, "10.1109/TVCG.2017.2743938": {"doi": "10.1109/TVCG.2017.2743938", "author": ["J. Tierny", "G. Favelier", "J. A. Levine", "C. Gueunet", "M. Michaux"], "title": "The Topology ToolKit", "year": "2018", "abstract": "This system paper presents the Topology ToolKit (TTK), a software platform designed for the topological analysis of scalar data in scientific visualization. While topological data analysis has gained in popularity over the last two decades, it has not yet been widely adopted as a standard data analysis tool for end users or developers. TTK aims at addressing this problem by providing a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK/C++) for fast prototyping or through direct, dependency-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code. online documentation and video tutorials are available on TTK's website [108].", "keywords": ["computational geometry", "data analysis", "data structures", "data visualisation", "graph theory", "software architecture", "solid modelling", "topological abstractions", "topological data simplification", "cached triangulation data structure", "direct accesses", "TTK features", "Topology ToolKit", "software platform", "topological analysis", "scalar data", "topological data analysis", "standard data analysis tool", "integral lines", "persistence diagrams", "persistence curves", "contour trees", "Morse-Smale complexes", "tight integration", "algorithmic software engineering challenges", "TTK website", "Data visualization", "Algorithm design and analysis", "Data analysis", "Tools", "Software algorithms", "Data structures", "Software", "Topological data analysis", "scalar data", "data segmentation", "feature extraction", "bivariate data", "uncertain data"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744038": {"doi": "10.1109/TVCG.2017.2744038", "author": ["L. Roy", "P. Kumar", "S. Golbabaei", "Y. Zhang", "E. Zhang"], "title": "Interactive Design and Visualization of Branched Covering Spaces", "year": "2018", "abstract": "Branched covering spaces are a mathematical concept which originates from complex analysis and topology and has applications in tensor field topology and geometry remeshing. Given a manifold surface and an $N$-way rotational symmetry field, a branched covering space is a manifold surface that has an $N$-to-1 map to the original surface except at the ramification points, which correspond to the singularities in the rotational symmetry field. Understanding the notion and mathematical properties of branched covering spaces is important to researchers in tensor field visualization and geometry processing, and their application areas. In this paper, we provide a framework to interactively design and visualize the branched covering space (BCS) of an input mesh surface and a rotational symmetry field defined on it. In our framework, the user can visualize not only the BCSs but also their construction process. In addition, our system allows the user to design the geometric realization of the BCS using mesh deformation techniques as well as connecting tubes. This enables the user to verify important facts about BCSs such as that they are manifold surfaces around singularities, as well as the Riemann-Hurwitz formula which relates the Euler characteristic of the BCS to that of the original mesh. Our system is evaluated by student researchers in scientific visualization and geometry processing as well as faculty members in mathematics at our university who teach topology. We include their evaluations and feedback in the paper.", "keywords": ["Visualization", "Tensile stress", "Topology", "Geometry", "Manifolds", "Electron tubes", "Surface treatment", "Tensor field topology", "math visualization", "branched covering spaces visualization", "rotational symmetries", "ramification points"], "referenced_by": [], "referencing": ["IKEY:5928344", "IKEY:5582087", "IKEY:6509098", "IKEY:5928344", "IKEY:5582087", "IKEY:6509098", "IKEY:5928344", "IKEY:5582087", "IKEY:6509098", "10.1145/1201775.882296", "10.1145/2508244.2508249", "10.1145/1576246.1531383", "10.1145/166117.166151", "10.1145/2185520.2185606", "10.1145/2461912.2462017", "10.1145/1399504.1360644", "10.1145/1275808.1276446", "10.1145/1183287.1183297", "10.1145/1356682.1356683", "10.1145/383259.383307", "10.1145/1186562.1015736", "10.1145/218380.218473", "10.1145/1201775.882290", "10.1145/1037957.1037958", "10.1145/1201775.882296", "10.1145/2508244.2508249", "10.1145/1576246.1531383", "10.1145/166117.166151", "10.1145/2185520.2185606", "10.1145/2461912.2462017", "10.1145/1399504.1360644", "10.1145/1275808.1276446", "10.1145/1183287.1183297", "10.1145/1356682.1356683", "10.1145/383259.383307", "10.1145/1186562.1015736", "10.1145/218380.218473", "10.1145/1201775.882290", "10.1145/1037957.1037958", "10.1145/1201775.882296", "10.1145/2508244.2508249", "10.1145/1576246.1531383", "10.1145/166117.166151", "10.1145/2185520.2185606", "10.1145/2461912.2462017", "10.1145/1399504.1360644", "10.1145/1275808.1276446", "10.1145/1183287.1183297", "10.1145/1356682.1356683", "10.1145/383259.383307", "10.1145/1186562.1015736", "10.1145/218380.218473", "10.1145/1201775.882290", "10.1145/1037957.1037958", "10.1111/cgf.12014", "10.1007/978-1-4757-3849-0", "10.1111/j.1467-8659.2007.01060.x", "10.1007/978-3-642-13411-1_11", "10.1111/j.1467-8659.2011.02014.x", "10.1111/cgf.12014", "10.1007/978-1-4757-3849-0", "10.1111/j.1467-8659.2007.01060.x", "10.1007/978-3-642-13411-1_11", "10.1111/j.1467-8659.2011.02014.x", "10.1111/cgf.12014", "10.1007/978-1-4757-3849-0", "10.1111/j.1467-8659.2007.01060.x", "10.1007/978-3-642-13411-1_11", "10.1111/j.1467-8659.2011.02014.x"]}, "10.1109/TVCG.2017.2744278": {"doi": "10.1109/TVCG.2017.2744278", "author": ["H. Mohammed", "A. K. Al-Awami", "J. Beyer", "C. Cali", "P. Magistretti", "H. Pfister", "M. Hadwiger"], "title": "Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells", "year": "2018", "abstract": "This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.", "keywords": ["biological tissues", "biomedical imaging", "brain", "cellular biophysics", "data visualisation", "electron microscopy", "neurophysiology", "abstractocyte", "nervous system", "astrocyte morphology", "glycogen distribution", "synapses", "brain tissue", "astrocytic-mitochondria coverage", "astrocytic coverage", "interactive analysis", "abstracted 2D views", "visual context", "specific joint visualization", "neighboring neurons", "visual abstraction levels", "high-resolution electron microscopy imaging", "widely-branching structure", "complex branching structure", "brain function", "nonneuronal cells", "glial cells", "nanoscale astroglial cells", "visual tool", "Neurons", "Data visualization", "Visualization", "Three-dimensional displays", "Two dimensional displays", "Tools", "Nanoscale devices", "Connectomics", "Neuroscience", "Data Abstraction", "Interactive 3D Visualization", "Astrocytes", "Computer Graphics", "Connectome", "Humans", "Imaging, Three-Dimensional", "Neurons", "Software"], "referenced_by": ["IKEY:8823785", "IKEY:8823764", "IKEY:8842614", "IKEY:8805465"], "referencing": ["IKEY:7192653", "IKEY:6875935", "IKEY:6634132", "IKEY:5290766", "IKEY:4658123", "IKEY:4376146", "IKEY:5753898", "IKEY:1021579", "IKEY:6664349", "IKEY:7534826", "IKEY:7192653", "IKEY:6875935", "IKEY:6634132", "IKEY:5290766", "IKEY:4658123", "IKEY:4376146", "IKEY:5753898", "IKEY:1021579", "IKEY:6664349", "IKEY:7534826", "IKEY:7192653", "IKEY:6875935", "IKEY:6634132", "IKEY:5290766", "IKEY:4658123", "IKEY:4376146", "IKEY:5753898", "IKEY:1021579", "IKEY:6664349", "IKEY:7534826", "10.1145/2254556.2254653", "10.1145/2702123.2702129", "10.1145/2254556.2254653", "10.1145/2702123.2702129", "10.1145/2254556.2254653", "10.1145/2702123.2702129", "10.1111/j.1365-2818.2010.03402.x", "10.1002/cne.21974", "10.1016/j.gmod.2015.05.001", "10.1007/3-540-32137-3_77", "10.1002/cne.23852", "10.1371/journal.pone.0038011", "10.1101/cshperspect.a020370", "10.1002/spe.4380211102", "10.3389/fninf.2011.00003", "10.1007/s12021-014-9242-5", "10.1016/j.neuroimage.2012.02.075", "10.1126/science.1209168", "10.1016/j.neuroimage.2013.04.111", "10.1007/978-3-319-27857-5_1", "10.1111/j.1365-2818.2010.03402.x", "10.1002/cne.21974", "10.1016/j.gmod.2015.05.001", "10.1007/3-540-32137-3_77", "10.1002/cne.23852", "10.1371/journal.pone.0038011", "10.1101/cshperspect.a020370", "10.1002/spe.4380211102", "10.3389/fninf.2011.00003", "10.1007/s12021-014-9242-5", "10.1016/j.neuroimage.2012.02.075", "10.1126/science.1209168", "10.1016/j.neuroimage.2013.04.111", "10.1007/978-3-319-27857-5_1", "10.1111/j.1365-2818.2010.03402.x", "10.1002/cne.21974", "10.1016/j.gmod.2015.05.001", "10.1007/3-540-32137-3_77", "10.1002/cne.23852", "10.1371/journal.pone.0038011", "10.1101/cshperspect.a020370", "10.1002/spe.4380211102", "10.3389/fninf.2011.00003", "10.1007/s12021-014-9242-5", "10.1016/j.neuroimage.2012.02.075", "10.1126/science.1209168", "10.1016/j.neuroimage.2013.04.111", "10.1007/978-3-319-27857-5_1"]}, "10.1109/TVCG.2017.2744258": {"doi": "10.1109/TVCG.2017.2744258", "author": ["T. Klein", "L. Autin", "B. Kozl\u00edkov\u00e1", "D. S. Goodsell", "A. Olson", "M. E. Gr\u00f6ller", "I. Viola"], "title": "Instant Construction and Visualization of Crowded Biological Environments", "year": "2018", "abstract": "We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.", "keywords": ["biological tissues", "biology computing", "biomembranes", "blood", "cellular biophysics", "data visualisation", "lipid bilayers", "molecular biophysics", "physiological models", "proteins", "random processes", "atomic resolution mesoscale models", "rapid construction", "complex biological structures", "multiple membrane-enclosed compartments", "soluble molecules", "fibrous structures", "interactive construction", "cellular compartments", "crowded biological environments", "integrative structural modeling", "biological mesoscale", "interactive visual environment", "complex models", "defined atomic structures", "noninteractive environment", "solution unites the modeling", "visualization aspect", "GPU algorithms", "Computational modeling", "Biomembranes", "Three-dimensional displays", "Visualization", "Biological system modeling", "Tools", "Interactive modeling", "population", "biological data", "interactive visualization", "Algorithms", "Bacteria", "Cell Membrane", "Computational Biology", "Computer Graphics", "Data Visualization", "Humans", "Image Processing, Computer-Assisted", "Models, Biological"], "referenced_by": ["IKEY:8590012", "IKEY:8807314"], "referencing": []}, "10.1109/TVCG.2017.2744299": {"doi": "10.1109/TVCG.2017.2744299", "author": ["J. Kreiser", "A. Hann", "E. Zizer", "T. Ropinski"], "title": "Decision Graph Embedding for High-Resolution Manometry Diagnosis", "year": "2018", "abstract": "High-resolution manometry is an imaging modality which enables the categorization of esophageal motility disorders. Spatio-temporal pressure data along the esophagus is acquired using a tubular device and multiple test swallows are performed by the patient. Current approaches visualize these swallows as individual instances, despite the fact that aggregated metrics are relevant in the diagnostic process. Based on the current Chicago Classification, which serves as the gold standard in this area, we introduce a visualization supporting an efficient and correct diagnosis. To reach this goal, we propose a novel decision graph representing the Chicago Classification with workflow optimization in mind. Based on this graph, we are further able to prioritize the different metrics used during diagnosis and can exploit this prioritization in the actual data visualization. Thus, different disorders and their related parameters are directly represented and intuitively influence the appearance of our visualization. Within this paper, we introduce our novel visualization, justify the design decisions, and provide the results of a user study we performed with medical students as well as a domain expert. On top of the presented visualization, we further discuss how to derive a visual signature for individual patients that allows us for the first time to perform an intuitive comparison between subjects, in the form of small multiples.", "keywords": ["biological organs", "data visualisation", "graph theory", "image classification", "manometers", "medical image processing", "patient diagnosis", "patient treatment", "pressure sensors", "statistical analysis", "Chicago classification", "data visualization", "visual signature", "workflow optimization", "diagnostic process", "aggregated metrics", "multiple test swallows", "tubular device", "spatio-temporal pressure data", "esophageal motility disorders", "imaging modality", "high-resolution manometry diagnosis", "decision graph embedding", "Data visualization", "Medical diagnostic imaging", "Medical services", "Esophagus", "Visualization", "Image color analysis", "Standards", "Small multiples", "manometry", "chicago classification", "Adult", "Computer Graphics", "Data Visualization", "Esophageal Motility Disorders", "Esophagus", "Female", "Humans", "Image Interpretation, Computer-Assisted", "Male", "Manometry", "Young Adult"], "referenced_by": [], "referencing": []}, "10.1109/TVCG.2017.2744518": {"doi": "10.1109/TVCG.2017.2744518", "author": ["P. Mindek", "D. Kou\u0159il", "J. Sorger", "D. Toloudis", "B. Lyons", "G. Johnson", "M. E. Gr\u00f6ller", "I. Viola"], "title": "Visualization Multi-Pipeline for Communicating Biology", "year": "2018", "abstract": "We propose a system to facilitate biology communication by developing a pipeline to support the instructional visualization of heterogeneous biological data on heterogeneous user-devices. Discoveries and concepts in biology are typically summarized with illustrations assembled manually from the interpretation and application of heterogenous data. The creation of such illustrations is time consuming, which makes it incompatible with frequent updates to the measured data as new discoveries are made. Illustrations are typically non-interactive, and when an illustration is updated, it still has to reach the user. Our system is designed to overcome these three obstacles. It supports the integration of heterogeneous datasets, reflecting the knowledge that is gained from different data sources in biology. After pre-processing the datasets, the system transforms them into visual representations as inspired by scientific illustrations. As opposed to traditional scientific illustration these representations are generated in real-time - they are interactive. The code generating the visualizations can be embedded in various software environments. To demonstrate this, we implemented both a desktop application and a remote-rendering server in which the pipeline is embedded. The remote-rendering server supports multi-threaded rendering and it is able to handle multiple users simultaneously. This scalability to different hardware environments, including multi-GPU setups, makes our system useful for efficient public dissemination of biological discoveries.", "keywords": ["biology computing", "colour graphics", "data visualisation", "multi-threading", "public domain software", "rendering (computer graphics)", "visualization multipipeline", "biology communication", "instructional visualization", "heterogeneous biological data", "heterogeneous user-devices", "time consuming", "frequent updates", "heterogeneous datasets", "visual representations", "scientific illustrations", "traditional scientific illustration", "remote-rendering server", "multithreaded rendering", "multiple users", "system useful", "biological discoveries", "heterogenous data interpretation", "heterogenous data application", "data sources", "software environment", "desktop application", "biology discoveries", "Rendering (computer graphics)", "Data visualization", "Biology", "Servers", "Pipelines", "Computer architecture", "Tools", "Biological visualization", "remote rendering", "public dissemination", "Biomedical Research", "Computer Graphics", "Data Visualization", "Databases, Factual", "Humans", "Induced Pluripotent Stem Cells", "Models, Biological"], "referenced_by": ["IKEY:8440077", "IKEY:8805465", "IKEY:8807314"], "referencing": []}, "10.1109/TVCG.2017.2743989": {"doi": "10.1109/TVCG.2017.2743989", "author": ["M. Kern", "T. Hewson", "F. Sadlo", "R. Westermann", "M. Rautenhaus"], "title": "Robust Detection and Visualization of Jet-Stream Core Lines in Atmospheric Flow", "year": "2018", "abstract": "Jet-streams, their core lines and their role in atmospheric dynamics have been subject to considerable meteorological research since the first half of the twentieth century. Yet, until today no consistent automated feature detection approach has been proposed to identify jet-stream core lines from 3D wind fields. Such 3D core lines can facilitate meteorological analyses previously not possible. Although jet-stream cores can be manually analyzed by meteorologists in 2D as height ridges in the wind speed field, to the best of our knowledge no automated ridge detection approach has been applied to jet-stream core detection. In this work, we -a team of visualization scientists and meteorologists-propose a method that exploits directional information in the wind field to extract core lines in a robust and numerically less involved manner than traditional 3D ridge detection. For the first time, we apply the extracted 3D core lines to meteorological analysis, considering real-world case studies and demonstrating our method's benefits for weather forecasting and meteorological research.", "keywords": ["atmospheric techniques", "geophysical fluid dynamics", "meteorology", "weather forecasting", "wind", "jet-stream core line detection", "jet-stream core line visualization", "automated feature detection approach", "weather forecasting", "atmospheric flow", "traditional 3D ridge detection", "Three-dimensional displays", "Feature extraction", "Wind speed", "Weather forecasting", "Visualization", "Meteorology", "weather forecast", "jet-stream", "feature detection"], "referenced_by": ["IKEY:8126857", "IKEY:8440076", "IKEY:8440839", "IKEY:8405549", "IKEY:8805433", "10.1111/cgf.13731", "10.1007/s00287-019-01222-w", "10.1175/JCLI-D-19-0449.1", "10.1175/JAS-D-19-0349.1"], "referencing": []}, "10.1109/TVCG.2017.2744058": {"doi": "10.1109/TVCG.2017.2744058", "author": ["A. Jallepalli", "J. Docampo-S\u00e1nchez", "J. K. Ryan", "R. Haimes", "R. M. Kirby"], "title": "On the Treatment of Field Quantities and Elemental Continuity in FEM Solutions", "year": "2018", "abstract": "As the finite element method (FEM) and the finite volume method (FVM), both traditional and high-order variants, continue their proliferation into various applied engineering disciplines, it is important that the visualization techniques and corresponding data analysis tools that act on the results produced by these methods faithfully represent the underlying data. To state this in another way: the interpretation of data generated by simulation needs to be consistent with the numerical schemes that underpin the specific solver technology. As the verifiable visualization literature has demonstrated: visual artifacts produced by the introduction of either explicit or implicit data transformations, such as data resampling, can sometimes distort or even obfuscate key scientific features in the data. In this paper, we focus on the handling of elemental continuity, which is often only $C^{0}$ continuous or piecewise discontinuous, when visualizing primary or derived fields from FEM or FVM simulations. We demonstrate that traditional data handling and visualization of these fields introduce visual errors. In addition, we show how the use of the recently proposed line-SIAC filter provides a way of handling elemental continuity issues in an accuracy-conserving manner with the added benefit of casting the data in a smooth context even if the representation is element discontinuous.", "keywords": ["Data visualization", "Finite element analysis", "Data models", "Mathematical model", "Pipelines", "Tools", "Computational modeling", "Flow Visualization", "discontinuous Galerkin (dG) methods", "continuous Galerkin (cG) methods", "finite element methods (FEM)", "finite volume methods", "filtering techniques", "Scalar Field Data", "Irregular and Unstructured Grids", "Extraction of Surfaces((Isosurfaces)"], "referenced_by": ["IKEY:8805451", "IKEY:8933623", "10.1007/s10915-017-0517-5"], "referencing": ["IKEY:1372174", "IKEY:4359498", "IKEY:6532282", "IKEY:5928335", "IKEY:5290733", "IKEY:4607318", "IKEY:1542005", "IKEY:6064943", "IKEY:6327237", "IKEY:1634311", "IKEY:4429178", "IKEY:1372174", "IKEY:4359498", "IKEY:6532282", "IKEY:5928335", "IKEY:5290733", "IKEY:4607318", "IKEY:1542005", "IKEY:6064943", "IKEY:6327237", "IKEY:1634311", "IKEY:4429178", "IKEY:1372174", "IKEY:4359498", "IKEY:6532282", "IKEY:5928335", "IKEY:5290733", "IKEY:4607318", "IKEY:1542005", "IKEY:6064943", "IKEY:6327237", "IKEY:1634311", "IKEY:4429178", "10.1016/j.cpc.2015.02.008", "10.1098/rsif.2016.0234", "10.1007/978-3-642-59721-3_24", "10.1137/070681284", "10.1137/16M1097845", "10.1090/S0025-5718-2014-02835-4", "10.1007/s10915-012-9593-8", "10.2514/1.J054181", "10.1137/120874059", "10.1007/s10915-009-9342-9", "10.1007/s10915-011-9535-x", "10.1007/s10915-015-0081-9", "10.1016/j.camwa.2015.06.034", "10.1016/j.jcp.2009.08.017", "10.1007/s10915-014-9946-6", "10.1137/100782188", "10.1007/s10915-008-9230-8", "10.1016/j.cpc.2015.02.008", "10.1098/rsif.2016.0234", "10.1007/978-3-642-59721-3_24", "10.1137/070681284", "10.1137/16M1097845", "10.1090/S0025-5718-2014-02835-4", "10.1007/s10915-012-9593-8", "10.2514/1.J054181", "10.1137/120874059", "10.1007/s10915-009-9342-9", "10.1007/s10915-011-9535-x", "10.1007/s10915-015-0081-9", "10.1016/j.camwa.2015.06.034", "10.1016/j.jcp.2009.08.017", "10.1007/s10915-014-9946-6", "10.1137/100782188", "10.1007/s10915-008-9230-8", "10.1016/j.cpc.2015.02.008", "10.1098/rsif.2016.0234", "10.1007/978-3-642-59721-3_24", "10.1137/070681284", "10.1137/16M1097845", "10.1090/S0025-5718-2014-02835-4", "10.1007/s10915-012-9593-8", "10.2514/1.J054181", "10.1137/120874059", "10.1007/s10915-009-9342-9", "10.1007/s10915-011-9535-x", "10.1007/s10915-015-0081-9", "10.1016/j.camwa.2015.06.034", "10.1016/j.jcp.2009.08.017", "10.1007/s10915-014-9946-6", "10.1137/100782188", "10.1007/s10915-008-9230-8"]}, "10.1109/TVCG.2017.2744459": {"doi": "10.1109/TVCG.2017.2744459", "author": ["G. E. Marai"], "title": "Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization", "year": "2018", "abstract": "Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage - and its evaluation - of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.", "keywords": ["data visualisation", "software engineering", "user centred design", "activity-centered domain characterization", "domain characterization step", "problem-driven visualization application design", "activity-centered design paradigm", "human-centered design", "activity-centered approach", "requirements engineering process", "requirements process", "software engineering literature", "visualization design nested model", "data visualization", "domain characterization model", "visualization design literature", "problem-driven scientific visualization", "methodological frameworks", "visualization design models", "Data visualization", "Computational modeling", "Data models", "Human computer interaction", "Tools", "Interviews", "Requirements engineering", "Design studies", "Tasks and requirements analysis", "Visualization models", "Domain characterization", "Activity-centered design", "Functional specifications", "Algorithms", "Computer Graphics", "Humans", "Software", "User-Computer Interface", "Visual Perception"], "referenced_by": ["IKEY:8534026", "IKEY:8440850", "IKEY:8440830", "IKEY:8320386", "IKEY:8807351", "IKEY:8809842", "IKEY:8977377", "IKEY:9307751"], "referencing": []}, "10.1109/TVCG.2017.2743978": {"doi": "10.1109/TVCG.2017.2743978", "author": ["R. Bujack", "T. L. Turton", "F. Samsel", "C. Ware", "D. H. Rogers", "J. Ahrens"], "title": "The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps", "year": "2018", "abstract": "A myriad of design rules for what constitutes a \u201cgood\u201d colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.", "keywords": ["colour graphics", "design of experiments", "uniformity", "high discriminative power", "collaborative work", "design of experiments", "automated colormap generation", "current guidelines", "historical guidelines", "colormap design", "common design rules", "theoretical framework", "continuous colormaps", "unambiguous mathematical definitions", "Image color analysis", "Linearity", "Color", "Taxonomy", "Sensitivity", "Brightness", "colormap", "survey", "taxonomy", "order", "uniformity", "discriminative power", "smoothness", "monotonicity", "linearity", "speed"], "referenced_by": [], "referencing": ["10.1145/2207676.2208547", "10.1145/965105.807502", "10.1145/2207676.2208547", "10.1145/965105.807502", "10.1145/2207676.2208547", "10.1145/965105.807502", "10.1016/B978-012387582-2/50038-1", "10.1037/0278-7393.14.4.579", "10.1111/j.1467-8659.2011.01938.x", "10.1016/B978-0-08-042415-6.50014-4", "10.1002/col.10051", "10.1002/9781118653128", "10.2307/2683729", "10.1364/JOSAA.23.002077", "10.1016/j.visres.2010.09.012", "10.1002/(SICI)1098-1098(199622)7:2&lt;97::AID-IMA5&gt;3.0.CO;2-N", "10.1029/2004EO400002", "10.1111/cgf.12127", "10.1002/col.20227", "10.1002/col.1049", "10.1007/978-1-4419-6190-7_2", "10.1364/JOSA.32.000247", "10.1111/cgf.12633", "10.1111/cgf.12633", "10.1111/cgf.12379", "10.1007/978-3-642-10520-3_9", "10.1016/0146-664X(81)90006-X", "10.1117/12.934586", "10.1002/col.20710", "10.1007/BF00275798", "10.1117/12.384882", "10.1063/1.4822401", "10.1167/15.12.1317", "10.1016/j.cag.2010.11.015", "10.1016/S0146-664X(79)80040-4", "10.1037/1076-898X.5.4.393", "10.1016/S0734-189X(83)80046-2", "10.5670/oceanog.2016.66", "10.2307/2683294", "10.2307/2684111", "10.1016/j.cag.2016.06.004", "10.1111/j.1467-8659.2008.01203.x", "10.1088/0959-5309/46/3/317", "10.1016/j.csda.2008.11.033", "10.1016/B978-012387582-2/50038-1", "10.1037/0278-7393.14.4.579", "10.1111/j.1467-8659.2011.01938.x", "10.1016/B978-0-08-042415-6.50014-4", "10.1002/col.10051", "10.1002/9781118653128", "10.2307/2683729", "10.1364/JOSAA.23.002077", "10.1016/j.visres.2010.09.012", "10.1002/(SICI)1098-1098(199622)7:2&lt;97::AID-IMA5&gt;3.0.CO;2-N", "10.1029/2004EO400002", "10.1111/cgf.12127", "10.1002/col.20227", "10.1002/col.1049", "10.1007/978-1-4419-6190-7_2", "10.1364/JOSA.32.000247", "10.1111/cgf.12633", "10.1111/cgf.12633", "10.1111/cgf.12379", "10.1007/978-3-642-10520-3_9", "10.1016/0146-664X(81)90006-X", "10.1117/12.934586", "10.1002/col.20710", "10.1007/BF00275798", "10.1117/12.384882", "10.1063/1.4822401", "10.1167/15.12.1317", "10.1016/j.cag.2010.11.015", "10.1016/S0146-664X(79)80040-4", "10.1037/1076-898X.5.4.393", "10.1016/S0734-189X(83)80046-2", "10.5670/oceanog.2016.66", "10.2307/2683294", "10.2307/2684111", "10.1016/j.cag.2016.06.004", "10.1111/j.1467-8659.2008.01203.x", "10.1088/0959-5309/46/3/317", "10.1016/j.csda.2008.11.033", "10.1016/B978-012387582-2/50038-1", "10.1037/0278-7393.14.4.579", "10.1111/j.1467-8659.2011.01938.x", "10.1016/B978-0-08-042415-6.50014-4", "10.1002/col.10051", "10.1002/9781118653128", "10.2307/2683729", "10.1364/JOSAA.23.002077", "10.1016/j.visres.2010.09.012", "10.1002/(SICI)1098-1098(199622)7:2&lt;97::AID-IMA5&gt;3.0.CO;2-N", "10.1029/2004EO400002", "10.1111/cgf.12127", "10.1002/col.20227", "10.1002/col.1049", "10.1007/978-1-4419-6190-7_2", "10.1364/JOSA.32.000247", "10.1111/cgf.12633", "10.1111/cgf.12633", "10.1111/cgf.12379", "10.1007/978-3-642-10520-3_9", "10.1016/0146-664X(81)90006-X", "10.1117/12.934586", "10.1002/col.20710", "10.1007/BF00275798", "10.1117/12.384882", "10.1063/1.4822401", "10.1167/15.12.1317", "10.1016/j.cag.2010.11.015", "10.1016/S0146-664X(79)80040-4", "10.1037/1076-898X.5.4.393", "10.1016/S0734-189X(83)80046-2", "10.5670/oceanog.2016.66", "10.2307/2683294", "10.2307/2684111", "10.1016/j.cag.2016.06.004", "10.1111/j.1467-8659.2008.01203.x", "10.1088/0959-5309/46/3/317", "10.1016/j.csda.2008.11.033"]}, "10.1109/TVCG.2017.2744099": {"doi": "10.1109/TVCG.2017.2744099", "author": ["S. Hazarika", "A. Biswas", "H. Shen"], "title": "Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models", "year": "2018", "abstract": "Distributions are often used to model uncertainty in many scientific datasets. To preserve the correlation among the spatially sampled grid locations in the dataset, various standard multivariate distribution models have been proposed in visualization literature. These models treat each grid location as a univariate random variable which models the uncertainty at that location. Standard multivariate distributions (both parametric and nonparametric) assume that all the univariate marginals are of the same type/family of distribution. But in reality, different grid locations show different statistical behavior which may not be modeled best by the same type of distribution. In this paper, we propose a new multivariate uncertainty modeling strategy to address the needs of uncertainty modeling in scientific datasets. Our proposed method is based on a statistically sound multivariate technique called Copula, which makes it possible to separate the process of estimating the univariate marginals and the process of modeling dependency, unlike the standard multivariate distributions. The modeling flexibility offered by our proposed method makes it possible to design distribution fields which can have different types of distribution (Gaussian, Histogram, KDE etc.) at the grid locations, while maintaining the correlation structure at the same time. Depending on the results of various standard statistical tests, we can choose an optimal distribution representation at each location, resulting in a more cost efficient modeling without significantly sacrificing on the analysis quality. To demonstrate the efficacy of our proposed modeling strategy, we extract and visualize uncertain features like isocontours and vortices in various real world datasets. We also study various modeling criterion to help users in the task of univariate model selection.", "keywords": ["data analysis", "data visualisation", "Gaussian processes", "nonparametric statistics", "statistical distributions", "statistical testing", "uncertainty visualization", "mixed distribution models", "scientific datasets", "spatially sampled grid locations", "standard multivariate distribution models", "visualization literature", "grid location", "univariate random variable", "standard multivariate distributions", "univariate marginals", "multivariate uncertainty modeling strategy", "statistically sound multivariate technique", "modeling flexibility", "distribution fields", "standard statistical tests", "optimal distribution representation", "modeling criterion", "univariate model selection", "statistical behavior", "Copula-Based Analysis", "grid locations", "Computational modeling", "Uncertainty", "Analytical models", "Feature extraction", "Correlation", "Data models", "Standards", "Uncertainty visualization", "probability distribution", "probabilistic feature", "statistical modeling", "copula"], "referenced_by": ["IKEY:8440034", "IKEY:8440043", "IKEY:8405549", "IKEY:8525340"], "referencing": ["IKEY:6634171", "IKEY:7192629", "IKEY:7352364", "IKEY:7539561", "IKEY:7192664", "IKEY:1310205", "IKEY:1231171", "IKEY:1438260", "IKEY:5620906", "IKEY:6327235", "IKEY:6092313", "IKEY:6634129", "IKEY:6634171", "IKEY:7192629", "IKEY:7352364", "IKEY:7539561", "IKEY:7192664", "IKEY:1310205", "IKEY:1231171", "IKEY:1438260", "IKEY:5620906", "IKEY:6327235", "IKEY:6092313", "IKEY:6634129", "IKEY:6634171", "IKEY:7192629", "IKEY:7352364", "IKEY:7539561", "IKEY:7192664", "IKEY:1310205", "IKEY:1231171", "IKEY:1438260", "IKEY:5620906", "IKEY:6327235", "IKEY:6092313", "IKEY:6634129", "10.1145/2020408.2020509", "10.1145/1390156.1390215", "10.1145/2020408.2020509", "10.1145/1390156.1390215", "10.1145/2020408.2020509", "10.1145/1390156.1390215", "10.1080/13504860210136721a", "10.1017/CBO9780511761676", "10.1016/B978-044450896-6.50010-8", "10.1007/BF00053369", "10.1002/sta4.39", "10.5812/ijem.3505", "10.1111/cgf.12912", "10.1007/978-3-540-74494-8_10", "10.1016/j.cag.2014.01.007", "10.1111/j.1467-8659.2009.01604.x", "10.1111/j.1467-8659.2012.03096.x", "10.1007/s003710050111", "10.1111/j.1467-8659.2012.03097.x", "10.1111/j.1467-8659.2011.01944.x", "10.1111/j.1467-8659.2012.03095.x", "10.1111/cgf.12100", "10.1615/Int.J.UncertaintyQuantification.2012003958", "10.1111/j.1467-8659.2011.01942.x", "10.1093/biomet/52.3-4.591", "10.2307/2286009", "10.1080/13504860210136721a", "10.1017/CBO9780511761676", "10.1016/B978-044450896-6.50010-8", "10.1007/BF00053369", "10.1002/sta4.39", "10.5812/ijem.3505", "10.1111/cgf.12912", "10.1007/978-3-540-74494-8_10", "10.1016/j.cag.2014.01.007", "10.1111/j.1467-8659.2009.01604.x", "10.1111/j.1467-8659.2012.03096.x", "10.1007/s003710050111", "10.1111/j.1467-8659.2012.03097.x", "10.1111/j.1467-8659.2011.01944.x", "10.1111/j.1467-8659.2012.03095.x", "10.1111/cgf.12100", "10.1615/Int.J.UncertaintyQuantification.2012003958", "10.1111/j.1467-8659.2011.01942.x", "10.1093/biomet/52.3-4.591", "10.2307/2286009", "10.1080/13504860210136721a", "10.1017/CBO9780511761676", "10.1016/B978-044450896-6.50010-8", "10.1007/BF00053369", "10.1002/sta4.39", "10.5812/ijem.3505", "10.1111/cgf.12912", "10.1007/978-3-540-74494-8_10", "10.1016/j.cag.2014.01.007", "10.1111/j.1467-8659.2009.01604.x", "10.1111/j.1467-8659.2012.03096.x", "10.1007/s003710050111", "10.1111/j.1467-8659.2012.03097.x", "10.1111/j.1467-8659.2011.01944.x", "10.1111/j.1467-8659.2012.03095.x", "10.1111/cgf.12100", "10.1615/Int.J.UncertaintyQuantification.2012003958", "10.1111/j.1467-8659.2011.01942.x", "10.1093/biomet/52.3-4.591", "10.2307/2286009"]}, "10.1109/TVCG.2017.2743979": {"doi": "10.1109/TVCG.2017.2743979", "author": ["M. Ibrahim", "P. Wickenh\u00e4user", "P. Rautek", "G. Reina", "M. Hadwiger"], "title": "Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data", "year": "2018", "abstract": "Molecular dynamics (MD) simulations are crucial to investigating important processes in physics and thermodynamics. The simulated atoms are usually visualized as hard spheres with Phong shading, where individual particles and their local density can be perceived well in close-up views. However, for large-scale simulations with 10 million particles or more, the visualization of large fields-of-view usually suffers from strong aliasing artifacts, because the mismatch between data size and output resolution leads to severe under-sampling of the geometry. Excessive super-sampling can alleviate this problem, but is prohibitively expensive. This paper presents a novel visualization method for large-scale particle data that addresses aliasing while enabling interactive high-quality rendering. We introduce the novel concept of screen-space normal distribution functions (S-NDFs) for particle data. S-NDFs represent the distribution of surface normals that map to a given pixel in screen space, which enables high-quality re-lighting without re-rendering particles. In order to facilitate interactive zooming, we cache S-NDFs in a screen-space mipmap (S-MIP). Together, these two concepts enable interactive, scale-consistent re-lighting and shading changes, as well as zooming, without having to re-sample the particle data. We show how our method facilitates the interactive exploration of real-world large-scale MD simulation data in different scenarios.", "keywords": ["data visualisation", "molecular dynamics method", "normal distribution", "rendering (computer graphics)", "re-rendering particles", "screen-space mipmap", "scale-consistent re-lighting", "simulation data", "screen-space normal distribution function caching", "molecular dynamics simulations", "simulated atoms", "large-scale simulations", "data size", "output resolution", "large-scale particle data", "high-quality rendering", "screen-space normal distribution functions", "surface normals", "aliasing artifacts", "visualization method", "multiresolution rendering", "Rendering (computer graphics)", "Data visualization", "Casting", "Gaussian distribution", "Probability density function", "Lighting", "Force", "Multiresolution Techniques", "Point-Based Data", "Glyph-based Techniques", "Scalability Issues", "Molecular Visualization"], "referenced_by": ["IKEY:8637795"], "referencing": []}, "10.1109/TVCG.2017.2744059": {"doi": "10.1109/TVCG.2017.2744059", "author": ["J. Zhang", "H. Guo", "F. Hong", "X. Yuan", "T. Peterka"], "title": "Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for Parallel Particle Tracing", "year": "2018", "abstract": "We propose a dynamically load-balanced algorithm for parallel particle tracing, which periodically attempts to evenly redistribute particles across processes based on k-d tree decomposition. Each process is assigned with (1) a statically partitioned, axis-aligned data block that partially overlaps with neighboring blocks in other processes and (2) a dynamically determined k-d tree leaf node that bounds the active particles for computation; the bounds of the k-d tree nodes are constrained by the geometries of data blocks. Given a certain degree of overlap between blocks, our method can balance the number of particles as much as possible. Compared with other load-balancing algorithms for parallel particle tracing, the proposed method does not require any preanalysis, does not use any heuristics based on flow features, does not make any assumptions about seed distribution, does not move any data blocks during the run, and does not need any master process for work redistribution. Based on a comprehensive performance study up to 8K processes on a Blue Gene/Q system, the proposed algorithm outperforms baseline approaches in both load balance and scalability on various flow visualization and analysis problems.", "keywords": ["resource allocation", "trees (mathematics)", "constrained K-D tree decomposition", "parallel particle tracing", "dynamically load-balanced algorithm", "data block", "neighboring blocks", "dynamically determined k-d tree leaf", "active particles", "k-d tree nodes", "load-balancing algorithms", "master process", "scalability", "flow visualization", "flow analysis", "Blue Gene/Q system", "Heuristic algorithms", "Load management", "Partitioning algorithms", "Algorithm design and analysis", "Data visualization", "Scalability", "Load modeling", "Parallel particle tracing", "dynamic load balancing", "k-d trees", "performance analysis"], "referenced_by": ["IKEY:8365979", "IKEY:8739165", "IKEY:8419325", "IKEY:8809847", "IKEY:8916488", "10.1049/iet-cdt.2019.0166"], "referencing": ["IKEY:6634133", "IKEY:4475463", "IKEY:4376175", "IKEY:6634188", "IKEY:5611509", "IKEY:7874307", "IKEY:6675152", "IKEY:6064941", "IKEY:7465254", "IKEY:6634133", "IKEY:4475463", "IKEY:4376175", "IKEY:6634188", "IKEY:5611509", "IKEY:7874307", "IKEY:6675152", "IKEY:6064941", "IKEY:7465254", "IKEY:6634133", "IKEY:4475463", "IKEY:4376175", "IKEY:6634188", "IKEY:5611509", "IKEY:7874307", "IKEY:6675152", "IKEY:6064941", "IKEY:7465254", "10.1145/361002.361007", "10.1145/166117.166151", "10.1145/1654059.1654113", "10.1145/2063384.2063397", "10.1145/2807591.2807616", "10.1145/1362622.1362655", "10.1145/361002.361007", "10.1145/166117.166151", "10.1145/1654059.1654113", "10.1145/2063384.2063397", "10.1145/2807591.2807616", "10.1145/1362622.1362655", "10.1145/361002.361007", "10.1145/166117.166151", "10.1145/1654059.1654113", "10.1145/2063384.2063397", "10.1145/2807591.2807616", "10.1145/1362622.1362655", "10.1038/324446a0", "10.1137/0909044", "10.1016/j.cag.2012.07.006", "10.1016/S0167-2789(00)00199-8", "10.1111/j.1467-8659.2004.00753.x", "10.1111/j.1467-8659.2010.01650.x", "10.1111/j.1467-8659.2003.00723.x", "10.1007/11558989_5", "10.1038/324446a0", "10.1137/0909044", "10.1016/j.cag.2012.07.006", "10.1016/S0167-2789(00)00199-8", "10.1111/j.1467-8659.2004.00753.x", "10.1111/j.1467-8659.2010.01650.x", "10.1111/j.1467-8659.2003.00723.x", "10.1007/11558989_5", "10.1038/324446a0", "10.1137/0909044", "10.1016/j.cag.2012.07.006", "10.1016/S0167-2789(00)00199-8", "10.1111/j.1467-8659.2004.00753.x", "10.1111/j.1467-8659.2010.01650.x", "10.1111/j.1467-8659.2003.00723.x", "10.1007/11558989_5"]}, "10.1109/TVCG.2017.2744078": {"doi": "10.1109/TVCG.2017.2744078", "author": ["T. M. Quan", "J. Choi", "H. Jeong", "W. Jeong"], "title": "An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding", "year": "2018", "abstract": "In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.", "keywords": ["feature extraction", "image classification", "image coding", "image representation", "image segmentation", "learning (artificial intelligence)", "pattern classification", "probability", "rendering (computer graphics)", "dictionary based features", "hierarchical multiscale 3D convolutional sparse coding", "sparse feature representation method", "high-dimensional feature vectors", "random forest classifier", "probabilistic transfer function", "synthetic world volume datasets", "real-world volume datasets", "intelligent system approach", "hierarchical 3D convolutional sparse coding", "voxel classification method", "probabilistic volume rendering", "machine learning-based voxel classification method", "Transfer functions", "Three-dimensional displays", "Rendering (computer graphics)", "Convolutional codes", "Encoding", "Dictionaries", "Intelligent systems", "Volume Rendering", "Machine Learning", "Hierarchically Convolutional Sparse Coding"], "referenced_by": ["IKEY:8681072", "IKEY:8781576", "IKEY:8823764"], "referencing": ["IKEY:1710377", "IKEY:4658153", "IKEY:1640847", "IKEY:6064975", "IKEY:4775894", "IKEY:729588", "IKEY:1021579", "IKEY:511", "IKEY:6185542", "IKEY:920623", "IKEY:1407860", "IKEY:7308045", "IKEY:7164109", "IKEY:1710377", "IKEY:4658153", "IKEY:1640847", "IKEY:6064975", "IKEY:4775894", "IKEY:729588", "IKEY:1021579", "IKEY:511", "IKEY:6185542", "IKEY:920623", "IKEY:1407860", "IKEY:7308045", "IKEY:7164109", "IKEY:1710377", "IKEY:4658153", "IKEY:1640847", "IKEY:6064975", "IKEY:4775894", "IKEY:729588", "IKEY:1021579", "IKEY:511", "IKEY:6185542", "IKEY:920623", "IKEY:1407860", "IKEY:7308045", "IKEY:7164109", "10.1145/54852.378484", "10.1145/360825.360839", "10.1145/54852.378484", "10.1145/360825.360839", "10.1145/54852.378484", "10.1145/360825.360839", "10.1023/A:1010933404324", "10.1111/cgf.12624", "10.2307/1932409", "10.1111/cgf.12934", "10.1207/s15327051hci0701_3", "10.1007/978-3-319-12643-2_31", "10.1007/978-3-319-46726-9_56", "10.1111/cgf.12623", "10.1023/A:1010933404324", "10.1111/cgf.12624", "10.2307/1932409", "10.1111/cgf.12934", "10.1207/s15327051hci0701_3", "10.1007/978-3-319-12643-2_31", "10.1007/978-3-319-46726-9_56", "10.1111/cgf.12623", "10.1023/A:1010933404324", "10.1111/cgf.12624", "10.2307/1932409", "10.1111/cgf.12934", "10.1207/s15327051hci0701_3", "10.1007/978-3-319-12643-2_31", "10.1007/978-3-319-46726-9_56", "10.1111/cgf.12623"]}, "10.1109/TVCG.2017.2744238": {"doi": "10.1109/TVCG.2017.2744238", "author": ["M. Hadwiger", "A. K. Al-Awami", "J. Beyer", "M. Agus", "H. Pfister"], "title": "SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering", "year": "2018", "abstract": "Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.", "keywords": ["computer graphic equipment", "data acquisition", "octrees", "rendering (computer graphics)", "highly fragmented space subdivisions", "regions changes", "space subdivision", "fine structures", "ray-casting stage", "volume data", "ray segment list", "active objects changes", "SparseLeap scales", "sparse data", "large-scale volume rendering", "data acquisition", "high resolution", "microscopy volumes", "intricate structures", "SparseLeap method", "view-independent bounding boxes", "standard octree empty space skipping", "Rendering (computer graphics)", "Octrees", "Graphics processing units", "Geometry", "Image segmentation", "Standards", "Histograms", "Empty Space Skipping", "Volume Rendering", "Segmented Volume Data", "Hybrid Image/Object-Order Approaches"], "referenced_by": ["IKEY:8781571", "IKEY:8795066", "IKEY:8794560", "IKEY:8933631", "IKEY:8944267", "IKEY:8637795"], "referencing": []}, "10.1109/TVCG.2017.2744438": {"doi": "10.1109/TVCG.2017.2744438", "author": ["J. G. Magnus", "S. Bruckner"], "title": "Interactive Dynamic Volume Illumination with Refraction and Caustics", "year": "2018", "abstract": "In recent years, significant progress has been made in developing high-quality interactive methods for realistic volume illumination. However, refraction - despite being an important aspect of light propagation in participating media - has so far only received little attention. In this paper, we present a novel approach for refractive volume illumination including caustics capable of interactive frame rates. By interleaving light and viewing ray propagation, our technique avoids memory-intensive storage of illumination information and does not require any precomputation. It is fully dynamic and all parameters such as light position and transfer function can be modified interactively without a performance penalty.", "keywords": ["interactive systems", "rendering (computer graphics)", "interactive dynamic volume illumination", "refraction", "high-quality interactive methods", "light propagation", "interactive frame rates", "viewing ray propagation", "memory-intensive storage", "illumination information", "light position", "volume illumination", "refractive volume illumination", "Lighting", "Rendering (computer graphics)", "Mathematical model", "Media", "Scattering", "Refractive index", "Ray tracing", "Interactive volume rendering", "illumination", "refraction", "shadows", "caustics"], "referenced_by": ["IKEY:8823764", "IKEY:8637795", "IKEY:9190937"], "referencing": []}, "10.1109/TVCG.2017.2744079": {"doi": "10.1109/TVCG.2017.2744079", "author": ["W. Usher", "P. Klacansky", "F. Federer", "P. Bremer", "A. Knoll", "J. Yarch", "A. Angelucci", "V. Pascucci"], "title": "A Virtual Reality Visualization Tool for Neuron Tracing", "year": "2018", "abstract": "Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.", "keywords": ["brain", "data visualisation", "medical image processing", "neural nets", "neurophysiology", "virtual reality", "wiring diagram", "terabyte-sized image volumes", "design process", "consumer-grade VR technology", "virtual reality system", "2D image stacks", "complex datasets", "brain process information", "large-scale microscopy data", "neuron tracing", "virtual reality visualization tool", "Neurons", "Three-dimensional displays", "Tools", "Microscopy", "Image reconstruction", "Rendering (computer graphics)", "Data visualization", "Virtual reality", "interaction design", "design studies", "Computer Graphics", "Humans", "Microscopy", "Neuroanatomy", "Neurons", "User-Computer Interface", "Virtual Reality", "Vision, Ocular"], "referenced_by": ["IKEY:8365995", "IKEY:8440858", "IKEY:8440805", "IKEY:8440808", "IKEY:8781584", "IKEY:8823763", "IKEY:8933605", "IKEY:8943760", "IKEY:8961433", "IKEY:9086291", "IKEY:9288591"], "referencing": []}, "10.1109/TVCG.2017.2744159": {"doi": "10.1109/TVCG.2017.2744159", "author": ["Q. Shen", "W. Zeng", "Y. Ye", "S. M. Arisona", "S. Schubiger", "R. Burkhard", "H. Qu"], "title": "StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views", "year": "2018", "abstract": "Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, smell, and touch) in their daily lives, can provide unprecedented insights on a variety of applications, such as urban planning and environment auditing. The analysis of urban forms can help planners develop high-quality urban spaces through evidence-based design. However, such analysis is complex because of the involvement of spatial, multi-scale (i.e., city, region, and street), and multivariate (e.g., greenery and sky ratios) natures of urban forms. In addition, current methods either lack quantitative measurements or are limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge in exploring human-scale urban forms based on street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer for the visual comparison of spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scales; 2) and a Street Explorer with a novel parallel coordinate plot for the exploration of the fine-grained details of the urban forms at the street-scale. We integrate visualization techniques with machine learning models to facilitate the detection of street view patterns. We illustrate the applicability of our approach with case studies on the real-world datasets of four cities, i.e., Hong Kong, Singapore, Greater London and New York City. Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.", "keywords": ["building management systems", "computer vision", "data analysis", "data visualisation", "learning (artificial intelligence)", "town and country planning", "interactive visual analytics system", "human-scale urban forms", "street view images", "two-stage visual exploration", "Street Explorer", "street-scale", "street view patterns", "urban environments", "urban planning", "environment auditing", "high-quality urban spaces", "Data visualization", "Urban areas", "Green products", "Visual analytics", "Correlation", "Layout", "Urban forms", "human scale", "street view", "visual analytics", "Cities", "Computer Graphics", "Humans", "Image Processing, Computer-Assisted", "Maps as Topic", "User-Computer Interface"], "referenced_by": ["IKEY:8474512", "IKEY:8633409", "IKEY:8807274", "IKEY:8897648"], "referencing": []}, "10.1109/TVCG.2017.2743981": {"doi": "10.1109/TVCG.2017.2743981", "author": ["H. Miao", "E. De Llano", "J. Sorger", "Y. Ahmadi", "T. Kekic", "T. Isenberg", "M. E. Gr\u00f6ller", "I. Bari\u0161i\u0107", "I. Viola"], "title": "Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures", "year": "2018", "abstract": "We present an approach to represent DNA nanostructures in varying forms of semantic abstraction, describe ways to smoothly transition between them, and thus create a continuous multiscale visualization and interaction space for applications in DNA nanotechnology. This new way of observing, interacting with, and creating DNA nanostructures enables domain experts to approach their work in any of the semantic abstraction levels, supporting both low-level manipulations and high-level visualization and modifications. Our approach allows them to deal with the increasingly complex DNA objects that they are designing, to improve their features, and to add novel functions in a way that no existing single-scale approach offers today. For this purpose we collaborated with DNA nanotechnology experts to design a set of ten semantic scales. These scales take the DNA's chemical and structural behavior into account and depict it from atoms to the targeted architecture with increasing levels of abstraction. To create coherence between the discrete scales, we seamlessly transition between them in a well-defined manner. We use special encodings to allow experts to estimate the nanoscale object's stability. We also add scale-adaptive interactions that facilitate the intuitive modification of complex structures at multiple scales. We demonstrate the applicability of our approach on an experimental use case. Moreover, feedback from our collaborating domain experts confirmed an increased time efficiency and certainty for analysis and modification tasks on complex DNA structures. Our method thus offers exciting new opportunities with promising applications in medicine and biotechnology.", "keywords": ["biology computing", "data visualisation", "DNA", "nanotechnology", "discrete scales", "semantic scales", "DNA nanotechnology experts", "high-level visualization", "low-level manipulations", "semantic abstraction levels", "continuous multiscale visualization", "scale-adaptive modification", "complex DNA structures", "scale-adaptive interactions", "DNA", "Nanostructures", "Tools", "Nanoscale devices", "Shape", "Visualization", "Semantics", "Nano", "nanotechnology", "assembly", "multiscale", "abstraction", "DNA", "origami", "scale-adaptive modification", "Computer Graphics", "DNA", "Image Processing, Computer-Assisted", "Models, Molecular", "Nanostructures", "Nanotechnology", "Semantics"], "referenced_by": ["IKEY:8025425", "IKEY:8805465"], "referencing": []}, "10.1109/TVCG.2017.2744479": {"doi": "10.1109/TVCG.2017.2744479", "author": ["O. R\u00fcbel", "B. P. Bowen"], "title": "BASTet: Shareable and Reproducible Analysis and Visualization of Mass Spectrometry Imaging Data via OpenMSI", "year": "2018", "abstract": "Mass spectrometry imaging (MSI) is a transformative imaging method that supports the untargeted, quantitative measurement of the chemical composition and spatial heterogeneity of complex samples with broad applications in life sciences, bioenergy, and health. While MSI data can be routinely collected, its broad application is currently limited by the lack of easily accessible analysis methods that can process data of the size, volume, diversity, and complexity generated by MSI experiments. The development and application of cutting-edge analytical methods is a core driver in MSI research for new scientific discoveries, medical diagnostics, and commercial-innovation. However, the lack of means to share, apply, and reproduce analyses hinders the broad application, validation, and use of novel MSI analysis methods. To address this central challenge, we introduce the Berkeley Analysis and Storage Toolkit (BASTet), a novel framework for shareable and reproducible data analysis that supports standardized data and analysis interfaces, integrated data storage, data provenance, workflow management, and a broad set of integrated tools. Based on BASTet, we describe the extension of the OpenMSI mass spectrometry imaging science gateway to enable web-based sharing, reuse, analysis, and visualization of data analyses and derived data products. We demonstrate the application of BASTet and OpenMSI in practice to identify and compare characteristic substructures in the mouse brain based on their chemical composition measured via MSI.", "keywords": ["biomedical MRI", "brain", "medical image processing", "BASTet", "shareable analysis", "reproducible analysis", "mass spectrometry imaging data", "transformative imaging method", "untargeted measurement", "quantitative measurement", "chemical composition", "spatial heterogeneity", "MSI analysis methods", "shareable data analysis", "reproducible data analysis", "integrated data storage", "data provenance", "OpenMSI mass spectrometry", "data products", "Berkeley analysis", "Data visualization", "Imaging", "Data analysis", "Mass spectroscopy", "Tools", "Software", "Ions", "Mass spectrometry imaging", "Data provenance", "Visualization", "Data management", "Analysis Workflows", "Data sharing", "Database Management Systems", "Databases, Factual", "Information Dissemination", "Mass Spectrometry", "Molecular Imaging", "User-Computer Interface"], "referenced_by": ["IKEY:8823788"], "referencing": []}}