{"10.1109/TVCG.2018.2866106": {"doi": "10.1109/TVCG.2018.2866106", "author": ["Y. Kim", "H. Eun", "C. Jung", "C. Kim"], "title": "A Quad Edge-Based Grid Encoding Model for Content-Aware Image Retargeting", "year": "2019", "abstract": "In this paper, we present a novel grid encoding model for content-aware image retargeting. In contrast to previous approaches such as vertex-based and axis-aligned grid encoding models, our approach takes each horizontal/vertical distance between two adjacent vertices as an optimization variable. Upon this difference-based encoding scheme, every vertex position of a target grid is subsequently determined after optimizing the one-dimensional values. Our quad edge-based grid model has two major advantages for image retargeting. First, the model enables a grid optimization problem to be developed in a simple quadratic program while ensuring the global convexity of objective functions. Second, due to the independency of variables, spatial regularizations can be applied in a locally adaptive manner to preserve structural components. Based on this model, we propose three quadratic objective functions. Note that, in our work, their linear combination guides a grid deformation process to obtain a visually comfortable retargeting result by preserving salient regions and structural components of an input image. Comparative evaluations have been conducted with ten existing state-of-the-art image retargeting methods, and the results show that our method built upon the quad edge-based model consistently outperforms other previous methods both on qualitative and quantitative perspectives.", "keywords": ["image resolution", "quadratic programming", "quad edge-based grid encoding model", "content-aware image retargeting", "axis-aligned grid encoding models", "optimization variable", "quad edge-based grid model", "grid optimization problem", "quadratic objective functions", "grid deformation process", "grid encoding model", "difference-based encoding scheme", "Computational modeling", "Image segmentation", "Image quality", "Distortion", "Encoding", "Adaptation models", "Image retargeting", "2D grid deformation", "saliency detection", "line segment detection", "image quality assessment"], "referenced_by": ["IKEY:8826244"], "referencing": ["IKEY:6775267", "IKEY:6247786", "IKEY:5571001", "IKEY:5543281", "IKEY:6357312", "IKEY:6738219", "IKEY:7532625", "IKEY:7328742", "IKEY:7466112", "IKEY:7403996", "IKEY:7117450", "IKEY:6871397", "IKEY:4767851", "IKEY:4731268", "IKEY:7501594", "IKEY:6767067", "IKEY:7293665", "IKEY:7381692", "IKEY:7226835", "IKEY:6909756", "IKEY:1284395", "IKEY:5551153", "IKEY:6618997", "IKEY:6775267", "IKEY:6247786", "IKEY:5571001", "IKEY:5543281", "IKEY:6357312", "IKEY:6738219", "IKEY:7532625", "IKEY:7328742", "IKEY:7466112", "IKEY:7403996", "IKEY:7117450", "IKEY:6871397", "IKEY:4767851", "IKEY:4731268", "IKEY:7501594", "IKEY:6767067", "IKEY:7293665", "IKEY:7381692", "IKEY:7226835", "IKEY:6909756", "IKEY:1284395", "IKEY:5551153", "IKEY:6618997", "IKEY:6775267", "IKEY:6247786", "IKEY:5571001", "IKEY:5543281", "IKEY:6357312", "IKEY:6738219", "IKEY:7532625", "IKEY:7328742", "IKEY:7466112", "IKEY:7403996", "IKEY:7117450", "IKEY:6871397", "IKEY:4767851", "IKEY:4731268", "IKEY:7501594", "IKEY:6767067", "IKEY:7293665", "IKEY:7381692", "IKEY:7226835", "IKEY:6909756", "IKEY:1284395", "IKEY:5551153", "IKEY:6618997", "10.1145/1276377.1276390", "10.1145/1360612.1360615", "10.1145/1409060.1409071", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1882261.1866186", "10.1145/2010324.1964935", "10.1145/1276377.1276390", "10.1145/1360612.1360615", "10.1145/1409060.1409071", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1882261.1866186", "10.1145/2010324.1964935", "10.1145/1276377.1276390", "10.1145/1360612.1360615", "10.1145/1409060.1409071", "10.1145/1531326.1531329", "10.1145/1618452.1618471", "10.1145/1882261.1866186", "10.1145/2010324.1964935", "10.1117/12.840263", "10.1007/s11265-015-1084-3", "10.1111/j.1467-8659.2009.01568.x", "10.1111/j.1467-8659.2009.01567.x", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11042-017-4674-1", "10.1007/s11042-016-3318-1", "10.1049/iet-ipr.2012.0308", "10.1117/12.862419", "10.1016/0734-189X(85)90002-7", "10.1017/CBO9780511804441", "10.1117/12.840263", "10.1007/s11265-015-1084-3", "10.1111/j.1467-8659.2009.01568.x", "10.1111/j.1467-8659.2009.01567.x", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11042-017-4674-1", "10.1007/s11042-016-3318-1", "10.1049/iet-ipr.2012.0308", "10.1117/12.862419", "10.1016/0734-189X(85)90002-7", "10.1017/CBO9780511804441", "10.1117/12.840263", "10.1007/s11265-015-1084-3", "10.1111/j.1467-8659.2009.01568.x", "10.1111/j.1467-8659.2009.01567.x", "10.1111/j.1467-8659.2012.03001.x", "10.1007/s11042-017-4674-1", "10.1007/s11042-016-3318-1", "10.1049/iet-ipr.2012.0308", "10.1117/12.862419", "10.1016/0734-189X(85)90002-7", "10.1017/CBO9780511804441"]}, "10.1109/TVCG.2018.2866090": {"doi": "10.1109/TVCG.2018.2866090", "author": ["B. Sheng", "P. Li", "C. Gao", "K. -L. Ma"], "title": "Deep Neural Representation Guided Face Sketch Synthesis", "year": "2019", "abstract": "Face sketch synthesis shows great applications in a lot of fields such as online entertainment and suspects identification. Existing face sketch synthesis methods learn the patch-wise sketch style from the training dataset containing photo-sketch pairs. These methods manipulate the whole process directly in the field of RGB space, which unavoidably results in unsmooth noises at patch boundaries. If denoising methods are used, the sketch edges would be blurred and face structures could not be restored. Recent researches of feature maps, which are the outputs of a certain neural network layer, have achieved great success in texture synthesis and artistic image generation. In this paper, we reformulate the face sketch synthesis problem into a neural network feature maps based optimization task. Our results accurately capture the sketch drawing style and make full use of the whole stylistic information hidden in the training dataset. Unlike former feature map based methods, we utilize the Enhanced 3D PatchMatch and cross-layer cost aggregation methods to obtain the target feature maps for the final results. Multiple experiments have shown that our approach imitates hand-drawn sketch style vividly, and has high-quality visual effects on CUHK, AR, XM2VTS and CUFSF face sketch datasets.", "keywords": ["face recognition", "feature extraction", "image denoising", "image representation", "neural nets", "optimisation", "deep neural representation guided face sketch synthesis", "patch boundaries", "neural network layer", "texture synthesis", "cross-layer cost aggregation methods", "RGB space", "unsmooth noises", "denoising methods", "artistic image generation", "Enhanced 3D PatchMatch", "Rendering (computer graphics)", "Face recognition", "Feature extraction", "Image restoration", "Three-dimensional displays", "Noise reduction", "Convolutional neural networks", "Non-photorealistic rendering", "face sketch synthesis", "convolutional neural network (CNN)", "style transformation", "Algorithms", "Art", "Computer Graphics", "Databases, Factual", "Face", "Humans", "Image Processing, Computer-Assisted", "Neural Networks, Computer"], "referenced_by": ["IKEY:8520880", "IKEY:9272961"], "referencing": ["IKEY:5995324", "IKEY:7833186", "IKEY:7835631", "IKEY:4522547", "IKEY:4276083", "IKEY:6522845", "IKEY:962888", "IKEY:7726076", "IKEY:7414488", "IKEY:7272134", "IKEY:1238414", "IKEY:4624272", "IKEY:7475915", "IKEY:6196209", "IKEY:7115171", "IKEY:7410412", "IKEY:7780641", "IKEY:7812784", "IKEY:6909602", "IKEY:7299155", "IKEY:5705575", "IKEY:1284395", "IKEY:8237506", "IKEY:8100115", "IKEY:7084655", "IKEY:5995324", "IKEY:7833186", "IKEY:7835631", "IKEY:4522547", "IKEY:4276083", "IKEY:6522845", "IKEY:962888", "IKEY:7726076", "IKEY:7414488", "IKEY:7272134", "IKEY:1238414", "IKEY:4624272", "IKEY:7475915", "IKEY:6196209", "IKEY:7115171", "IKEY:7410412", "IKEY:7780641", "IKEY:7812784", "IKEY:6909602", "IKEY:7299155", "IKEY:5705575", "IKEY:1284395", "IKEY:8237506", "IKEY:8100115", "IKEY:7084655", "IKEY:5995324", "IKEY:7833186", "IKEY:7835631", "IKEY:4522547", "IKEY:4276083", "IKEY:6522845", "IKEY:962888", "IKEY:7726076", "IKEY:7414488", "IKEY:7272134", "IKEY:1238414", "IKEY:4624272", "IKEY:7475915", "IKEY:6196209", "IKEY:7115171", "IKEY:7410412", "IKEY:7780641", "IKEY:7812784", "IKEY:6909602", "IKEY:7299155", "IKEY:5705575", "IKEY:1284395", "IKEY:8237506", "IKEY:8100115", "IKEY:7084655", "10.1145/1124728.1124747", "10.1145/280814.280950", "10.1145/192161.192184", "10.1145/3065386", "10.1145/2897824.2925972", "10.1145/2671188.2749321", "10.1145/279232.279236", "10.1145/1124728.1124747", "10.1145/280814.280950", "10.1145/192161.192184", "10.1145/3065386", "10.1145/2897824.2925972", "10.1145/2671188.2749321", "10.1145/279232.279236", "10.1145/1124728.1124747", "10.1145/280814.280950", "10.1145/192161.192184", "10.1145/3065386", "10.1145/2897824.2925972", "10.1145/2671188.2749321", "10.1145/279232.279236", "10.1007/978-3-642-15567-3_31", "10.1007/978-3-319-10599-4_51", "10.1007/s11263-013-0645-9", "10.24963/ijcai.2017/500", "10.1016/j.cviu.2017.08.009", "10.1016/j.patcog.2017.11.008", "10.24963/ijcai.2017/632", "10.1007/978-3-319-10590-1_53", "10.1002/ima.20059", "10.1007/978-3-642-15567-3_31", "10.1007/978-3-319-10599-4_51", "10.1007/s11263-013-0645-9", "10.24963/ijcai.2017/500", "10.1016/j.cviu.2017.08.009", "10.1016/j.patcog.2017.11.008", "10.24963/ijcai.2017/632", "10.1007/978-3-319-10590-1_53", "10.1002/ima.20059", "10.1007/978-3-642-15567-3_31", "10.1007/978-3-319-10599-4_51", "10.1007/s11263-013-0645-9", "10.24963/ijcai.2017/500", "10.1016/j.cviu.2017.08.009", "10.1016/j.patcog.2017.11.008", "10.24963/ijcai.2017/632", "10.1007/978-3-319-10590-1_53", "10.1002/ima.20059"]}, "10.1109/TVCG.2018.2866436": {"doi": "10.1109/TVCG.2018.2866436", "author": ["T. Weiss", "A. Litteneker", "N. Duncan", "M. Nakada", "C. Jiang", "L. Yu", "D. Terzopoulos"], "title": "Fast and Scalable Position-Based Layout Synthesis", "year": "2019", "abstract": "The arrangement of objects into a layout can be challenging for non-experts, as is affirmed by the existence of interior design professionals. Recent research into the automation of this task has yielded methods that can synthesize layouts of objects respecting aesthetic and functional constraints that are non-linear and competing. These methods usually adopt a stochastic optimization scheme, which samples from different layout configurations, a process that is slow and inefficient. We introduce an physics-motivated, continuous layout synthesis technique, which results in a significant gain in speed and is readily scalable. We demonstrate our method on a variety of examples and show that it achieves results similar to conventional layout synthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster by at least an order of magnitude and can handle layouts of unprecedented size as well as tightly-packed layouts that can overwhelm McMC.", "keywords": ["integrated circuit layout", "Markov processes", "Monte Carlo methods", "optimisation", "interior design professionals", "aesthetic constraints", "functional constraints", "nonlinear", "stochastic optimization scheme", "layout configurations", "continuous layout synthesis technique", "conventional layout synthesis", "Markov chain Monte Carlo state-search", "tightly-packed layouts", "Layout", "Content management", "Probabilistic logic", "Three-dimensional displays", "Computational modeling", "Automatic layout synthesis", "3D scene modeling", "automatic content creation", "position-based methods", "constraints"], "referenced_by": ["10.1016/j.cag.2018.10.008"], "referencing": ["IKEY:6636298", "IKEY:489389", "IKEY:5246818", "IKEY:7803544", "IKEY:5539970", "IKEY:7410521", "IKEY:8014746", "IKEY:6636298", "IKEY:489389", "IKEY:5246818", "IKEY:7803544", "IKEY:5539970", "IKEY:7410521", "IKEY:8014746", "IKEY:6636298", "IKEY:489389", "IKEY:5246818", "IKEY:7803544", "IKEY:5539970", "IKEY:7410521", "IKEY:8014746", "10.1145/2010324.1964981", "10.1145/2010324.1964982", "10.1145/2185520.2185552", "10.1145/3130800.3130805", "10.1145/2897824.2925894", "10.1145/2366145.2366154", "10.1145/2601097.2601164", "10.1145/2461912.2461977", "10.1145/2366145.2366146", "10.1145/2366145.2366160", "10.1145/2601097.2601183", "10.1145/2508363.2508409", "10.1145/37402.37427", "10.1145/218380.218443", "10.1145/2461912.2461984", "10.1145/2601097.2601152", "10.1145/2614106.2614158", "10.1145/964460.964462", "10.1145/1015706.1015720", "10.1145/2010324.1964981", "10.1145/2010324.1964982", "10.1145/2185520.2185552", "10.1145/3130800.3130805", "10.1145/2897824.2925894", "10.1145/2366145.2366154", "10.1145/2601097.2601164", "10.1145/2461912.2461977", "10.1145/2366145.2366146", "10.1145/2366145.2366160", "10.1145/2601097.2601183", "10.1145/2508363.2508409", "10.1145/37402.37427", "10.1145/218380.218443", "10.1145/2461912.2461984", "10.1145/2601097.2601152", "10.1145/2614106.2614158", "10.1145/964460.964462", "10.1145/1015706.1015720", "10.1145/2010324.1964981", "10.1145/2010324.1964982", "10.1145/2185520.2185552", "10.1145/3130800.3130805", "10.1145/2897824.2925894", "10.1145/2366145.2366154", "10.1145/2601097.2601164", "10.1145/2461912.2461977", "10.1145/2366145.2366146", "10.1145/2366145.2366160", "10.1145/2601097.2601183", "10.1145/2508363.2508409", "10.1145/37402.37427", "10.1145/218380.218443", "10.1145/2461912.2461984", "10.1145/2601097.2601152", "10.1145/2614106.2614158", "10.1145/964460.964462", "10.1145/1015706.1015720", "10.1111/cgf.12276", "10.1080/00031305.1995.10476177", "10.1111/cgf.13380", "10.1111/cgf.12941", "10.1016/S0926-5805(00)00099-6", "10.1016/j.jvcir.2007.01.005", "10.1002/cav.1614", "10.1111/cgf.12346", "10.1137/1.9780898719154", "10.1111/cgf.12276", "10.1080/00031305.1995.10476177", "10.1111/cgf.13380", "10.1111/cgf.12941", "10.1016/S0926-5805(00)00099-6", "10.1016/j.jvcir.2007.01.005", "10.1002/cav.1614", "10.1111/cgf.12346", "10.1137/1.9780898719154", "10.1111/cgf.12276", "10.1080/00031305.1995.10476177", "10.1111/cgf.13380", "10.1111/cgf.12941", "10.1016/S0926-5805(00)00099-6", "10.1016/j.jvcir.2007.01.005", "10.1002/cav.1614", "10.1111/cgf.12346", "10.1137/1.9780898719154"]}, "10.1109/TVCG.2018.2866793": {"doi": "10.1109/TVCG.2018.2866793", "author": ["S. Chen", "L. Zheng", "Y. Zhang", "Z. Sun", "K. Xu"], "title": "VERAM: View-Enhanced Recurrent Attention Model for 3D Shape Classification", "year": "2019", "abstract": "Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. However, the fusion of multi-view features based on max or average pooling lacks a view selection mechanism, limiting its application in, e.g., multi-view active object recognition by a robot. This paper presents VERAM, a view-enhanced recurrent attention model capable of actively selecting a sequence of views for highly accurate 3D shape classification. VERAM addresses an important issue commonly found in existing attention-based models, i.e., the unbalanced training of the subnetworks corresponding to next view estimation and shape classification. The classification subnetwork is easily overfitted while the view estimation one is usually poorly trained, leading to a suboptimal classification performance. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95.5 and 95.3 percent on ModelNet10, 93.7 and 92.1 percent on ModelNet40, both are the state-of-the-art performance under the same number of views.", "keywords": ["backpropagation", "convolutional neural nets", "image classification", "image colour analysis", "neural net architecture", "recurrent neural nets", "shape recognition", "stereo image processing", "view estimation subnetwork", "VERAM", "view-enhanced recurrent attention model", "view selection", "attention-based models", "3D shape classification", "multiview deep neural network", "multiview feature fusion", "information flow", "gradient backpropagation", "grayscale image", "AlexNet", "CNN architecture", "reinforcement training", "Three-dimensional displays", "Image recognition", "Solid modeling", "Reinforcement learning", "Recurrent neural networks", "Convolutional neural networks", "Computational modeling", "3D shape classification", "multi-view 3D shape recognition", "visual attention model", "recurrent neural network", "reinforcement learning", "convolutional neural network"], "referenced_by": ["IKEY:8813053", "IKEY:9144466", "IKEY:9247094"], "referencing": ["IKEY:937526", "IKEY:6112769", "IKEY:7410471", "IKEY:7780978", "IKEY:5206848", "IKEY:7780783", "IKEY:8578624", "IKEY:7850113", "IKEY:7353481", "IKEY:8579077", "IKEY:7273863", "IKEY:7139782", "IKEY:7780662", "IKEY:7780459", "IKEY:6795963", "IKEY:8099577", "IKEY:937526", "IKEY:6112769", "IKEY:7410471", "IKEY:7780978", "IKEY:5206848", "IKEY:7780783", "IKEY:8578624", "IKEY:7850113", "IKEY:7353481", "IKEY:8579077", "IKEY:7273863", "IKEY:7139782", "IKEY:7780662", "IKEY:7780459", "IKEY:6795963", "IKEY:8099577", "IKEY:937526", "IKEY:6112769", "IKEY:7410471", "IKEY:7780978", "IKEY:5206848", "IKEY:7780783", "IKEY:8578624", "IKEY:7850113", "IKEY:7353481", "IKEY:8579077", "IKEY:7273863", "IKEY:7139782", "IKEY:7780662", "IKEY:7780459", "IKEY:6795963", "IKEY:8099577", "10.1145/1899404.1899405", "10.1145/2980179.2980224", "10.1145/3072959.3073608", "10.1145/1899404.1899405", "10.1145/2980179.2980224", "10.1145/3072959.3073608", "10.1145/1899404.1899405", "10.1145/2980179.2980224", "10.1145/3072959.3073608", "10.1111/1467-8659.00669", "10.1016/j.patcog.2006.04.034", "10.1007/s11263-009-0281-6", "10.1007/978-3-642-15567-3_43", "10.5244/C.31.64", "10.1038/323533a0", "10.1023/A:1022672621406", "10.1016/j.cag.2017.12.001", "10.1111/cgf.12740", "10.1007/978-3-540-74690-4_71", "10.1142/S0218488598000094", "10.1111/1467-8659.00669", "10.1016/j.patcog.2006.04.034", "10.1007/s11263-009-0281-6", "10.1007/978-3-642-15567-3_43", "10.5244/C.31.64", "10.1038/323533a0", "10.1023/A:1022672621406", "10.1016/j.cag.2017.12.001", "10.1111/cgf.12740", "10.1007/978-3-540-74690-4_71", "10.1142/S0218488598000094", "10.1111/1467-8659.00669", "10.1016/j.patcog.2006.04.034", "10.1007/s11263-009-0281-6", "10.1007/978-3-642-15567-3_43", "10.5244/C.31.64", "10.1038/323533a0", "10.1023/A:1022672621406", "10.1016/j.cag.2017.12.001", "10.1111/cgf.12740", "10.1007/978-3-540-74690-4_71", "10.1142/S0218488598000094"]}, "10.1109/TVCG.2019.2941055": {"doi": "10.1109/TVCG.2019.2941055", "author": [""], "title": "2019 Reviewers List", "year": "2019", "abstract": "Presents the list of reviewers who contributed to this publication in 2019.", "keywords": ["IEEE publishing"], "referenced_by": [], "referencing": []}}