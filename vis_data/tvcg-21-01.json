{"10.1109/TVCG.2019.2933196": {"doi": "10.1109/TVCG.2019.2933196", "author": ["P. Valdivia", "P. Buono", "C. Plaisant", "N. Dufournaud", "J. -D. Fekete"], "title": "Analyzing Dynamic Hypergraphs with Parallel Aggregated Ordered Hypergraph Visualization", "year": "2021", "abstract": "Parallel Aggregated Ordered Hypergraph(PAOH) is a novel technique to visualize dynamic hypergraphs. Hypergraphs are a generalization of graphs where edges can connect several vertices. Hypergraphs can be used to model networks of business partners or co-authorship networks with multiple authors per article. A dynamic hypergraph evolves over discrete time slots. PAOH represents vertices as parallel horizontal bars and hyperedges as vertical lines, using dots to depict the connections to one or more vertices. We describe a prototype implementation of Parallel Aggregated Ordered Hypergraph, report on a usability study with 9 participants analyzing publication data, and summarize the improvements made. Two case studies and several examples are provided. We believe that PAOH is the first technique to provide a highly readable representation of dynamic hypergraphs. It is easy to learn and well suited for medium size dynamic hypergraphs (50-500 vertices) such as those commonly generated by digital humanities projects\u2014our driving application domain.", "keywords": ["Visualization", "Contracts", "Motion pictures", "Usability", "Topology", "Computational modeling", "dynamic graph", "interaction", "case study", "dynamic hypergraph", "digital humanities", "usability "], "referenced_by": [], "referencing": ["10.1109/TVCG.2013.263", "10.1109/TVCG.2014.2346249", "10.1016/j.intcom.2007.05.004", "10.1109/TVCG.2014.2346248", "10.1109/WI.2006.118", "10.1109/TVCG.2015.2467691", "10.1109/TVCG.2015.2392771", "10.1109/TVCG.2011.226", "10.1109/TVCG.2013.254", "10.1109/TVCG.2015.2424889", "10.1109/PASSAT/SocialCom.2011.139", "10.1109/TVCG.2013.263", "10.1109/TVCG.2014.2346249", "10.1016/j.intcom.2007.05.004", "10.1109/TVCG.2014.2346248", "10.1109/WI.2006.118", "10.1109/TVCG.2015.2467691", "10.1109/TVCG.2015.2392771", "10.1109/TVCG.2011.226", "10.1109/TVCG.2013.254", "10.1109/TVCG.2015.2424889", "10.1109/PASSAT/SocialCom.2011.139", "10.1109/TVCG.2013.263", "10.1109/TVCG.2014.2346249", "10.1016/j.intcom.2007.05.004", "10.1109/WI.2006.118", "10.1109/TVCG.2015.2467691", "10.1109/TVCG.2015.2392771", "10.1109/TVCG.2011.226", "10.1109/TVCG.2013.254", "10.1109/TVCG.2015.2424889", "10.1109/PASSAT/SocialCom.2011.139", "10.1145/2858036.2858488", "10.1145/1168149.1168168", "10.1145/2470654.2466444", "10.1145/800195.805928", "10.1145/2858036.2858488", "10.1145/1168149.1168168", "10.1145/2470654.2466444", "10.1145/800195.805928", "10.1145/2858036.2858488", "10.1145/1168149.1168168", "10.1145/2470654.2466444", "10.1145/800195.805928", "10.1186/1471-2105-13-275", "10.1111/cgf.12791", "10.1073/pnas.98.2.404", "10.1057/palgrave.ivs.9500143", "10.1016/j.socnet.2007.04.006", "10.1111/cgf.12722", "10.1111/j.1467-8659.2012.03080.x", "10.1117/12.890220", "10.1371/journal.pone.0008694", "10.1111/cgf.12880", "10.1111/cgf.12512", "10.1007/978-3-319-64471-4_31", "10.1111/cgf.12615", "10.1111/cgf.12882", "10.1111/j.1467-8306.1994.tb01869.x", "10.1111/cgf.12935", "10.1186/1471-2105-13-275", "10.1111/cgf.12791", "10.1073/pnas.98.2.404", "10.1057/palgrave.ivs.9500143", "10.1016/j.socnet.2007.04.006", "10.1111/cgf.12722", "10.1111/j.1467-8659.2012.03080.x", "10.1117/12.890220", "10.1371/journal.pone.0008694", "10.1111/cgf.12880", "10.1111/cgf.12512", "10.1007/978-3-319-64471-4_31", "10.1111/cgf.12615", "10.1111/cgf.12882", "10.1111/j.1467-8306.1994.tb01869.x", "10.1111/cgf.12935", "10.1186/1471-2105-13-275", "10.1111/cgf.12791", "10.1073/pnas.98.2.404", "10.1057/palgrave.ivs.9500143", "10.1016/j.socnet.2007.04.006", "10.1111/cgf.12722", "10.1111/j.1467-8659.2012.03080.x", "10.1117/12.890220", "10.1371/journal.pone.0008694", "10.1111/cgf.12880", "10.1111/cgf.12512", "10.1007/978-3-319-64471-4_31", "10.1111/cgf.12615", "10.1111/cgf.12882", "10.1111/j.1467-8306.1994.tb01869.x", "10.1111/cgf.12935"]}, "10.1109/TVCG.2019.2938520": {"doi": "10.1109/TVCG.2019.2938520", "author": ["Z. Wang", "J. Chai", "S. Xia"], "title": "Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control", "year": "2021", "abstract": "This paper introduces a new generative deep learning network for human motion synthesis and control. Our key idea is to combine recurrent neural networks (RNNs) and adversarial training for human motion modeling. We first describe an efficient method for training an RNN model from prerecorded motion data. We implement RNNs with long short-term memory (LSTM) cells because they are capable of addressing the nonlinear dynamics and long term temporal dependencies present in human motions. Next, we train a refiner network using an adversarial loss, similar to generative adversarial networks (GANs), such that refined motion sequences are indistinguishable from real mocap data using a discriminative network. The resulting model is appealing for motion synthesis and control because it is compact, contact-aware, and can generate an infinite number of naturally looking motions with infinite lengths. Our experiments show that motions generated by our deep learning model are always highly realistic and comparable to high-quality motion capture data. We demonstrate the power and effectiveness of our models by exploring a variety of applications, ranging from random motion synthesis, online/offline motion control, and motion filtering. We show the superiority of our generative model by comparison against baseline models.", "keywords": ["Hidden Markov models", "Data models", "Training", "Generators", "Mathematical model", "Recurrent neural networks", "Animation", "Deep learning", "adversarial training", "human motion modeling", "synthesis and control"], "referenced_by": [], "referencing": ["10.1162/neco.1997.9.8.1735", "10.1109/ICASSP.2013.6638947", "10.1109/CVPR.2017.632", "10.1109/CVPR.2017.241", "10.1109/ICCV.2015.494", "10.1109/CVPR.2016.573", "10.1109/CVPR.2017.173", "10.1109/78.650093", "10.1162/neco.1997.9.8.1735", "10.1109/ICASSP.2013.6638947", "10.1109/CVPR.2017.632", "10.1109/CVPR.2017.241", "10.1109/ICCV.2015.494", "10.1109/CVPR.2016.573", "10.1109/CVPR.2017.497", "10.1109/CVPR.2017.173", "10.1109/CVPR.2016.573", "10.1109/78.650093", "10.1162/neco.1997.9.8.1735", "10.1109/ICASSP.2013.6638947", "10.1109/CVPR.2017.632", "10.1109/CVPR.2017.241", "10.1109/ICCV.2015.494", "10.1109/CVPR.2016.573", "10.1109/CVPR.2017.497", "10.1109/CVPR.2017.173", "10.1109/CVPR.2016.573", "10.1109/78.650093", "10.1145/344779.344865", "10.1145/566654.566604", "10.1145/1276377.1276387", "10.1145/1618452.1618517", "10.1145/1640443.1640452", "10.1145/1966394.1966398", "10.1145/2366145.2366172", "10.1145/3072959.3073663", "10.1145/1553374.1553505", "10.1145/2766999", "10.1145/344779.344865", "10.1145/566654.566604", "10.1145/1276377.1276387", "10.1145/1618452.1618517", "10.1145/1640443.1640452", "10.1145/1966394.1966398", "10.1145/2366145.2366172", "10.1145/3072959.3073663", "10.1145/1553374.1553505", "10.1145/2766999", "10.1145/344779.344865", "10.1145/566654.566604", "10.1145/1276377.1276387", "10.1145/1618452.1618517", "10.1145/1640443.1640452", "10.1145/1966394.1966398", "10.1145/2366145.2366172", "10.1145/3072959.3073663", "10.1145/1553374.1553505", "10.1145/2766999", "10.5244/C.31.119", "10.1016/0893-6080(88)90007-X", "10.1007/BF01589116", "10.5244/C.31.119", "10.1016/0893-6080(88)90007-X", "10.1007/BF01589116", "10.5244/C.31.119", "10.1016/0893-6080(88)90007-X", "10.1007/BF01589116"]}, "10.1109/TVCG.2020.3016327": {"doi": "10.1109/TVCG.2020.3016327", "author": ["D. Kim", "H. Kye", "J. Lee", "Y. -G. Shin"], "title": "Confidence-Controlled Local Isosurfacing", "year": "2021", "abstract": "This article presents a novel framework that can generate a high-fidelity isosurface model of X-ray computed tomography (CT) data. CT surfaces with subvoxel precision and smoothness can be simply modeled via isosurfacing, where a single CT value represents an isosurface. However, this inevitably results in geometric distortion of the CT data containing CT artifacts. An alternative is to treat this challenge as a segmentation problem. However, in general, segmentation techniques are not robust against noisy data and require heavy computation to handle the artifacts that occur in three-dimensional CT data. Furthermore, the surfaces generated from segmentation results may contain jagged, overly smooth, or distorted geometries. We present a novel local isosurfacing framework that can address these issues simultaneously. The proposed framework exploits two primary techniques: 1) Canny edge approach for obtaining surface candidate boundary points and evaluating their confidence and 2) screened Poisson optimization for fitting a surface to the boundary points in which the confidence term is incorporated. This combination facilitates local isosurfacing that can produce high-fidelity surface models. We also implement an intuitive user interface to alleviate the burden of selecting the appropriate confidence computing parameters. Our experimental results demonstrate the effectiveness of the proposed framework.", "keywords": ["Computed tomography", "Isosurfaces", "Three-dimensional displays", "Image segmentation", "Computational modeling", "Data models", "Surface treatment", "High-fidelity CT surface generation", "local isosurfacing", "Canny edge approach", "screened Poisson equation"], "referenced_by": [], "referencing": ["10.1109/TVCG.2012.259", "10.1109/MPUL.2017.2701493", "10.1109/TVCG.2007.70598", "10.1109/83.902291", "10.1109/TVCG.2004.2", "10.1109/34.368173", "10.1109/TPAMI.2013.106", "10.1109/TCYB.2015.2409119", "10.1109/TIP.2017.2666042", "10.1109/34.537343", "10.1109/TVCG.2010.208", "10.1109/TVCG.2013.208", "10.1109/TVCG.2015.2467958", "10.1109/TVCG.2018.2864505", "10.1109/TPAMI.2007.70751", "10.1109/CVPR.2017.188", "10.1109/TVCG.2006.149", "10.1109/TVCG.2007.70604", "10.1109/VISUAL.2002.1183771", "10.1109/VISUAL.2003.1250414", "10.1109/TSMC.1979.4310076", "10.1109/TVCG.2012.259", "10.1109/MPUL.2017.2701493", "10.1109/TVCG.2007.70598", "10.1109/83.902291", "10.1109/TVCG.2004.2", "10.1109/34.368173", "10.1109/TPAMI.2013.106", "10.1109/TCYB.2015.2409119", "10.1109/TIP.2017.2666042", "10.1109/34.537343", "10.1109/TVCG.2010.208", "10.1109/TVCG.2013.208", "10.1109/TVCG.2015.2467958", "10.1109/TVCG.2018.2864505", "10.1109/TPAMI.2007.70751", "10.1109/CVPR.2017.188", "10.1109/TVCG.2006.149", "10.1109/TVCG.2007.70604", "10.1109/VISUAL.2002.1183771", "10.1109/VISUAL.2003.1250414", "10.1109/TSMC.1979.4310076", "10.1109/TVCG.2012.259", "10.1109/MPUL.2017.2701493", "10.1109/TVCG.2007.70598", "10.1109/83.902291", "10.1109/TVCG.2004.2", "10.1109/34.368173", "10.1109/TPAMI.2013.106", "10.1109/TCYB.2015.2409119", "10.1109/TIP.2017.2666042", "10.1109/34.537343", "10.1109/TVCG.2010.208", "10.1109/TVCG.2013.208", "10.1109/TVCG.2015.2467958", "10.1109/TVCG.2018.2864505", "10.1109/TPAMI.2007.70751", "10.1109/CVPR.2017.188", "10.1109/TVCG.2006.149", "10.1109/TVCG.2007.70604", "10.1109/VISUAL.2002.1183771", "10.1109/VISUAL.2003.1250414", "10.1109/TSMC.1979.4310076", "10.1145/37402.37422", "10.1145/1015706.1015720", "10.1145/1364901.1364931", "10.1145/37402.37422", "10.1145/1015706.1015720", "10.1145/1364901.1364931", "10.1145/37402.37422", "10.1145/1015706.1015720", "10.1145/1364901.1364931", "10.1088/0031-9155/21/3/004", "10.1007/s11263-006-7934-5", "10.1016/j.media.2004.06.022", "10.1016/B978-0-08-051581-6.50024-6", "10.1006/jcph.1995.1098", "10.1016/0021-9991(88)90002-2", "10.1007/BF00133570", "10.1006/jcph.1999.6345", "10.1016/j.cviu.2009.05.002", "10.1080/00207160.2011.632410", "10.1016/j.optlaseng.2013.04.019", "10.1023/A:1007979827043", "10.1137/S0036144598347059", "10.1007/3-540-76076-8_117", "10.1016/j.cad.2009.08.003", "10.1007/s11432-013-4987-2", "10.1007/BFb0033757", "10.1007/978-3-642-85949-6_2", "10.1088/0031-9155/21/3/004", "10.1007/s11263-006-7934-5", "10.1016/j.media.2004.06.022", "10.1016/B978-0-08-051581-6.50024-6", "10.1006/jcph.1995.1098", "10.1016/0021-9991(88)90002-2", "10.1007/BF00133570", "10.1006/jcph.1999.6345", "10.1016/j.cviu.2009.05.002", "10.1080/00207160.2011.632410", "10.1016/j.optlaseng.2013.04.019", "10.1023/A:1007979827043", "10.1137/S0036144598347059", "10.1007/3-540-76076-8_117", "10.1016/j.cad.2009.08.003", "10.1007/s11432-013-4987-2", "10.1007/BFb0033757", "10.1007/978-3-642-85949-6_2", "10.1088/0031-9155/21/3/004", "10.1007/s11263-006-7934-5", "10.1016/j.media.2004.06.022", "10.1016/B978-0-08-051581-6.50024-6", "10.1006/jcph.1995.1098", "10.1016/0021-9991(88)90002-2", "10.1007/BF00133570", "10.1006/jcph.1999.6345", "10.1016/j.cviu.2009.05.002", "10.1080/00207160.2011.632410", "10.1016/j.optlaseng.2013.04.019", "10.1023/A:1007979827043", "10.1137/S0036144598347059", "10.1007/3-540-76076-8_117", "10.1016/j.cad.2009.08.003", "10.1007/s11432-013-4987-2", "10.1007/BFb0033757", "10.1007/978-3-642-85949-6_2"]}, "10.1109/TVCG.2019.2938946": {"doi": "10.1109/TVCG.2019.2938946", "author": ["J. Wu", "W. Wang", "X. Gao"], "title": "Design and Optimization of Conforming Lattice Structures", "year": "2021", "abstract": "Inspired by natural cellular materials such as trabecular bone, lattice structures have been developed as a new type of lightweight material. In this paper we present a novel method to design lattice structures that conform with both the principal stress directions and the boundary of the optimized shape. Our method consists of two major steps: the first optimizes concurrently the shape (including its topology) and the distribution of orthotropic lattice materials inside the shape to maximize stiffness under application-specific external loads; the second takes the optimized configuration (i.e., locally-defined orientation, porosity, and anisotropy) of lattice materials from the previous step, and extracts a globally consistent lattice structure by field-aligned parameterization. Our approach is robust and works for both 2D planar and 3D volumetric domains. Numerical results and physical verifications demonstrate remarkable structural properties of conforming lattice structures generated by our method.", "keywords": ["Lattices", "Optimization", "Shape", "Stress", "Three-dimensional printing", "Anisotropic magnetoresistance", "Three-dimensional displays", "Lattice structures", "topology optimization", "homogenization", "3D printing"], "referenced_by": ["10.1109/MACISE49704.2020.00065"], "referencing": ["10.1145/3095815", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2897824.2925922", "10.1145/3072959.3073638", "10.1145/3130800.3130827", "10.1145/566654.566580", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/3072959.3073649", "10.1145/2980179.2982436", "10.1145/2601097.2601189", "10.1145/3072959.3073626", "10.1145/3272127.3275012", "10.1145/2642918.2647359", "10.1145/2897824.2925966", "10.1145/2980179.2982401", "10.1145/3328939.3328999", "10.1145/2070781.2024177", "10.1145/2366145.2366196", "10.1145/3072959.2930662", "10.1145/3072959.3073676", "10.1145/2980179.2982408", "10.1145/2897824.2925976", "10.1145/3065254", "10.1145/3197517.3201344", "10.1145/2816795.2818078", "10.1145/1531326.1531383", "10.1145/2766927", "10.1145/3095815", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2897824.2925922", "10.1145/3072959.3073638", "10.1145/3130800.3130827", "10.1145/566654.566580", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/3072959.3073649", "10.1145/2980179.2982436", "10.1145/2601097.2601189", "10.1145/3072959.3073626", "10.1145/3272127.3275012", "10.1145/2642918.2647359", "10.1145/2897824.2925966", "10.1145/2980179.2982401", "10.1145/3328939.3328999", "10.1145/2070781.2024177", "10.1145/2366145.2366196", "10.1145/3072959.2930662", "10.1145/3072959.3073676", "10.1145/2980179.2982408", "10.1145/2897824.2925976", "10.1145/3065254", "10.1145/3197517.3201344", "10.1145/2816795.2818078", "10.1145/1531326.1531383", "10.1145/2766927", "10.1145/3095815", "10.1145/2508363.2508382", "10.1145/2601097.2601168", "10.1145/2897824.2925922", "10.1145/3072959.3073638", "10.1145/3130800.3130827", "10.1145/566654.566580", "10.1145/2185520.2185544", "10.1145/2461912.2461967", "10.1145/3072959.3073649", "10.1145/2980179.2982436", "10.1145/2601097.2601189", "10.1145/3072959.3073626", "10.1145/3272127.3275012", "10.1145/2642918.2647359", "10.1145/2897824.2925966", "10.1145/2980179.2982401", "10.1145/3328939.3328999", "10.1145/2070781.2024177", "10.1145/2366145.2366196", "10.1145/3072959.2930662", "10.1145/3072959.3073676", "10.1145/2980179.2982408", "10.1145/2897824.2925976", "10.1145/3065254", "10.1145/3197517.3201344", "10.1145/2816795.2818078", "10.1145/1531326.1531383", "10.1145/2766927", "10.1007/s00158-018-1994-3", "10.1007/BF01637666", "10.1016/0045-7825(88)90086-2", "10.1137/070688900", "10.1002/nme.5575", "10.1016/j.camwa.2018.08.007", "10.2200/S00847ED1V01Y201804VCP031", "10.1016/j.cagd.2015.03.012", "10.1016/j.cma.2014.05.022", "10.1007/s00158-016-1420-7", "10.1115/1.4037305", "10.1115/1.4040131", "10.1115/DETC2005-85366", "10.1080/14786440409463229", "10.1016/j.cad.2016.07.005", "10.20898/j.iass.2016.190.856", "10.1111/cgf.13268", "10.1007/s00158-014-1085-z", "10.1007/s001580050176", "10.1038/nature23911", "10.1038/s41598-018-33454-3", "10.1111/cgf.12014", "10.1111/j.1467-8659.2011.02014.x", "10.1111/cgf.12864", "10.1016/j.cma.2016.09.044", "10.1016/j.commatsci.2013.09.006", "10.1115/1.4040555", "10.1002/nme.1620240207", "10.1007/s00158-010-0602-y", "10.1007/978-1-4684-9286-6", "10.1007/s00158-013-1015-5", "10.1016/j.addma.2018.12.007", "10.1016/J.ENG.2016.02.006", "10.1007/s00158-019-02253-3", "10.1007/s00158-018-1994-3", "10.1007/BF01637666", "10.1016/0045-7825(88)90086-2", "10.1137/070688900", "10.1002/nme.5575", "10.1016/j.camwa.2018.08.007", "10.2200/S00847ED1V01Y201804VCP031", "10.1016/j.cagd.2015.03.012", "10.1016/j.cma.2014.05.022", "10.1007/s00158-016-1420-7", "10.1115/1.4037305", "10.1115/1.4040131", "10.1115/DETC2005-85366", "10.1080/14786440409463229", "10.1016/j.cad.2016.07.005", "10.20898/j.iass.2016.190.856", "10.1111/cgf.13268", "10.1007/s00158-014-1085-z", "10.1007/s001580050176", "10.1038/nature23911", "10.1038/s41598-018-33454-3", "10.1111/cgf.12014", "10.1111/j.1467-8659.2011.02014.x", "10.1111/cgf.12864", "10.1016/j.cma.2016.09.044", "10.1016/j.commatsci.2013.09.006", "10.1115/1.4040555", "10.1002/nme.1620240207", "10.1007/s00158-010-0602-y", "10.1007/978-1-4684-9286-6", "10.1007/s00158-013-1015-5", "10.1016/j.addma.2018.12.007", "10.1016/J.ENG.2016.02.006", "10.1007/s00158-019-02253-3", "10.1007/s00158-018-1994-3", "10.1007/BF01637666", "10.1016/0045-7825(88)90086-2", "10.1137/070688900", "10.1002/nme.5575", "10.1016/j.camwa.2018.08.007", "10.2200/S00847ED1V01Y201804VCP031", "10.1016/j.cagd.2015.03.012", "10.1016/j.cma.2014.05.022", "10.1007/s00158-016-1420-7", "10.1115/1.4037305", "10.1115/1.4040131", "10.1115/DETC2005-85366", "10.1080/14786440409463229", "10.1016/j.cad.2016.07.005", "10.20898/j.iass.2016.190.856", "10.1111/cgf.13268", "10.1007/s00158-014-1085-z", "10.1007/s001580050176", "10.1038/nature23911", "10.1038/s41598-018-33454-3", "10.1111/cgf.12014", "10.1111/j.1467-8659.2011.02014.x", "10.1111/cgf.12864", "10.1016/j.cma.2016.09.044", "10.1016/j.commatsci.2013.09.006", "10.1115/1.4040555", "10.1002/nme.1620240207", "10.1007/s00158-010-0602-y", "10.1007/978-1-4684-9286-6", "10.1007/s00158-013-1015-5", "10.1016/j.addma.2018.12.007", "10.1016/J.ENG.2016.02.006", "10.1007/s00158-019-02253-3"]}, "10.1109/TVCG.2019.2929041": {"doi": "10.1109/TVCG.2019.2929041", "author": ["H. Zhou", "K. Chen", "W. Zhang", "C. Qin", "N. Yu"], "title": "Feature-Preserving Tensor Voting Model for Mesh Steganalysis", "year": "2021", "abstract": "The standard tensor voting technique shows its versatility in tasks such as object recognition and semantic segmentation by recognizing feature points and sharp edges that can segment a model into several patches. We propose a neighborhood-level representation-guided tensor voting model for 3D mesh steganalysis. Because existing steganalytic methods do not analyze correlations among neighborhood faces, they are not very effective at discriminating stego meshes from cover meshes. In this paper, we propose to utilize a tensor voting model to reveal the artifacts caused by embedding data. In the proposed steganalytic scheme, the normal voting tensor (NVT) operation is performed on original mesh faces and smoothed mesh faces separately. Then, the absolute values of the differences between the eigenvalues of the two tensors (from the original face and the smoothed face) are regarded as features that capture intricate relationships among the vertices. Subsequently, the extracted features are processed with a nonlinear mapping to boost the feature effectiveness. The experimental results show that the proposed feature sets prevail over state-of-the-art feature sets including LFS64 and ELFS124 under various steganographic schemes.", "keywords": ["Feature extraction", "Faces", "Three-dimensional displays", "Tensors", "Shape", "Laplace equations", "Correlation", "Mesh steganography", "mesh steganalysis", "normal voting tensor", "feature extraction", "ensemble classifier"], "referenced_by": [], "referencing": ["10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/ACCESS.2017.2767072", "10.1109/ICIP.2017.8296333", "10.1109/TMM.2018.2882088", "10.1109/ICASSP.2016.7472056", "10.1109/ICIP.2018.8451643", "10.1109/5254.708428", "10.1109/TIFS.2011.2175919", "10.1109/ICIP.2002.1039099", "10.1109/TVCG.2017.2740384", "10.1109/TIFS.2012.2190402", "10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/ACCESS.2017.2767072", "10.1109/ICIP.2017.8296333", "10.1109/TMM.2018.2882088", "10.1109/ICASSP.2016.7472056", "10.1109/ICIP.2018.8451643", "10.1109/5254.708428", "10.1109/TIFS.2011.2175919", "10.1109/ICIP.2002.1039099", "10.1109/TVCG.2017.2740384", "10.1109/TIFS.2012.2190402", "10.1109/TSP.2003.809380", "10.1109/TVCG.2008.94", "10.1109/TVCG.2012.106", "10.1109/ACCESS.2017.2767072", "10.1109/ICIP.2017.8296333", "10.1109/TMM.2018.2882088", "10.1109/ICASSP.2016.7472056", "10.1109/ICIP.2018.8451643", "10.1109/5254.708428", "10.1109/TIFS.2011.2175919", "10.1109/ICIP.2002.1039099", "10.1109/TVCG.2017.2740384", "10.1109/TIFS.2012.2190402", "10.1145/2535555", "10.1145/1597817.1597830", "10.1145/218380.218473", "10.1145/1531326.1531379", "10.1145/2535555", "10.1145/1597817.1597830", "10.1145/218380.218473", "10.1145/1531326.1531379", "10.1145/2535555", "10.1145/1597817.1597830", "10.1145/218380.218473", "10.1145/1531326.1531379", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-006-0069-4", "10.1016/j.image.2010.05.002", "10.1007/s11042-016-4163-y", "10.1111/j.1467-8659.2010.01767.x", "10.1016/j.ins.2017.06.011", "10.1007/978-981-10-4154-9_42", "10.1111/j.1467-8659.2008.01161.x", "10.1016/j.cag.2012.06.002", "10.1007/11949534_14", "10.1016/j.cad.2008.12.003", "10.1007/BF00058655", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-006-0069-4", "10.1016/j.image.2010.05.002", "10.1007/s11042-016-4163-y", "10.1111/j.1467-8659.2010.01767.x", "10.1016/j.ins.2017.06.011", "10.1007/978-981-10-4154-9_42", "10.1111/j.1467-8659.2008.01161.x", "10.1016/j.cag.2012.06.002", "10.1007/11949534_14", "10.1016/j.cad.2008.12.003", "10.1007/BF00058655", "10.1111/j.1467-8659.2005.00884.x", "10.1007/s00371-006-0069-4", "10.1016/j.image.2010.05.002", "10.1007/s11042-016-4163-y", "10.1111/j.1467-8659.2010.01767.x", "10.1016/j.ins.2017.06.011", "10.1007/978-981-10-4154-9_42", "10.1111/j.1467-8659.2008.01161.x", "10.1016/j.cag.2012.06.002", "10.1007/11949534_14", "10.1016/j.cad.2008.12.003", "10.1007/BF00058655"]}, "10.1109/TVCG.2019.2930691": {"doi": "10.1109/TVCG.2019.2930691", "author": ["L. Xu", "W. Cheng", "K. Guo", "L. Han", "Y. Liu", "L. Fang"], "title": "FlyFusion: Realtime Dynamic Scene Reconstruction Using a Flying Depth Camera", "year": "2021", "abstract": "While dynamic scene reconstruction has made revolutionary progress from the earliest setup using a mass of static cameras in studio environment to the latest egocentric or hand-held moving camera based schemes, it is still restricted by the recording volume, user comfortability, human labor and expertise. In this paper, a novel solution is proposed through a real-time and robust dynamic fusion scheme using a single flying depth camera, denoted as FlyFusion. By proposing a novel topology compactness strategy for effectively regularizing the complex topology changes, and the Geometry And Motion Energy (GAME) metric for guiding the viewpoint optimization in the volumetric space, FlyFusion succeeds to enable intelligent viewpoint selection based on the immediate dynamic reconstruction result. The merit of FlyFusion lies in its concurrent robustness, efficiency, and adaptation in producing fused and denoised 3D geometry and motions of a moving target interacting with different non-rigid objects in a large space.", "keywords": ["Cameras", "Image reconstruction", "Dynamics", "Geometry", "Real-time systems", "Topology", "Measurement", "Non-rigid", "real-time", "4D reconstruction", "markerless motion capture", "flying camera"], "referenced_by": [], "referencing": ["10.1109/CVPR.2009.5206755", "10.1109/CVPR.2009.5206859", "10.1109/CVPR.2015.7298631", "10.1109/IROS.2015.7354212", "10.1109/ICRA.2016.7487527", "10.1109/LRA.2017.2660769", "10.1109/ICCV.2011.6126338", "10.1109/ICCV.2015.381", "10.1109/CVPR.2011.5995316", "10.1109/CVPR.2010.5540141", "10.1109/CVPR.2017.143", "10.1109/CVPR.2000.854758", "10.1109/ICCV.2015.353", "10.1109/CVPR.2011.5995424", "10.1109/ICCV.2011.6126358", "10.1109/TPAMI.2019.2915229", "10.1109/ICCV.2017.104", "10.1109/ICCV.2011.6126310", "10.1109/CVPR.2017.581", "10.1109/TVCG.2017.2728660", "10.1109/LRA.2018.2850224", "10.1109/LRA.2019.2932570", "10.1109/ROBOT.1985.1087372", "10.1109/CVPRW.2016.11", "10.1109/LRA.2017.2665693", "10.1109/ICCV.2017.569", "10.1109/CVPRW.2015.7301360", "10.1109/CVPR.2017.492", "10.1109/CVPR.2009.5206755", "10.1109/CVPR.2009.5206859", "10.1109/CVPR.2015.7298631", "10.1109/IROS.2015.7354212", "10.1109/ICRA.2016.7487527", "10.1109/LRA.2017.2660769", "10.1109/ICCV.2011.6126338", "10.1109/ICCV.2015.381", "10.1109/CVPR.2011.5995316", "10.1109/CVPR.2010.5540141", "10.1109/CVPR.2017.143", "10.1109/CVPR.2000.854758", "10.1109/ICCV.2015.353", "10.1109/CVPR.2011.5995424", "10.1109/ICCV.2011.6126358", "10.1109/ICCV.2017.104", "10.1109/ICCV.2011.6126310", "10.1109/CVPR.2017.581", "10.1109/TVCG.2017.2728660", "10.1109/LRA.2018.2850224", "10.1109/LRA.2019.2932570", "10.1109/ROBOT.1985.1087372", "10.1109/CVPRW.2016.11", "10.1109/LRA.2017.2665693", "10.1109/ICCV.2017.569", "10.1109/CVPRW.2015.7301360", "10.1109/CVPR.2017.492", "10.1109/CVPR.2009.5206755", "10.1109/CVPR.2009.5206859", "10.1109/CVPR.2015.7298631", "10.1109/IROS.2015.7354212", "10.1109/ICRA.2016.7487527", "10.1109/LRA.2017.2660769", "10.1109/ICCV.2011.6126338", "10.1109/ICCV.2015.381", "10.1109/CVPR.2011.5995316", "10.1109/CVPR.2010.5540141", "10.1109/CVPR.2017.143", "10.1109/CVPR.2000.854758", "10.1109/ICCV.2015.353", "10.1109/CVPR.2011.5995424", "10.1109/ICCV.2011.6126358", "10.1109/TPAMI.2019.2915229", "10.1109/ICCV.2017.104", "10.1109/ICCV.2011.6126310", "10.1109/CVPR.2017.581", "10.1109/TVCG.2017.2728660", "10.1109/LRA.2018.2850224", "10.1109/LRA.2019.2932570", "10.1109/ROBOT.1985.1087372", "10.1109/CVPRW.2016.11", "10.1109/LRA.2017.2665693", "10.1109/ICCV.2017.569", "10.1109/CVPRW.2015.7301360", "10.1109/CVPR.2017.492", "10.1145/1360612.1360697", "10.1145/1360612.1360696", "10.1145/2010324.1964926", "10.1145/2980179.2980235", "10.1145/1276377.1276421", "10.1145/3083722", "10.1145/1276377.1276422", "10.1145/2661229.2661286", "10.1145/1360612.1360697", "10.1145/2508363.2508418", "10.1145/1276377.1276478", "10.1145/2601097.2601165", "10.1145/2897824.2925969", "10.1145/2984511.2984517", "10.1145/3130800.3130801", "10.1145/641865.641868", "10.1145/3072959.3073712", "10.1145/3233794", "10.1145/2816795.2818106", "10.1145/237170.237269", "10.1145/1360612.1360697", "10.1145/1360612.1360696", "10.1145/2010324.1964926", "10.1145/2980179.2980235", "10.1145/1276377.1276421", "10.1145/3083722", "10.1145/1276377.1276422", "10.1145/2661229.2661286", "10.1145/1360612.1360697", "10.1145/2508363.2508418", "10.1145/1276377.1276478", "10.1145/2601097.2601165", "10.1145/2897824.2925969", "10.1145/2984511.2984517", "10.1145/3130800.3130801", "10.1145/641865.641868", "10.1145/3072959.3073712", "10.1145/3233794", "10.1145/2816795.2818106", "10.1145/237170.237269", "10.1145/1360612.1360697", "10.1145/1360612.1360696", "10.1145/2010324.1964926", "10.1145/2980179.2980235", "10.1145/1276377.1276421", "10.1145/3083722", "10.1145/1276377.1276422", "10.1145/2661229.2661286", "10.1145/1360612.1360697", "10.1145/2508363.2508418", "10.1145/1276377.1276478", "10.1145/2601097.2601165", "10.1145/2897824.2925969", "10.1145/2984511.2984517", "10.1145/3130800.3130801", "10.1145/641865.641868", "10.1145/3072959.3073712", "10.1145/3233794", "10.1145/2816795.2818106", "10.1145/237170.237269", "10.1111/cgf.13131", "10.1007/978-3-319-46478-7_17", "10.1007/978-3-642-33709-3_59", "10.1007/978-3-319-46484-8_22", "10.1007/978-3-642-14061-7_19", "10.1023/B:VISI.0000011203.00237.9b", "10.1007/BF00133571", "10.1177/0278364911410755", "10.1007/s11554-013-0386-6", "10.1111/cgf.13131", "10.1007/978-3-319-46478-7_17", "10.1007/978-3-642-33709-3_59", "10.1007/978-3-319-46484-8_22", "10.1007/978-3-642-14061-7_19", "10.1023/B:VISI.0000011203.00237.9b", "10.1007/BF00133571", "10.1177/0278364911410755", "10.1007/s11554-013-0386-6", "10.1111/cgf.13131", "10.1007/978-3-319-46478-7_17", "10.1007/978-3-642-33709-3_59", "10.1007/978-3-319-46484-8_22", "10.1007/978-3-642-14061-7_19", "10.1023/B:VISI.0000011203.00237.9b", "10.1007/BF00133571", "10.1177/0278364911410755", "10.1007/s11554-013-0386-6"]}, "10.1109/TVCG.2019.2937300": {"doi": "10.1109/TVCG.2019.2937300", "author": ["Z. -N. Liu", "Y. -P. Cao", "Z. -F. Kuang", "L. Kobbelt", "S. -M. Hu"], "title": "High-Quality Textured 3D Shape Reconstruction with Cascaded Fully Convolutional Networks", "year": "2021", "abstract": "We present a learning-based approach to reconstructing high-resolution three-dimensional (3D) shapes with detailed geometry and high-fidelity textures. Albeit extensively studied, algorithms for 3D reconstruction from multi-view depth-and-color (RGB-D) scans are still prone to measurement noise and occlusions; limited scanning or capturing angles also often lead to incomplete reconstructions. Propelled by recent advances in 3D deep learning techniques, in this paper, we introduce a novel computation- and memory-efficient cascaded 3D convolutional network architecture, which learns to reconstruct implicit surface representations as well as the corresponding color information from noisy and imperfect RGB-D maps. The proposed 3D neural network performs reconstruction in a progressive and coarse-to-fine manner, achieving unprecedented output resolution and fidelity. Meanwhile, an algorithm for end-to-end training of the proposed cascaded structure is developed. We further introduce Human10, a newly created dataset containing both detailed and textured full-body reconstructions as well as corresponding raw RGB-D scans of 10 subjects. Qualitative and quantitative experimental results on both synthetic and real-world datasets demonstrate that the presented approach outperforms existing state-of-the-art work regarding visual quality and accuracy of reconstructed models.", "keywords": ["Three-dimensional displays", "Shape", "Image reconstruction", "Geometry", "Image color analysis", "Solid modeling", "Surface reconstruction", "High-fidelity reconstruction", "texture reconstruction", "3D vision", "cascaded architecture"], "referenced_by": [], "referencing": ["10.1145/3054739", "10.1145/2487228.2487237", "10.1145/237170.237269", "10.1145/2366145.2366199", "10.1145/2661229.2661239", "10.1145/3072959.3073608", "10.1145/383259.383266", "10.1145/1276377.1276406", "10.1145/2070781.2024182", "10.1145/2366145.2366155", "10.1145/2816795.2818013", "10.1145/2766945", "10.1145/2984511.2984517", "10.1145/1073204.1073325", "10.1145/2601097.2601134", "10.1145/3190834.3190843", "10.1145/237170.237200", "10.1145/383259.383309", "10.1145/237170.237191", "10.1145/2487228.2487238", "10.1145/2980179.2982420", "10.1145/3272127.3275084", "10.1145/3272127.3275099", "10.1145/566654.566590", "10.1145/882262.882269", "10.1145/37402.37422", "10.1145/1576246.1531330", "10.1145/3072959.3073659", "10.1145/3054739", "10.1145/2487228.2487237", "10.1145/237170.237269", "10.1145/2366145.2366199", "10.1145/2661229.2661239", "10.1145/3072959.3073608", "10.1145/383259.383266", "10.1145/1276377.1276406", "10.1145/2070781.2024182", "10.1145/2366145.2366155", "10.1145/2816795.2818013", "10.1145/2766945", "10.1145/2984511.2984517", "10.1145/1073204.1073325", "10.1145/2601097.2601134", "10.1145/3190834.3190843", "10.1145/237170.237200", "10.1145/383259.383309", "10.1145/237170.237191", "10.1145/2487228.2487238", "10.1145/2980179.2982420", "10.1145/3272127.3275084", "10.1145/3272127.3275099", "10.1145/566654.566590", "10.1145/882262.882269", "10.1145/37402.37422", "10.1145/1576246.1531330", "10.1145/3072959.3073659", "10.1145/3054739", "10.1145/2487228.2487237", "10.1145/237170.237269", "10.1145/2366145.2366199", "10.1145/2661229.2661239", "10.1145/3072959.3073608", "10.1145/383259.383266", "10.1145/1276377.1276406", "10.1145/2070781.2024182", "10.1145/2366145.2366155", "10.1145/2816795.2818013", "10.1145/2766945", "10.1145/2984511.2984517", "10.1145/1073204.1073325", "10.1145/2601097.2601134", "10.1145/3190834.3190843", "10.1145/237170.237200", "10.1145/383259.383309", "10.1145/237170.237191", "10.1145/2487228.2487238", "10.1145/2980179.2982420", "10.1145/3272127.3275084", "10.1145/3272127.3275099", "10.1145/566654.566590", "10.1145/882262.882269", "10.1145/37402.37422", "10.1145/1576246.1531330", "10.1145/3072959.3073659", "10.1177/0278364916669237", "10.1007/978-3-319-46484-8_30", "10.1111/j.1467-8659.2009.01388.x", "10.1007/978-3-319-46484-8_38", "10.1007/978-3-319-46478-7_20", "10.1007/978-3-319-49409-8_20", "10.1007/978-3-030-01240-3_38", "10.1111/cgf.12802", "10.1007/s41095-015-0029-x", "10.1111/j.1467-8659.2011.02058.x", "10.1111/j.1467-8659.2010.01785.x", "10.1111/j.1467-8659.2009.01388.x", "10.1111/j.1467-8659.2009.01389.x", "10.1007/978-3-030-01234-2_2", "10.1007/978-3-030-01270-0_21", "10.1007/978-3-030-01252-6_35", "10.1111/j.1467-8659.2009.01617.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46493-0_18", "10.15607/RSS.2015.XI.001", "10.1007/978-3-642-15986-2_1", "10.1016/j.media.2017.05.001", "10.5244/C.25.14", "10.1023/A:1008191222954", "10.1177/0278364916669237", "10.1007/978-3-319-46484-8_30", "10.1111/j.1467-8659.2009.01388.x", "10.1007/978-3-319-46484-8_38", "10.1007/978-3-319-46478-7_20", "10.1007/978-3-319-49409-8_20", "10.1007/978-3-030-01240-3_38", "10.1111/cgf.12802", "10.1007/s41095-015-0029-x", "10.1111/j.1467-8659.2011.02058.x", "10.1111/j.1467-8659.2010.01785.x", "10.1111/j.1467-8659.2009.01388.x", "10.1111/j.1467-8659.2009.01389.x", "10.1007/978-3-030-01234-2_2", "10.1007/978-3-030-01270-0_21", "10.1007/978-3-030-01252-6_35", "10.1111/j.1467-8659.2009.01617.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46493-0_18", "10.15607/RSS.2015.XI.001", "10.1007/978-3-642-15986-2_1", "10.1016/j.media.2017.05.001", "10.5244/C.25.14", "10.1023/A:1008191222954", "10.1177/0278364916669237", "10.1007/978-3-319-46484-8_30", "10.1111/j.1467-8659.2009.01388.x", "10.1007/978-3-319-46484-8_38", "10.1007/978-3-319-46478-7_20", "10.1007/978-3-319-49409-8_20", "10.1007/978-3-030-01240-3_38", "10.1111/cgf.12802", "10.1007/s41095-015-0029-x", "10.1111/j.1467-8659.2011.02058.x", "10.1111/j.1467-8659.2010.01785.x", "10.1111/j.1467-8659.2009.01388.x", "10.1111/j.1467-8659.2009.01389.x", "10.1007/978-3-030-01234-2_2", "10.1007/978-3-030-01270-0_21", "10.1007/978-3-030-01252-6_35", "10.1111/j.1467-8659.2009.01617.x", "10.1007/978-3-7091-6453-2_10", "10.1111/j.1467-8659.2008.01138.x", "10.1007/978-3-319-46493-0_18", "10.15607/RSS.2015.XI.001", "10.1007/978-3-642-15986-2_1", "10.1016/j.media.2017.05.001", "10.5244/C.25.14", "10.1023/A:1008191222954"]}, "10.1109/TVCG.2019.2931299": {"doi": "10.1109/TVCG.2019.2931299", "author": ["A. Somarakis", "V. Van Unen", "F. Koning", "B. Lelieveldt", "T. H\u00f6llt"], "title": "ImaCytE: Visual Exploration of Cellular Micro-Environments for Imaging Mass Cytometry Data", "year": "2021", "abstract": "Tissue functionality is determined by the characteristics of tissue-resident cells and their interactions within their microenvironment. Imaging Mass Cytometry offers the opportunity to distinguish cell types with high precision and link them to their spatial location in intact tissues at sub-cellular resolution. This technology produces large amounts of spatially-resolved high-dimensional data, which constitutes a serious challenge for the data analysis. We present an interactive visual analysis workflow for the end-to-end analysis of Imaging Mass Cytometry data that was developed in close collaboration with domain expert partners. We implemented the presented workflow in an interactive visual analysis tool; ImaCytE. Our workflow is designed to allow the user to discriminate cell types according to their protein expression profiles and analyze their cellular microenvironments, aiding in the formulation or verification of hypotheses on tissue architecture and function. Finally, we show the effectiveness of our workflow and ImaCytE through a case study performed by a collaborating specialist.", "keywords": ["Proteins", "Imaging", "Spatial resolution", "Tools", "Task analysis", "Visualization", "Visual analytics", "imaging mass cytometry", "spatial omics data", "high-dimensional images"], "referenced_by": [], "referencing": ["10.1109/TVCG.2010.137", "10.1109/TVCG.2014.2346574", "10.1109/MCSE.2007.55", "10.1109/TVCG.2016.2570755", "10.1109/34.400568", "10.1109/TVCG.2011.185", "10.1109/TVCG.2010.137", "10.1109/TVCG.2014.2346574", "10.1109/TVCG.2014.2346578", "10.1109/TVCG.2013.124", "10.1109/MCSE.2007.55", "10.1109/TVCG.2016.2570755", "10.1109/34.400568", "10.1109/TVCG.2011.185", "10.1109/TVCG.2010.137", "10.1109/TVCG.2014.2346574", "10.1109/TVCG.2014.2346578", "10.1109/TVCG.2013.124", "10.1109/MCSE.2007.55", "10.1109/TVCG.2016.2570755", "10.1109/34.400568", "10.1109/TVCG.2011.185", "10.1145/2470654.2466444", "10.1145/2669557.2669559", "10.1145/22949.22950", "10.1145/2470654.2466444", "10.1145/2669557.2669559", "10.1145/22949.22950", "10.1145/2470654.2466444", "10.1145/2669557.2669559", "10.1145/22949.22950", "10.1038/8964", "10.1038/nmeth.2869", "10.1038/nrg3832", "10.1111/cgf.12893", "10.1073/pnas.1510227113", "10.1002/9783527335961", "10.1016/j.cels.2018.03.010", "10.1021/ac901049w", "10.1038/nmeth.3863", "10.1016/j.stem.2015.01.015", "10.1002/cyto.a.22625", "10.1016/j.cell.2015.05.047", "10.1038/nbt.2594", "10.1038/s41467-017-01689-9", "10.1038/nmeth.4391", "10.1016/j.cag.2011.01.011", "10.1038/nmeth.1436", "10.1126/science.280.5363.585", "10.1038/srep00028", "10.1073/pnas.1321405111", "10.1057/palgrave.ivs.9500092", "10.1002/acp.2350050106", "10.1016/0169-2607(91)90103-Z", "10.1038/8964", "10.1038/nmeth.2869", "10.1038/nrg3832", "10.1111/cgf.12893", "10.1073/pnas.1510227113", "10.1002/9783527335961", "10.1016/j.cels.2018.03.010", "10.1021/ac901049w", "10.1038/nmeth.3863", "10.1016/j.stem.2015.01.015", "10.1002/cyto.a.22625", "10.1016/j.cell.2015.05.047", "10.1038/nbt.2594", "10.1038/s41467-017-01689-9", "10.1038/nmeth.4391", "10.1016/j.cag.2011.01.011", "10.1038/nmeth.1436", "10.1126/science.280.5363.585", "10.1038/srep00028", "10.1073/pnas.1321405111", "10.1057/palgrave.ivs.9500092", "10.1002/acp.2350050106", "10.1016/0169-2607(91)90103-Z", "10.1038/8964", "10.1038/nmeth.2869", "10.1038/nrg3832", "10.1111/cgf.12893", "10.1073/pnas.1510227113", "10.1002/9783527335961", "10.1016/j.cels.2018.03.010", "10.1021/ac901049w", "10.1038/nmeth.3863", "10.1016/j.stem.2015.01.015", "10.1002/cyto.a.22625", "10.1016/j.cell.2015.05.047", "10.1038/nbt.2594", "10.1038/s41467-017-01689-9", "10.1038/nmeth.4391", "10.1016/j.cag.2011.01.011", "10.1038/nmeth.1436", "10.1126/science.280.5363.585", "10.1038/srep00028", "10.1073/pnas.1321405111", "10.1057/palgrave.ivs.9500092", "10.1002/acp.2350050106", "10.1016/0169-2607(91)90103-Z"]}, "10.1109/TVCG.2019.2938961": {"doi": "10.1109/TVCG.2019.2938961", "author": ["G. Berseth", "B. Haworth", "M. Usman", "D. Schaumann", "M. Khayatkhoei", "M. Kapadia", "P. Faloutsos"], "title": "Interactive Architectural Design with Diverse Solution Exploration", "year": "2021", "abstract": "In architectural design, architects explore a vast amount of design options to maximize various performance criteria, while adhering to specific constraints. In an effort to assist architects in such a complex endeavour, we propose IDOME, an interactive system for computer-aided design optimization. Our approach balances automation and control by efficiently exploring, analyzing, and filtering space layouts to inform architects\u2019 decision-making better. At each design iteration, IDOME provides a set of alternative building layouts which satisfy user-defined constraints and optimality criteria concerning a user-defined space parametrization. When the user selects a design generated by IDOME, the system performs a similar optimization process with the same (or different) parameters and objectives. A user may iterate this exploration process as many times as needed. In this work, we focus on optimizing built environments using architectural metrics by improving the degree of visibility, accessibility, and information gaining for navigating a proposed space. This approach, however, can be extended to support other kinds of analysis as well. We demonstrate the capabilities of IDOME through a series of examples, performance analysis, user studies, and a usability test. The results indicate that IDOME successfully optimizes the proposed designs concerning the chosen metrics and offers a satisfactory experience for users with minimal training.", "keywords": ["Optimization", "Layout", "Measurement", "Tools", "Design automation", "Automation", "Aerospace electronics", "Design exploration", "design optimization", "user-in-the-loop"], "referenced_by": [], "referencing": ["10.1109/CIES.2013.6611723", "10.1109/TVCG.2014.2314658", "10.1109/VLHCC.2012.6344477", "10.1109/ICEC.1996.542381", "10.1109/CIES.2013.6611723", "10.1109/TVCG.2014.2314658", "10.1109/VLHCC.2012.6344477", "10.1109/ICEC.1996.542381", "10.1109/CIES.2013.6611723", "10.1109/TVCG.2014.2314658", "10.1109/VLHCC.2012.6344477", "10.1109/ICEC.1996.542381", "10.1145/2897824.2925935", "10.1145/2461912.2461977", "10.1145/2508363.2508405", "10.1145/358800.358804", "10.1145/1882261.1866203", "10.1145/2010324.1964981", "10.1145/2897824.2925894", "10.1145/2702123.2702149", "10.1145/2185520.2185582", "10.1145/2601097.2601117", "10.1145/218380.218443", "10.1145/2010324.1964982", "10.1145/2636240.2636849", "10.1145/1275808.1276395", "10.1145/258734.258887", "10.1145/3274247.3274503", "10.1145/1978942.1979266", "10.1145/2897824.2925935", "10.1145/2461912.2461977", "10.1145/2508363.2508405", "10.1145/358800.358804", "10.1145/1882261.1866203", "10.1145/2010324.1964981", "10.1145/2897824.2925894", "10.1145/2702123.2702149", "10.1145/2185520.2185582", "10.1145/2601097.2601117", "10.1145/218380.218443", "10.1145/2010324.1964982", "10.1145/2636240.2636849", "10.1145/1275808.1276395", "10.1145/258734.258887", "10.1145/3274247.3274503", "10.1145/1978942.1979266", "10.1145/2897824.2925935", "10.1145/2461912.2461977", "10.1145/2508363.2508405", "10.1145/358800.358804", "10.1145/1882261.1866203", "10.1145/2010324.1964981", "10.1145/2897824.2925894", "10.1145/2702123.2702149", "10.1145/2185520.2185582", "10.1145/2601097.2601117", "10.1145/218380.218443", "10.1145/2010324.1964982", "10.1145/2636240.2636849", "10.1145/1275808.1276395", "10.1145/258734.258887", "10.1145/3274247.3274503", "10.1145/1978942.1979266", "10.1080/10464883.1971.11102482", "10.1002/cav.1898", "10.1080/19401493.2017.1332687", "10.1002/cav.1749", "10.1007/BF01405730", "10.1177/0013916502238863", "10.1016/j.cag.2014.11.002", "10.1080/03052150214021", "10.1007/s00371-013-0825-1", "10.1016/S0926-5805(00)00099-6", "10.1016/S0926-5805(00)00096-0", "10.1260/1351-010X.21.1.75", "10.1007/BF01888721", "10.1016/j.autcon.2013.01.015", "10.1016/j.aei.2011.07.009", "10.1111/cgf.12314", "10.1017/CBO9780511597237", "10.1177/0013916590225001", "10.1068/b34048t", "10.1061/(ASCE)CO.1943-7862.0000881", "10.2200/S00673ED1V01Y201509CGR020", "10.1016/S0303-2434(00)85010-2", "10.1068/b34050t", "10.1068/b2684", "10.1016/j.apenergy.2013.08.061", "10.1007/978-981-13-8410-3_20", "10.1080/10464883.1971.11102482", "10.1002/cav.1898", "10.1080/19401493.2017.1332687", "10.1002/cav.1749", "10.1007/BF01405730", "10.1177/0013916502238863", "10.1016/j.cag.2014.11.002", "10.1080/03052150214021", "10.1007/s00371-013-0825-1", "10.1016/S0926-5805(00)00099-6", "10.1016/S0926-5805(00)00096-0", "10.1260/1351-010X.21.1.75", "10.1007/BF01888721", "10.1016/j.autcon.2013.01.015", "10.1016/j.aei.2011.07.009", "10.1111/cgf.12314", "10.1017/CBO9780511597237", "10.1177/0013916590225001", "10.1068/b34048t", "10.1061/(ASCE)CO.1943-7862.0000881", "10.2200/S00673ED1V01Y201509CGR020", "10.1016/S0303-2434(00)85010-2", "10.1068/b34050t", "10.1068/b2684", "10.1016/j.apenergy.2013.08.061", "10.1007/978-981-13-8410-3_20", "10.1080/10464883.1971.11102482", "10.1002/cav.1898", "10.1080/19401493.2017.1332687", "10.1002/cav.1749", "10.1007/BF01405730", "10.1177/0013916502238863", "10.1016/j.cag.2014.11.002", "10.1080/03052150214021", "10.1007/s00371-013-0825-1", "10.1016/S0926-5805(00)00099-6", "10.1016/S0926-5805(00)00096-0", "10.1260/1351-010X.21.1.75", "10.1007/BF01888721", "10.1016/j.autcon.2013.01.015", "10.1016/j.aei.2011.07.009", "10.1111/cgf.12314", "10.1017/CBO9780511597237", "10.1177/0013916590225001", "10.1068/b34048t", "10.1061/(ASCE)CO.1943-7862.0000881", "10.2200/S00673ED1V01Y201509CGR020", "10.1016/S0303-2434(00)85010-2", "10.1068/b34050t", "10.1068/b2684", "10.1016/j.apenergy.2013.08.061", "10.1007/978-981-13-8410-3_20"]}, "10.1109/TVCG.2019.2928304": {"doi": "10.1109/TVCG.2019.2928304", "author": ["F. Buttussi", "L. Chittaro"], "title": "Locomotion in Place in Virtual Reality: A Comparative Evaluation of Joystick, Teleport, and Leaning", "year": "2021", "abstract": "Recent VR head-mounted displays for consumers feature 3-DOF or 6-DOF head tracking. However, position tracking (when available) is limited to a small area. Moreover, in small or cluttered physical spaces, users can safely experience VR only by staying in place, standing or seated. Different locomotion techniques have been proposed to allow users to explore virtual environments by staying in place. Two in-place locomotion techniques, frequently employed in the literature and in consumer applications, are based on joystick and teleport. Some authors explored leaning with the aim of proposing a more natural in-place locomotion technique. However, more research is needed to understand the effects of the three techniques, since no user study thoroughly compared them all together on a variety of fundamental aspects. Therefore, this paper presents a comparative evaluation with 75 users, assessing the effects of the three techniques on performance, sickness, presence, usability, and different aspects of comfort. Performance of teleport was better than the other techniques, and performance of leaning was better than joystick. Teleport also caused less nausea than the other techniques. Unexpectedly, no significant differences were found for presence. Teleport received a higher usability score than the other techniques. Finally, the techniques had different effects on comfort that we discuss in detail.", "keywords": ["Tracking", "Usability", "Task analysis", "Legged locomotion", "Head-mounted displays", "Virtual environments", "Immersive virtual reality", "locomotion techniques", "teleport", "joystick", "leaning", "user study", "comparative evaluation"], "referenced_by": ["10.1145/3385956.3422123", "10.3390/app10093012"], "referencing": ["10.1145/210079.210084", "10.1145/2804408.2804416", "10.1145/3102071.3102082", "10.1145/3234253.3234291", "10.1145/2967934.2968105", "10.1145/2670473.2670512", "10.1145/2043603.2043607", "10.1145/2522628.2522655", "10.1145/3013971.3014010", "10.1145/364338.364339", "10.1145/3025453.3025521", "10.1145/302979.303042", "10.1145/210079.210084", "10.1145/2804408.2804416", "10.1145/3102071.3102082", "10.1145/3234253.3234291", "10.1145/2967934.2968105", "10.1145/2670473.2670512", "10.1145/2043603.2043607", "10.1145/2522628.2522655", "10.1145/3013971.3014010", "10.1145/364338.364339", "10.1145/3025453.3025521", "10.1145/302979.303042", "10.1145/210079.210084", "10.1145/2804408.2804416", "10.1145/3102071.3102082", "10.1145/3234253.3234291", "10.1145/2967934.2968105", "10.1145/2670473.2670512", "10.1145/2043603.2043607", "10.1145/2522628.2522655", "10.1145/3013971.3014010", "10.1145/364338.364339", "10.1145/3025453.3025521", "10.1145/302979.303042", "10.1007/BF01417673", "10.1007/978-3-319-60928-7_37", "10.1007/s00371-009-0356-y", "10.1016/j.ijhcs.2018.08.002", "10.1207/s15327108ijap0303_3", "10.1007/978-3-642-02806-9_12", "10.1007/BF01417673", "10.1007/978-3-319-60928-7_37", "10.1007/s00371-009-0356-y", "10.1016/j.ijhcs.2018.08.002", "10.1207/s15327108ijap0303_3", "10.1007/978-3-642-02806-9_12", "10.1007/BF01417673", "10.1007/978-3-319-60928-7_37", "10.1007/s00371-009-0356-y", "10.1016/j.ijhcs.2018.08.002", "10.1207/s15327108ijap0303_3", "10.1007/978-3-642-02806-9_12"]}, "10.1109/TVCG.2019.2937301": {"doi": "10.1109/TVCG.2019.2937301", "author": ["Z. Montazeri", "C. Xiao", "Y. Fei", "C. Zheng", "S. Zhao"], "title": "Mechanics-Aware Modeling of Cloth Appearance", "year": "2021", "abstract": "Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric's microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a mechanically plausible way.", "keywords": ["Yarn", "Fabrics", "Computational modeling", "Microstructure", "Training", "Geometry", "Neural networks", "Cloth appearance", "cloth mechanics"], "referenced_by": ["10.1145/3386569.3392412"], "referencing": ["10.1145/344779.344814", "10.1145/280814.280821", "10.1145/1778765.1778853", "10.1145/1360612.1360662", "10.1145/2661229.2661279", "10.1145/357290.357293", "10.1145/2815618", "10.1145/3197517.3201392", "10.1145/3197517.3201346", "10.1145/2077341.2077352", "10.1145/2601097.2601186", "10.1145/3072959.3073623", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2818648", "10.1145/3130800.3130829", "10.1145/2980179.2980255", "10.1145/1073204.1073216", "10.1145/2451236.2451240", "10.1145/142920.134075", "10.1145/2070781.2024179", "10.1145/2010324.1964939", "10.1145/2185520.2185571", "10.1145/2897824.2925932", "10.1145/2485895.2485904", "10.1145/344779.344814", "10.1145/280814.280821", "10.1145/1778765.1778853", "10.1145/1360612.1360662", "10.1145/2661229.2661279", "10.1145/357290.357293", "10.1145/2815618", "10.1145/3197517.3201392", "10.1145/3197517.3201346", "10.1145/2077341.2077352", "10.1145/2601097.2601186", "10.1145/3072959.3073623", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2818648", "10.1145/3130800.3130829", "10.1145/2980179.2980255", "10.1145/1073204.1073216", "10.1145/2451236.2451240", "10.1145/142920.134075", "10.1145/2070781.2024179", "10.1145/2010324.1964939", "10.1145/2185520.2185571", "10.1145/2897824.2925932", "10.1145/2485895.2485904", "10.1145/344779.344814", "10.1145/280814.280821", "10.1145/1778765.1778853", "10.1145/1360612.1360662", "10.1145/2661229.2661279", "10.1145/357290.357293", "10.1145/2815618", "10.1145/3197517.3201392", "10.1145/3197517.3201346", "10.1145/2077341.2077352", "10.1145/2601097.2601186", "10.1145/3072959.3073623", "10.1145/1360612.1360664", "10.1145/1778765.1778842", "10.1145/2818648", "10.1145/3130800.3130829", "10.1145/2980179.2980255", "10.1145/1073204.1073216", "10.1145/2451236.2451240", "10.1145/142920.134075", "10.1145/2070781.2024179", "10.1145/2010324.1964939", "10.1145/2185520.2185571", "10.1145/2897824.2925932", "10.1145/2485895.2485904", "10.1111/cgf.13230", "10.1016/0045-7825(94)90112-0", "10.1111/cgf.13230", "10.1016/0045-7825(94)90112-0", "10.1111/cgf.13230", "10.1016/0045-7825(94)90112-0"]}, "10.1109/TVCG.2019.2928794": {"doi": "10.1109/TVCG.2019.2928794", "author": ["R. Song", "Y. Liu", "P. L. Rosin"], "title": "Mesh Saliency via Weakly Supervised Classification-for-Saliency CNN", "year": "2021", "abstract": "Recently, effort has been made to apply deep learning to the detection of mesh saliency. However, one major barrier is to collect a large amount of vertex-level annotation as saliency ground truth for training the neural networks. Quite a few pilot studies showed that this task is difficult. In this work, we solve this problem by developing a novel network trained in a weakly supervised manner. The training is end-to-end and does not require any saliency ground truth but only the class membership of meshes. Our Classification-for-Saliency CNN (CfS-CNN) employs a multi-view setup and contains a newly designed two-channel structure which integrates view-based features of both classification and saliency. It essentially transfers knowledge from 3D object classification to mesh saliency. Our approach significantly outperforms the existing state-of-the-art methods according to extensive experimental results. Also, the CfS-CNN can be directly used for scene saliency. We showcase two novel applications based on scene saliency to demonstrate its utility.", "keywords": ["Three-dimensional displays", "Training", "Two dimensional displays", "Neural networks", "Deep learning", "Task analysis", "Solid modeling", "Mesh saliency", "deep learning", "transfer learning", "weak supervision"], "referenced_by": ["10.1109/ICME46284.2020.9102796", "10.1109/ACCESS.2020.3023167", "10.1109/TII.2020.3003455"], "referencing": ["10.1109/TPAMI.2016.2522437", "10.1109/TVCG.2019.2892076", "10.1109/CVPRW.2015.7301289", "10.1109/CVPR.2018.00269", "10.1109/ICCV.2015.114", "10.1109/CVPR.2017.702", "10.1109/CVPR.2016.609", "10.1109/TPAMI.2016.2522437", "10.1109/CVPRW.2015.7301289", "10.1109/CVPR.2018.00269", "10.1109/ICCV.2015.114", "10.1109/CVPR.2017.702", "10.1109/CVPR.2016.609", "10.1109/TPAMI.2018.2815601", "10.1109/TPAMI.2016.2522437", "10.1109/TVCG.2019.2892076", "10.1109/CVPRW.2015.7301289", "10.1109/CVPR.2018.00269", "10.1109/ICCV.2015.114", "10.1109/CVPR.2017.702", "10.1109/CVPR.2016.609", "10.1109/TPAMI.2018.2815601", "10.1145/1073204.1073244", "10.1145/1243980.1243981", "10.1145/2897824.2925927", "10.1145/2185520.2185525", "10.1145/2980179.2980238", "10.1145/1122501.1122507", "10.1145/1670671.1670676", "10.1145/2530691", "10.1145/3137609", "10.1145/2019627.2019628", "10.1145/1399504.1360641", "10.1145/1877808.1877819", "10.1145/1073204.1073244", "10.1145/1243980.1243981", "10.1145/2897824.2925927", "10.1145/2185520.2185525", "10.1145/2980179.2980238", "10.1145/1122501.1122507", "10.1145/1670671.1670676", "10.1145/2530691", "10.1145/3137609", "10.1145/2019627.2019628", "10.1145/1399504.1360641", "10.1145/1877808.1877819", "10.1145/1073204.1073244", "10.1145/1243980.1243981", "10.1145/2897824.2925927", "10.1145/2185520.2185525", "10.1145/2980179.2980238", "10.1145/1122501.1122507", "10.1145/1670671.1670676", "10.1145/2530691", "10.1145/3137609", "10.1145/2019627.2019628", "10.1145/1399504.1360641", "10.1145/1877808.1877819", "10.1111/cgf.13353", "10.1007/s00371-012-0746-4", "10.1016/0010-0285(80)90005-5", "10.3758/BF03200774", "10.1038/4511", "10.1016/j.gmod.2013.05.002", "10.1016/j.cagd.2015.03.003", "10.1007/s00371-016-1334-9", "10.1016/j.neucom.2015.08.127", "10.1111/j.1467-8659.2009.01515.x", "10.5244/C.28.6", "10.1016/j.cag.2014.09.023", "10.1006/cogp.2001.0755", "10.1111/cgf.13353", "10.1007/s00371-012-0746-4", "10.1016/0010-0285(80)90005-5", "10.3758/BF03200774", "10.1038/4511", "10.1016/j.gmod.2013.05.002", "10.1016/j.cagd.2015.03.003", "10.1007/s00371-016-1334-9", "10.1016/j.neucom.2015.08.127", "10.1111/j.1467-8659.2009.01515.x", "10.5244/C.28.6", "10.1016/j.cag.2014.09.023", "10.1006/cogp.2001.0755", "10.1111/cgf.13353", "10.1007/s00371-012-0746-4", "10.1016/0010-0285(80)90005-5", "10.3758/BF03200774", "10.1038/4511", "10.1016/j.gmod.2013.05.002", "10.1016/j.cagd.2015.03.003", "10.1007/s00371-016-1334-9", "10.1016/j.neucom.2015.08.127", "10.1111/j.1467-8659.2009.01515.x", "10.5244/C.28.6", "10.1016/j.cag.2014.09.023", "10.1006/cogp.2001.0755"]}, "10.1109/TVCG.2019.2935730": {"doi": "10.1109/TVCG.2019.2935730", "author": ["T. Nguyen-Vo", "B. E. Riecke", "W. Stuerzlinger", "D. -M. Pham", "E. Kruijff"], "title": "NaviBoard and NaviChair: Limited Translation Combined with Full Rotation for Efficient Virtual Locomotion", "year": "2021", "abstract": "Walking has always been considered as the gold standard for navigation in Virtual Reality research. Though full rotation is no longer a technical challenge, physical translation is still restricted through limited tracked areas. While rotational information has been shown to be important, the benefit of the translational component is still unclear with mixed results in previous work. To address this gap, we conducted a mixed-method experiment to compare four levels of translational cues and control: none (using the trackpad of the HTC Vive controller to translate), upper-body leaning (sitting on a \u201cNaviChair\u201d, leaning the upper-body to locomote), whole-body leaning/stepping (standing on a platform called NaviBoard, leaning the whole body or stepping one foot off the center to navigate), and full translation (physically walking). Results showed that translational cues and control had significant effects on various measures including task performance, task load, and simulator sickness. While participants performed significantly worse when they used a controller with no embodied translational cues, there was no significant difference between the NaviChair, NaviBoard, and actual walking. These results suggest that translational body-based motion cues and control from a low-cost leaning/stepping interface might provide enough sensory information for supporting spatial updating, spatial awareness, and efficient locomotion in VR, although future work will need to investigate how these results might or might not generalize to other tasks and scenarios.", "keywords": ["Legged locomotion", "Task analysis", "Navigation", "Resists", "Wheelchairs", "Virtual reality", "Input devices", "Adaptive control", "cognitive informatics", "human computer interaction", "human factors", "user interface", "virtual reality"], "referenced_by": ["10.1109/VR46266.2020.00060", "10.1109/VRW50115.2020.00067", "10.1109/VRW50115.2020.00061", "10.1109/VRW50115.2020.00066"], "referencing": ["10.1109/3DUI.2012.6184180", "10.1109/3DUI.2017.7893344", "10.1109/3DUI.2014.6798851", "10.1109/3DUI.2017.7893320", "10.1109/3DUI.2016.7460053", "10.1162/105474698565659", "10.1109/TVCG.2018.2793038", "10.1109/3DUI.2012.6184180", "10.1109/3DUI.2017.7893344", "10.1109/VR.2018.8446383", "10.1109/3DUI.2014.6798851", "10.1109/3DUI.2017.7893320", "10.1109/3DUI.2016.7460053", "10.1162/105474698565659", "10.1109/TVCG.2018.2793038", "10.1109/3DUI.2012.6184180", "10.1109/3DUI.2017.7893344", "10.1109/VR.2018.8446383", "10.1109/3DUI.2014.6798851", "10.1109/3DUI.2017.7893320", "10.1109/3DUI.2016.7460053", "10.1162/105474698565659", "10.1109/TVCG.2018.2793038", "10.1145/2804408.2804416", "10.1145/1077399.1077401", "10.1145/1502800.1502805", "10.1145/2983298.2983307", "10.1145/3013971.3014010", "10.1145/2983310.2985759", "10.1145/2670473.2670512", "10.1145/2967934.2968105", "10.1145/1970378.1970384", "10.1145/2788940.2788943", "10.1145/2993369.2996327", "10.1145/2788940.2788956", "10.1145/2804408.2804416", "10.1145/1077399.1077401", "10.1145/1502800.1502805", "10.1145/2983298.2983307", "10.1145/3013971.3014010", "10.1145/2983310.2985759", "10.1145/2670473.2670512", "10.1145/2967934.2968105", "10.1145/1970378.1970384", "10.1145/2788940.2788943", "10.1145/2993369.2996327", "10.1145/2788940.2788956", "10.1145/2804408.2804416", "10.1145/1077399.1077401", "10.1145/1502800.1502805", "10.1145/2983298.2983307", "10.1145/3013971.3014010", "10.1145/2983310.2985759", "10.1145/2670473.2670512", "10.1145/2967934.2968105", "10.1145/1970378.1970384", "10.1145/2788940.2788943", "10.1145/2993369.2996327", "10.1145/2788940.2788956", "10.1080/10447318.2013.796441", "10.1007/978-1-4419-8432-6_1", "10.1016/B978-012370509-9.00176-5", "10.1111/j.1467-9280.2006.01728.x", "10.1007/978-3-642-14749-4_21", "10.1111/1467-9280.00058", "10.1068/p231447", "10.1037/0012-1649.27.1.97", "10.1201/b17360-29", "10.1007/s00426-006-0085-z", "10.1068/p3311", "10.1007/s00221-003-1652-9", "10.1007/BF00231984", "10.1007/s00371-016-1229-9", "10.1207/s15327108ijap0303_3", "10.1016/S0166-4115(08)62386-9", "10.3758/BF03210780", "10.1007/978-3-319-57987-0_2", "10.1073/pnas.1718648115", "10.1111/j.1469-7793.2001.0869e.x", "10.1007/978-1-4419-8432-6_5", "10.1080/10447318.2013.796441", "10.1007/978-1-4419-8432-6_1", "10.1016/B978-012370509-9.00176-5", "10.1111/j.1467-9280.2006.01728.x", "10.1007/978-3-642-14749-4_21", "10.1111/1467-9280.00058", "10.1068/p231447", "10.1037/0012-1649.27.1.97", "10.1201/b17360-29", "10.1007/s00426-006-0085-z", "10.1068/p3311", "10.1007/s00221-003-1652-9", "10.1007/BF00231984", "10.1007/s00371-016-1229-9", "10.1207/s15327108ijap0303_3", "10.1016/S0166-4115(08)62386-9", "10.3758/BF03210780", "10.1007/978-3-319-57987-0_2", "10.1073/pnas.1718648115", "10.1111/j.1469-7793.2001.0869e.x", "10.1007/978-1-4419-8432-6_5", "10.1080/10447318.2013.796441", "10.1007/978-1-4419-8432-6_1", "10.1016/B978-012370509-9.00176-5", "10.1111/j.1467-9280.2006.01728.x", "10.1007/978-3-642-14749-4_21", "10.1111/1467-9280.00058", "10.1068/p231447", "10.1037/0012-1649.27.1.97", "10.1201/b17360-29", "10.1007/s00426-006-0085-z", "10.1068/p3311", "10.1007/s00221-003-1652-9", "10.1007/BF00231984", "10.1007/s00371-016-1229-9", "10.1207/s15327108ijap0303_3", "10.1016/S0166-4115(08)62386-9", "10.3758/BF03210780", "10.1007/978-3-319-57987-0_2", "10.1073/pnas.1718648115", "10.1111/j.1469-7793.2001.0869e.x", "10.1007/978-1-4419-8432-6_5"]}, "10.1109/TVCG.2019.2930512": {"doi": "10.1109/TVCG.2019.2930512", "author": ["X. Xu", "M. Xie", "P. Miao", "W. Qu", "W. Xiao", "H. Zhang", "X. Liu", "T. -T. Wong"], "title": "Perceptual-Aware Sketch Simplification Based on Integrated VGG Layers", "year": "2021", "abstract": "Deep learning has been recently demonstrated as an effective tool for raster-based sketch simplification. Nevertheless, it remains challenging to simplify extremely rough sketches. We found that a simplification network trained with a simple loss, such as pixel loss or discriminator loss, may fail to retain the semantically meaningful details when simplifying a very sketchy and complicated drawing. In this paper, we show that, with a well-designed multi-layer perceptual loss, we are able to obtain aesthetic and neat simplification results preserving semantically important global structures as well as fine details without blurriness and excessive emphasis on local structures. To do so, we design a multi-layer discriminator by fusing all VGG feature layers to differentiate sketches and clean lines. The weights used in layer fusing are automatically learned via an intelligent adjustment mechanism. Furthermore, to evaluate our method, we compare our method to state-of-the-art methods through multiple experiments, including visual comparison and intensive user study.", "keywords": ["Feature extraction", "Semantics", "Task analysis", "Generative adversarial networks", "Visualization", "Lighting", "Image segmentation", "Convolutional neural network", "perceptual awareness", "sketch simplification"], "referenced_by": ["10.1109/ACCESS.2020.3022355"], "referencing": ["10.1109/CVPR.2015.7298795", "10.1109/CVPR.2017.728", "10.1109/CVPR.2017.723", "10.1109/ICCV.2017.31", "10.1109/TPAMI.2017.2699184", "10.1109/CVPR.2015.7298642", "10.1109/PCCGA.2004.1348362", "10.1109/TPAMI.2006.127", "10.1109/CVPR.2017.434", "10.1109/CVPR.2018.00068", "10.1109/CVPR.2016.90", "10.1109/CVPR.2015.7298795", "10.1109/CVPR.2017.19", "10.1109/CVPR.2017.728", "10.1109/CVPR.2017.723", "10.1109/ICCV.2017.31", "10.1109/TPAMI.2017.2699184", "10.1109/CVPR.2015.7298642", "10.1109/PCCGA.2004.1348362", "10.1109/TPAMI.2006.127", "10.1109/CVPR.2017.434", "10.1109/CVPR.2018.00068", "10.1109/CVPR.2016.90", "10.1109/CVPR.2015.7298795", "10.1109/CVPR.2017.19", "10.1109/CVPR.2017.728", "10.1109/CVPR.2017.723", "10.1109/ICCV.2017.31", "10.1109/CVPR.2015.7298642", "10.1109/PCCGA.2004.1348362", "10.1109/TPAMI.2006.127", "10.1109/CVPR.2017.434", "10.1109/CVPR.2018.00068", "10.1109/CVPR.2016.90", "10.1145/987657.987674", "10.1145/2816795.2818067", "10.1145/2897824.2925946", "10.1145/2897824.2925972", "10.1145/3132703", "10.1145/1449715.1449740", "10.1145/354401.354413", "10.1145/2421636.2421640", "10.1145/2897824.2925954", "10.1145/987657.987674", "10.1145/2816795.2818067", "10.1145/2897824.2925946", "10.1145/2897824.2925972", "10.1145/3132703", "10.1145/1449715.1449740", "10.1145/354401.354413", "10.1145/2421636.2421640", "10.1145/2897824.2925954", "10.1145/987657.987674", "10.1145/2816795.2818067", "10.1145/2897824.2925946", "10.1145/2897824.2925972", "10.1145/3132703", "10.1145/1449715.1449740", "10.1145/354401.354413", "10.1145/2421636.2421640", "10.1145/2897824.2925954", "10.1007/978-3-319-46493-0_35", "10.1111/j.1467-8659.2008.01151.x", "10.1007/978-3-319-46487-9_40", "10.1007/978-3-319-46493-0_35", "10.1111/j.1467-8659.2008.01151.x", "10.1007/978-3-319-46487-9_40", "10.1007/978-3-319-46493-0_35", "10.1111/j.1467-8659.2008.01151.x", "10.1007/978-3-319-46487-9_40"]}, "10.1109/TVCG.2019.2938165": {"doi": "10.1109/TVCG.2019.2938165", "author": ["Z. Wang", "J. Chai", "S. Xia"], "title": "Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation", "year": "2021", "abstract": "This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet [1] and Squeezenet [2] to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al. [3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera.", "keywords": ["Three-dimensional displays", "Gaze tracking", "Iris", "Cameras", "Convolutional neural nets", "Image reconstruction", "Videos", "3D eye gaze tracking", "convolutional neural network", "facial capture"], "referenced_by": ["10.1109/ACCESS.2020.3010011", "10.1109/CFIS49607.2020.9238677", "10.1109/IEMCON51383.2020.9284947"], "referencing": ["10.1109/ICCV.2013.449", "10.1109/CVPR.2015.7298776", "10.1109/CVPR.2012.6247980", "10.1109/CVPR.2013.75", "10.1109/CVPR.2014.218", "10.1109/CVPR.2014.241", "10.1109/AFGR.2000.840605", "10.1109/TCE.2012.6227433", "10.1109/ICPR.2000.903019", "10.1109/AFGR.2000.840610", "10.1109/AFGR.2000.840620", "10.1109/CVPRW.2017.284", "10.1109/CVPRW.2018.00290", "10.1109/TVCG.2016.2641442", "10.1109/CVPR.2015.7298965", "10.1109/TPAMI.2016.2644615", "10.1109/CVPR.2017.549", "10.1109/TPAMI.2015.2469286", "10.1109/CVPR.1996.517079", "10.1109/TPAMI.2014.2313123", "10.1109/TPAMI.1986.4767851", "10.1109/ICCV.2013.449", "10.1109/CVPR.2015.7298776", "10.1109/CVPR.2012.6247980", "10.1109/CVPR.2013.75", "10.1109/CVPR.2014.218", "10.1109/CVPR.2014.241", "10.1109/AFGR.2000.840605", "10.1109/TCE.2012.6227433", "10.1109/ICPR.2000.903019", "10.1109/AFGR.2000.840610", "10.1109/AFGR.2000.840620", "10.1109/CVPRW.2017.284", "10.1109/CVPRW.2018.00290", "10.1109/TVCG.2016.2641442", "10.1109/CVPR.2015.7298965", "10.1109/TPAMI.2016.2644615", "10.1109/CVPR.2017.549", "10.1109/TPAMI.2015.2469286", "10.1109/CVPR.1996.517079", "10.1109/TPAMI.2014.2313123", "10.1109/TPAMI.1986.4767851", "10.1109/ICCV.2013.449", "10.1109/CVPR.2015.7298776", "10.1109/CVPR.2012.6247980", "10.1109/CVPR.2013.75", "10.1109/CVPR.2014.218", "10.1109/CVPR.2014.241", "10.1109/AFGR.2000.840605", "10.1109/TCE.2012.6227433", "10.1109/ICPR.2000.903019", "10.1109/AFGR.2000.840610", "10.1109/AFGR.2000.840620", "10.1109/CVPRW.2017.284", "10.1109/CVPRW.2018.00290", "10.1109/TVCG.2016.2641442", "10.1109/CVPR.2015.7298965", "10.1109/TPAMI.2016.2644615", "10.1109/CVPR.2017.549", "10.1109/TPAMI.2015.2469286", "10.1109/CVPR.1996.517079", "10.1109/TPAMI.2014.2313123", "10.1109/TPAMI.1986.4767851", "10.1145/2897824.2925947", "10.1145/1276377.1276419", "10.1145/2010324.1964969", "10.1145/1015706.1015759", "10.1145/1409060.1409074", "10.1145/1618452.1618521", "10.1145/1599470.1599472", "10.1145/1778765.1778778", "10.1145/1778765.1778777", "10.1145/2010324.1964970", "10.1145/2366145.2366206", "10.1145/2010324.1964972", "10.1145/2461912.2461976", "10.1145/2461912.2462019", "10.1145/2766939", "10.1145/2816795.2818122", "10.1145/2461912.2462012", "10.1145/2601097.2601204", "10.1145/2766943", "10.1145/2508363.2508380", "10.1145/2661229.2661290", "10.1145/2897824.2925882", "10.1145/3130800.3130837", "10.1145/2661229.2661285", "10.1145/3293353.3293423", "10.1145/3272127.3275093", "10.1145/2647868.2654889", "10.1145/2897824.2925947", "10.1145/1276377.1276419", "10.1145/2010324.1964969", "10.1145/1015706.1015759", "10.1145/1409060.1409074", "10.1145/1618452.1618521", "10.1145/1599470.1599472", "10.1145/1778765.1778778", "10.1145/1778765.1778777", "10.1145/2010324.1964970", "10.1145/2366145.2366206", "10.1145/2010324.1964972", "10.1145/2461912.2461976", "10.1145/2461912.2462019", "10.1145/2766939", "10.1145/2816795.2818122", "10.1145/2461912.2462012", "10.1145/2601097.2601204", "10.1145/2766943", "10.1145/2508363.2508380", "10.1145/2661229.2661290", "10.1145/2897824.2925882", "10.1145/3130800.3130837", "10.1145/2661229.2661285", "10.1145/3293353.3293423", "10.1145/3272127.3275093", "10.1145/2647868.2654889", "10.1145/2897824.2925947", "10.1145/1276377.1276419", "10.1145/2010324.1964969", "10.1145/1015706.1015759", "10.1145/1409060.1409074", "10.1145/1618452.1618521", "10.1145/1599470.1599472", "10.1145/1778765.1778778", "10.1145/1778765.1778777", "10.1145/2010324.1964970", "10.1145/2366145.2366206", "10.1145/2010324.1964972", "10.1145/2461912.2461976", "10.1145/2461912.2462019", "10.1145/2766939", "10.1145/2816795.2818122", "10.1145/2461912.2462012", "10.1145/2601097.2601204", "10.1145/2766943", "10.1145/2508363.2508380", "10.1145/2661229.2661290", "10.1145/2897824.2925882", "10.1145/3130800.3130837", "10.1145/2661229.2661285", "10.1145/3293353.3293423", "10.1145/3272127.3275093", "10.1145/2647868.2654889", "10.1007/s11263-013-0667-3", "10.1142/S0218001499000562", "10.1111/cgf.13355", "10.1007/978-3-030-01264-9_7", "10.1007/978-3-030-01261-8_44", "10.1007/978-3-319-46493-0_29", "10.2307/1932409", "10.1007/s11263-013-0667-3", "10.1142/S0218001499000562", "10.1111/cgf.13355", "10.1007/978-3-030-01264-9_7", "10.1007/978-3-030-01261-8_44", "10.1007/978-3-319-46493-0_29", "10.2307/1932409", "10.1007/s11263-013-0667-3", "10.1142/S0218001499000562", "10.1111/cgf.13355", "10.1007/978-3-030-01264-9_7", "10.1007/978-3-030-01261-8_44", "10.1007/978-3-319-46493-0_29", "10.2307/1932409"]}, "10.1109/TVCG.2019.2927477": {"doi": "10.1109/TVCG.2019.2927477", "author": ["C. A. T. Cortes", "H. -T. Chen", "D. L. Sturnieks", "J. Garcia", "S. R. Lord", "C. -T. Lin"], "title": "Evaluating Balance Recovery Techniques for Users Wearing Head-Mounted Display in VR", "year": "2021", "abstract": "Room-scale 3D position tracking enables users to explore a virtual environment by physically walking, which improves comfort and the level of immersion. However, when users walk with their eyesight blocked by a head-mounted display, they may unexpectedly lose their balance and fall if they bump into real-world obstacles or unintentionally shift their center of mass outside the margin of stability. This paper evaluates balance recovery methods and intervention timing during the use of VR with the assumption that the onset of a fall is given. Our experiment followed the tether-release protocol during clinical research and induced a fall while a subject was engaged in a secondary 3D object selection task. The experiment employed a two-by-two design that evaluated two assistive techniques, i.e., video-see-through and auditory warning at two different timings, i.e., at fall onset and 500ms prior to fall onset. The data from 17 subjects showed that video-see-through triggered 500 ms before the onset of fall can effectively help users recover from falls. Surprisingly, video-see-through at fall onset has a significant negative impact on balance recovery and produces similar results to those of the baseline condition (no intervention).", "keywords": ["Protocols", "Virtual environments", "Three-dimensional displays", "Australia", "Legged locomotion", "Timing", "Resists", "VR", "fall", "balance"], "referenced_by": [], "referencing": ["10.1109/ISMAR.2003.1240725", "10.1109/ACCESS.2018.2832089", "10.1162/105474601300343603", "10.1109/JBHI.2014.2328593", "10.1109/JSEN.2013.2245231", "10.1109/ISMAR.2003.1240725", "10.1109/ACCESS.2018.2832089", "10.1162/105474601300343603", "10.1109/JBHI.2014.2328593", "10.1109/JSEN.2013.2245231", "10.1109/ISMAR.2003.1240725", "10.1109/ACCESS.2018.2832089", "10.1162/105474601300343603", "10.1109/JBHI.2014.2328593", "10.1109/JSEN.2013.2245231", "10.1145/311535.311589", "10.1145/3180658", "10.1145/2501988.2502049", "10.1145/2967102", "10.1145/3167918.3167943", "10.1145/2702123.2702190", "10.1145/2839462.2839484", "10.1145/2807442.2807463", "10.1145/3025453.3025753", "10.1145/566570.566630", "10.1145/3290605.3300657", "10.1145/3025453.3025600", "10.1145/2984511.2984535", "10.1145/311535.311589", "10.1145/3180658", "10.1145/2501988.2502049", "10.1145/2967102", "10.1145/3167918.3167943", "10.1145/2702123.2702190", "10.1145/2839462.2839484", "10.1145/2807442.2807463", "10.1145/3025453.3025753", "10.1145/566570.566630", "10.1145/3290605.3300657", "10.1145/3025453.3025600", "10.1145/2984511.2984535", "10.1145/311535.311589", "10.1145/3180658", "10.1145/2501988.2502049", "10.1145/2967102", "10.1145/3167918.3167943", "10.1145/2702123.2702190", "10.1145/2839462.2839484", "10.1145/2807442.2807463", "10.1145/3025453.3025753", "10.1145/566570.566630", "10.1145/3290605.3300657", "10.1145/3025453.3025600", "10.1145/2984511.2984535", "10.3389/fpsyg.2015.00472", "10.1016/S0140-6736(16)31325-3", "10.1111/j.1532-5415.2008.02014.x", "10.1016/j.jelekin.2007.06.007", "10.1093/ageing/afu054", "10.1016/j.jbiomech.2016.05.033", "10.1093/gerona/60.2.187", "10.1016/j.gaitpost.2010.11.017", "10.1016/0021-9290(82)90011-2", "10.1016/S0887-6185(00)00041-4", "10.1016/j.jbi.2015.12.013", "10.1093/ageing/afl084", "10.1016/S0140-6736(16)31347-2", "10.1016/S0966-6362(01)00156-4", "10.1007/s00221-009-1956-5", "10.1016/j.neulet.2015.02.049", "10.1007/s00221-009-1956-5", "10.1016/j.brainres.2009.04.050", "10.1016/j.clinph.2008.03.020", "10.1093/gerona/56.10.M627", "10.1016/j.neuroscience.2010.07.008", "10.1016/j.gaitpost.2004.09.008", "10.1016/j.jbiomech.2011.10.005", "10.1016/S0304-3940(00)00814-4", "10.1093/gerona/54.11.M583", "10.1016/j.jbiomech.2004.02.038", "10.1093/gerona/57.8.M496", "10.1016/j.jbiomech.2004.03.025", "10.3389/fpsyg.2015.00472", "10.1016/S0140-6736(16)31325-3", "10.1111/j.1532-5415.2008.02014.x", "10.1016/j.jelekin.2007.06.007", "10.1093/ageing/afu054", "10.1016/j.jbiomech.2016.05.033", "10.1093/gerona/60.2.187", "10.1016/j.gaitpost.2010.11.017", "10.1016/0021-9290(82)90011-2", "10.1016/S0887-6185(00)00041-4", "10.1016/j.jbi.2015.12.013", "10.1093/ageing/afl084", "10.1016/S0140-6736(16)31347-2", "10.1016/S0966-6362(01)00156-4", "10.1007/s00221-009-1956-5", "10.1016/j.neulet.2015.02.049", "10.1007/s00221-009-1956-5", "10.1016/j.brainres.2009.04.050", "10.1016/j.clinph.2008.03.020", "10.1093/gerona/56.10.M627", "10.1016/j.neuroscience.2010.07.008", "10.1016/j.gaitpost.2004.09.008", "10.1016/j.jbiomech.2011.10.005", "10.1016/S0304-3940(00)00814-4", "10.1093/gerona/54.11.M583", "10.1016/j.jbiomech.2004.02.038", "10.1093/gerona/57.8.M496", "10.1016/j.jbiomech.2004.03.025", "10.3389/fpsyg.2015.00472", "10.1016/S0140-6736(16)31325-3", "10.1111/j.1532-5415.2008.02014.x", "10.1016/j.jelekin.2007.06.007", "10.1093/ageing/afu054", "10.1016/j.jbiomech.2016.05.033", "10.1093/gerona/60.2.187", "10.1016/j.gaitpost.2010.11.017", "10.1016/0021-9290(82)90011-2", "10.1016/S0887-6185(00)00041-4", "10.1016/j.jbi.2015.12.013", "10.1093/ageing/afl084", "10.1016/S0140-6736(16)31347-2", "10.1016/S0966-6362(01)00156-4", "10.1007/s00221-009-1956-5", "10.1016/j.neulet.2015.02.049", "10.1007/s00221-009-1956-5", "10.1016/j.brainres.2009.04.050", "10.1016/j.clinph.2008.03.020", "10.1093/gerona/56.10.M627", "10.1016/j.neuroscience.2010.07.008", "10.1016/j.gaitpost.2004.09.008", "10.1016/j.jbiomech.2011.10.005", "10.1016/S0304-3940(00)00814-4", "10.1093/gerona/54.11.M583", "10.1016/j.jbiomech.2004.02.038", "10.1093/gerona/57.8.M496", "10.1016/j.jbiomech.2004.03.025"]}, "10.1109/TVCG.2019.2936810": {"doi": "10.1109/TVCG.2019.2936810", "author": ["H. Wang", "E. S. L. Ho", "H. P. H. Shum", "Z. Zhu"], "title": "Spatio-Temporal Manifold Learning for Human Motions via Long-Horizon Modeling", "year": "2021", "abstract": "Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning are correlated with a number of frames later. Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superior results comparing to existing work in multiple applications.", "keywords": ["Manifolds", "Deep learning", "Skeleton", "Three-dimensional displays", "Feature extraction", "Dynamics", "Animation", "Computer graphics", "computer animation", "character animation", "deep learning"], "referenced_by": ["10.1109/CVPR42600.2020.00029", "10.1109/ISPDS51347.2020.00016"], "referencing": ["10.1109/ICCV.2015.494", "10.1109/TCYB.2013.2275945", "10.1109/TVCG.2015.2510000", "10.1109/TVCG.2010.23", "10.1109/CVPR.2017.497", "10.1109/TPAMI.2016.2599174", "10.1109/CVPR.2016.138", "10.1109/WACV.2013.6474999", "10.1109/CVPRW.2010.5543273", "10.1109/TPAMI.2007.1167", "10.1162/neco.1997.9.8.1735", "10.1109/ICCV.2015.494", "10.1109/TCYB.2013.2275945", "10.1109/TVCG.2015.2510000", "10.1109/TVCG.2010.23", "10.1109/CVPR.2017.497", "10.1109/TPAMI.2016.2599174", "10.1109/CVPR.2016.138", "10.1109/WACV.2013.6474999", "10.1109/CVPRW.2010.5543273", "10.1109/TPAMI.2007.1167", "10.1162/neco.1997.9.8.1735", "10.1109/TPAMI.2013.248", "10.1109/ICCV.2015.494", "10.1109/TCYB.2013.2275945", "10.1109/TVCG.2015.2510000", "10.1109/TVCG.2010.23", "10.1109/CVPR.2017.497", "10.1109/TPAMI.2016.2599174", "10.1109/CVPR.2016.138", "10.1109/WACV.2013.6474999", "10.1109/CVPRW.2010.5543273", "10.1109/TPAMI.2007.1167", "10.1162/neco.1997.9.8.1735", "10.1109/TPAMI.2013.248", "10.1145/566654.566605", "10.1145/3072959.3073663", "10.1145/566654.566607", "10.1145/636886.636889", "10.1145/1015706.1015755", "10.1145/2766999", "10.1145/1944745.1944768", "10.1145/1015706.1015754", "10.1145/1073204.1073248", "10.1145/1966394.1966397", "10.1145/3065386", "10.1145/3072959.3073602", "10.1145/3272127.3275071", "10.1145/1618452.1618517", "10.1145/1015706.1015755", "10.1145/2407336.2407340", "10.1145/2820903.2820918", "10.1145/566654.566605", "10.1145/3072959.3073663", "10.1145/566654.566607", "10.1145/636886.636889", "10.1145/1015706.1015755", "10.1145/2766999", "10.1145/1944745.1944768", "10.1145/1015706.1015754", "10.1145/1073204.1073248", "10.1145/1966394.1966397", "10.1145/3065386", "10.1145/3072959.3073602", "10.1145/3272127.3275071", "10.1145/1618452.1618517", "10.1145/1015706.1015755", "10.1145/2407336.2407340", "10.1145/2820903.2820918", "10.1145/566654.566605", "10.1145/3072959.3073663", "10.1145/566654.566607", "10.1145/636886.636889", "10.1145/1015706.1015755", "10.1145/2766999", "10.1145/1944745.1944768", "10.1145/1015706.1015754", "10.1145/1073204.1073248", "10.1145/1966394.1966397", "10.1145/3065386", "10.1145/3072959.3073602", "10.1145/3272127.3275071", "10.1145/1618452.1618517", "10.1145/1015706.1015755", "10.1145/2407336.2407340", "10.1145/2820903.2820918", "10.1111/j.1467-8659.2009.01404.x", "10.1002/cav.1662", "10.1002/cav.125", "10.1007/978-3-030-27529-7_29", "10.3115/v1/W14-4012", "10.1111/j.1467-8659.2009.01404.x", "10.1002/cav.1662", "10.1002/cav.125", "10.1007/978-3-030-27529-7_29", "10.3115/v1/W14-4012", "10.1111/j.1467-8659.2009.01404.x", "10.1002/cav.1662", "10.1002/cav.125", "10.1007/978-3-030-27529-7_29", "10.3115/v1/W14-4012"]}, "10.1109/TVCG.2019.2929808": {"doi": "10.1109/TVCG.2019.2929808", "author": ["Y. Li", "X. Zhai", "F. Hou", "Y. Liu", "A. Hao", "H. Qin"], "title": "Vectorized Painting with Temporal Diffusion Curves", "year": "2021", "abstract": "This paper presents a vector painting system for digital artworks. We first propose Temporal Diffusion Curve (TDC), a new form of vector graphics, and a novel random-access solver for modeling the evolution of strokes. With the help of a procedural stroke processing function, the TDC strokes can achieve various shapes and effects for multiple art styles. Based on these, we build a painting system of great potential. Thanks to the random-access solver, our method has real-time performance regardless of the rendering resolution, provides straightforward editing possibilities on strokes both at runtime and afterward, and is effective and straightforward for art production. Compared with the previous Diffusion Curve, our method uses strokes as the basic graphics primitives, which are able to intersect each other and much more consistent with the intuition and painting habits of human. We finally demonstrate that professional artists can create multiple genres of artworks with our painting system.", "keywords": ["Painting", "Mathematical model", "Graphics", "Real-time systems", "Pigments", "Heating systems", "Oils", "Vector graphics", "heat equation", "procedural model", "real-time application"], "referenced_by": ["10.1109/ACCESS.2020.2982457"], "referencing": ["10.1109/TVCG.2012.76", "10.1109/TVCG.2018.2867478", "10.1109/CA.1995.393542", "10.1109/TVCG.2015.2403352", "10.1109/TVCG.2012.76", "10.1109/TVCG.2018.2867478", "10.1109/CA.1995.393542", "10.1109/TVCG.2015.2403352", "10.1109/TVCG.2012.76", "10.1109/TVCG.2018.2867478", "10.1109/CA.1995.393542", "10.1109/TVCG.2015.2403352", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/3098900.3098902", "10.1145/2816795.2818066", "10.1145/1618452.1618461", "10.1145/1360612.1360691", "10.1145/2159616.2159627", "10.1145/2897824.2925946", "10.1145/2185520.2185570", "10.1145/2070781.2024200", "10.1145/2508363.2508426", "10.1145/311535.311548", "10.1145/2936744.2936747", "10.1145/2087756.2087770", "10.1145/383259.383313", "10.1145/987657.987665", "10.1145/2787626.2792648", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/3098900.3098902", "10.1145/2816795.2818066", "10.1145/1618452.1618461", "10.1145/1360612.1360691", "10.1145/2159616.2159627", "10.1145/2897824.2925946", "10.1145/2185520.2185570", "10.1145/2070781.2024200", "10.1145/2508363.2508426", "10.1145/311535.311548", "10.1145/2936744.2936747", "10.1145/2087756.2087770", "10.1145/383259.383313", "10.1145/987657.987665", "10.1145/2787626.2792648", "10.1145/258734.258896", "10.1145/1073204.1073221", "10.1145/3098900.3098902", "10.1145/2816795.2818066", "10.1145/1618452.1618461", "10.1145/1360612.1360691", "10.1145/2159616.2159627", "10.1145/2897824.2925946", "10.1145/2185520.2185570", "10.1145/2070781.2024200", "10.1145/2508363.2508426", "10.1145/311535.311548", "10.1145/2936744.2936747", "10.1145/2087756.2087770", "10.1145/383259.383313", "10.1145/987657.987665", "10.1145/2787626.2792648", "10.1111/j.1467-8659.2011.01994.x", "10.1111/cgf.12812", "10.1111/cgf.12995", "10.1364/JOSA.44.000330", "10.1002/cav.95", "10.1016/j.jmaa.2005.07.012", "10.1111/j.1467-8659.2011.01994.x", "10.1111/cgf.12812", "10.1111/cgf.12995", "10.1364/JOSA.44.000330", "10.1002/cav.95", "10.1016/j.jmaa.2005.07.012", "10.1111/j.1467-8659.2011.01994.x", "10.1111/cgf.12812", "10.1111/cgf.12995", "10.1364/JOSA.44.000330", "10.1002/cav.95", "10.1016/j.jmaa.2005.07.012"]}, "10.1109/TVCG.2020.3011155": {"doi": "10.1109/TVCG.2020.3011155", "author": ["Y. Ma", "R. Maciejewski"], "title": "Visual Analysis of Class Separations With Locally Linear Segments", "year": "2021", "abstract": "High-dimensional labeled data widely exists in many real-world applications such as classification and clustering. One main task in analyzing such datasets is to explore class separations and class boundaries derived from machine learning models. Dimension reduction techniques are commonly applied to support analysts in exploring the underlying decision boundary structures by depicting a low-dimensional representation of the data distributions from multiple classes. However, such projection-based analyses are limited due to their lack of ability to show separations in complex non-linear decision boundary structures and can suffer from heavy distortion and low interpretability. To overcome these issues of separability and interpretability, we propose a visual analysis approach that utilizes the power of explainability from linear projections to support analysts when exploring non-linear separation structures. Our approach is to extract a set of locally linear segments that approximate the original non-linear separations. Unlike traditional projection-based analysis where the data instances are mapped to a single scatterplot, our approach supports the exploration of complex class separations through multiple local projection results. We conduct case studies on two labeled datasets to demonstrate the effectiveness of our approach.", "keywords": ["Dimensionality reduction", "Data visualization", "Visualization", "Analytical models", "Manifolds", "Data models", "Support vector machines", "Visual analysis", "dimension reduction", "class separation"], "referenced_by": [], "referencing": ["10.1145/2872427.2883041", "10.1145/2872427.2883041", "10.1145/2872427.2883041", "10.1111/cgf.12876", "10.1126/science.290.5500.2319", "10.1126/science.290.5500.2323", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.12632", "10.1111/cgf.12639", "10.1111/j.1467-8659.2009.01475.x", "10.1137/0906011", "10.3390/info10090280", "10.1146/annurev-statistics-031017-100045", "10.1016/j.neucom.2004.04.009", "10.1016/j.neucom.2007.12.028", "10.1023/A:1013968124284", "10.1016/0893-6080(94)90109-0", "10.1023/A:1018748928965", "10.18653/v1/N16-3020", "10.1023/A:1009715923555", "10.1371/journal.pone.0098679", "10.1111/cgf.12876", "10.1126/science.290.5500.2319", "10.1126/science.290.5500.2323", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.12632", "10.1111/cgf.12639", "10.1111/j.1467-8659.2009.01475.x", "10.1137/0906011", "10.3390/info10090280", "10.1146/annurev-statistics-031017-100045", "10.1016/j.neucom.2004.04.009", "10.1016/j.neucom.2007.12.028", "10.1023/A:1013968124284", "10.1016/0893-6080(94)90109-0", "10.1023/A:1018748928965", "10.18653/v1/N16-3020", "10.1023/A:1009715923555", "10.1371/journal.pone.0098679", "10.1111/cgf.12876", "10.1126/science.290.5500.2319", "10.1126/science.290.5500.2323", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.12632", "10.1111/cgf.12639", "10.1111/j.1467-8659.2009.01475.x", "10.1137/0906011", "10.3390/info10090280", "10.1146/annurev-statistics-031017-100045", "10.1016/j.neucom.2004.04.009", "10.1016/j.neucom.2007.12.028", "10.1023/A:1013968124284", "10.1016/0893-6080(94)90109-0", "10.1023/A:1018748928965", "10.18653/v1/N16-3020", "10.1023/A:1009715923555", "10.1371/journal.pone.0098679"]}, "10.1109/TVCG.2020.3010736": {"doi": "10.1109/TVCG.2020.3010736", "author": ["M. Livesu"], "title": "Scalable Mesh Refinement for Canonical Polygonal Schemas of Extremely High Genus Shapes", "year": "2021", "abstract": "Any closed manifold of genus $g$g can be cut open to form a topological disk and then mapped to a regular polygon with $4g$4g sides. This construction is called the canonical polygonal schema of the manifold, and is a key ingredient for many applications in graphics and engineering, where a parameterization between two shapes with same topology is often needed. The sides of the $4g-$4g-gon define on the manifold a system of loops, which all intersect at a single point and are disjoint elsewhere. Computing a shortest system of loops of this kind is NP-hard. A computationally tractable alternative consists of computing a set of shortest loops that are not fully disjoint in polynomial time using the greedy homotopy basis algorithm proposed by Erickson and Whittlesey and then detach them in post processing via mesh refinement. Despite this operation is conceptually simple, known refinement strategies do not scale well for high genus shapes, triggering a mesh growth that may exceed the amount of memory available in modern computers, leading to failures. In this article we study various local refinement operators to detach cycles in a system of loops, and show that there are important differences between them, both in terms of mesh complexity and preservation of the original surface. We ultimately propose two novel refinement approaches: the former greatly reduces the number of new elements in the mesh, possibly at the cost of a deviation from the input geometry. The latter allows to trade mesh complexity for geometric accuracy, bounding deviation from the input surface. Both strategies are trivial to implement, and experiments confirm that they allow to realize canonical polygonal schemas even for extremely high genus shapes where previous methods fail.", "keywords": ["Manifolds", "Shape", "Basis algorithms", "Geometry", "Complexity theory", "Topology", "Robustness", "Topology", "polygonal schema", "cut graph", "homology", "homotopy"], "referenced_by": [], "referencing": ["10.1109/TVCG.2008.57", "10.1109/TVCG.2008.57", "10.1109/TVCG.2008.57", "10.1145/98524.98546", "10.1145/1236246.1236281", "10.1145/1360612.1360644", "10.1145/378583.378630", "10.1145/2508363.2508388", "10.1145/1061347.1061354", "10.1145/566654.566588", "10.1145/274363.274365", "10.1145/1015706.1015811", "10.1145/3355089.3356519", "10.1145/3072959.2983621", "10.1145/2766947", "10.1145/3197517.3201331", "10.1145/98524.98546", "10.1145/1236246.1236281", "10.1145/1360612.1360644", "10.1145/378583.378630", "10.1145/2508363.2508388", "10.1145/1061347.1061354", "10.1145/566654.566588", "10.1145/274363.274365", "10.1145/1015706.1015811", "10.1145/3355089.3356519", "10.1145/3072959.2983621", "10.1145/2766947", "10.1145/3197517.3201331", "10.1145/98524.98546", "10.1145/1236246.1236281", "10.1145/1360612.1360644", "10.1145/378583.378630", "10.1145/2508363.2508388", "10.1145/1061347.1061354", "10.1145/566654.566588", "10.1145/274363.274365", "10.1145/1015706.1015811", "10.1145/3355089.3356519", "10.1145/3072959.2983621", "10.1145/2766947", "10.1145/3197517.3201331", "10.1007/s00454-003-2948-z", "10.1007/BF02570697", "10.1007/s00454-004-1150-2", "10.1007/978-3-662-59958-7_4", "10.1007/s00454-003-2948-z", "10.1007/BF02570697", "10.1007/s00454-004-1150-2", "10.1007/978-3-662-59958-7_4", "10.1007/s00454-003-2948-z", "10.1007/BF02570697", "10.1007/s00454-004-1150-2", "10.1007/978-3-662-59958-7_4"]}}